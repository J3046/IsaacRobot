################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 11538 steps/s (collection: 8.227s, learning 0.293s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 25.5712
                       Mean reward: 0.00
               Mean episode length: 21.31
    Episode_Reward/reaching_object: 0.0004
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0002
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.52s
                      Time elapsed: 00:00:08
                               ETA: 04:43:59

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14557 steps/s (collection: 6.635s, learning 0.118s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.6665
                       Mean reward: 0.00
               Mean episode length: 45.35
    Episode_Reward/reaching_object: 0.0011
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.75s
                      Time elapsed: 00:00:15
                               ETA: 04:14:24

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14317 steps/s (collection: 6.728s, learning 0.138s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.6934
                       Mean reward: 0.00
               Mean episode length: 69.71
    Episode_Reward/reaching_object: 0.0017
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.87s
                      Time elapsed: 00:00:22
                               ETA: 04:05:44

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14812 steps/s (collection: 6.494s, learning 0.143s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.7234
                       Mean reward: 0.00
               Mean episode length: 93.33
    Episode_Reward/reaching_object: 0.0022
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.64s
                      Time elapsed: 00:00:28
                               ETA: 03:59:25

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14627 steps/s (collection: 6.565s, learning 0.156s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.7224
                       Mean reward: 0.00
               Mean episode length: 117.98
    Episode_Reward/reaching_object: 0.0028
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.72s
                      Time elapsed: 00:00:35
                               ETA: 03:56:09

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14900 steps/s (collection: 6.470s, learning 0.128s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 25.6864
                       Mean reward: 0.00
               Mean episode length: 141.18
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.60s
                      Time elapsed: 00:00:42
                               ETA: 03:53:15

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 15544 steps/s (collection: 6.187s, learning 0.137s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 25.6631
                       Mean reward: 0.00
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.0039
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.32s
                      Time elapsed: 00:00:48
                               ETA: 03:49:51

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14463 steps/s (collection: 6.654s, learning 0.143s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 25.5944
                       Mean reward: -0.00
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.0041
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0030
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.80s
                      Time elapsed: 00:00:55
                               ETA: 03:49:15

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 19392 steps/s (collection: 4.961s, learning 0.108s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 25.4887
                       Mean reward: 0.00
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 0.0048
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.07s
                      Time elapsed: 00:01:00
                               ETA: 03:42:22

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 60145 steps/s (collection: 1.531s, learning 0.103s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 25.3800
                       Mean reward: 0.00
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.0058
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.63s
                      Time elapsed: 00:01:01
                               ETA: 03:25:27

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 62166 steps/s (collection: 1.489s, learning 0.093s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 25.2890
                       Mean reward: 0.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0064
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.58s
                      Time elapsed: 00:01:03
                               ETA: 03:11:27

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 62981 steps/s (collection: 1.472s, learning 0.089s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 25.2405
                       Mean reward: 0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0069
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.56s
                      Time elapsed: 00:01:05
                               ETA: 02:59:43

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 64090 steps/s (collection: 1.447s, learning 0.087s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 25.2286
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0085
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.53s
                      Time elapsed: 00:01:06
                               ETA: 02:49:43

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 60095 steps/s (collection: 1.540s, learning 0.096s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 25.1980
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0100
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.64s
                      Time elapsed: 00:01:08
                               ETA: 02:41:23

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 60783 steps/s (collection: 1.526s, learning 0.091s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 25.1851
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0124
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.62s
                      Time elapsed: 00:01:09
                               ETA: 02:34:07

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 62028 steps/s (collection: 1.492s, learning 0.093s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 25.1684
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0153
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.58s
                      Time elapsed: 00:01:11
                               ETA: 02:27:41

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 60207 steps/s (collection: 1.526s, learning 0.107s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 25.1594
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0222
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.63s
                      Time elapsed: 00:01:13
                               ETA: 02:22:07

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 60948 steps/s (collection: 1.523s, learning 0.090s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 25.1854
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0296
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.61s
                      Time elapsed: 00:01:14
                               ETA: 02:17:06

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 60407 steps/s (collection: 1.522s, learning 0.105s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0296
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.2826
                       Mean reward: 0.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0427
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.63s
                      Time elapsed: 00:01:16
                               ETA: 02:12:39

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 59872 steps/s (collection: 1.553s, learning 0.089s)
             Mean action noise std: 1.00
          Mean value_function loss: 4.6916
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.3698
                       Mean reward: -2.91
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.0571
     Episode_Reward/lifting_object: -0.1937
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.64s
                      Time elapsed: 00:01:17
                               ETA: 02:08:40

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 59972 steps/s (collection: 1.554s, learning 0.086s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.7290
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.6132
                       Mean reward: 0.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0795
     Episode_Reward/lifting_object: -0.2638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.64s
                      Time elapsed: 00:01:19
                               ETA: 02:05:03

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 55361 steps/s (collection: 1.668s, learning 0.108s)
             Mean action noise std: 1.01
          Mean value_function loss: 2.3709
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.7332
                       Mean reward: 0.50
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.0982
     Episode_Reward/lifting_object: -0.3873
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.78s
                      Time elapsed: 00:01:21
                               ETA: 02:01:58

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 56138 steps/s (collection: 1.663s, learning 0.088s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.9172
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.8368
                       Mean reward: -1.68
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.1085
     Episode_Reward/lifting_object: -0.2502
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.75s
                      Time elapsed: 00:01:23
                               ETA: 01:59:07

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 56735 steps/s (collection: 1.644s, learning 0.089s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 25.9323
                       Mean reward: 0.65
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.1293
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.73s
                      Time elapsed: 00:01:24
                               ETA: 01:56:29

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 57689 steps/s (collection: 1.615s, learning 0.089s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.4469
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.0107
                       Mean reward: 0.69
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.1292
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.70s
                      Time elapsed: 00:01:26
                               ETA: 01:54:00

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 58328 steps/s (collection: 1.588s, learning 0.097s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.0588
                       Mean reward: 0.65
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.1325
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.69s
                      Time elapsed: 00:01:28
                               ETA: 01:51:42

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 57084 steps/s (collection: 1.632s, learning 0.090s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.0261
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.2089
                       Mean reward: 0.67
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.1359
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.72s
                      Time elapsed: 00:01:29
                               ETA: 01:49:36

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 56929 steps/s (collection: 1.626s, learning 0.101s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.2563
                       Mean reward: 0.58
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.1161
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.73s
                      Time elapsed: 00:01:31
                               ETA: 01:47:40

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 56359 steps/s (collection: 1.638s, learning 0.106s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.1138
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.3499
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1037
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.74s
                      Time elapsed: 00:01:33
                               ETA: 01:45:53

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 59207 steps/s (collection: 1.551s, learning 0.109s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0631
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.3944
                       Mean reward: 0.43
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.0939
     Episode_Reward/lifting_object: -0.0417
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.66s
                      Time elapsed: 00:01:35
                               ETA: 01:44:07

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 59169 steps/s (collection: 1.555s, learning 0.106s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.5874
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.5780
                       Mean reward: 0.36
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.0824
     Episode_Reward/lifting_object: -0.1384
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.66s
                      Time elapsed: 00:01:36
                               ETA: 01:42:28

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 56652 steps/s (collection: 1.624s, learning 0.111s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3369
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.7091
                       Mean reward: 0.35
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.0760
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.74s
                      Time elapsed: 00:01:38
                               ETA: 01:40:59

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 57578 steps/s (collection: 1.614s, learning 0.094s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1235
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.8044
                       Mean reward: 0.30
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.0672
     Episode_Reward/lifting_object: -0.0934
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.71s
                      Time elapsed: 00:01:40
                               ETA: 01:39:35

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 57204 steps/s (collection: 1.613s, learning 0.105s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 26.8968
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0725
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.72s
                      Time elapsed: 00:01:41
                               ETA: 01:38:15

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 57609 steps/s (collection: 1.615s, learning 0.091s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0252
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.9622
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0676
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.71s
                      Time elapsed: 00:01:43
                               ETA: 01:37:00

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 60052 steps/s (collection: 1.542s, learning 0.095s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.0014
                       Mean reward: 0.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0676
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.64s
                      Time elapsed: 00:01:45
                               ETA: 01:35:45

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 59042 steps/s (collection: 1.568s, learning 0.097s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 27.1095
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0660
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.66s
                      Time elapsed: 00:01:46
                               ETA: 01:34:35

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 57180 steps/s (collection: 1.625s, learning 0.094s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.1319
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0661
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.72s
                      Time elapsed: 00:01:48
                               ETA: 01:33:31

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 58766 steps/s (collection: 1.557s, learning 0.116s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 27.1756
                       Mean reward: 0.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0721
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.67s
                      Time elapsed: 00:01:50
                               ETA: 01:32:29

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 60206 steps/s (collection: 1.535s, learning 0.098s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 27.1900
                       Mean reward: 0.35
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.0775
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.63s
                      Time elapsed: 00:01:51
                               ETA: 01:31:28

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 58632 steps/s (collection: 1.592s, learning 0.085s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0849
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.2510
                       Mean reward: 0.38
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.0776
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.68s
                      Time elapsed: 00:01:53
                               ETA: 01:30:31

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 57929 steps/s (collection: 1.602s, learning 0.095s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0301
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.2945
                       Mean reward: 0.21
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.0855
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.70s
                      Time elapsed: 00:01:55
                               ETA: 01:29:38

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 56823 steps/s (collection: 1.633s, learning 0.097s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.7107
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.4116
                       Mean reward: 0.47
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.0989
     Episode_Reward/lifting_object: -0.0893
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.73s
                      Time elapsed: 00:01:57
                               ETA: 01:28:49

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 55001 steps/s (collection: 1.683s, learning 0.105s)
             Mean action noise std: 1.12
          Mean value_function loss: 1.4570
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.4979
                       Mean reward: -0.65
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.1102
     Episode_Reward/lifting_object: -0.1377
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.79s
                      Time elapsed: 00:01:58
                               ETA: 01:28:05

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 55072 steps/s (collection: 1.694s, learning 0.091s)
             Mean action noise std: 1.12
          Mean value_function loss: 1.1439
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 27.5635
                       Mean reward: -0.93
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.1303
     Episode_Reward/lifting_object: -0.2070
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.78s
                      Time elapsed: 00:02:00
                               ETA: 01:27:22

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 54500 steps/s (collection: 1.704s, learning 0.100s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.3480
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.6161
                       Mean reward: 0.69
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.1429
     Episode_Reward/lifting_object: -0.1107
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.80s
                      Time elapsed: 00:02:02
                               ETA: 01:26:42

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 54115 steps/s (collection: 1.704s, learning 0.113s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 27.6865
                       Mean reward: 0.74
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.1591
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.82s
                      Time elapsed: 00:02:04
                               ETA: 01:26:05

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 54261 steps/s (collection: 1.706s, learning 0.106s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 27.7905
                       Mean reward: 0.80
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.1803
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.81s
                      Time elapsed: 00:02:06
                               ETA: 01:25:28

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 54733 steps/s (collection: 1.676s, learning 0.121s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0547
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 27.8372
                       Mean reward: 0.93
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.1813
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.80s
                      Time elapsed: 00:02:07
                               ETA: 01:24:53

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 55655 steps/s (collection: 1.669s, learning 0.098s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.8704
                       Mean reward: 0.97
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 0.1946
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.77s
                      Time elapsed: 00:02:09
                               ETA: 01:24:17

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 55518 steps/s (collection: 1.681s, learning 0.090s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 27.9723
                       Mean reward: 0.92
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 0.1958
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.77s
                      Time elapsed: 00:02:11
                               ETA: 01:23:43

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 54628 steps/s (collection: 1.712s, learning 0.088s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 27.9888
                       Mean reward: 0.81
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.1862
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.80s
                      Time elapsed: 00:02:13
                               ETA: 01:23:11

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 55172 steps/s (collection: 1.690s, learning 0.092s)
             Mean action noise std: 1.15
          Mean value_function loss: 2.9658
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.0454
                       Mean reward: 0.78
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.1707
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.78s
                      Time elapsed: 00:02:14
                               ETA: 01:22:40

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 54694 steps/s (collection: 1.698s, learning 0.100s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0507
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.0781
                       Mean reward: 0.52
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.1639
     Episode_Reward/lifting_object: -0.1639
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.80s
                      Time elapsed: 00:02:16
                               ETA: 01:22:11

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 56697 steps/s (collection: 1.644s, learning 0.090s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.2151
                       Mean reward: 0.71
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.1555
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.73s
                      Time elapsed: 00:02:18
                               ETA: 01:21:40

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 55926 steps/s (collection: 1.667s, learning 0.091s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.3668
                       Mean reward: 0.68
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 0.1569
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.76s
                      Time elapsed: 00:02:20
                               ETA: 01:21:11

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 55669 steps/s (collection: 1.676s, learning 0.090s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0540
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 28.3572
                       Mean reward: 0.66
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 0.1549
     Episode_Reward/lifting_object: -0.0114
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.77s
                      Time elapsed: 00:02:22
                               ETA: 01:20:43

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 53997 steps/s (collection: 1.711s, learning 0.110s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0450
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.3851
                       Mean reward: 0.67
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 0.1695
     Episode_Reward/lifting_object: -0.0143
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.82s
                      Time elapsed: 00:02:23
                               ETA: 01:20:18

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 54685 steps/s (collection: 1.697s, learning 0.101s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1615
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.4738
                       Mean reward: 0.42
               Mean episode length: 220.86
    Episode_Reward/reaching_object: 0.1695
     Episode_Reward/lifting_object: -0.0360
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.80s
                      Time elapsed: 00:02:25
                               ETA: 01:19:53

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 55118 steps/s (collection: 1.695s, learning 0.089s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0607
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.5104
                       Mean reward: 0.52
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 0.1867
     Episode_Reward/lifting_object: -0.0202
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.78s
                      Time elapsed: 00:02:27
                               ETA: 01:19:29

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 54899 steps/s (collection: 1.696s, learning 0.095s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.3334
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.5551
                       Mean reward: 0.89
               Mean episode length: 207.72
    Episode_Reward/reaching_object: 0.1877
     Episode_Reward/lifting_object: -0.0455
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.79s
                      Time elapsed: 00:02:29
                               ETA: 01:19:05

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 55401 steps/s (collection: 1.685s, learning 0.089s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0892
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.6034
                       Mean reward: 0.93
               Mean episode length: 209.29
    Episode_Reward/reaching_object: 0.1938
     Episode_Reward/lifting_object: -0.0101
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.77s
                      Time elapsed: 00:02:30
                               ETA: 01:18:41

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 56551 steps/s (collection: 1.652s, learning 0.086s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0579
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.7341
                       Mean reward: 1.00
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 0.2022
     Episode_Reward/lifting_object: -0.0183
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.74s
                      Time elapsed: 00:02:32
                               ETA: 01:18:18

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 55966 steps/s (collection: 1.657s, learning 0.100s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.1857
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.9191
                       Mean reward: 1.11
               Mean episode length: 218.79
    Episode_Reward/reaching_object: 0.2173
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.76s
                      Time elapsed: 00:02:34
                               ETA: 01:17:55

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 54495 steps/s (collection: 1.708s, learning 0.095s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 28.9524
                       Mean reward: 1.06
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 0.2191
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.80s
                      Time elapsed: 00:02:36
                               ETA: 01:17:34

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 55189 steps/s (collection: 1.683s, learning 0.098s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.0429
                       Mean reward: 1.16
               Mean episode length: 219.28
    Episode_Reward/reaching_object: 0.2371
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.78s
                      Time elapsed: 00:02:38
                               ETA: 01:17:14

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 55010 steps/s (collection: 1.678s, learning 0.109s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.2269
                       Mean reward: 1.30
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 0.2267
     Episode_Reward/lifting_object: 0.0103
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.79s
                      Time elapsed: 00:02:39
                               ETA: 01:16:54

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 54663 steps/s (collection: 1.696s, learning 0.103s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1068
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.3847
                       Mean reward: 1.16
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 0.2360
     Episode_Reward/lifting_object: -0.0133
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.80s
                      Time elapsed: 00:02:41
                               ETA: 01:16:35

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 55785 steps/s (collection: 1.671s, learning 0.091s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.3306
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.4137
                       Mean reward: 1.08
               Mean episode length: 227.18
    Episode_Reward/reaching_object: 0.2321
     Episode_Reward/lifting_object: -0.0481
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.76s
                      Time elapsed: 00:02:43
                               ETA: 01:16:15

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 54717 steps/s (collection: 1.700s, learning 0.097s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0311
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.4597
                       Mean reward: 1.06
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 0.2241
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.80s
                      Time elapsed: 00:02:45
                               ETA: 01:15:57

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 52826 steps/s (collection: 1.751s, learning 0.110s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0313
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.5862
                       Mean reward: 1.08
               Mean episode length: 225.82
    Episode_Reward/reaching_object: 0.2231
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.86s
                      Time elapsed: 00:02:47
                               ETA: 01:15:41

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 55065 steps/s (collection: 1.680s, learning 0.105s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0310
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.6770
                       Mean reward: 1.16
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 0.2205
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.79s
                      Time elapsed: 00:02:48
                               ETA: 01:15:23

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 53458 steps/s (collection: 1.728s, learning 0.111s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.4281
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.7099
                       Mean reward: 0.88
               Mean episode length: 209.74
    Episode_Reward/reaching_object: 0.2210
     Episode_Reward/lifting_object: -0.0727
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.84s
                      Time elapsed: 00:02:50
                               ETA: 01:15:08

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 53647 steps/s (collection: 1.738s, learning 0.095s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0747
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.7333
                       Mean reward: 1.12
               Mean episode length: 211.08
    Episode_Reward/reaching_object: 0.2299
     Episode_Reward/lifting_object: -0.0142
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.83s
                      Time elapsed: 00:02:52
                               ETA: 01:14:52

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 53948 steps/s (collection: 1.735s, learning 0.087s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.3745
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8006
                       Mean reward: 0.86
               Mean episode length: 204.65
    Episode_Reward/reaching_object: 0.2284
     Episode_Reward/lifting_object: -0.0456
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.82s
                      Time elapsed: 00:02:54
                               ETA: 01:14:37

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 53007 steps/s (collection: 1.761s, learning 0.094s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1702
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.8277
                       Mean reward: 1.14
               Mean episode length: 200.99
    Episode_Reward/reaching_object: 0.2404
     Episode_Reward/lifting_object: -0.0466
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.85s
                      Time elapsed: 00:02:56
                               ETA: 01:14:23

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 53044 steps/s (collection: 1.762s, learning 0.091s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2899
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8734
                       Mean reward: 1.20
               Mean episode length: 204.53
    Episode_Reward/reaching_object: 0.2556
     Episode_Reward/lifting_object: -0.0590
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.85s
                      Time elapsed: 00:02:58
                               ETA: 01:14:09

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 53493 steps/s (collection: 1.740s, learning 0.098s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.4535
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.9029
                       Mean reward: 1.15
               Mean episode length: 200.15
    Episode_Reward/reaching_object: 0.2598
     Episode_Reward/lifting_object: -0.0430
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.84s
                      Time elapsed: 00:02:59
                               ETA: 01:13:55

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 53921 steps/s (collection: 1.734s, learning 0.090s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1221
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.9473
                       Mean reward: 1.08
               Mean episode length: 206.13
    Episode_Reward/reaching_object: 0.2640
     Episode_Reward/lifting_object: -0.1029
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.82s
                      Time elapsed: 00:03:01
                               ETA: 01:13:41

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 54009 steps/s (collection: 1.730s, learning 0.090s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0906
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.0643
                       Mean reward: 1.30
               Mean episode length: 207.26
    Episode_Reward/reaching_object: 0.2725
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.82s
                      Time elapsed: 00:03:03
                               ETA: 01:13:27

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 53558 steps/s (collection: 1.741s, learning 0.094s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0453
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.1870
                       Mean reward: 1.49
               Mean episode length: 216.47
    Episode_Reward/reaching_object: 0.2992
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.84s
                      Time elapsed: 00:03:05
                               ETA: 01:13:14

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 54198 steps/s (collection: 1.708s, learning 0.106s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.2739
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 30.2599
                       Mean reward: 1.39
               Mean episode length: 214.42
    Episode_Reward/reaching_object: 0.2980
     Episode_Reward/lifting_object: -0.0569
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.81s
                      Time elapsed: 00:03:07
                               ETA: 01:13:00

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 54040 steps/s (collection: 1.701s, learning 0.118s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0765
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.2860
                       Mean reward: 1.13
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 0.2949
     Episode_Reward/lifting_object: -0.0156
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.82s
                      Time elapsed: 00:03:09
                               ETA: 01:12:47

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 53203 steps/s (collection: 1.749s, learning 0.099s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 30.3488
                       Mean reward: 1.60
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.85s
                      Time elapsed: 00:03:10
                               ETA: 01:12:35

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 54341 steps/s (collection: 1.715s, learning 0.094s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.2627
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.3909
                       Mean reward: 1.35
               Mean episode length: 222.01
    Episode_Reward/reaching_object: 0.3095
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.81s
                      Time elapsed: 00:03:12
                               ETA: 01:12:22

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 53007 steps/s (collection: 1.758s, learning 0.097s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.1598
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.4018
                       Mean reward: 1.59
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 0.3247
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.85s
                      Time elapsed: 00:03:14
                               ETA: 01:12:11

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 54387 steps/s (collection: 1.702s, learning 0.106s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0302
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.4492
                       Mean reward: 1.51
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 0.3102
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.81s
                      Time elapsed: 00:03:16
                               ETA: 01:11:59

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 53206 steps/s (collection: 1.752s, learning 0.096s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0714
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.5415
                       Mean reward: 1.50
               Mean episode length: 218.91
    Episode_Reward/reaching_object: 0.3191
     Episode_Reward/lifting_object: -0.0594
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.85s
                      Time elapsed: 00:03:18
                               ETA: 01:11:48

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 53804 steps/s (collection: 1.721s, learning 0.106s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0509
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.6550
                       Mean reward: 1.53
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 0.3183
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.83s
                      Time elapsed: 00:03:20
                               ETA: 01:11:36

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 53435 steps/s (collection: 1.747s, learning 0.093s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.3049
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.7410
                       Mean reward: 1.43
               Mean episode length: 209.40
    Episode_Reward/reaching_object: 0.3160
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.84s
                      Time elapsed: 00:03:21
                               ETA: 01:11:25

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 52907 steps/s (collection: 1.767s, learning 0.092s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0979
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.7565
                       Mean reward: 1.55
               Mean episode length: 208.90
    Episode_Reward/reaching_object: 0.3226
     Episode_Reward/lifting_object: -0.0824
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.86s
                      Time elapsed: 00:03:23
                               ETA: 01:11:15

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 54554 steps/s (collection: 1.704s, learning 0.098s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0345
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.8290
                       Mean reward: 1.36
               Mean episode length: 195.11
    Episode_Reward/reaching_object: 0.3197
     Episode_Reward/lifting_object: 0.0071
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.80s
                      Time elapsed: 00:03:25
                               ETA: 01:11:04

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 53920 steps/s (collection: 1.727s, learning 0.096s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1432
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.9122
                       Mean reward: 1.82
               Mean episode length: 206.68
    Episode_Reward/reaching_object: 0.3366
     Episode_Reward/lifting_object: -0.0331
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.82s
                      Time elapsed: 00:03:27
                               ETA: 01:10:53

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 54600 steps/s (collection: 1.712s, learning 0.088s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 30.9458
                       Mean reward: 1.90
               Mean episode length: 210.24
    Episode_Reward/reaching_object: 0.3552
     Episode_Reward/lifting_object: 0.0038
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.80s
                      Time elapsed: 00:03:29
                               ETA: 01:10:42

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 54520 steps/s (collection: 1.718s, learning 0.085s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0968
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.9970
                       Mean reward: 1.76
               Mean episode length: 214.67
    Episode_Reward/reaching_object: 0.3911
     Episode_Reward/lifting_object: -0.0365
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.80s
                      Time elapsed: 00:03:30
                               ETA: 01:10:31

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 53693 steps/s (collection: 1.739s, learning 0.092s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.4136
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 31.0324
                       Mean reward: 1.95
               Mean episode length: 215.08
    Episode_Reward/reaching_object: 0.3865
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.83s
                      Time elapsed: 00:03:32
                               ETA: 01:10:21

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 54759 steps/s (collection: 1.691s, learning 0.104s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 31.0428
                       Mean reward: 2.10
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 0.4218
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.80s
                      Time elapsed: 00:03:34
                               ETA: 01:10:11

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 54039 steps/s (collection: 1.710s, learning 0.109s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 31.0547
                       Mean reward: 2.34
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.4768
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.82s
                      Time elapsed: 00:03:36
                               ETA: 01:10:01

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 53869 steps/s (collection: 1.709s, learning 0.115s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0673
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.0800
                       Mean reward: 2.39
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 0.4701
     Episode_Reward/lifting_object: -0.0085
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.82s
                      Time elapsed: 00:03:38
                               ETA: 01:09:52

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 54381 steps/s (collection: 1.705s, learning 0.102s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0894
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.1531
                       Mean reward: 1.52
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.4774
     Episode_Reward/lifting_object: -0.0619
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.81s
                      Time elapsed: 00:03:40
                               ETA: 01:09:42

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 54955 steps/s (collection: 1.699s, learning 0.090s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0527
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.2463
                       Mean reward: 2.09
               Mean episode length: 229.94
    Episode_Reward/reaching_object: 0.4840
     Episode_Reward/lifting_object: 0.0031
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.79s
                      Time elapsed: 00:03:41
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 54265 steps/s (collection: 1.718s, learning 0.094s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0503
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.3608
                       Mean reward: 2.50
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 0.4729
     Episode_Reward/lifting_object: -0.0130
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.81s
                      Time elapsed: 00:03:43
                               ETA: 01:09:22

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 55844 steps/s (collection: 1.674s, learning 0.086s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1384
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 31.4412
                       Mean reward: 2.14
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 0.4923
     Episode_Reward/lifting_object: -0.0242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.76s
                      Time elapsed: 00:03:45
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 53399 steps/s (collection: 1.740s, learning 0.101s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1543
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.4600
                       Mean reward: 2.46
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 0.4983
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.84s
                      Time elapsed: 00:03:47
                               ETA: 01:09:04

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 54283 steps/s (collection: 1.714s, learning 0.097s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0416
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.5149
                       Mean reward: 2.30
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 0.4883
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.81s
                      Time elapsed: 00:03:49
                               ETA: 01:08:55

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 54622 steps/s (collection: 1.713s, learning 0.087s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2169
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.5831
                       Mean reward: 2.24
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.4877
     Episode_Reward/lifting_object: -0.0259
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.80s
                      Time elapsed: 00:03:50
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 55337 steps/s (collection: 1.687s, learning 0.089s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1187
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.6279
                       Mean reward: 1.94
               Mean episode length: 220.91
    Episode_Reward/reaching_object: 0.4782
     Episode_Reward/lifting_object: -0.0474
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.78s
                      Time elapsed: 00:03:52
                               ETA: 01:08:37

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 54709 steps/s (collection: 1.704s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0702
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.6588
                       Mean reward: 2.41
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 0.4831
     Episode_Reward/lifting_object: -0.0013
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.80s
                      Time elapsed: 00:03:54
                               ETA: 01:08:28

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 54394 steps/s (collection: 1.707s, learning 0.100s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1066
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.7031
                       Mean reward: 2.79
               Mean episode length: 231.40
    Episode_Reward/reaching_object: 0.5175
     Episode_Reward/lifting_object: 0.0181
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.81s
                      Time elapsed: 00:03:56
                               ETA: 01:08:19

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 54112 steps/s (collection: 1.726s, learning 0.091s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.7655
                       Mean reward: 2.54
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 0.5289
     Episode_Reward/lifting_object: 0.0092
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.82s
                      Time elapsed: 00:03:58
                               ETA: 01:08:11

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 53442 steps/s (collection: 1.746s, learning 0.093s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.2358
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 31.7820
                       Mean reward: 2.40
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 0.5453
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.84s
                      Time elapsed: 00:03:59
                               ETA: 01:08:03

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 54402 steps/s (collection: 1.717s, learning 0.090s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0420
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 31.7892
                       Mean reward: 2.90
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 0.5527
     Episode_Reward/lifting_object: 0.0001
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.81s
                      Time elapsed: 00:04:01
                               ETA: 01:07:55

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 53092 steps/s (collection: 1.759s, learning 0.093s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0333
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.8078
                       Mean reward: 2.85
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.5839
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.85s
                      Time elapsed: 00:04:03
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 54596 steps/s (collection: 1.704s, learning 0.097s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 31.8345
                       Mean reward: 2.94
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 0.5646
     Episode_Reward/lifting_object: 0.0190
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.80s
                      Time elapsed: 00:04:05
                               ETA: 01:07:40

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 54477 steps/s (collection: 1.695s, learning 0.109s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.8660
                       Mean reward: 2.58
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 0.0051
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.80s
                      Time elapsed: 00:04:07
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 53638 steps/s (collection: 1.717s, learning 0.116s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1513
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.9174
                       Mean reward: 2.36
               Mean episode length: 230.34
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: -0.0110
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.83s
                      Time elapsed: 00:04:08
                               ETA: 01:07:25

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 53884 steps/s (collection: 1.737s, learning 0.087s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1138
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.9646
                       Mean reward: 2.64
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 0.5718
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.82s
                      Time elapsed: 00:04:10
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 54027 steps/s (collection: 1.732s, learning 0.088s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.0103
                       Mean reward: 2.98
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 0.5676
     Episode_Reward/lifting_object: 0.0274
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.82s
                      Time elapsed: 00:04:12
                               ETA: 01:07:10

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 53149 steps/s (collection: 1.760s, learning 0.089s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1432
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.0546
                       Mean reward: 2.84
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 0.5532
     Episode_Reward/lifting_object: -0.0140
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.85s
                      Time elapsed: 00:04:14
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 52856 steps/s (collection: 1.763s, learning 0.097s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.3147
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 32.0827
                       Mean reward: 2.79
               Mean episode length: 226.00
    Episode_Reward/reaching_object: 0.5712
     Episode_Reward/lifting_object: 0.0126
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.86s
                      Time elapsed: 00:04:16
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 54885 steps/s (collection: 1.698s, learning 0.094s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0781
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.0928
                       Mean reward: 3.16
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 0.0421
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.79s
                      Time elapsed: 00:04:18
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 55098 steps/s (collection: 1.689s, learning 0.096s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0931
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 32.1300
                       Mean reward: 2.90
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 0.5877
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.78s
                      Time elapsed: 00:04:19
                               ETA: 01:06:42

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 55017 steps/s (collection: 1.690s, learning 0.097s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0393
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 32.1848
                       Mean reward: 2.93
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.5820
     Episode_Reward/lifting_object: -0.0623
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.79s
                      Time elapsed: 00:04:21
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 55231 steps/s (collection: 1.686s, learning 0.094s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0759
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.2180
                       Mean reward: 2.28
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.5595
     Episode_Reward/lifting_object: -0.0281
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.78s
                      Time elapsed: 00:04:23
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 54614 steps/s (collection: 1.703s, learning 0.097s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0775
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 32.2542
                       Mean reward: 3.14
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: -0.0096
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.80s
                      Time elapsed: 00:04:25
                               ETA: 01:06:20

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 54384 steps/s (collection: 1.707s, learning 0.101s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0594
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 32.2818
                       Mean reward: 3.41
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 0.5955
     Episode_Reward/lifting_object: 0.0396
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.81s
                      Time elapsed: 00:04:27
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 54125 steps/s (collection: 1.729s, learning 0.088s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1275
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.3260
                       Mean reward: 3.37
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 0.6020
     Episode_Reward/lifting_object: 0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.82s
                      Time elapsed: 00:04:28
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 53379 steps/s (collection: 1.745s, learning 0.097s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1015
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.3479
                       Mean reward: 3.24
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 0.5914
     Episode_Reward/lifting_object: 0.0243
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.84s
                      Time elapsed: 00:04:30
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 52514 steps/s (collection: 1.774s, learning 0.098s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1579
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 32.3809
                       Mean reward: 2.98
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.6169
     Episode_Reward/lifting_object: 0.0156
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.87s
                      Time elapsed: 00:04:32
                               ETA: 01:05:55

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 52552 steps/s (collection: 1.779s, learning 0.092s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.2292
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.4040
                       Mean reward: 3.31
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 0.5841
     Episode_Reward/lifting_object: 0.0389
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.87s
                      Time elapsed: 00:04:34
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 51983 steps/s (collection: 1.801s, learning 0.090s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1887
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.4477
                       Mean reward: 3.29
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 0.5791
     Episode_Reward/lifting_object: 0.0465
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.89s
                      Time elapsed: 00:04:36
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 51163 steps/s (collection: 1.812s, learning 0.110s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1601
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.4674
                       Mean reward: 3.26
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 0.5593
     Episode_Reward/lifting_object: 0.0479
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.92s
                      Time elapsed: 00:04:38
                               ETA: 01:05:39

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51049 steps/s (collection: 1.803s, learning 0.123s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1467
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.5027
                       Mean reward: 3.12
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 0.5694
     Episode_Reward/lifting_object: 0.0547
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.93s
                      Time elapsed: 00:04:40
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 52222 steps/s (collection: 1.793s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.2209
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.5358
                       Mean reward: 2.79
               Mean episode length: 222.44
    Episode_Reward/reaching_object: 0.5720
     Episode_Reward/lifting_object: 0.0506
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.88s
                      Time elapsed: 00:04:42
                               ETA: 01:05:30

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 53473 steps/s (collection: 1.747s, learning 0.092s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1708
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 32.5669
                       Mean reward: 3.41
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 0.5751
     Episode_Reward/lifting_object: 0.0344
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.84s
                      Time elapsed: 00:04:43
                               ETA: 01:05:24

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 54610 steps/s (collection: 1.708s, learning 0.092s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1882
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.6056
                       Mean reward: 3.10
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 0.5959
     Episode_Reward/lifting_object: 0.0615
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.80s
                      Time elapsed: 00:04:45
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 50055 steps/s (collection: 1.859s, learning 0.105s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1241
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 32.6648
                       Mean reward: 2.88
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 0.5702
     Episode_Reward/lifting_object: 0.0800
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.96s
                      Time elapsed: 00:04:47
                               ETA: 01:05:14

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 51912 steps/s (collection: 1.787s, learning 0.107s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0926
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 32.7103
                       Mean reward: 3.00
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 0.5863
     Episode_Reward/lifting_object: 0.0336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.89s
                      Time elapsed: 00:04:49
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 53431 steps/s (collection: 1.750s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1944
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.7458
                       Mean reward: 3.53
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 0.5819
     Episode_Reward/lifting_object: 0.0661
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.84s
                      Time elapsed: 00:04:51
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 53662 steps/s (collection: 1.736s, learning 0.096s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2659
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.7817
                       Mean reward: 2.96
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 0.5935
     Episode_Reward/lifting_object: 0.0781
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.83s
                      Time elapsed: 00:04:53
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 53476 steps/s (collection: 1.751s, learning 0.087s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2012
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.8075
                       Mean reward: 3.16
               Mean episode length: 233.06
    Episode_Reward/reaching_object: 0.5863
     Episode_Reward/lifting_object: 0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.84s
                      Time elapsed: 00:04:55
                               ETA: 01:04:52

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 52599 steps/s (collection: 1.771s, learning 0.098s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4242
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.8551
                       Mean reward: 3.93
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.5992
     Episode_Reward/lifting_object: 0.0902
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.87s
                      Time elapsed: 00:04:56
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 51812 steps/s (collection: 1.807s, learning 0.091s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.3729
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.8993
                       Mean reward: 3.59
               Mean episode length: 227.00
    Episode_Reward/reaching_object: 0.5697
     Episode_Reward/lifting_object: 0.1301
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.90s
                      Time elapsed: 00:04:58
                               ETA: 01:04:42

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 51768 steps/s (collection: 1.798s, learning 0.101s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4569
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 32.9377
                       Mean reward: 3.15
               Mean episode length: 233.40
    Episode_Reward/reaching_object: 0.5903
     Episode_Reward/lifting_object: 0.0957
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.90s
                      Time elapsed: 00:05:00
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 54060 steps/s (collection: 1.727s, learning 0.092s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3271
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.9957
                       Mean reward: 3.81
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 0.5534
     Episode_Reward/lifting_object: 0.1196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.82s
                      Time elapsed: 00:05:02
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 53625 steps/s (collection: 1.747s, learning 0.087s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3333
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.0399
                       Mean reward: 3.60
               Mean episode length: 207.28
    Episode_Reward/reaching_object: 0.5378
     Episode_Reward/lifting_object: 0.1659
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.83s
                      Time elapsed: 00:05:04
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 52417 steps/s (collection: 1.769s, learning 0.106s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.6243
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.0755
                       Mean reward: 3.27
               Mean episode length: 210.76
    Episode_Reward/reaching_object: 0.5355
     Episode_Reward/lifting_object: 0.1761
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.88s
                      Time elapsed: 00:05:06
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 52386 steps/s (collection: 1.784s, learning 0.093s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.5755
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.1012
                       Mean reward: 4.02
               Mean episode length: 210.71
    Episode_Reward/reaching_object: 0.5382
     Episode_Reward/lifting_object: 0.1298
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.88s
                      Time elapsed: 00:05:08
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 53368 steps/s (collection: 1.724s, learning 0.118s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2924
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.1435
                       Mean reward: 4.22
               Mean episode length: 213.96
    Episode_Reward/reaching_object: 0.5421
     Episode_Reward/lifting_object: 0.2063
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.84s
                      Time elapsed: 00:05:09
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 53333 steps/s (collection: 1.748s, learning 0.095s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3627
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 33.1763
                       Mean reward: 3.04
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 0.5314
     Episode_Reward/lifting_object: 0.1669
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.84s
                      Time elapsed: 00:05:11
                               ETA: 01:04:08

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 53707 steps/s (collection: 1.738s, learning 0.093s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3573
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.2143
                       Mean reward: 3.42
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 0.5159
     Episode_Reward/lifting_object: 0.1693
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.83s
                      Time elapsed: 00:05:13
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 51122 steps/s (collection: 1.822s, learning 0.101s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.4204
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.2398
                       Mean reward: 5.08
               Mean episode length: 225.47
    Episode_Reward/reaching_object: 0.5637
     Episode_Reward/lifting_object: 0.1814
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.92s
                      Time elapsed: 00:05:15
                               ETA: 01:03:58

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 52287 steps/s (collection: 1.774s, learning 0.106s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.4249
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 33.2750
                       Mean reward: 3.39
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 0.5355
     Episode_Reward/lifting_object: 0.2555
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.88s
                      Time elapsed: 00:05:17
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 54137 steps/s (collection: 1.717s, learning 0.099s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.5277
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 33.3096
                       Mean reward: 3.84
               Mean episode length: 226.53
    Episode_Reward/reaching_object: 0.5413
     Episode_Reward/lifting_object: 0.2196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.82s
                      Time elapsed: 00:05:19
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 53529 steps/s (collection: 1.740s, learning 0.096s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.7220
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.3564
                       Mean reward: 4.62
               Mean episode length: 230.91
    Episode_Reward/reaching_object: 0.5677
     Episode_Reward/lifting_object: 0.2177
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.84s
                      Time elapsed: 00:05:21
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 53526 steps/s (collection: 1.746s, learning 0.091s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.4211
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.3971
                       Mean reward: 3.42
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 0.5220
     Episode_Reward/lifting_object: 0.2669
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.84s
                      Time elapsed: 00:05:22
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 53435 steps/s (collection: 1.740s, learning 0.100s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.5670
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 33.4323
                       Mean reward: 3.21
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.5620
     Episode_Reward/lifting_object: 0.2612
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.84s
                      Time elapsed: 00:05:24
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 53646 steps/s (collection: 1.742s, learning 0.090s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.6817
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.4623
                       Mean reward: 3.63
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 0.5669
     Episode_Reward/lifting_object: 0.2757
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.83s
                      Time elapsed: 00:05:26
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 53882 steps/s (collection: 1.731s, learning 0.093s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.8212
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.5083
                       Mean reward: 3.74
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 0.5328
     Episode_Reward/lifting_object: 0.2292
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.82s
                      Time elapsed: 00:05:28
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 54242 steps/s (collection: 1.723s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.8050
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.5573
                       Mean reward: 3.41
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 0.5436
     Episode_Reward/lifting_object: 0.2384
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.81s
                      Time elapsed: 00:05:30
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 52557 steps/s (collection: 1.776s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.7879
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.5986
                       Mean reward: 4.37
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 0.5171
     Episode_Reward/lifting_object: 0.3752
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.87s
                      Time elapsed: 00:05:32
                               ETA: 01:03:15

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 54305 steps/s (collection: 1.715s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 2.6271
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 33.6302
                       Mean reward: 3.12
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 0.4866
     Episode_Reward/lifting_object: 0.1555
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.81s
                      Time elapsed: 00:05:33
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 51790 steps/s (collection: 1.786s, learning 0.113s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.9076
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.6637
                       Mean reward: 3.78
               Mean episode length: 231.01
    Episode_Reward/reaching_object: 0.4982
     Episode_Reward/lifting_object: 0.4111
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.90s
                      Time elapsed: 00:05:35
                               ETA: 01:03:06

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 53229 steps/s (collection: 1.738s, learning 0.109s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.2217
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 33.6969
                       Mean reward: 4.09
               Mean episode length: 209.96
    Episode_Reward/reaching_object: 0.4650
     Episode_Reward/lifting_object: 0.3526
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.85s
                      Time elapsed: 00:05:37
                               ETA: 01:03:02

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 52913 steps/s (collection: 1.754s, learning 0.104s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.5208
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 33.7294
                       Mean reward: 5.18
               Mean episode length: 204.62
    Episode_Reward/reaching_object: 0.4425
     Episode_Reward/lifting_object: 0.4847
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.86s
                      Time elapsed: 00:05:39
                               ETA: 01:02:58

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 53599 steps/s (collection: 1.746s, learning 0.088s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.9227
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.7589
                       Mean reward: 3.08
               Mean episode length: 221.37
    Episode_Reward/reaching_object: 0.4840
     Episode_Reward/lifting_object: 0.4126
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.83s
                      Time elapsed: 00:05:41
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 54315 steps/s (collection: 1.722s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 1.0338
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 33.8019
                       Mean reward: 4.59
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 0.4757
     Episode_Reward/lifting_object: 0.3942
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.81s
                      Time elapsed: 00:05:43
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 53830 steps/s (collection: 1.738s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 4.6365
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.8421
                       Mean reward: 3.66
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 0.4363
     Episode_Reward/lifting_object: 0.3796
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.83s
                      Time elapsed: 00:05:45
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 53933 steps/s (collection: 1.733s, learning 0.090s)
             Mean action noise std: 1.59
          Mean value_function loss: 1.3969
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.8732
                       Mean reward: 5.62
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 0.4472
     Episode_Reward/lifting_object: 0.4830
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.82s
                      Time elapsed: 00:05:46
                               ETA: 01:02:39

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 53915 steps/s (collection: 1.718s, learning 0.105s)
             Mean action noise std: 1.60
          Mean value_function loss: 1.1452
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 33.9248
                       Mean reward: 4.89
               Mean episode length: 217.04
    Episode_Reward/reaching_object: 0.4419
     Episode_Reward/lifting_object: 0.6668
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.82s
                      Time elapsed: 00:05:48
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 54355 steps/s (collection: 1.707s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 1.2193
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 33.9602
                       Mean reward: 5.20
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 0.4202
     Episode_Reward/lifting_object: 0.5550
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.81s
                      Time elapsed: 00:05:50
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 52430 steps/s (collection: 1.768s, learning 0.107s)
             Mean action noise std: 1.60
          Mean value_function loss: 1.0299
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 33.9902
                       Mean reward: 4.54
               Mean episode length: 238.28
    Episode_Reward/reaching_object: 0.4228
     Episode_Reward/lifting_object: 0.5317
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.87s
                      Time elapsed: 00:05:52
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 53002 steps/s (collection: 1.749s, learning 0.106s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.8812
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.0094
                       Mean reward: 5.60
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 0.4174
     Episode_Reward/lifting_object: 0.5758
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.85s
                      Time elapsed: 00:05:54
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 52131 steps/s (collection: 1.787s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.3474
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.0338
                       Mean reward: 4.84
               Mean episode length: 225.99
    Episode_Reward/reaching_object: 0.3930
     Episode_Reward/lifting_object: 0.5266
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.89s
                      Time elapsed: 00:05:56
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 51641 steps/s (collection: 1.806s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.4632
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.0557
                       Mean reward: 5.24
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 0.3875
     Episode_Reward/lifting_object: 0.5732
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.90s
                      Time elapsed: 00:05:57
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 40500 steps/s (collection: 2.253s, learning 0.175s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.0053
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.0676
                       Mean reward: 4.52
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 0.3630
     Episode_Reward/lifting_object: 0.5641
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.43s
                      Time elapsed: 00:06:00
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 42646 steps/s (collection: 2.216s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.5626
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.0868
                       Mean reward: 4.46
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.3702
     Episode_Reward/lifting_object: 0.6977
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.31s
                      Time elapsed: 00:06:02
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 50565 steps/s (collection: 1.854s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.9962
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.1133
                       Mean reward: 4.33
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 0.3559
     Episode_Reward/lifting_object: 0.5466
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.94s
                      Time elapsed: 00:06:04
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 52326 steps/s (collection: 1.778s, learning 0.101s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.2110
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.1405
                       Mean reward: 5.32
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 0.3399
     Episode_Reward/lifting_object: 0.5814
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.88s
                      Time elapsed: 00:06:06
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 53771 steps/s (collection: 1.737s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.0876
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.1692
                       Mean reward: 5.85
               Mean episode length: 210.67
    Episode_Reward/reaching_object: 0.3129
     Episode_Reward/lifting_object: 0.4665
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.83s
                      Time elapsed: 00:06:08
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 52952 steps/s (collection: 1.766s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.2757
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.2031
                       Mean reward: 5.87
               Mean episode length: 200.86
    Episode_Reward/reaching_object: 0.3284
     Episode_Reward/lifting_object: 0.6114
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.86s
                      Time elapsed: 00:06:10
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 46569 steps/s (collection: 1.958s, learning 0.153s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.3069
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.2348
                       Mean reward: 4.39
               Mean episode length: 195.13
    Episode_Reward/reaching_object: 0.3289
     Episode_Reward/lifting_object: 0.5990
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.11s
                      Time elapsed: 00:06:12
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 48838 steps/s (collection: 1.917s, learning 0.096s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.4980
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.2607
                       Mean reward: 3.57
               Mean episode length: 183.32
    Episode_Reward/reaching_object: 0.3292
     Episode_Reward/lifting_object: 0.4581
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.01s
                      Time elapsed: 00:06:14
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 54040 steps/s (collection: 1.733s, learning 0.086s)
             Mean action noise std: 1.63
          Mean value_function loss: 2.3003
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.2937
                       Mean reward: 4.02
               Mean episode length: 210.29
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: 0.5885
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.82s
                      Time elapsed: 00:06:16
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 54496 steps/s (collection: 1.715s, learning 0.089s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.0682
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.3265
                       Mean reward: 6.72
               Mean episode length: 219.91
    Episode_Reward/reaching_object: 0.3755
     Episode_Reward/lifting_object: 0.7335
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.80s
                      Time elapsed: 00:06:17
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 55190 steps/s (collection: 1.678s, learning 0.104s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.4316
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.3550
                       Mean reward: 6.00
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 0.3801
     Episode_Reward/lifting_object: 0.7927
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.78s
                      Time elapsed: 00:06:19
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 54624 steps/s (collection: 1.706s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.5033
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.3843
                       Mean reward: 7.70
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 0.4000
     Episode_Reward/lifting_object: 0.7580
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.80s
                      Time elapsed: 00:06:21
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 54858 steps/s (collection: 1.694s, learning 0.098s)
             Mean action noise std: 1.64
          Mean value_function loss: 2.0652
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.4105
                       Mean reward: 6.32
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 0.4148
     Episode_Reward/lifting_object: 0.8902
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.79s
                      Time elapsed: 00:06:23
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 54613 steps/s (collection: 1.708s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 2.8349
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.4444
                       Mean reward: 5.89
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 0.4070
     Episode_Reward/lifting_object: 0.7741
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.80s
                      Time elapsed: 00:06:25
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 52815 steps/s (collection: 1.765s, learning 0.097s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.3402
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.4780
                       Mean reward: 5.92
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 0.4198
     Episode_Reward/lifting_object: 0.8535
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.86s
                      Time elapsed: 00:06:27
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 53784 steps/s (collection: 1.724s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.9483
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.5136
                       Mean reward: 7.84
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 0.4047
     Episode_Reward/lifting_object: 0.8720
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.83s
                      Time elapsed: 00:06:28
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 54529 steps/s (collection: 1.707s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.8366
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 34.5518
                       Mean reward: 6.11
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 0.4049
     Episode_Reward/lifting_object: 0.9211
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.80s
                      Time elapsed: 00:06:30
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 52555 steps/s (collection: 1.782s, learning 0.089s)
             Mean action noise std: 1.66
          Mean value_function loss: 2.0638
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.5837
                       Mean reward: 6.94
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.4151
     Episode_Reward/lifting_object: 1.0092
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.87s
                      Time elapsed: 00:06:32
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 49468 steps/s (collection: 1.875s, learning 0.112s)
             Mean action noise std: 1.66
          Mean value_function loss: 2.4071
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.5989
                       Mean reward: 7.14
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 0.4220
     Episode_Reward/lifting_object: 0.9889
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.99s
                      Time elapsed: 00:06:34
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 50082 steps/s (collection: 1.846s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 2.1673
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.6184
                       Mean reward: 9.15
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 0.4371
     Episode_Reward/lifting_object: 1.0714
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.96s
                      Time elapsed: 00:06:36
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 48114 steps/s (collection: 1.888s, learning 0.156s)
             Mean action noise std: 1.66
          Mean value_function loss: 2.1021
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.6426
                       Mean reward: 7.48
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 0.4126
     Episode_Reward/lifting_object: 1.0548
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.04s
                      Time elapsed: 00:06:38
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 45578 steps/s (collection: 2.018s, learning 0.138s)
             Mean action noise std: 1.67
          Mean value_function loss: 2.4704
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.6632
                       Mean reward: 7.02
               Mean episode length: 222.52
    Episode_Reward/reaching_object: 0.4100
     Episode_Reward/lifting_object: 1.1779
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.16s
                      Time elapsed: 00:06:40
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 49262 steps/s (collection: 1.865s, learning 0.131s)
             Mean action noise std: 1.67
          Mean value_function loss: 2.4107
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.6924
                       Mean reward: 7.96
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 0.4096
     Episode_Reward/lifting_object: 1.2447
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.00s
                      Time elapsed: 00:06:42
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 52562 steps/s (collection: 1.758s, learning 0.112s)
             Mean action noise std: 1.67
          Mean value_function loss: 2.4038
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.7166
                       Mean reward: 8.38
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 0.3986
     Episode_Reward/lifting_object: 1.4878
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.87s
                      Time elapsed: 00:06:44
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 53446 steps/s (collection: 1.739s, learning 0.101s)
             Mean action noise std: 1.67
          Mean value_function loss: 4.0380
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.7335
                       Mean reward: 8.68
               Mean episode length: 216.15
    Episode_Reward/reaching_object: 0.3856
     Episode_Reward/lifting_object: 1.2648
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.84s
                      Time elapsed: 00:06:46
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 53358 steps/s (collection: 1.734s, learning 0.108s)
             Mean action noise std: 1.68
          Mean value_function loss: 2.8582
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.7566
                       Mean reward: 10.51
               Mean episode length: 209.15
    Episode_Reward/reaching_object: 0.3833
     Episode_Reward/lifting_object: 1.3691
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.84s
                      Time elapsed: 00:06:48
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 53924 steps/s (collection: 1.708s, learning 0.115s)
             Mean action noise std: 1.68
          Mean value_function loss: 3.0201
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.7770
                       Mean reward: 9.15
               Mean episode length: 210.11
    Episode_Reward/reaching_object: 0.3677
     Episode_Reward/lifting_object: 1.4962
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.82s
                      Time elapsed: 00:06:50
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 54632 steps/s (collection: 1.702s, learning 0.097s)
             Mean action noise std: 1.68
          Mean value_function loss: 5.0045
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.7919
                       Mean reward: 6.75
               Mean episode length: 204.02
    Episode_Reward/reaching_object: 0.3620
     Episode_Reward/lifting_object: 1.3820
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.80s
                      Time elapsed: 00:06:51
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 53858 steps/s (collection: 1.724s, learning 0.101s)
             Mean action noise std: 1.68
          Mean value_function loss: 2.6028
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.8134
                       Mean reward: 9.80
               Mean episode length: 206.21
    Episode_Reward/reaching_object: 0.3742
     Episode_Reward/lifting_object: 1.4539
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.83s
                      Time elapsed: 00:06:53
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 54345 steps/s (collection: 1.709s, learning 0.100s)
             Mean action noise std: 1.68
          Mean value_function loss: 3.0533
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8353
                       Mean reward: 6.08
               Mean episode length: 197.58
    Episode_Reward/reaching_object: 0.3559
     Episode_Reward/lifting_object: 1.3791
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.81s
                      Time elapsed: 00:06:55
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 54496 steps/s (collection: 1.716s, learning 0.088s)
             Mean action noise std: 1.68
          Mean value_function loss: 3.0340
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8480
                       Mean reward: 9.70
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 0.3565
     Episode_Reward/lifting_object: 1.4818
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.80s
                      Time elapsed: 00:06:57
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 54693 steps/s (collection: 1.707s, learning 0.090s)
             Mean action noise std: 1.69
          Mean value_function loss: 2.9781
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.8678
                       Mean reward: 10.10
               Mean episode length: 193.27
    Episode_Reward/reaching_object: 0.3520
     Episode_Reward/lifting_object: 1.6031
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.80s
                      Time elapsed: 00:06:59
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 53402 steps/s (collection: 1.733s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 3.3638
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.8888
                       Mean reward: 9.52
               Mean episode length: 199.80
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: 1.6221
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.84s
                      Time elapsed: 00:07:00
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 54108 steps/s (collection: 1.718s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 3.7597
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.9014
                       Mean reward: 10.15
               Mean episode length: 196.68
    Episode_Reward/reaching_object: 0.3432
     Episode_Reward/lifting_object: 1.5081
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.82s
                      Time elapsed: 00:07:02
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 54104 steps/s (collection: 1.717s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 2.9936
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.9161
                       Mean reward: 10.71
               Mean episode length: 201.63
    Episode_Reward/reaching_object: 0.3451
     Episode_Reward/lifting_object: 1.4315
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.82s
                      Time elapsed: 00:07:04
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 53574 steps/s (collection: 1.748s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 2.8742
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.9332
                       Mean reward: 9.09
               Mean episode length: 185.98
    Episode_Reward/reaching_object: 0.3388
     Episode_Reward/lifting_object: 1.6599
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.83s
                      Time elapsed: 00:07:06
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 53372 steps/s (collection: 1.742s, learning 0.100s)
             Mean action noise std: 1.69
          Mean value_function loss: 4.7322
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.9442
                       Mean reward: 9.10
               Mean episode length: 184.45
    Episode_Reward/reaching_object: 0.3156
     Episode_Reward/lifting_object: 1.4233
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.84s
                      Time elapsed: 00:07:08
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 55234 steps/s (collection: 1.689s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 3.2900
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.9567
                       Mean reward: 10.36
               Mean episode length: 180.48
    Episode_Reward/reaching_object: 0.3136
     Episode_Reward/lifting_object: 1.5276
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.78s
                      Time elapsed: 00:07:09
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 54405 steps/s (collection: 1.718s, learning 0.089s)
             Mean action noise std: 1.70
          Mean value_function loss: 3.4215
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.9758
                       Mean reward: 7.19
               Mean episode length: 180.27
    Episode_Reward/reaching_object: 0.3024
     Episode_Reward/lifting_object: 1.4287
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.81s
                      Time elapsed: 00:07:11
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 53675 steps/s (collection: 1.731s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 4.4189
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.9980
                       Mean reward: 8.27
               Mean episode length: 175.21
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: 1.4994
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.83s
                      Time elapsed: 00:07:13
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 54067 steps/s (collection: 1.719s, learning 0.099s)
             Mean action noise std: 1.70
          Mean value_function loss: 4.3803
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.0239
                       Mean reward: 10.76
               Mean episode length: 166.07
    Episode_Reward/reaching_object: 0.2922
     Episode_Reward/lifting_object: 1.5390
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.82s
                      Time elapsed: 00:07:15
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 54592 steps/s (collection: 1.708s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 4.4382
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 35.0501
                       Mean reward: 9.01
               Mean episode length: 158.36
    Episode_Reward/reaching_object: 0.2748
     Episode_Reward/lifting_object: 1.6088
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.80s
                      Time elapsed: 00:07:17
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 53160 steps/s (collection: 1.751s, learning 0.098s)
             Mean action noise std: 1.70
          Mean value_function loss: 4.0876
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.0649
                       Mean reward: 7.41
               Mean episode length: 153.69
    Episode_Reward/reaching_object: 0.2609
     Episode_Reward/lifting_object: 1.3537
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.85s
                      Time elapsed: 00:07:19
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 54736 steps/s (collection: 1.708s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 7.7545
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.0889
                       Mean reward: 6.53
               Mean episode length: 143.71
    Episode_Reward/reaching_object: 0.2513
     Episode_Reward/lifting_object: 1.2130
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.80s
                      Time elapsed: 00:07:20
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 53848 steps/s (collection: 1.737s, learning 0.089s)
             Mean action noise std: 1.71
          Mean value_function loss: 6.3369
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.1073
                       Mean reward: 8.39
               Mean episode length: 135.61
    Episode_Reward/reaching_object: 0.2514
     Episode_Reward/lifting_object: 1.2964
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.83s
                      Time elapsed: 00:07:22
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 53978 steps/s (collection: 1.731s, learning 0.091s)
             Mean action noise std: 1.71
          Mean value_function loss: 5.4158
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.1264
                       Mean reward: 5.66
               Mean episode length: 131.07
    Episode_Reward/reaching_object: 0.2391
     Episode_Reward/lifting_object: 1.1770
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.82s
                      Time elapsed: 00:07:24
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 53691 steps/s (collection: 1.739s, learning 0.092s)
             Mean action noise std: 1.71
          Mean value_function loss: 6.1346
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.1512
                       Mean reward: 6.16
               Mean episode length: 137.97
    Episode_Reward/reaching_object: 0.2355
     Episode_Reward/lifting_object: 1.3136
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.83s
                      Time elapsed: 00:07:26
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 54283 steps/s (collection: 1.723s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.0320
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.1703
                       Mean reward: 8.43
               Mean episode length: 137.83
    Episode_Reward/reaching_object: 0.2583
     Episode_Reward/lifting_object: 1.4587
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.81s
                      Time elapsed: 00:07:28
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 53923 steps/s (collection: 1.736s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.0809
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 35.1843
                       Mean reward: 10.44
               Mean episode length: 141.48
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: 1.9269
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.82s
                      Time elapsed: 00:07:30
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 53122 steps/s (collection: 1.763s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.1500
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.1879
                       Mean reward: 10.08
               Mean episode length: 145.69
    Episode_Reward/reaching_object: 0.2589
     Episode_Reward/lifting_object: 1.6231
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.85s
                      Time elapsed: 00:07:31
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 54415 steps/s (collection: 1.696s, learning 0.110s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.1625
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.1972
                       Mean reward: 9.51
               Mean episode length: 137.39
    Episode_Reward/reaching_object: 0.2533
     Episode_Reward/lifting_object: 1.8302
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.81s
                      Time elapsed: 00:07:33
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 53726 steps/s (collection: 1.744s, learning 0.086s)
             Mean action noise std: 1.72
          Mean value_function loss: 4.6226
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.2074
                       Mean reward: 9.38
               Mean episode length: 140.24
    Episode_Reward/reaching_object: 0.2493
     Episode_Reward/lifting_object: 1.7388
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.83s
                      Time elapsed: 00:07:35
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 52388 steps/s (collection: 1.771s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.2392
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.2148
                       Mean reward: 7.97
               Mean episode length: 143.44
    Episode_Reward/reaching_object: 0.2459
     Episode_Reward/lifting_object: 1.7421
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.88s
                      Time elapsed: 00:07:37
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 53146 steps/s (collection: 1.736s, learning 0.114s)
             Mean action noise std: 1.72
          Mean value_function loss: 8.2248
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2282
                       Mean reward: 12.15
               Mean episode length: 137.39
    Episode_Reward/reaching_object: 0.2461
     Episode_Reward/lifting_object: 1.7679
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.85s
                      Time elapsed: 00:07:39
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 53706 steps/s (collection: 1.736s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 6.2518
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.2479
                       Mean reward: 9.97
               Mean episode length: 138.30
    Episode_Reward/reaching_object: 0.2405
     Episode_Reward/lifting_object: 1.6239
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.83s
                      Time elapsed: 00:07:41
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 53712 steps/s (collection: 1.734s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 6.9130
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.2692
                       Mean reward: 8.50
               Mean episode length: 141.37
    Episode_Reward/reaching_object: 0.2442
     Episode_Reward/lifting_object: 1.6084
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.83s
                      Time elapsed: 00:07:42
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 54418 steps/s (collection: 1.713s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 5.7283
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.2822
                       Mean reward: 8.94
               Mean episode length: 140.22
    Episode_Reward/reaching_object: 0.2518
     Episode_Reward/lifting_object: 1.6725
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.81s
                      Time elapsed: 00:07:44
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 51960 steps/s (collection: 1.807s, learning 0.085s)
             Mean action noise std: 1.73
          Mean value_function loss: 11.3066
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.2955
                       Mean reward: 9.95
               Mean episode length: 136.90
    Episode_Reward/reaching_object: 0.2362
     Episode_Reward/lifting_object: 1.8129
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.89s
                      Time elapsed: 00:07:46
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 53656 steps/s (collection: 1.745s, learning 0.087s)
             Mean action noise std: 1.73
          Mean value_function loss: 6.0417
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.3188
                       Mean reward: 9.21
               Mean episode length: 138.64
    Episode_Reward/reaching_object: 0.2457
     Episode_Reward/lifting_object: 1.4686
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.83s
                      Time elapsed: 00:07:48
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 53555 steps/s (collection: 1.747s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 6.2671
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.3330
                       Mean reward: 11.64
               Mean episode length: 144.13
    Episode_Reward/reaching_object: 0.2472
     Episode_Reward/lifting_object: 1.9775
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.84s
                      Time elapsed: 00:07:50
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 53485 steps/s (collection: 1.750s, learning 0.088s)
             Mean action noise std: 1.73
          Mean value_function loss: 7.5845
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.3468
                       Mean reward: 9.88
               Mean episode length: 141.42
    Episode_Reward/reaching_object: 0.2462
     Episode_Reward/lifting_object: 1.9020
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.84s
                      Time elapsed: 00:07:52
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 52980 steps/s (collection: 1.764s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 6.1906
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.3539
                       Mean reward: 11.50
               Mean episode length: 132.00
    Episode_Reward/reaching_object: 0.2468
     Episode_Reward/lifting_object: 1.8545
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.86s
                      Time elapsed: 00:07:53
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 53271 steps/s (collection: 1.756s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 6.0837
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.3582
                       Mean reward: 10.23
               Mean episode length: 133.36
    Episode_Reward/reaching_object: 0.2604
     Episode_Reward/lifting_object: 2.1066
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.85s
                      Time elapsed: 00:07:55
                               ETA: 00:58:44

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 53318 steps/s (collection: 1.753s, learning 0.091s)
             Mean action noise std: 1.73
          Mean value_function loss: 7.7911
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.3660
                       Mean reward: 11.72
               Mean episode length: 146.56
    Episode_Reward/reaching_object: 0.2560
     Episode_Reward/lifting_object: 2.0837
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.84s
                      Time elapsed: 00:07:57
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 54019 steps/s (collection: 1.734s, learning 0.086s)
             Mean action noise std: 1.74
          Mean value_function loss: 6.1781
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.3784
                       Mean reward: 13.53
               Mean episode length: 139.81
    Episode_Reward/reaching_object: 0.2652
     Episode_Reward/lifting_object: 2.3581
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.82s
                      Time elapsed: 00:07:59
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 51899 steps/s (collection: 1.801s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.2460
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3899
                       Mean reward: 13.04
               Mean episode length: 136.95
    Episode_Reward/reaching_object: 0.2608
     Episode_Reward/lifting_object: 2.2358
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.89s
                      Time elapsed: 00:08:01
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 53649 steps/s (collection: 1.743s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 6.7585
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.4018
                       Mean reward: 11.27
               Mean episode length: 131.02
    Episode_Reward/reaching_object: 0.2612
     Episode_Reward/lifting_object: 2.4238
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.83s
                      Time elapsed: 00:08:03
                               ETA: 00:58:32

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 53676 steps/s (collection: 1.740s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 8.8112
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.4113
                       Mean reward: 14.18
               Mean episode length: 142.06
    Episode_Reward/reaching_object: 0.2479
     Episode_Reward/lifting_object: 2.3112
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.83s
                      Time elapsed: 00:08:05
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 53468 steps/s (collection: 1.739s, learning 0.099s)
             Mean action noise std: 1.74
          Mean value_function loss: 6.5796
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.4256
                       Mean reward: 11.97
               Mean episode length: 134.09
    Episode_Reward/reaching_object: 0.2471
     Episode_Reward/lifting_object: 2.3344
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 1.84s
                      Time elapsed: 00:08:06
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 53248 steps/s (collection: 1.745s, learning 0.101s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.1851
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.4359
                       Mean reward: 9.93
               Mean episode length: 140.42
    Episode_Reward/reaching_object: 0.2451
     Episode_Reward/lifting_object: 2.1042
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 1.85s
                      Time elapsed: 00:08:08
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 53181 steps/s (collection: 1.756s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.0145
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.4429
                       Mean reward: 12.72
               Mean episode length: 132.74
    Episode_Reward/reaching_object: 0.2422
     Episode_Reward/lifting_object: 2.2948
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.85s
                      Time elapsed: 00:08:10
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 54390 steps/s (collection: 1.720s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.7322
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.4476
                       Mean reward: 12.36
               Mean episode length: 131.12
    Episode_Reward/reaching_object: 0.2396
     Episode_Reward/lifting_object: 2.1644
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.81s
                      Time elapsed: 00:08:12
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 53343 steps/s (collection: 1.753s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 8.1198
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.4550
                       Mean reward: 13.17
               Mean episode length: 131.48
    Episode_Reward/reaching_object: 0.2472
     Episode_Reward/lifting_object: 2.2928
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.84s
                      Time elapsed: 00:08:14
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 53229 steps/s (collection: 1.758s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.2363
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.4643
                       Mean reward: 12.67
               Mean episode length: 126.70
    Episode_Reward/reaching_object: 0.2415
     Episode_Reward/lifting_object: 2.3947
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.85s
                      Time elapsed: 00:08:16
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 53201 steps/s (collection: 1.760s, learning 0.088s)
             Mean action noise std: 1.75
          Mean value_function loss: 7.7685
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.4721
                       Mean reward: 13.16
               Mean episode length: 127.23
    Episode_Reward/reaching_object: 0.2471
     Episode_Reward/lifting_object: 2.5271
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.85s
                      Time elapsed: 00:08:17
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 53251 steps/s (collection: 1.745s, learning 0.101s)
             Mean action noise std: 1.75
          Mean value_function loss: 10.9615
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.4824
                       Mean reward: 8.31
               Mean episode length: 118.14
    Episode_Reward/reaching_object: 0.2447
     Episode_Reward/lifting_object: 2.2495
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.85s
                      Time elapsed: 00:08:19
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 53008 steps/s (collection: 1.769s, learning 0.085s)
             Mean action noise std: 1.75
          Mean value_function loss: 8.5185
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.4894
                       Mean reward: 14.53
               Mean episode length: 131.24
    Episode_Reward/reaching_object: 0.2469
     Episode_Reward/lifting_object: 2.5505
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.85s
                      Time elapsed: 00:08:21
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 53637 steps/s (collection: 1.748s, learning 0.085s)
             Mean action noise std: 1.75
          Mean value_function loss: 8.7694
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 35.4929
                       Mean reward: 18.89
               Mean episode length: 134.97
    Episode_Reward/reaching_object: 0.2487
     Episode_Reward/lifting_object: 2.9486
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.83s
                      Time elapsed: 00:08:23
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 52874 steps/s (collection: 1.767s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 12.7170
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.4939
                       Mean reward: 12.79
               Mean episode length: 132.28
    Episode_Reward/reaching_object: 0.2499
     Episode_Reward/lifting_object: 2.5986
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.86s
                      Time elapsed: 00:08:25
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 53114 steps/s (collection: 1.758s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 10.6274
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 35.4943
                       Mean reward: 14.62
               Mean episode length: 124.03
    Episode_Reward/reaching_object: 0.2466
     Episode_Reward/lifting_object: 2.8581
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.85s
                      Time elapsed: 00:08:27
                               ETA: 00:57:52

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 53155 steps/s (collection: 1.762s, learning 0.087s)
             Mean action noise std: 1.75
          Mean value_function loss: 7.8437
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.4946
                       Mean reward: 14.87
               Mean episode length: 130.78
    Episode_Reward/reaching_object: 0.2576
     Episode_Reward/lifting_object: 3.0831
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 1.85s
                      Time elapsed: 00:08:28
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 51622 steps/s (collection: 1.812s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 8.7810
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.4967
                       Mean reward: 17.14
               Mean episode length: 131.80
    Episode_Reward/reaching_object: 0.2610
     Episode_Reward/lifting_object: 3.3180
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 1.90s
                      Time elapsed: 00:08:30
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 52934 steps/s (collection: 1.765s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 9.6148
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.5031
                       Mean reward: 18.29
               Mean episode length: 129.51
    Episode_Reward/reaching_object: 0.2464
     Episode_Reward/lifting_object: 2.7818
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.86s
                      Time elapsed: 00:08:32
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 54301 steps/s (collection: 1.721s, learning 0.090s)
             Mean action noise std: 1.75
          Mean value_function loss: 9.7217
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.5128
                       Mean reward: 14.02
               Mean episode length: 118.84
    Episode_Reward/reaching_object: 0.2442
     Episode_Reward/lifting_object: 3.0320
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.81s
                      Time elapsed: 00:08:34
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 53500 steps/s (collection: 1.745s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 9.8156
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.5208
                       Mean reward: 18.05
               Mean episode length: 125.29
    Episode_Reward/reaching_object: 0.2381
     Episode_Reward/lifting_object: 2.8274
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.84s
                      Time elapsed: 00:08:36
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 53665 steps/s (collection: 1.726s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 15.4795
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.5318
                       Mean reward: 15.30
               Mean episode length: 126.98
    Episode_Reward/reaching_object: 0.2369
     Episode_Reward/lifting_object: 2.9622
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.83s
                      Time elapsed: 00:08:38
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 53166 steps/s (collection: 1.743s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 10.6607
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.5445
                       Mean reward: 17.95
               Mean episode length: 128.43
    Episode_Reward/reaching_object: 0.2375
     Episode_Reward/lifting_object: 3.0762
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 1.85s
                      Time elapsed: 00:08:40
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 52814 steps/s (collection: 1.757s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 11.3857
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.5527
                       Mean reward: 15.87
               Mean episode length: 121.67
    Episode_Reward/reaching_object: 0.2358
     Episode_Reward/lifting_object: 3.2022
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 1.86s
                      Time elapsed: 00:08:41
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 54155 steps/s (collection: 1.725s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 10.2537
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.5572
                       Mean reward: 18.75
               Mean episode length: 122.60
    Episode_Reward/reaching_object: 0.2377
     Episode_Reward/lifting_object: 3.2292
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 1.82s
                      Time elapsed: 00:08:43
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 53321 steps/s (collection: 1.719s, learning 0.125s)
             Mean action noise std: 1.76
          Mean value_function loss: 10.8115
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.5652
                       Mean reward: 15.30
               Mean episode length: 120.65
    Episode_Reward/reaching_object: 0.2353
     Episode_Reward/lifting_object: 3.0806
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.84s
                      Time elapsed: 00:08:45
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 52722 steps/s (collection: 1.773s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 9.9362
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.5764
                       Mean reward: 19.16
               Mean episode length: 122.60
    Episode_Reward/reaching_object: 0.2353
     Episode_Reward/lifting_object: 3.1070
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 1.86s
                      Time elapsed: 00:08:47
                               ETA: 00:57:20

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 53230 steps/s (collection: 1.758s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 11.0952
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.5869
                       Mean reward: 16.19
               Mean episode length: 126.92
    Episode_Reward/reaching_object: 0.2361
     Episode_Reward/lifting_object: 3.2414
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.85s
                      Time elapsed: 00:08:49
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 51333 steps/s (collection: 1.822s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 14.3767
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.5970
                       Mean reward: 13.34
               Mean episode length: 123.24
    Episode_Reward/reaching_object: 0.2299
     Episode_Reward/lifting_object: 2.9883
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.92s
                      Time elapsed: 00:08:51
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 52114 steps/s (collection: 1.798s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 11.5036
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.6122
                       Mean reward: 19.47
               Mean episode length: 122.34
    Episode_Reward/reaching_object: 0.2338
     Episode_Reward/lifting_object: 3.3848
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 1.89s
                      Time elapsed: 00:08:53
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 52568 steps/s (collection: 1.783s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 15.0977
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 35.6186
                       Mean reward: 18.45
               Mean episode length: 113.96
    Episode_Reward/reaching_object: 0.2382
     Episode_Reward/lifting_object: 3.2727
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 1.87s
                      Time elapsed: 00:08:54
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 52522 steps/s (collection: 1.781s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 15.3450
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6204
                       Mean reward: 16.41
               Mean episode length: 121.19
    Episode_Reward/reaching_object: 0.2314
     Episode_Reward/lifting_object: 3.2743
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 1.87s
                      Time elapsed: 00:08:56
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 52609 steps/s (collection: 1.767s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 12.7956
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.6245
                       Mean reward: 18.39
               Mean episode length: 115.86
    Episode_Reward/reaching_object: 0.2305
     Episode_Reward/lifting_object: 3.3667
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.87s
                      Time elapsed: 00:08:58
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 52451 steps/s (collection: 1.786s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 14.5600
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.6306
                       Mean reward: 12.86
               Mean episode length: 115.36
    Episode_Reward/reaching_object: 0.2352
     Episode_Reward/lifting_object: 3.1191
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 1.87s
                      Time elapsed: 00:09:00
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 51071 steps/s (collection: 1.838s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 15.8462
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.6368
                       Mean reward: 15.16
               Mean episode length: 124.97
    Episode_Reward/reaching_object: 0.2366
     Episode_Reward/lifting_object: 3.3911
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.92s
                      Time elapsed: 00:09:02
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 51960 steps/s (collection: 1.804s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 14.3806
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.6400
                       Mean reward: 16.82
               Mean episode length: 116.32
    Episode_Reward/reaching_object: 0.2374
     Episode_Reward/lifting_object: 3.5439
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.89s
                      Time elapsed: 00:09:04
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 51763 steps/s (collection: 1.792s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 13.7775
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.6464
                       Mean reward: 16.85
               Mean episode length: 117.82
    Episode_Reward/reaching_object: 0.2375
     Episode_Reward/lifting_object: 3.4178
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.90s
                      Time elapsed: 00:09:06
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 52519 steps/s (collection: 1.770s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 15.9722
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.6583
                       Mean reward: 21.64
               Mean episode length: 115.90
    Episode_Reward/reaching_object: 0.2356
     Episode_Reward/lifting_object: 3.7179
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 1.87s
                      Time elapsed: 00:09:08
                               ETA: 00:56:51

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 51355 steps/s (collection: 1.811s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 14.0223
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.6679
                       Mean reward: 18.78
               Mean episode length: 120.83
    Episode_Reward/reaching_object: 0.2298
     Episode_Reward/lifting_object: 3.5730
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.91s
                      Time elapsed: 00:09:10
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 51782 steps/s (collection: 1.787s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 26.2677
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.6772
                       Mean reward: 23.31
               Mean episode length: 113.50
    Episode_Reward/reaching_object: 0.2337
     Episode_Reward/lifting_object: 3.7034
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.90s
                      Time elapsed: 00:09:11
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 52007 steps/s (collection: 1.782s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 15.8397
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.6960
                       Mean reward: 22.10
               Mean episode length: 111.39
    Episode_Reward/reaching_object: 0.2274
     Episode_Reward/lifting_object: 3.3460
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 1.89s
                      Time elapsed: 00:09:13
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 53122 steps/s (collection: 1.757s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 17.8392
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.7034
                       Mean reward: 23.21
               Mean episode length: 116.34
    Episode_Reward/reaching_object: 0.2371
     Episode_Reward/lifting_object: 3.8839
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 1.85s
                      Time elapsed: 00:09:15
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 53038 steps/s (collection: 1.756s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 15.1178
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7060
                       Mean reward: 18.05
               Mean episode length: 114.53
    Episode_Reward/reaching_object: 0.2316
     Episode_Reward/lifting_object: 3.8164
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.85s
                      Time elapsed: 00:09:17
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 51967 steps/s (collection: 1.791s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 15.9993
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.7120
                       Mean reward: 22.02
               Mean episode length: 114.62
    Episode_Reward/reaching_object: 0.2330
     Episode_Reward/lifting_object: 3.8333
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 1.89s
                      Time elapsed: 00:09:19
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 52583 steps/s (collection: 1.758s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 15.9098
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7247
                       Mean reward: 16.76
               Mean episode length: 114.08
    Episode_Reward/reaching_object: 0.2320
     Episode_Reward/lifting_object: 4.0144
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.87s
                      Time elapsed: 00:09:21
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 53475 steps/s (collection: 1.748s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 17.7401
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7337
                       Mean reward: 20.85
               Mean episode length: 117.95
    Episode_Reward/reaching_object: 0.2268
     Episode_Reward/lifting_object: 3.8980
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.84s
                      Time elapsed: 00:09:23
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 53282 steps/s (collection: 1.756s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 15.5099
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.7415
                       Mean reward: 18.49
               Mean episode length: 113.75
    Episode_Reward/reaching_object: 0.2195
     Episode_Reward/lifting_object: 3.7807
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 1.84s
                      Time elapsed: 00:09:25
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 52277 steps/s (collection: 1.789s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 24.1878
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.7474
                       Mean reward: 19.26
               Mean episode length: 116.12
    Episode_Reward/reaching_object: 0.2307
     Episode_Reward/lifting_object: 4.0965
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 1.88s
                      Time elapsed: 00:09:26
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 52625 steps/s (collection: 1.783s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 24.9766
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7570
                       Mean reward: 20.24
               Mean episode length: 119.32
    Episode_Reward/reaching_object: 0.2345
     Episode_Reward/lifting_object: 4.0960
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.87s
                      Time elapsed: 00:09:28
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 51067 steps/s (collection: 1.832s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 19.0098
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.7736
                       Mean reward: 21.54
               Mean episode length: 116.01
    Episode_Reward/reaching_object: 0.2312
     Episode_Reward/lifting_object: 4.0285
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 1.92s
                      Time elapsed: 00:09:30
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 50845 steps/s (collection: 1.849s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 25.6629
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.7833
                       Mean reward: 22.08
               Mean episode length: 113.60
    Episode_Reward/reaching_object: 0.2275
     Episode_Reward/lifting_object: 4.1033
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.93s
                      Time elapsed: 00:09:32
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 52720 steps/s (collection: 1.769s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 18.5519
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.7888
                       Mean reward: 24.34
               Mean episode length: 118.58
    Episode_Reward/reaching_object: 0.2327
     Episode_Reward/lifting_object: 4.2018
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.86s
                      Time elapsed: 00:09:34
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 51148 steps/s (collection: 1.808s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 18.8478
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.7953
                       Mean reward: 26.26
               Mean episode length: 112.26
    Episode_Reward/reaching_object: 0.2328
     Episode_Reward/lifting_object: 4.3716
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.92s
                      Time elapsed: 00:09:36
                               ETA: 00:56:13

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 52133 steps/s (collection: 1.785s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 27.3211
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.7990
                       Mean reward: 23.58
               Mean episode length: 108.25
    Episode_Reward/reaching_object: 0.2336
     Episode_Reward/lifting_object: 4.3937
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.89s
                      Time elapsed: 00:09:38
                               ETA: 00:56:11

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 51396 steps/s (collection: 1.799s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 21.9334
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.8053
                       Mean reward: 23.00
               Mean episode length: 104.68
    Episode_Reward/reaching_object: 0.2253
     Episode_Reward/lifting_object: 4.1933
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 37.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.91s
                      Time elapsed: 00:09:40
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 51083 steps/s (collection: 1.815s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 23.5032
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 35.8136
                       Mean reward: 23.93
               Mean episode length: 111.95
    Episode_Reward/reaching_object: 0.2362
     Episode_Reward/lifting_object: 4.7554
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.92s
                      Time elapsed: 00:09:42
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 52509 steps/s (collection: 1.774s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 18.3300
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.8211
                       Mean reward: 24.94
               Mean episode length: 108.52
    Episode_Reward/reaching_object: 0.2349
     Episode_Reward/lifting_object: 4.4899
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.87s
                      Time elapsed: 00:09:44
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 52038 steps/s (collection: 1.797s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 16.5047
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.8241
                       Mean reward: 22.69
               Mean episode length: 113.90
    Episode_Reward/reaching_object: 0.2252
     Episode_Reward/lifting_object: 4.4327
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.89s
                      Time elapsed: 00:09:45
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 52961 steps/s (collection: 1.767s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 20.6029
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.8252
                       Mean reward: 25.03
               Mean episode length: 109.19
    Episode_Reward/reaching_object: 0.2278
     Episode_Reward/lifting_object: 4.4142
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 1.86s
                      Time elapsed: 00:09:47
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 52358 steps/s (collection: 1.786s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 23.5893
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.8263
                       Mean reward: 24.83
               Mean episode length: 105.52
    Episode_Reward/reaching_object: 0.2321
     Episode_Reward/lifting_object: 4.4295
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.88s
                      Time elapsed: 00:09:49
                               ETA: 00:55:56

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 51742 steps/s (collection: 1.809s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 22.2334
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.8323
                       Mean reward: 22.83
               Mean episode length: 112.92
    Episode_Reward/reaching_object: 0.2438
     Episode_Reward/lifting_object: 4.8557
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 1.90s
                      Time elapsed: 00:09:51
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 52371 steps/s (collection: 1.785s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 19.2349
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.8428
                       Mean reward: 27.41
               Mean episode length: 114.06
    Episode_Reward/reaching_object: 0.2407
     Episode_Reward/lifting_object: 4.6843
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.88s
                      Time elapsed: 00:09:53
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 52085 steps/s (collection: 1.802s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 33.7454
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.8480
                       Mean reward: 19.45
               Mean episode length: 117.97
    Episode_Reward/reaching_object: 0.2507
     Episode_Reward/lifting_object: 4.6835
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 1.89s
                      Time elapsed: 00:09:55
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 51522 steps/s (collection: 1.819s, learning 0.089s)
             Mean action noise std: 1.79
          Mean value_function loss: 30.2940
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.8534
                       Mean reward: 23.59
               Mean episode length: 109.39
    Episode_Reward/reaching_object: 0.2464
     Episode_Reward/lifting_object: 4.7744
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.91s
                      Time elapsed: 00:09:57
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 52427 steps/s (collection: 1.788s, learning 0.087s)
             Mean action noise std: 1.79
          Mean value_function loss: 34.0003
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.8597
                       Mean reward: 21.71
               Mean episode length: 109.65
    Episode_Reward/reaching_object: 0.2435
     Episode_Reward/lifting_object: 4.8228
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 1.88s
                      Time elapsed: 00:09:59
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 52502 steps/s (collection: 1.774s, learning 0.098s)
             Mean action noise std: 1.79
          Mean value_function loss: 25.5936
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.8631
                       Mean reward: 26.43
               Mean episode length: 108.23
    Episode_Reward/reaching_object: 0.2447
     Episode_Reward/lifting_object: 4.8418
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 1.87s
                      Time elapsed: 00:10:00
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 51572 steps/s (collection: 1.815s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 28.0739
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.8707
                       Mean reward: 27.37
               Mean episode length: 110.39
    Episode_Reward/reaching_object: 0.2480
     Episode_Reward/lifting_object: 5.1080
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 1.91s
                      Time elapsed: 00:10:02
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 52085 steps/s (collection: 1.792s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 29.9585
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8811
                       Mean reward: 22.54
               Mean episode length: 109.47
    Episode_Reward/reaching_object: 0.2418
     Episode_Reward/lifting_object: 4.8043
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 1.89s
                      Time elapsed: 00:10:04
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 52157 steps/s (collection: 1.776s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 21.7856
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.8893
                       Mean reward: 30.02
               Mean episode length: 103.70
    Episode_Reward/reaching_object: 0.2348
     Episode_Reward/lifting_object: 5.2882
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 1.88s
                      Time elapsed: 00:10:06
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 51366 steps/s (collection: 1.821s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 23.6751
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8921
                       Mean reward: 24.78
               Mean episode length: 107.46
    Episode_Reward/reaching_object: 0.2345
     Episode_Reward/lifting_object: 4.9674
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 1.91s
                      Time elapsed: 00:10:08
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 51121 steps/s (collection: 1.818s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 19.0698
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.8959
                       Mean reward: 25.83
               Mean episode length: 110.19
    Episode_Reward/reaching_object: 0.2346
     Episode_Reward/lifting_object: 4.8240
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 1.92s
                      Time elapsed: 00:10:10
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 51717 steps/s (collection: 1.809s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 21.8863
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.8984
                       Mean reward: 28.34
               Mean episode length: 103.62
    Episode_Reward/reaching_object: 0.2287
     Episode_Reward/lifting_object: 4.8026
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.90s
                      Time elapsed: 00:10:12
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 51675 steps/s (collection: 1.813s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 24.5073
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.9020
                       Mean reward: 24.78
               Mean episode length: 103.68
    Episode_Reward/reaching_object: 0.2273
     Episode_Reward/lifting_object: 4.9905
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 1.90s
                      Time elapsed: 00:10:14
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 50079 steps/s (collection: 1.859s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 27.9402
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.9091
                       Mean reward: 27.33
               Mean episode length: 104.97
    Episode_Reward/reaching_object: 0.2221
     Episode_Reward/lifting_object: 4.7619
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 1.96s
                      Time elapsed: 00:10:16
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 50989 steps/s (collection: 1.833s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 36.9610
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.9151
                       Mean reward: 22.72
               Mean episode length: 96.77
    Episode_Reward/reaching_object: 0.2192
     Episode_Reward/lifting_object: 4.8948
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 1.93s
                      Time elapsed: 00:10:18
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 51378 steps/s (collection: 1.822s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 34.6880
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.9216
                       Mean reward: 25.23
               Mean episode length: 102.21
    Episode_Reward/reaching_object: 0.2276
     Episode_Reward/lifting_object: 4.8944
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 1.91s
                      Time elapsed: 00:10:20
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 51706 steps/s (collection: 1.800s, learning 0.102s)
             Mean action noise std: 1.79
          Mean value_function loss: 22.9909
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.9289
                       Mean reward: 25.67
               Mean episode length: 106.53
    Episode_Reward/reaching_object: 0.2272
     Episode_Reward/lifting_object: 4.9821
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 1.90s
                      Time elapsed: 00:10:21
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 51541 steps/s (collection: 1.793s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 28.5873
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.9322
                       Mean reward: 23.58
               Mean episode length: 101.68
    Episode_Reward/reaching_object: 0.2212
     Episode_Reward/lifting_object: 5.1347
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 1.91s
                      Time elapsed: 00:10:23
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 50560 steps/s (collection: 1.840s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 40.0858
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.9382
                       Mean reward: 30.73
               Mean episode length: 102.89
    Episode_Reward/reaching_object: 0.2217
     Episode_Reward/lifting_object: 5.1588
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 1.94s
                      Time elapsed: 00:10:25
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 51409 steps/s (collection: 1.821s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 35.6578
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.9464
                       Mean reward: 25.94
               Mean episode length: 97.01
    Episode_Reward/reaching_object: 0.2220
     Episode_Reward/lifting_object: 5.0800
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 1.91s
                      Time elapsed: 00:10:27
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 51283 steps/s (collection: 1.825s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 24.8317
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.9561
                       Mean reward: 24.54
               Mean episode length: 104.77
    Episode_Reward/reaching_object: 0.2235
     Episode_Reward/lifting_object: 5.0741
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 1.92s
                      Time elapsed: 00:10:29
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 51246 steps/s (collection: 1.825s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 30.8916
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.9632
                       Mean reward: 29.03
               Mean episode length: 104.85
    Episode_Reward/reaching_object: 0.2355
     Episode_Reward/lifting_object: 5.3953
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 1.92s
                      Time elapsed: 00:10:31
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 50990 steps/s (collection: 1.829s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 40.7247
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.9708
                       Mean reward: 27.11
               Mean episode length: 98.36
    Episode_Reward/reaching_object: 0.2306
     Episode_Reward/lifting_object: 5.2449
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 1.93s
                      Time elapsed: 00:10:33
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 48885 steps/s (collection: 1.917s, learning 0.094s)
             Mean action noise std: 1.80
          Mean value_function loss: 28.4335
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.9781
                       Mean reward: 28.39
               Mean episode length: 102.75
    Episode_Reward/reaching_object: 0.2318
     Episode_Reward/lifting_object: 5.5517
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 37.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.01s
                      Time elapsed: 00:10:35
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 50220 steps/s (collection: 1.852s, learning 0.106s)
             Mean action noise std: 1.80
          Mean value_function loss: 34.9831
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.9836
                       Mean reward: 30.11
               Mean episode length: 103.95
    Episode_Reward/reaching_object: 0.2344
     Episode_Reward/lifting_object: 5.5811
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 1.96s
                      Time elapsed: 00:10:37
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 50869 steps/s (collection: 1.825s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 33.3504
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.9889
                       Mean reward: 27.27
               Mean episode length: 116.49
    Episode_Reward/reaching_object: 0.2374
     Episode_Reward/lifting_object: 5.6074
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 1.93s
                      Time elapsed: 00:10:39
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 50207 steps/s (collection: 1.859s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 47.6970
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.9908
                       Mean reward: 26.52
               Mean episode length: 102.64
    Episode_Reward/reaching_object: 0.2308
     Episode_Reward/lifting_object: 5.1284
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 41.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 1.96s
                      Time elapsed: 00:10:41
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 50821 steps/s (collection: 1.842s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 28.4487
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.9923
                       Mean reward: 25.38
               Mean episode length: 105.16
    Episode_Reward/reaching_object: 0.2349
     Episode_Reward/lifting_object: 5.3610
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 1.93s
                      Time elapsed: 00:10:43
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 50117 steps/s (collection: 1.870s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 40.5137
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.9966
                       Mean reward: 33.65
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.2323
     Episode_Reward/lifting_object: 5.6579
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 1.96s
                      Time elapsed: 00:10:45
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 50599 steps/s (collection: 1.846s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 32.8979
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.0030
                       Mean reward: 30.57
               Mean episode length: 93.85
    Episode_Reward/reaching_object: 0.2276
     Episode_Reward/lifting_object: 5.4938
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 1.94s
                      Time elapsed: 00:10:47
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 51065 steps/s (collection: 1.814s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 37.0469
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.0112
                       Mean reward: 29.84
               Mean episode length: 104.90
    Episode_Reward/reaching_object: 0.2362
     Episode_Reward/lifting_object: 5.8334
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 42.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 1.93s
                      Time elapsed: 00:10:49
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 51491 steps/s (collection: 1.814s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 33.8315
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.0187
                       Mean reward: 27.88
               Mean episode length: 102.34
    Episode_Reward/reaching_object: 0.2348
     Episode_Reward/lifting_object: 6.1580
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 1.91s
                      Time elapsed: 00:10:51
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 50661 steps/s (collection: 1.841s, learning 0.100s)
             Mean action noise std: 1.80
          Mean value_function loss: 28.7578
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.0278
                       Mean reward: 31.52
               Mean episode length: 99.81
    Episode_Reward/reaching_object: 0.2383
     Episode_Reward/lifting_object: 5.9415
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.1250
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 1.94s
                      Time elapsed: 00:10:52
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 50975 steps/s (collection: 1.837s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 35.2614
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.0300
                       Mean reward: 33.47
               Mean episode length: 104.43
    Episode_Reward/reaching_object: 0.2335
     Episode_Reward/lifting_object: 5.6134
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 1.93s
                      Time elapsed: 00:10:54
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 19554 steps/s (collection: 4.893s, learning 0.135s)
             Mean action noise std: 1.80
          Mean value_function loss: 37.6928
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 36.0331
                       Mean reward: 28.78
               Mean episode length: 107.24
    Episode_Reward/reaching_object: 0.2431
     Episode_Reward/lifting_object: 5.7264
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.03s
                      Time elapsed: 00:10:59
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14330 steps/s (collection: 6.743s, learning 0.117s)
             Mean action noise std: 1.81
          Mean value_function loss: 32.1533
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.0383
                       Mean reward: 32.77
               Mean episode length: 106.80
    Episode_Reward/reaching_object: 0.2398
     Episode_Reward/lifting_object: 5.8081
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.86s
                      Time elapsed: 00:11:06
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14883 steps/s (collection: 6.484s, learning 0.122s)
             Mean action noise std: 1.81
          Mean value_function loss: 48.5338
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.0463
                       Mean reward: 28.85
               Mean episode length: 95.48
    Episode_Reward/reaching_object: 0.2400
     Episode_Reward/lifting_object: 5.8585
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.61s
                      Time elapsed: 00:11:13
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14871 steps/s (collection: 6.486s, learning 0.124s)
             Mean action noise std: 1.81
          Mean value_function loss: 31.9311
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.0492
                       Mean reward: 31.81
               Mean episode length: 107.70
    Episode_Reward/reaching_object: 0.2412
     Episode_Reward/lifting_object: 5.8608
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.61s
                      Time elapsed: 00:11:20
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14797 steps/s (collection: 6.527s, learning 0.116s)
             Mean action noise std: 1.81
          Mean value_function loss: 45.2395
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.0526
                       Mean reward: 30.56
               Mean episode length: 102.06
    Episode_Reward/reaching_object: 0.2485
     Episode_Reward/lifting_object: 6.1497
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.64s
                      Time elapsed: 00:11:26
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 15479 steps/s (collection: 6.224s, learning 0.127s)
             Mean action noise std: 1.81
          Mean value_function loss: 43.7874
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.0592
                       Mean reward: 35.31
               Mean episode length: 102.62
    Episode_Reward/reaching_object: 0.2512
     Episode_Reward/lifting_object: 6.1708
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.35s
                      Time elapsed: 00:11:33
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14626 steps/s (collection: 6.605s, learning 0.116s)
             Mean action noise std: 1.81
          Mean value_function loss: 44.2458
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.0630
                       Mean reward: 26.05
               Mean episode length: 100.52
    Episode_Reward/reaching_object: 0.2452
     Episode_Reward/lifting_object: 5.9022
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.72s
                      Time elapsed: 00:11:39
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 15086 steps/s (collection: 6.399s, learning 0.117s)
             Mean action noise std: 1.81
          Mean value_function loss: 46.3375
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.0676
                       Mean reward: 34.48
               Mean episode length: 105.52
    Episode_Reward/reaching_object: 0.2522
     Episode_Reward/lifting_object: 6.1859
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.52s
                      Time elapsed: 00:11:46
                               ETA: 00:57:18

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13876 steps/s (collection: 6.963s, learning 0.121s)
             Mean action noise std: 1.81
          Mean value_function loss: 48.4948
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.0737
                       Mean reward: 27.34
               Mean episode length: 101.59
    Episode_Reward/reaching_object: 0.2408
     Episode_Reward/lifting_object: 6.3339
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.08s
                      Time elapsed: 00:11:53
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 50582 steps/s (collection: 1.831s, learning 0.113s)
             Mean action noise std: 1.81
          Mean value_function loss: 49.2655
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.0786
                       Mean reward: 33.20
               Mean episode length: 104.69
    Episode_Reward/reaching_object: 0.2431
     Episode_Reward/lifting_object: 6.1869
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 1.94s
                      Time elapsed: 00:11:55
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 51567 steps/s (collection: 1.798s, learning 0.108s)
             Mean action noise std: 1.81
          Mean value_function loss: 44.9163
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.0843
                       Mean reward: 34.26
               Mean episode length: 97.16
    Episode_Reward/reaching_object: 0.2399
     Episode_Reward/lifting_object: 5.7769
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.91s
                      Time elapsed: 00:11:57
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 52620 steps/s (collection: 1.773s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 49.6059
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.0926
                       Mean reward: 30.52
               Mean episode length: 98.04
    Episode_Reward/reaching_object: 0.2340
     Episode_Reward/lifting_object: 5.9137
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.87s
                      Time elapsed: 00:11:59
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 52596 steps/s (collection: 1.774s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 33.8908
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.0968
                       Mean reward: 25.78
               Mean episode length: 101.83
    Episode_Reward/reaching_object: 0.2401
     Episode_Reward/lifting_object: 6.0650
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 43.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 1.87s
                      Time elapsed: 00:12:00
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 52949 steps/s (collection: 1.767s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 29.2530
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.0977
                       Mean reward: 33.14
               Mean episode length: 94.43
    Episode_Reward/reaching_object: 0.2389
     Episode_Reward/lifting_object: 6.4881
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 1.86s
                      Time elapsed: 00:12:02
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 52519 steps/s (collection: 1.779s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 36.9393
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.0998
                       Mean reward: 30.18
               Mean episode length: 96.94
    Episode_Reward/reaching_object: 0.2317
     Episode_Reward/lifting_object: 5.9491
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 1.87s
                      Time elapsed: 00:12:04
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 52059 steps/s (collection: 1.780s, learning 0.109s)
             Mean action noise std: 1.81
          Mean value_function loss: 40.5126
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.1062
                       Mean reward: 32.65
               Mean episode length: 100.00
    Episode_Reward/reaching_object: 0.2348
     Episode_Reward/lifting_object: 6.1665
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.89s
                      Time elapsed: 00:12:06
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 51986 steps/s (collection: 1.797s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 32.0595
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.1123
                       Mean reward: 30.54
               Mean episode length: 98.11
    Episode_Reward/reaching_object: 0.2336
     Episode_Reward/lifting_object: 5.7629
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.89s
                      Time elapsed: 00:12:08
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 52625 steps/s (collection: 1.778s, learning 0.090s)
             Mean action noise std: 1.81
          Mean value_function loss: 48.0726
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.1188
                       Mean reward: 35.38
               Mean episode length: 94.81
    Episode_Reward/reaching_object: 0.2319
     Episode_Reward/lifting_object: 5.9251
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 41.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 1.87s
                      Time elapsed: 00:12:10
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 51940 steps/s (collection: 1.808s, learning 0.085s)
             Mean action noise std: 1.81
          Mean value_function loss: 39.7173
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.1261
                       Mean reward: 31.69
               Mean episode length: 91.76
    Episode_Reward/reaching_object: 0.2344
     Episode_Reward/lifting_object: 6.1174
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.89s
                      Time elapsed: 00:12:12
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 53004 steps/s (collection: 1.766s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 43.0799
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.1347
                       Mean reward: 36.89
               Mean episode length: 100.07
    Episode_Reward/reaching_object: 0.2392
     Episode_Reward/lifting_object: 6.2857
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 40.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.85s
                      Time elapsed: 00:12:14
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 52988 steps/s (collection: 1.764s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 36.3888
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.1419
                       Mean reward: 34.00
               Mean episode length: 95.35
    Episode_Reward/reaching_object: 0.2354
     Episode_Reward/lifting_object: 6.2854
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 1.86s
                      Time elapsed: 00:12:15
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 52500 steps/s (collection: 1.785s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 51.5746
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.1450
                       Mean reward: 27.78
               Mean episode length: 101.96
    Episode_Reward/reaching_object: 0.2427
     Episode_Reward/lifting_object: 6.2655
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.87s
                      Time elapsed: 00:12:17
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 52467 steps/s (collection: 1.782s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 54.1296
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.1456
                       Mean reward: 33.26
               Mean episode length: 93.84
    Episode_Reward/reaching_object: 0.2388
     Episode_Reward/lifting_object: 6.3311
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 1.87s
                      Time elapsed: 00:12:19
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 52103 steps/s (collection: 1.782s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 47.4154
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 36.1462
                       Mean reward: 28.81
               Mean episode length: 98.43
    Episode_Reward/reaching_object: 0.2385
     Episode_Reward/lifting_object: 5.9854
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 40.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.89s
                      Time elapsed: 00:12:21
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 52253 steps/s (collection: 1.774s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 55.4595
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.1468
                       Mean reward: 37.76
               Mean episode length: 96.01
    Episode_Reward/reaching_object: 0.2392
     Episode_Reward/lifting_object: 6.1752
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 42.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.88s
                      Time elapsed: 00:12:23
                               ETA: 00:56:51

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 51945 steps/s (collection: 1.781s, learning 0.111s)
             Mean action noise std: 1.82
          Mean value_function loss: 47.3015
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.1482
                       Mean reward: 29.63
               Mean episode length: 98.36
    Episode_Reward/reaching_object: 0.2391
     Episode_Reward/lifting_object: 5.9879
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 41.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.89s
                      Time elapsed: 00:12:25
                               ETA: 00:56:48

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 53433 steps/s (collection: 1.753s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 47.0768
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 36.1489
                       Mean reward: 38.29
               Mean episode length: 98.52
    Episode_Reward/reaching_object: 0.2396
     Episode_Reward/lifting_object: 6.6712
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.84s
                      Time elapsed: 00:12:27
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 53442 steps/s (collection: 1.746s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 32.5085
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.1502
                       Mean reward: 32.02
               Mean episode length: 99.36
    Episode_Reward/reaching_object: 0.2416
     Episode_Reward/lifting_object: 6.4211
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.84s
                      Time elapsed: 00:12:28
                               ETA: 00:56:42

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 53127 steps/s (collection: 1.758s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 38.1314
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.1535
                       Mean reward: 31.40
               Mean episode length: 95.89
    Episode_Reward/reaching_object: 0.2406
     Episode_Reward/lifting_object: 6.7101
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.85s
                      Time elapsed: 00:12:30
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 53802 steps/s (collection: 1.739s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 53.2576
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.1579
                       Mean reward: 35.61
               Mean episode length: 99.13
    Episode_Reward/reaching_object: 0.2377
     Episode_Reward/lifting_object: 6.7539
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.83s
                      Time elapsed: 00:12:32
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 51871 steps/s (collection: 1.798s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 39.9533
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.1622
                       Mean reward: 35.14
               Mean episode length: 94.50
    Episode_Reward/reaching_object: 0.2374
     Episode_Reward/lifting_object: 6.6893
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 41.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.90s
                      Time elapsed: 00:12:34
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 52572 steps/s (collection: 1.768s, learning 0.102s)
             Mean action noise std: 1.82
          Mean value_function loss: 60.0436
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.1657
                       Mean reward: 36.05
               Mean episode length: 95.78
    Episode_Reward/reaching_object: 0.2415
     Episode_Reward/lifting_object: 6.8878
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.87s
                      Time elapsed: 00:12:36
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 54032 steps/s (collection: 1.721s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 50.7177
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.1714
                       Mean reward: 29.54
               Mean episode length: 92.80
    Episode_Reward/reaching_object: 0.2433
     Episode_Reward/lifting_object: 6.6670
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.82s
                      Time elapsed: 00:12:38
                               ETA: 00:56:27

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 53590 steps/s (collection: 1.741s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 49.3222
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.1786
                       Mean reward: 32.59
               Mean episode length: 97.05
    Episode_Reward/reaching_object: 0.2429
     Episode_Reward/lifting_object: 6.5321
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.83s
                      Time elapsed: 00:12:40
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 52837 steps/s (collection: 1.763s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 66.3615
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.1848
                       Mean reward: 38.87
               Mean episode length: 104.69
    Episode_Reward/reaching_object: 0.2519
     Episode_Reward/lifting_object: 6.5423
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 43.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.86s
                      Time elapsed: 00:12:41
                               ETA: 00:56:21

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 53699 steps/s (collection: 1.737s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 64.3488
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.1918
                       Mean reward: 35.29
               Mean episode length: 94.78
    Episode_Reward/reaching_object: 0.2410
     Episode_Reward/lifting_object: 6.6183
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 42.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.83s
                      Time elapsed: 00:12:43
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 51839 steps/s (collection: 1.780s, learning 0.117s)
             Mean action noise std: 1.82
          Mean value_function loss: 46.6949
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.2024
                       Mean reward: 34.88
               Mean episode length: 92.97
    Episode_Reward/reaching_object: 0.2422
     Episode_Reward/lifting_object: 6.5106
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.90s
                      Time elapsed: 00:12:45
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 52586 steps/s (collection: 1.776s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 50.6437
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.2096
                       Mean reward: 31.25
               Mean episode length: 101.66
    Episode_Reward/reaching_object: 0.2347
     Episode_Reward/lifting_object: 6.2726
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 42.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.87s
                      Time elapsed: 00:12:47
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 52334 steps/s (collection: 1.779s, learning 0.099s)
             Mean action noise std: 1.82
          Mean value_function loss: 53.7300
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.2130
                       Mean reward: 30.79
               Mean episode length: 91.55
    Episode_Reward/reaching_object: 0.2412
     Episode_Reward/lifting_object: 6.5915
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 44.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.88s
                      Time elapsed: 00:12:49
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 52389 steps/s (collection: 1.785s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 47.0031
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.2164
                       Mean reward: 38.96
               Mean episode length: 99.57
    Episode_Reward/reaching_object: 0.2466
     Episode_Reward/lifting_object: 6.5139
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 42.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.88s
                      Time elapsed: 00:12:51
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 51345 steps/s (collection: 1.818s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 58.3788
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.2179
                       Mean reward: 30.93
               Mean episode length: 89.64
    Episode_Reward/reaching_object: 0.2374
     Episode_Reward/lifting_object: 6.2891
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 1.91s
                      Time elapsed: 00:12:53
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 52614 steps/s (collection: 1.767s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 50.8136
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.2230
                       Mean reward: 29.35
               Mean episode length: 92.38
    Episode_Reward/reaching_object: 0.2299
     Episode_Reward/lifting_object: 6.2699
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.87s
                      Time elapsed: 00:12:55
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 52530 steps/s (collection: 1.774s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 39.4409
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.2297
                       Mean reward: 31.69
               Mean episode length: 88.70
    Episode_Reward/reaching_object: 0.2403
     Episode_Reward/lifting_object: 6.7610
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.87s
                      Time elapsed: 00:12:56
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 51906 steps/s (collection: 1.783s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 51.2249
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.2345
                       Mean reward: 35.64
               Mean episode length: 93.46
    Episode_Reward/reaching_object: 0.2431
     Episode_Reward/lifting_object: 6.4733
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.89s
                      Time elapsed: 00:12:58
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 50910 steps/s (collection: 1.830s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 62.1150
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.2436
                       Mean reward: 29.30
               Mean episode length: 94.67
    Episode_Reward/reaching_object: 0.2386
     Episode_Reward/lifting_object: 6.4438
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 42.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 1.93s
                      Time elapsed: 00:13:00
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 52347 steps/s (collection: 1.783s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 76.7187
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.2501
                       Mean reward: 32.04
               Mean episode length: 92.26
    Episode_Reward/reaching_object: 0.2409
     Episode_Reward/lifting_object: 6.9170
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.88s
                      Time elapsed: 00:13:02
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 51161 steps/s (collection: 1.813s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 61.7822
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.2557
                       Mean reward: 28.78
               Mean episode length: 96.28
    Episode_Reward/reaching_object: 0.2386
     Episode_Reward/lifting_object: 6.5057
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.92s
                      Time elapsed: 00:13:04
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 51096 steps/s (collection: 1.817s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 54.3398
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.2613
                       Mean reward: 36.12
               Mean episode length: 96.58
    Episode_Reward/reaching_object: 0.2384
     Episode_Reward/lifting_object: 6.9694
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 43.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 1.92s
                      Time elapsed: 00:13:06
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 50649 steps/s (collection: 1.843s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 48.0130
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.2667
                       Mean reward: 38.42
               Mean episode length: 97.90
    Episode_Reward/reaching_object: 0.2418
     Episode_Reward/lifting_object: 7.1036
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.94s
                      Time elapsed: 00:13:08
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 51444 steps/s (collection: 1.824s, learning 0.087s)
             Mean action noise std: 1.83
          Mean value_function loss: 56.4919
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 36.2713
                       Mean reward: 39.73
               Mean episode length: 91.40
    Episode_Reward/reaching_object: 0.2340
     Episode_Reward/lifting_object: 7.0352
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 44.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 1.91s
                      Time elapsed: 00:13:10
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 51852 steps/s (collection: 1.805s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 36.0647
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.2733
                       Mean reward: 29.82
               Mean episode length: 95.19
    Episode_Reward/reaching_object: 0.2416
     Episode_Reward/lifting_object: 6.6860
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 42.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 1.90s
                      Time elapsed: 00:13:12
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 52651 steps/s (collection: 1.768s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 37.2718
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 36.2767
                       Mean reward: 36.53
               Mean episode length: 91.98
    Episode_Reward/reaching_object: 0.2412
     Episode_Reward/lifting_object: 7.1737
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 1.87s
                      Time elapsed: 00:13:14
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 52733 steps/s (collection: 1.775s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 64.0573
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.2787
                       Mean reward: 41.35
               Mean episode length: 92.16
    Episode_Reward/reaching_object: 0.2409
     Episode_Reward/lifting_object: 7.4995
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 44.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 1.86s
                      Time elapsed: 00:13:15
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 52661 steps/s (collection: 1.769s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 47.7505
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.2792
                       Mean reward: 33.03
               Mean episode length: 89.53
    Episode_Reward/reaching_object: 0.2448
     Episode_Reward/lifting_object: 7.1765
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 41.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 1.87s
                      Time elapsed: 00:13:17
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 48389 steps/s (collection: 1.911s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 45.1153
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.2798
                       Mean reward: 37.52
               Mean episode length: 92.67
    Episode_Reward/reaching_object: 0.2445
     Episode_Reward/lifting_object: 7.4461
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 42.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.03s
                      Time elapsed: 00:13:19
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 46312 steps/s (collection: 2.004s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 51.0777
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.2799
                       Mean reward: 37.63
               Mean episode length: 90.82
    Episode_Reward/reaching_object: 0.2462
     Episode_Reward/lifting_object: 7.4774
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.12s
                      Time elapsed: 00:13:21
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 49016 steps/s (collection: 1.873s, learning 0.133s)
             Mean action noise std: 1.83
          Mean value_function loss: 45.4587
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.2801
                       Mean reward: 34.02
               Mean episode length: 91.73
    Episode_Reward/reaching_object: 0.2473
     Episode_Reward/lifting_object: 7.5066
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.01s
                      Time elapsed: 00:13:23
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 44425 steps/s (collection: 2.105s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 42.5197
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 36.2801
                       Mean reward: 35.78
               Mean episode length: 94.12
    Episode_Reward/reaching_object: 0.2512
     Episode_Reward/lifting_object: 7.8047
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 43.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.21s
                      Time elapsed: 00:13:26
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 51115 steps/s (collection: 1.837s, learning 0.086s)
             Mean action noise std: 1.83
          Mean value_function loss: 53.4809
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 36.2802
                       Mean reward: 31.98
               Mean episode length: 88.82
    Episode_Reward/reaching_object: 0.2488
     Episode_Reward/lifting_object: 7.5709
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 42.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 1.92s
                      Time elapsed: 00:13:28
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 51437 steps/s (collection: 1.817s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 58.1541
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.2806
                       Mean reward: 37.73
               Mean episode length: 102.59
    Episode_Reward/reaching_object: 0.2472
     Episode_Reward/lifting_object: 7.6581
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 1.91s
                      Time elapsed: 00:13:30
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 50678 steps/s (collection: 1.851s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 56.8703
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.2828
                       Mean reward: 36.30
               Mean episode length: 91.90
    Episode_Reward/reaching_object: 0.2458
     Episode_Reward/lifting_object: 7.4347
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 1.94s
                      Time elapsed: 00:13:31
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 48016 steps/s (collection: 1.892s, learning 0.155s)
             Mean action noise std: 1.83
          Mean value_function loss: 44.8983
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.2885
                       Mean reward: 44.50
               Mean episode length: 96.49
    Episode_Reward/reaching_object: 0.2476
     Episode_Reward/lifting_object: 7.3930
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 41.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.05s
                      Time elapsed: 00:13:34
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 45798 steps/s (collection: 2.023s, learning 0.124s)
             Mean action noise std: 1.83
          Mean value_function loss: 59.4212
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 36.2956
                       Mean reward: 39.93
               Mean episode length: 93.46
    Episode_Reward/reaching_object: 0.2500
     Episode_Reward/lifting_object: 7.6511
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 42.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.15s
                      Time elapsed: 00:13:36
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 51189 steps/s (collection: 1.820s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 56.6180
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 36.2974
                       Mean reward: 35.18
               Mean episode length: 87.53
    Episode_Reward/reaching_object: 0.2526
     Episode_Reward/lifting_object: 7.5516
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 41.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 1.92s
                      Time elapsed: 00:13:38
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 51277 steps/s (collection: 1.824s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 51.3852
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.2978
                       Mean reward: 35.90
               Mean episode length: 94.51
    Episode_Reward/reaching_object: 0.2561
     Episode_Reward/lifting_object: 7.7474
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 40.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 1.92s
                      Time elapsed: 00:13:40
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 49928 steps/s (collection: 1.850s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 54.3595
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 36.2992
                       Mean reward: 39.59
               Mean episode length: 92.25
    Episode_Reward/reaching_object: 0.2550
     Episode_Reward/lifting_object: 7.9477
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 43.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 1.97s
                      Time elapsed: 00:13:41
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 50252 steps/s (collection: 1.825s, learning 0.132s)
             Mean action noise std: 1.83
          Mean value_function loss: 45.5337
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 36.3001
                       Mean reward: 44.41
               Mean episode length: 96.30
    Episode_Reward/reaching_object: 0.2528
     Episode_Reward/lifting_object: 8.0849
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 1.96s
                      Time elapsed: 00:13:43
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 49235 steps/s (collection: 1.893s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 52.2797
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 36.3002
                       Mean reward: 41.71
               Mean episode length: 100.02
    Episode_Reward/reaching_object: 0.2619
     Episode_Reward/lifting_object: 8.1670
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.4167
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.00s
                      Time elapsed: 00:13:45
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 50886 steps/s (collection: 1.835s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 54.5176
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.3004
                       Mean reward: 39.63
               Mean episode length: 98.22
    Episode_Reward/reaching_object: 0.2585
     Episode_Reward/lifting_object: 8.2549
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 1.93s
                      Time elapsed: 00:13:47
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 50377 steps/s (collection: 1.849s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 49.0205
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.3006
                       Mean reward: 44.97
               Mean episode length: 98.09
    Episode_Reward/reaching_object: 0.2634
     Episode_Reward/lifting_object: 8.3311
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.95s
                      Time elapsed: 00:13:49
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 49859 steps/s (collection: 1.850s, learning 0.122s)
             Mean action noise std: 1.83
          Mean value_function loss: 65.4267
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.3017
                       Mean reward: 38.49
               Mean episode length: 99.44
    Episode_Reward/reaching_object: 0.2676
     Episode_Reward/lifting_object: 7.8935
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 40.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.97s
                      Time elapsed: 00:13:51
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 49888 steps/s (collection: 1.851s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 56.3770
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.3031
                       Mean reward: 37.68
               Mean episode length: 90.18
    Episode_Reward/reaching_object: 0.2586
     Episode_Reward/lifting_object: 7.8951
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 40.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 1.97s
                      Time elapsed: 00:13:53
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 49501 steps/s (collection: 1.839s, learning 0.147s)
             Mean action noise std: 1.83
          Mean value_function loss: 49.1868
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 36.3049
                       Mean reward: 39.32
               Mean episode length: 92.33
    Episode_Reward/reaching_object: 0.2691
     Episode_Reward/lifting_object: 8.4924
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 1.99s
                      Time elapsed: 00:13:55
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 50598 steps/s (collection: 1.840s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 61.1666
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.3062
                       Mean reward: 43.86
               Mean episode length: 92.21
    Episode_Reward/reaching_object: 0.2669
     Episode_Reward/lifting_object: 8.4068
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 1.94s
                      Time elapsed: 00:13:57
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 52155 steps/s (collection: 1.791s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 55.3365
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.3107
                       Mean reward: 40.42
               Mean episode length: 104.12
    Episode_Reward/reaching_object: 0.2790
     Episode_Reward/lifting_object: 8.8911
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 1.88s
                      Time elapsed: 00:13:59
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 50888 steps/s (collection: 1.821s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 60.3642
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.3163
                       Mean reward: 42.96
               Mean episode length: 102.16
    Episode_Reward/reaching_object: 0.2767
     Episode_Reward/lifting_object: 8.3875
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 1.93s
                      Time elapsed: 00:14:01
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 50770 steps/s (collection: 1.837s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 67.7692
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.3219
                       Mean reward: 43.07
               Mean episode length: 97.86
    Episode_Reward/reaching_object: 0.2724
     Episode_Reward/lifting_object: 8.7337
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 1.94s
                      Time elapsed: 00:14:03
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 51573 steps/s (collection: 1.820s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 53.9598
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.3287
                       Mean reward: 44.27
               Mean episode length: 96.07
    Episode_Reward/reaching_object: 0.2758
     Episode_Reward/lifting_object: 8.5657
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 1.91s
                      Time elapsed: 00:14:05
                               ETA: 00:54:30

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 49045 steps/s (collection: 1.908s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 70.8876
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.3324
                       Mean reward: 43.05
               Mean episode length: 101.41
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 8.6381
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.00s
                      Time elapsed: 00:14:07
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 51818 steps/s (collection: 1.803s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 60.8121
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.3360
                       Mean reward: 40.25
               Mean episode length: 94.63
    Episode_Reward/reaching_object: 0.2795
     Episode_Reward/lifting_object: 8.9284
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 1.90s
                      Time elapsed: 00:14:09
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 51383 steps/s (collection: 1.825s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 61.8051
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.3393
                       Mean reward: 37.33
               Mean episode length: 95.47
    Episode_Reward/reaching_object: 0.2747
     Episode_Reward/lifting_object: 8.4775
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 1.91s
                      Time elapsed: 00:14:11
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 51909 steps/s (collection: 1.801s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 64.4643
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.3403
                       Mean reward: 46.54
               Mean episode length: 107.43
    Episode_Reward/reaching_object: 0.2830
     Episode_Reward/lifting_object: 8.7195
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 1.89s
                      Time elapsed: 00:14:13
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 50381 steps/s (collection: 1.835s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 58.3107
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 36.3413
                       Mean reward: 49.29
               Mean episode length: 104.51
    Episode_Reward/reaching_object: 0.2850
     Episode_Reward/lifting_object: 8.5532
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 1.95s
                      Time elapsed: 00:14:15
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 50764 steps/s (collection: 1.843s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 48.4498
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 36.3420
                       Mean reward: 42.11
               Mean episode length: 95.62
    Episode_Reward/reaching_object: 0.2850
     Episode_Reward/lifting_object: 8.5517
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 38.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 1.94s
                      Time elapsed: 00:14:16
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 49725 steps/s (collection: 1.858s, learning 0.119s)
             Mean action noise std: 1.84
          Mean value_function loss: 62.9038
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.3426
                       Mean reward: 39.50
               Mean episode length: 97.84
    Episode_Reward/reaching_object: 0.2911
     Episode_Reward/lifting_object: 8.7632
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 1.98s
                      Time elapsed: 00:14:18
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 50218 steps/s (collection: 1.862s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 72.7991
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.3435
                       Mean reward: 46.64
               Mean episode length: 105.84
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 9.0594
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 1.96s
                      Time elapsed: 00:14:20
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 49333 steps/s (collection: 1.887s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 63.1549
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.3469
                       Mean reward: 45.10
               Mean episode length: 106.87
    Episode_Reward/reaching_object: 0.2956
     Episode_Reward/lifting_object: 8.7986
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 1.99s
                      Time elapsed: 00:14:22
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 50439 steps/s (collection: 1.857s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 56.5220
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.3523
                       Mean reward: 48.71
               Mean episode length: 105.65
    Episode_Reward/reaching_object: 0.3103
     Episode_Reward/lifting_object: 10.0496
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 1.95s
                      Time elapsed: 00:14:24
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 50541 steps/s (collection: 1.845s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 55.2294
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.3576
                       Mean reward: 52.24
               Mean episode length: 107.33
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: 9.9986
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.95s
                      Time elapsed: 00:14:26
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 49500 steps/s (collection: 1.842s, learning 0.144s)
             Mean action noise std: 1.84
          Mean value_function loss: 61.6087
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.3612
                       Mean reward: 53.86
               Mean episode length: 101.85
    Episode_Reward/reaching_object: 0.3127
     Episode_Reward/lifting_object: 9.4944
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 1.99s
                      Time elapsed: 00:14:28
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 48008 steps/s (collection: 1.867s, learning 0.181s)
             Mean action noise std: 1.84
          Mean value_function loss: 67.9129
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.3632
                       Mean reward: 49.62
               Mean episode length: 112.18
    Episode_Reward/reaching_object: 0.3253
     Episode_Reward/lifting_object: 10.0361
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.05s
                      Time elapsed: 00:14:30
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 51211 steps/s (collection: 1.808s, learning 0.112s)
             Mean action noise std: 1.84
          Mean value_function loss: 80.9834
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.3673
                       Mean reward: 50.62
               Mean episode length: 97.86
    Episode_Reward/reaching_object: 0.3174
     Episode_Reward/lifting_object: 10.1251
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 1.92s
                      Time elapsed: 00:14:32
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 47225 steps/s (collection: 1.940s, learning 0.142s)
             Mean action noise std: 1.84
          Mean value_function loss: 96.3064
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.3688
                       Mean reward: 48.97
               Mean episode length: 115.01
    Episode_Reward/reaching_object: 0.3267
     Episode_Reward/lifting_object: 10.0069
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.08s
                      Time elapsed: 00:14:34
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 50539 steps/s (collection: 1.841s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 83.1693
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.3694
                       Mean reward: 51.83
               Mean episode length: 116.22
    Episode_Reward/reaching_object: 0.3260
     Episode_Reward/lifting_object: 10.2044
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 1.95s
                      Time elapsed: 00:14:36
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 50796 steps/s (collection: 1.846s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 63.6035
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.3720
                       Mean reward: 50.31
               Mean episode length: 102.70
    Episode_Reward/reaching_object: 0.3291
     Episode_Reward/lifting_object: 9.8561
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 1.94s
                      Time elapsed: 00:14:38
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 48870 steps/s (collection: 1.855s, learning 0.157s)
             Mean action noise std: 1.84
          Mean value_function loss: 64.3214
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.3747
                       Mean reward: 60.32
               Mean episode length: 118.59
    Episode_Reward/reaching_object: 0.3512
     Episode_Reward/lifting_object: 10.8465
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.01s
                      Time elapsed: 00:14:40
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 47545 steps/s (collection: 1.933s, learning 0.135s)
             Mean action noise std: 1.84
          Mean value_function loss: 78.5308
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.3770
                       Mean reward: 54.56
               Mean episode length: 110.97
    Episode_Reward/reaching_object: 0.3492
     Episode_Reward/lifting_object: 11.0993
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.07s
                      Time elapsed: 00:14:42
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 48212 steps/s (collection: 1.944s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 73.5960
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.3803
                       Mean reward: 54.14
               Mean episode length: 110.20
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: 11.7141
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.04s
                      Time elapsed: 00:14:44
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 50880 steps/s (collection: 1.843s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 72.9082
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.3822
                       Mean reward: 55.22
               Mean episode length: 112.72
    Episode_Reward/reaching_object: 0.3577
     Episode_Reward/lifting_object: 11.1234
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 1.93s
                      Time elapsed: 00:14:46
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 50144 steps/s (collection: 1.843s, learning 0.117s)
             Mean action noise std: 1.84
          Mean value_function loss: 96.2267
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.3839
                       Mean reward: 53.62
               Mean episode length: 117.38
    Episode_Reward/reaching_object: 0.3835
     Episode_Reward/lifting_object: 11.7255
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.96s
                      Time elapsed: 00:14:48
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 50975 steps/s (collection: 1.842s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 80.1341
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.3870
                       Mean reward: 49.02
               Mean episode length: 121.62
    Episode_Reward/reaching_object: 0.3625
     Episode_Reward/lifting_object: 11.1639
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.93s
                      Time elapsed: 00:14:50
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 49443 steps/s (collection: 1.881s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 71.3411
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.3923
                       Mean reward: 61.17
               Mean episode length: 115.60
    Episode_Reward/reaching_object: 0.3708
     Episode_Reward/lifting_object: 11.1773
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 1.99s
                      Time elapsed: 00:14:52
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 50475 steps/s (collection: 1.853s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 78.9865
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.3967
                       Mean reward: 66.50
               Mean episode length: 120.93
    Episode_Reward/reaching_object: 0.3795
     Episode_Reward/lifting_object: 11.6180
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 1.95s
                      Time elapsed: 00:14:54
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 46239 steps/s (collection: 1.951s, learning 0.175s)
             Mean action noise std: 1.84
          Mean value_function loss: 76.7472
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.3985
                       Mean reward: 60.46
               Mean episode length: 116.88
    Episode_Reward/reaching_object: 0.3791
     Episode_Reward/lifting_object: 12.0259
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.13s
                      Time elapsed: 00:14:56
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 45998 steps/s (collection: 1.973s, learning 0.164s)
             Mean action noise std: 1.84
          Mean value_function loss: 71.7939
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.4005
                       Mean reward: 65.27
               Mean episode length: 120.28
    Episode_Reward/reaching_object: 0.3839
     Episode_Reward/lifting_object: 12.5796
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.14s
                      Time elapsed: 00:14:58
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 48154 steps/s (collection: 1.916s, learning 0.125s)
             Mean action noise std: 1.84
          Mean value_function loss: 67.6639
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.4028
                       Mean reward: 61.38
               Mean episode length: 122.96
    Episode_Reward/reaching_object: 0.3754
     Episode_Reward/lifting_object: 12.0221
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.04s
                      Time elapsed: 00:15:00
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 49653 steps/s (collection: 1.876s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 76.2686
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.4049
                       Mean reward: 56.28
               Mean episode length: 113.89
    Episode_Reward/reaching_object: 0.3738
     Episode_Reward/lifting_object: 12.2282
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 1.98s
                      Time elapsed: 00:15:02
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 46691 steps/s (collection: 1.998s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 78.0635
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.4067
                       Mean reward: 60.60
               Mean episode length: 124.26
    Episode_Reward/reaching_object: 0.3719
     Episode_Reward/lifting_object: 12.2001
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.11s
                      Time elapsed: 00:15:04
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 49579 steps/s (collection: 1.856s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 70.8470
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.4081
                       Mean reward: 58.01
               Mean episode length: 121.81
    Episode_Reward/reaching_object: 0.3596
     Episode_Reward/lifting_object: 11.7528
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 1.98s
                      Time elapsed: 00:15:06
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 49664 steps/s (collection: 1.870s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 79.2885
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.4094
                       Mean reward: 61.66
               Mean episode length: 113.22
    Episode_Reward/reaching_object: 0.3558
     Episode_Reward/lifting_object: 11.7851
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 1.98s
                      Time elapsed: 00:15:08
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 48243 steps/s (collection: 1.920s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 63.8906
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.4110
                       Mean reward: 51.18
               Mean episode length: 118.61
    Episode_Reward/reaching_object: 0.3532
     Episode_Reward/lifting_object: 11.4091
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.04s
                      Time elapsed: 00:15:10
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 48964 steps/s (collection: 1.883s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.0916
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.4131
                       Mean reward: 56.99
               Mean episode length: 116.81
    Episode_Reward/reaching_object: 0.3570
     Episode_Reward/lifting_object: 11.6720
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.01s
                      Time elapsed: 00:15:12
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 50219 steps/s (collection: 1.840s, learning 0.117s)
             Mean action noise std: 1.85
          Mean value_function loss: 67.6539
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 36.4164
                       Mean reward: 66.54
               Mean episode length: 120.08
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: 13.3205
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 1.96s
                      Time elapsed: 00:15:14
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 50605 steps/s (collection: 1.816s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 67.0670
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 36.4177
                       Mean reward: 71.21
               Mean episode length: 125.11
    Episode_Reward/reaching_object: 0.3940
     Episode_Reward/lifting_object: 13.9760
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 1.94s
                      Time elapsed: 00:15:16
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 50803 steps/s (collection: 1.841s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.7443
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.4185
                       Mean reward: 74.81
               Mean episode length: 125.12
    Episode_Reward/reaching_object: 0.3811
     Episode_Reward/lifting_object: 13.2371
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 1.93s
                      Time elapsed: 00:15:18
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 50398 steps/s (collection: 1.840s, learning 0.111s)
             Mean action noise std: 1.85
          Mean value_function loss: 68.8673
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 36.4201
                       Mean reward: 71.65
               Mean episode length: 126.20
    Episode_Reward/reaching_object: 0.3940
     Episode_Reward/lifting_object: 14.0668
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 1.95s
                      Time elapsed: 00:15:20
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 49242 steps/s (collection: 1.873s, learning 0.123s)
             Mean action noise std: 1.85
          Mean value_function loss: 71.4781
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 36.4206
                       Mean reward: 73.76
               Mean episode length: 129.56
    Episode_Reward/reaching_object: 0.4048
     Episode_Reward/lifting_object: 14.5472
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.00s
                      Time elapsed: 00:15:22
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 50555 steps/s (collection: 1.857s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 73.2167
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.4216
                       Mean reward: 74.24
               Mean episode length: 130.06
    Episode_Reward/reaching_object: 0.4012
     Episode_Reward/lifting_object: 14.2764
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 1.94s
                      Time elapsed: 00:15:24
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 50755 steps/s (collection: 1.836s, learning 0.101s)
             Mean action noise std: 1.85
          Mean value_function loss: 76.7038
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.4221
                       Mean reward: 79.99
               Mean episode length: 129.67
    Episode_Reward/reaching_object: 0.4004
     Episode_Reward/lifting_object: 14.6421
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 1.94s
                      Time elapsed: 00:15:26
                               ETA: 00:52:55

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 48568 steps/s (collection: 1.907s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 72.4302
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.4231
                       Mean reward: 83.14
               Mean episode length: 127.09
    Episode_Reward/reaching_object: 0.4071
     Episode_Reward/lifting_object: 15.2378
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.02s
                      Time elapsed: 00:15:28
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 50706 steps/s (collection: 1.842s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 73.7142
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.4236
                       Mean reward: 77.20
               Mean episode length: 124.60
    Episode_Reward/reaching_object: 0.4007
     Episode_Reward/lifting_object: 15.0312
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 1.94s
                      Time elapsed: 00:15:30
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 50865 steps/s (collection: 1.820s, learning 0.112s)
             Mean action noise std: 1.85
          Mean value_function loss: 78.3813
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 36.4241
                       Mean reward: 92.26
               Mean episode length: 134.32
    Episode_Reward/reaching_object: 0.3975
     Episode_Reward/lifting_object: 14.9573
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 1.93s
                      Time elapsed: 00:15:32
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 51214 steps/s (collection: 1.809s, learning 0.110s)
             Mean action noise std: 1.85
          Mean value_function loss: 66.6604
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.4244
                       Mean reward: 84.96
               Mean episode length: 128.10
    Episode_Reward/reaching_object: 0.3985
     Episode_Reward/lifting_object: 15.2528
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 1.92s
                      Time elapsed: 00:15:34
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 50919 steps/s (collection: 1.846s, learning 0.085s)
             Mean action noise std: 1.85
          Mean value_function loss: 73.4311
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.4249
                       Mean reward: 79.21
               Mean episode length: 119.62
    Episode_Reward/reaching_object: 0.3934
     Episode_Reward/lifting_object: 15.2455
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.93s
                      Time elapsed: 00:15:36
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 50167 steps/s (collection: 1.840s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 70.5080
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.4256
                       Mean reward: 87.29
               Mean episode length: 135.58
    Episode_Reward/reaching_object: 0.4046
     Episode_Reward/lifting_object: 15.6811
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 1.96s
                      Time elapsed: 00:15:38
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 51514 steps/s (collection: 1.821s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 68.8677
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.4283
                       Mean reward: 74.08
               Mean episode length: 117.85
    Episode_Reward/reaching_object: 0.3923
     Episode_Reward/lifting_object: 14.9661
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 1.91s
                      Time elapsed: 00:15:40
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 49407 steps/s (collection: 1.873s, learning 0.116s)
             Mean action noise std: 1.85
          Mean value_function loss: 70.4166
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.4292
                       Mean reward: 79.40
               Mean episode length: 120.69
    Episode_Reward/reaching_object: 0.3912
     Episode_Reward/lifting_object: 15.4671
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 1.99s
                      Time elapsed: 00:15:42
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 48473 steps/s (collection: 1.880s, learning 0.148s)
             Mean action noise std: 1.85
          Mean value_function loss: 73.9284
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 36.4289
                       Mean reward: 86.80
               Mean episode length: 135.45
    Episode_Reward/reaching_object: 0.4088
     Episode_Reward/lifting_object: 16.2543
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.03s
                      Time elapsed: 00:15:44
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 49553 steps/s (collection: 1.876s, learning 0.108s)
             Mean action noise std: 1.85
          Mean value_function loss: 71.3822
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.4286
                       Mean reward: 87.77
               Mean episode length: 138.65
    Episode_Reward/reaching_object: 0.4030
     Episode_Reward/lifting_object: 15.8070
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 1.98s
                      Time elapsed: 00:15:46
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 49540 steps/s (collection: 1.844s, learning 0.140s)
             Mean action noise std: 1.85
          Mean value_function loss: 70.3034
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.4290
                       Mean reward: 84.28
               Mean episode length: 124.13
    Episode_Reward/reaching_object: 0.3998
     Episode_Reward/lifting_object: 16.2376
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 1.98s
                      Time elapsed: 00:15:48
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 49290 steps/s (collection: 1.903s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 75.8367
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.4294
                       Mean reward: 85.10
               Mean episode length: 127.50
    Episode_Reward/reaching_object: 0.4094
     Episode_Reward/lifting_object: 16.5563
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 1.99s
                      Time elapsed: 00:15:50
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 47402 steps/s (collection: 1.917s, learning 0.156s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.7157
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.4315
                       Mean reward: 87.02
               Mean episode length: 127.91
    Episode_Reward/reaching_object: 0.4070
     Episode_Reward/lifting_object: 16.7190
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.07s
                      Time elapsed: 00:15:52
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 48511 steps/s (collection: 1.898s, learning 0.128s)
             Mean action noise std: 1.85
          Mean value_function loss: 77.6834
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.4323
                       Mean reward: 87.66
               Mean episode length: 131.43
    Episode_Reward/reaching_object: 0.4037
     Episode_Reward/lifting_object: 16.8494
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.03s
                      Time elapsed: 00:15:54
                               ETA: 00:52:23

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 48854 steps/s (collection: 1.897s, learning 0.115s)
             Mean action noise std: 1.85
          Mean value_function loss: 77.7193
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.4336
                       Mean reward: 80.19
               Mean episode length: 125.99
    Episode_Reward/reaching_object: 0.4034
     Episode_Reward/lifting_object: 16.4985
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.01s
                      Time elapsed: 00:15:56
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 48660 steps/s (collection: 1.902s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 98.2592
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.4367
                       Mean reward: 89.32
               Mean episode length: 132.33
    Episode_Reward/reaching_object: 0.4131
     Episode_Reward/lifting_object: 17.5171
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.02s
                      Time elapsed: 00:15:58
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 49538 steps/s (collection: 1.879s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.7834
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.4383
                       Mean reward: 87.05
               Mean episode length: 132.62
    Episode_Reward/reaching_object: 0.4094
     Episode_Reward/lifting_object: 17.4104
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 1.98s
                      Time elapsed: 00:16:00
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 50403 steps/s (collection: 1.843s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 72.6909
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.4392
                       Mean reward: 96.61
               Mean episode length: 129.14
    Episode_Reward/reaching_object: 0.4151
     Episode_Reward/lifting_object: 18.1194
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 1.95s
                      Time elapsed: 00:16:02
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 49936 steps/s (collection: 1.846s, learning 0.123s)
             Mean action noise std: 1.85
          Mean value_function loss: 83.4214
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 36.4411
                       Mean reward: 83.70
               Mean episode length: 121.77
    Episode_Reward/reaching_object: 0.4035
     Episode_Reward/lifting_object: 17.3366
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 1.97s
                      Time elapsed: 00:16:04
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 50052 steps/s (collection: 1.855s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.9673
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.4425
                       Mean reward: 88.16
               Mean episode length: 116.56
    Episode_Reward/reaching_object: 0.3902
     Episode_Reward/lifting_object: 16.7549
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 1.96s
                      Time elapsed: 00:16:06
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47832 steps/s (collection: 1.920s, learning 0.135s)
             Mean action noise std: 1.85
          Mean value_function loss: 81.5731
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.4438
                       Mean reward: 85.92
               Mean episode length: 127.77
    Episode_Reward/reaching_object: 0.4100
     Episode_Reward/lifting_object: 17.4906
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.06s
                      Time elapsed: 00:16:08
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 47904 steps/s (collection: 1.915s, learning 0.138s)
             Mean action noise std: 1.85
          Mean value_function loss: 85.8373
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.4444
                       Mean reward: 78.40
               Mean episode length: 124.24
    Episode_Reward/reaching_object: 0.4033
     Episode_Reward/lifting_object: 17.4811
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.05s
                      Time elapsed: 00:16:10
                               ETA: 00:52:05

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 48355 steps/s (collection: 1.915s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 87.9188
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.4452
                       Mean reward: 93.28
               Mean episode length: 128.21
    Episode_Reward/reaching_object: 0.4004
     Episode_Reward/lifting_object: 17.4169
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.03s
                      Time elapsed: 00:16:12
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 48704 steps/s (collection: 1.916s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.8534
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.4459
                       Mean reward: 90.02
               Mean episode length: 128.46
    Episode_Reward/reaching_object: 0.4222
     Episode_Reward/lifting_object: 18.6173
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.02s
                      Time elapsed: 00:16:14
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 48990 steps/s (collection: 1.893s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 89.1188
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.4470
                       Mean reward: 83.09
               Mean episode length: 123.83
    Episode_Reward/reaching_object: 0.3956
     Episode_Reward/lifting_object: 17.1043
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.01s
                      Time elapsed: 00:16:16
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 45580 steps/s (collection: 2.008s, learning 0.149s)
             Mean action noise std: 1.85
          Mean value_function loss: 84.4060
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.4473
                       Mean reward: 95.43
               Mean episode length: 120.59
    Episode_Reward/reaching_object: 0.4067
     Episode_Reward/lifting_object: 18.0691
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.16s
                      Time elapsed: 00:16:18
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 47141 steps/s (collection: 1.952s, learning 0.133s)
             Mean action noise std: 1.85
          Mean value_function loss: 90.7545
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.4472
                       Mean reward: 94.05
               Mean episode length: 119.73
    Episode_Reward/reaching_object: 0.3985
     Episode_Reward/lifting_object: 17.6953
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.09s
                      Time elapsed: 00:16:20
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 46379 steps/s (collection: 1.995s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 89.5474
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 36.4471
                       Mean reward: 88.22
               Mean episode length: 122.29
    Episode_Reward/reaching_object: 0.3902
     Episode_Reward/lifting_object: 17.9393
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.12s
                      Time elapsed: 00:16:22
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 46679 steps/s (collection: 1.987s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.6826
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 36.4474
                       Mean reward: 91.72
               Mean episode length: 119.84
    Episode_Reward/reaching_object: 0.4096
     Episode_Reward/lifting_object: 18.6653
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.11s
                      Time elapsed: 00:16:24
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 45782 steps/s (collection: 1.992s, learning 0.155s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.5660
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.4480
                       Mean reward: 81.23
               Mean episode length: 117.37
    Episode_Reward/reaching_object: 0.3942
     Episode_Reward/lifting_object: 17.9106
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.15s
                      Time elapsed: 00:16:26
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 45674 steps/s (collection: 2.023s, learning 0.129s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.9025
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.4497
                       Mean reward: 86.49
               Mean episode length: 122.55
    Episode_Reward/reaching_object: 0.3889
     Episode_Reward/lifting_object: 17.0522
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.15s
                      Time elapsed: 00:16:29
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 47976 steps/s (collection: 1.915s, learning 0.134s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.6103
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 36.4518
                       Mean reward: 80.17
               Mean episode length: 114.96
    Episode_Reward/reaching_object: 0.3766
     Episode_Reward/lifting_object: 16.6741
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.05s
                      Time elapsed: 00:16:31
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 43194 steps/s (collection: 2.109s, learning 0.167s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.6405
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 36.4526
                       Mean reward: 87.66
               Mean episode length: 111.23
    Episode_Reward/reaching_object: 0.3644
     Episode_Reward/lifting_object: 16.1692
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.28s
                      Time elapsed: 00:16:33
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 47797 steps/s (collection: 1.939s, learning 0.117s)
             Mean action noise std: 1.85
          Mean value_function loss: 88.0575
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.4530
                       Mean reward: 80.07
               Mean episode length: 114.82
    Episode_Reward/reaching_object: 0.3747
     Episode_Reward/lifting_object: 17.0135
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.06s
                      Time elapsed: 00:16:35
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 48182 steps/s (collection: 1.930s, learning 0.111s)
             Mean action noise std: 1.85
          Mean value_function loss: 96.3942
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.4545
                       Mean reward: 98.78
               Mean episode length: 116.82
    Episode_Reward/reaching_object: 0.3714
     Episode_Reward/lifting_object: 17.1434
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.04s
                      Time elapsed: 00:16:37
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 48790 steps/s (collection: 1.896s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.1735
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.4557
                       Mean reward: 86.27
               Mean episode length: 116.89
    Episode_Reward/reaching_object: 0.3537
     Episode_Reward/lifting_object: 16.1325
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.01s
                      Time elapsed: 00:16:39
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 48515 steps/s (collection: 1.906s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.3975
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 36.4564
                       Mean reward: 92.78
               Mean episode length: 117.00
    Episode_Reward/reaching_object: 0.3819
     Episode_Reward/lifting_object: 18.0800
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.03s
                      Time elapsed: 00:16:41
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 47611 steps/s (collection: 1.920s, learning 0.145s)
             Mean action noise std: 1.85
          Mean value_function loss: 80.7445
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.4572
                       Mean reward: 90.95
               Mean episode length: 114.84
    Episode_Reward/reaching_object: 0.3912
     Episode_Reward/lifting_object: 18.3984
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.06s
                      Time elapsed: 00:16:43
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 48029 steps/s (collection: 1.947s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.7931
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.4583
                       Mean reward: 99.69
               Mean episode length: 129.18
    Episode_Reward/reaching_object: 0.3930
     Episode_Reward/lifting_object: 18.6531
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.05s
                      Time elapsed: 00:16:45
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 47873 steps/s (collection: 1.939s, learning 0.115s)
             Mean action noise std: 1.85
          Mean value_function loss: 84.9106
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.4586
                       Mean reward: 95.67
               Mean episode length: 120.76
    Episode_Reward/reaching_object: 0.3934
     Episode_Reward/lifting_object: 18.9066
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.05s
                      Time elapsed: 00:16:47
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 47782 steps/s (collection: 1.938s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.8343
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.4589
                       Mean reward: 101.93
               Mean episode length: 122.89
    Episode_Reward/reaching_object: 0.4220
     Episode_Reward/lifting_object: 20.1105
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.06s
                      Time elapsed: 00:16:49
                               ETA: 00:51:28

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 48572 steps/s (collection: 1.917s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 99.4277
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 36.4592
                       Mean reward: 107.77
               Mean episode length: 125.14
    Episode_Reward/reaching_object: 0.4145
     Episode_Reward/lifting_object: 20.5936
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.02s
                      Time elapsed: 00:16:51
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 48332 steps/s (collection: 1.912s, learning 0.122s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.7799
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.4591
                       Mean reward: 99.74
               Mean episode length: 126.65
    Episode_Reward/reaching_object: 0.4135
     Episode_Reward/lifting_object: 19.6420
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.03s
                      Time elapsed: 00:16:53
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 48603 steps/s (collection: 1.926s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 93.8754
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.4590
                       Mean reward: 112.43
               Mean episode length: 131.68
    Episode_Reward/reaching_object: 0.4142
     Episode_Reward/lifting_object: 20.4700
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.02s
                      Time elapsed: 00:16:55
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 47210 steps/s (collection: 1.973s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 99.1334
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.4594
                       Mean reward: 127.51
               Mean episode length: 143.63
    Episode_Reward/reaching_object: 0.4118
     Episode_Reward/lifting_object: 20.6103
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.08s
                      Time elapsed: 00:16:57
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 49244 steps/s (collection: 1.892s, learning 0.104s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.7103
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 36.4605
                       Mean reward: 113.44
               Mean episode length: 131.91
    Episode_Reward/reaching_object: 0.4040
     Episode_Reward/lifting_object: 20.4926
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.00s
                      Time elapsed: 00:16:59
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 47712 steps/s (collection: 1.917s, learning 0.143s)
             Mean action noise std: 1.85
          Mean value_function loss: 79.5368
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.4609
                       Mean reward: 108.67
               Mean episode length: 131.75
    Episode_Reward/reaching_object: 0.4011
     Episode_Reward/lifting_object: 20.2139
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.06s
                      Time elapsed: 00:17:02
                               ETA: 00:51:16

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 48621 steps/s (collection: 1.863s, learning 0.159s)
             Mean action noise std: 1.85
          Mean value_function loss: 85.9774
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.4613
                       Mean reward: 102.08
               Mean episode length: 125.58
    Episode_Reward/reaching_object: 0.4125
     Episode_Reward/lifting_object: 20.1308
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.02s
                      Time elapsed: 00:17:04
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 48704 steps/s (collection: 1.867s, learning 0.151s)
             Mean action noise std: 1.85
          Mean value_function loss: 88.1326
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.4626
                       Mean reward: 95.44
               Mean episode length: 121.98
    Episode_Reward/reaching_object: 0.4146
     Episode_Reward/lifting_object: 20.6132
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.02s
                      Time elapsed: 00:17:06
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 45488 steps/s (collection: 2.026s, learning 0.135s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.9285
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 36.4635
                       Mean reward: 109.88
               Mean episode length: 128.77
    Episode_Reward/reaching_object: 0.4331
     Episode_Reward/lifting_object: 21.6795
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.16s
                      Time elapsed: 00:17:08
                               ETA: 00:51:10

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 48921 steps/s (collection: 1.890s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.7917
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.4637
                       Mean reward: 111.67
               Mean episode length: 130.80
    Episode_Reward/reaching_object: 0.4246
     Episode_Reward/lifting_object: 21.6821
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.01s
                      Time elapsed: 00:17:10
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 49145 steps/s (collection: 1.872s, learning 0.129s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.4742
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.4641
                       Mean reward: 117.63
               Mean episode length: 136.84
    Episode_Reward/reaching_object: 0.4411
     Episode_Reward/lifting_object: 22.2845
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.00s
                      Time elapsed: 00:17:12
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 49202 steps/s (collection: 1.874s, learning 0.124s)
             Mean action noise std: 1.85
          Mean value_function loss: 85.5739
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.4646
                       Mean reward: 127.09
               Mean episode length: 137.43
    Episode_Reward/reaching_object: 0.4307
     Episode_Reward/lifting_object: 22.0102
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.00s
                      Time elapsed: 00:17:14
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 49914 steps/s (collection: 1.865s, learning 0.104s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.6666
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.4652
                       Mean reward: 109.51
               Mean episode length: 127.74
    Episode_Reward/reaching_object: 0.4350
     Episode_Reward/lifting_object: 22.3030
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 1.97s
                      Time elapsed: 00:17:16
                               ETA: 00:51:01

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 49505 steps/s (collection: 1.870s, learning 0.116s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.9960
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 36.4652
                       Mean reward: 113.04
               Mean episode length: 129.69
    Episode_Reward/reaching_object: 0.4338
     Episode_Reward/lifting_object: 22.5252
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 1.99s
                      Time elapsed: 00:17:18
                               ETA: 00:50:59

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 49727 steps/s (collection: 1.859s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 81.5087
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 36.4652
                       Mean reward: 108.53
               Mean episode length: 132.68
    Episode_Reward/reaching_object: 0.4475
     Episode_Reward/lifting_object: 22.5358
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 1.98s
                      Time elapsed: 00:17:20
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 48490 steps/s (collection: 1.914s, learning 0.113s)
             Mean action noise std: 1.85
          Mean value_function loss: 85.0991
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 36.4653
                       Mean reward: 122.34
               Mean episode length: 144.85
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 22.3790
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.03s
                      Time elapsed: 00:17:22
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 49825 steps/s (collection: 1.851s, learning 0.122s)
             Mean action noise std: 1.85
          Mean value_function loss: 87.0532
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 36.4654
                       Mean reward: 118.92
               Mean episode length: 140.95
    Episode_Reward/reaching_object: 0.4486
     Episode_Reward/lifting_object: 23.0209
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 1.97s
                      Time elapsed: 00:17:24
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 49541 steps/s (collection: 1.864s, learning 0.121s)
             Mean action noise std: 1.85
          Mean value_function loss: 100.9957
               Mean surrogate loss: 0.0132
                 Mean entropy loss: 36.4655
                       Mean reward: 110.47
               Mean episode length: 129.07
    Episode_Reward/reaching_object: 0.4296
     Episode_Reward/lifting_object: 21.3317
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.98s
                      Time elapsed: 00:17:26
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 49750 steps/s (collection: 1.852s, learning 0.124s)
             Mean action noise std: 1.85
          Mean value_function loss: 93.7665
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 36.4656
                       Mean reward: 117.69
               Mean episode length: 143.41
    Episode_Reward/reaching_object: 0.4563
     Episode_Reward/lifting_object: 23.3424
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 1.98s
                      Time elapsed: 00:17:28
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 50166 steps/s (collection: 1.835s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 88.9633
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 36.4657
                       Mean reward: 121.03
               Mean episode length: 139.10
    Episode_Reward/reaching_object: 0.4543
     Episode_Reward/lifting_object: 23.2840
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 1.96s
                      Time elapsed: 00:17:30
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 49001 steps/s (collection: 1.881s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 85.9788
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 36.4658
                       Mean reward: 115.79
               Mean episode length: 131.44
    Episode_Reward/reaching_object: 0.4360
     Episode_Reward/lifting_object: 21.9450
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.01s
                      Time elapsed: 00:17:32
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 48210 steps/s (collection: 1.904s, learning 0.136s)
             Mean action noise std: 1.85
          Mean value_function loss: 85.5607
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.4658
                       Mean reward: 113.23
               Mean episode length: 133.19
    Episode_Reward/reaching_object: 0.4410
     Episode_Reward/lifting_object: 22.4191
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.04s
                      Time elapsed: 00:17:34
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 48499 steps/s (collection: 1.901s, learning 0.126s)
             Mean action noise std: 1.85
          Mean value_function loss: 97.2549
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.4656
                       Mean reward: 112.12
               Mean episode length: 127.94
    Episode_Reward/reaching_object: 0.4312
     Episode_Reward/lifting_object: 22.5956
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.03s
                      Time elapsed: 00:17:36
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 49229 steps/s (collection: 1.866s, learning 0.131s)
             Mean action noise std: 1.85
          Mean value_function loss: 97.2555
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 36.4652
                       Mean reward: 106.36
               Mean episode length: 125.95
    Episode_Reward/reaching_object: 0.4381
     Episode_Reward/lifting_object: 22.8135
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.00s
                      Time elapsed: 00:17:38
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 49965 steps/s (collection: 1.850s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 90.7740
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.4656
                       Mean reward: 113.81
               Mean episode length: 130.01
    Episode_Reward/reaching_object: 0.4300
     Episode_Reward/lifting_object: 22.7833
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 1.97s
                      Time elapsed: 00:17:40
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 48801 steps/s (collection: 1.892s, learning 0.122s)
             Mean action noise std: 1.85
          Mean value_function loss: 94.2754
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.4670
                       Mean reward: 111.13
               Mean episode length: 126.21
    Episode_Reward/reaching_object: 0.4220
     Episode_Reward/lifting_object: 22.5357
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.01s
                      Time elapsed: 00:17:42
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 47922 steps/s (collection: 1.949s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 97.2939
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.4680
                       Mean reward: 102.42
               Mean episode length: 118.29
    Episode_Reward/reaching_object: 0.4217
     Episode_Reward/lifting_object: 22.0462
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.05s
                      Time elapsed: 00:17:44
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 49529 steps/s (collection: 1.882s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 93.2635
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 36.4686
                       Mean reward: 117.22
               Mean episode length: 127.86
    Episode_Reward/reaching_object: 0.4249
     Episode_Reward/lifting_object: 23.3060
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 1.98s
                      Time elapsed: 00:17:46
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 49195 steps/s (collection: 1.902s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 83.8041
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.4687
                       Mean reward: 97.32
               Mean episode length: 116.94
    Episode_Reward/reaching_object: 0.4183
     Episode_Reward/lifting_object: 22.2208
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.00s
                      Time elapsed: 00:17:48
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 48924 steps/s (collection: 1.917s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 89.3999
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.4689
                       Mean reward: 114.35
               Mean episode length: 128.78
    Episode_Reward/reaching_object: 0.4264
     Episode_Reward/lifting_object: 22.5560
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.01s
                      Time elapsed: 00:17:50
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 49283 steps/s (collection: 1.873s, learning 0.121s)
             Mean action noise std: 1.85
          Mean value_function loss: 93.0005
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.4689
                       Mean reward: 113.16
               Mean episode length: 134.84
    Episode_Reward/reaching_object: 0.4358
     Episode_Reward/lifting_object: 22.4100
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 1.99s
                      Time elapsed: 00:17:52
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 49693 steps/s (collection: 1.863s, learning 0.115s)
             Mean action noise std: 1.85
          Mean value_function loss: 93.5571
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.4696
                       Mean reward: 129.84
               Mean episode length: 143.16
    Episode_Reward/reaching_object: 0.4334
     Episode_Reward/lifting_object: 22.7616
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 1.98s
                      Time elapsed: 00:17:54
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 47853 steps/s (collection: 1.935s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.0081
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.4695
                       Mean reward: 107.16
               Mean episode length: 127.44
    Episode_Reward/reaching_object: 0.4439
     Episode_Reward/lifting_object: 23.9806
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.05s
                      Time elapsed: 00:17:56
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 48948 steps/s (collection: 1.878s, learning 0.131s)
             Mean action noise std: 1.85
          Mean value_function loss: 97.5584
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.4677
                       Mean reward: 115.95
               Mean episode length: 129.76
    Episode_Reward/reaching_object: 0.4374
     Episode_Reward/lifting_object: 23.2657
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.01s
                      Time elapsed: 00:17:58
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 48517 steps/s (collection: 1.898s, learning 0.128s)
             Mean action noise std: 1.85
          Mean value_function loss: 89.4385
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 36.4668
                       Mean reward: 126.04
               Mean episode length: 141.78
    Episode_Reward/reaching_object: 0.4572
     Episode_Reward/lifting_object: 24.6320
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.03s
                      Time elapsed: 00:18:00
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 49322 steps/s (collection: 1.867s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.8626
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.4666
                       Mean reward: 126.51
               Mean episode length: 136.71
    Episode_Reward/reaching_object: 0.4332
     Episode_Reward/lifting_object: 24.0593
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 1.99s
                      Time elapsed: 00:18:02
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 48007 steps/s (collection: 1.929s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 89.5756
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.4664
                       Mean reward: 121.10
               Mean episode length: 133.52
    Episode_Reward/reaching_object: 0.4349
     Episode_Reward/lifting_object: 23.7634
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.05s
                      Time elapsed: 00:18:04
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 49146 steps/s (collection: 1.876s, learning 0.124s)
             Mean action noise std: 1.85
          Mean value_function loss: 99.0031
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.4651
                       Mean reward: 116.37
               Mean episode length: 133.75
    Episode_Reward/reaching_object: 0.4291
     Episode_Reward/lifting_object: 23.6075
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.00s
                      Time elapsed: 00:18:06
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 49436 steps/s (collection: 1.872s, learning 0.117s)
             Mean action noise std: 1.85
          Mean value_function loss: 107.2744
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.4647
                       Mean reward: 115.11
               Mean episode length: 131.18
    Episode_Reward/reaching_object: 0.4252
     Episode_Reward/lifting_object: 23.2952
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 1.99s
                      Time elapsed: 00:18:08
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 48763 steps/s (collection: 1.898s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 110.0504
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.4647
                       Mean reward: 124.45
               Mean episode length: 135.06
    Episode_Reward/reaching_object: 0.4366
     Episode_Reward/lifting_object: 24.2569
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.02s
                      Time elapsed: 00:18:10
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 49258 steps/s (collection: 1.877s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 104.8355
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.4641
                       Mean reward: 123.89
               Mean episode length: 132.42
    Episode_Reward/reaching_object: 0.4296
     Episode_Reward/lifting_object: 23.8970
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.00s
                      Time elapsed: 00:18:12
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 49453 steps/s (collection: 1.861s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 100.5550
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.4643
                       Mean reward: 114.57
               Mean episode length: 132.96
    Episode_Reward/reaching_object: 0.4460
     Episode_Reward/lifting_object: 25.0386
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 1.99s
                      Time elapsed: 00:18:14
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 49017 steps/s (collection: 1.891s, learning 0.115s)
             Mean action noise std: 1.85
          Mean value_function loss: 100.9563
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.4648
                       Mean reward: 122.66
               Mean episode length: 127.37
    Episode_Reward/reaching_object: 0.4372
     Episode_Reward/lifting_object: 25.1082
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.01s
                      Time elapsed: 00:18:16
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 49039 steps/s (collection: 1.881s, learning 0.124s)
             Mean action noise std: 1.86
          Mean value_function loss: 106.3112
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.4651
                       Mean reward: 134.29
               Mean episode length: 129.74
    Episode_Reward/reaching_object: 0.4391
     Episode_Reward/lifting_object: 25.1456
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.00s
                      Time elapsed: 00:18:18
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 49624 steps/s (collection: 1.863s, learning 0.118s)
             Mean action noise std: 1.86
          Mean value_function loss: 115.6175
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.4658
                       Mean reward: 120.38
               Mean episode length: 125.31
    Episode_Reward/reaching_object: 0.4292
     Episode_Reward/lifting_object: 24.8338
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 1.98s
                      Time elapsed: 00:18:20
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 48992 steps/s (collection: 1.880s, learning 0.127s)
             Mean action noise std: 1.86
          Mean value_function loss: 111.4897
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 36.4661
                       Mean reward: 137.41
               Mean episode length: 134.57
    Episode_Reward/reaching_object: 0.4325
     Episode_Reward/lifting_object: 25.5205
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.01s
                      Time elapsed: 00:18:22
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 47550 steps/s (collection: 1.944s, learning 0.124s)
             Mean action noise std: 1.86
          Mean value_function loss: 108.4980
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 36.4660
                       Mean reward: 126.18
               Mean episode length: 130.10
    Episode_Reward/reaching_object: 0.4150
     Episode_Reward/lifting_object: 24.3471
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.07s
                      Time elapsed: 00:18:24
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 47251 steps/s (collection: 1.948s, learning 0.133s)
             Mean action noise std: 1.86
          Mean value_function loss: 118.3477
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.4662
                       Mean reward: 120.57
               Mean episode length: 127.40
    Episode_Reward/reaching_object: 0.3991
     Episode_Reward/lifting_object: 23.0912
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.08s
                      Time elapsed: 00:18:26
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 48894 steps/s (collection: 1.883s, learning 0.128s)
             Mean action noise std: 1.86
          Mean value_function loss: 118.8100
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.4665
                       Mean reward: 127.43
               Mean episode length: 120.27
    Episode_Reward/reaching_object: 0.4019
     Episode_Reward/lifting_object: 23.3776
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.01s
                      Time elapsed: 00:18:28
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 49158 steps/s (collection: 1.874s, learning 0.126s)
             Mean action noise std: 1.86
          Mean value_function loss: 116.8526
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 36.4668
                       Mean reward: 123.93
               Mean episode length: 122.87
    Episode_Reward/reaching_object: 0.3731
     Episode_Reward/lifting_object: 21.8105
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.00s
                      Time elapsed: 00:18:30
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 49165 steps/s (collection: 1.888s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 110.8527
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4669
                       Mean reward: 104.10
               Mean episode length: 117.70
    Episode_Reward/reaching_object: 0.3710
     Episode_Reward/lifting_object: 21.8498
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.00s
                      Time elapsed: 00:18:32
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 49772 steps/s (collection: 1.862s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 117.7302
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.4675
                       Mean reward: 99.36
               Mean episode length: 105.55
    Episode_Reward/reaching_object: 0.3422
     Episode_Reward/lifting_object: 20.2174
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 1.98s
                      Time elapsed: 00:18:34
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 49314 steps/s (collection: 1.871s, learning 0.123s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.0478
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.4686
                       Mean reward: 96.56
               Mean episode length: 109.15
    Episode_Reward/reaching_object: 0.3458
     Episode_Reward/lifting_object: 20.8483
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 1.99s
                      Time elapsed: 00:18:36
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 49271 steps/s (collection: 1.881s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 112.3828
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 36.4695
                       Mean reward: 102.16
               Mean episode length: 105.51
    Episode_Reward/reaching_object: 0.3329
     Episode_Reward/lifting_object: 19.9797
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.00s
                      Time elapsed: 00:18:38
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 49807 steps/s (collection: 1.862s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 111.0029
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.4700
                       Mean reward: 107.48
               Mean episode length: 106.90
    Episode_Reward/reaching_object: 0.3357
     Episode_Reward/lifting_object: 19.7626
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 1.97s
                      Time elapsed: 00:18:40
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 50950 steps/s (collection: 1.833s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.5845
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 36.4704
                       Mean reward: 100.78
               Mean episode length: 111.45
    Episode_Reward/reaching_object: 0.3435
     Episode_Reward/lifting_object: 20.2803
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 1.93s
                      Time elapsed: 00:18:42
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 49179 steps/s (collection: 1.894s, learning 0.105s)
             Mean action noise std: 1.86
          Mean value_function loss: 109.1134
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.4707
                       Mean reward: 95.94
               Mean episode length: 99.20
    Episode_Reward/reaching_object: 0.3415
     Episode_Reward/lifting_object: 20.3336
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.00s
                      Time elapsed: 00:18:44
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 49551 steps/s (collection: 1.876s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 105.4812
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.4709
                       Mean reward: 96.81
               Mean episode length: 104.02
    Episode_Reward/reaching_object: 0.3391
     Episode_Reward/lifting_object: 19.9728
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 1.98s
                      Time elapsed: 00:18:46
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 48191 steps/s (collection: 1.935s, learning 0.105s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.6717
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 36.4708
                       Mean reward: 111.33
               Mean episode length: 116.47
    Episode_Reward/reaching_object: 0.3623
     Episode_Reward/lifting_object: 22.0364
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.04s
                      Time elapsed: 00:18:48
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 48736 steps/s (collection: 1.912s, learning 0.106s)
             Mean action noise std: 1.86
          Mean value_function loss: 113.4421
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4711
                       Mean reward: 113.94
               Mean episode length: 121.15
    Episode_Reward/reaching_object: 0.3605
     Episode_Reward/lifting_object: 21.9027
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.02s
                      Time elapsed: 00:18:50
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 48858 steps/s (collection: 1.908s, learning 0.105s)
             Mean action noise std: 1.86
          Mean value_function loss: 109.7859
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 36.4714
                       Mean reward: 114.97
               Mean episode length: 114.94
    Episode_Reward/reaching_object: 0.3488
     Episode_Reward/lifting_object: 21.1647
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 35.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.01s
                      Time elapsed: 00:18:52
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 47836 steps/s (collection: 1.959s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 100.6877
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.4714
                       Mean reward: 128.37
               Mean episode length: 122.41
    Episode_Reward/reaching_object: 0.3685
     Episode_Reward/lifting_object: 22.9835
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.06s
                      Time elapsed: 00:18:54
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 49080 steps/s (collection: 1.900s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 106.3284
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.4711
                       Mean reward: 108.84
               Mean episode length: 109.30
    Episode_Reward/reaching_object: 0.3496
     Episode_Reward/lifting_object: 21.1767
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.00s
                      Time elapsed: 00:18:56
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 49525 steps/s (collection: 1.872s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 117.9006
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.4713
                       Mean reward: 103.33
               Mean episode length: 104.67
    Episode_Reward/reaching_object: 0.3644
     Episode_Reward/lifting_object: 21.9136
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 1.98s
                      Time elapsed: 00:18:58
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 49523 steps/s (collection: 1.875s, learning 0.110s)
             Mean action noise std: 1.86
          Mean value_function loss: 106.0001
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.4713
                       Mean reward: 112.50
               Mean episode length: 119.89
    Episode_Reward/reaching_object: 0.3660
     Episode_Reward/lifting_object: 22.0133
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 1.98s
                      Time elapsed: 00:19:00
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 49423 steps/s (collection: 1.878s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.1165
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.4717
                       Mean reward: 102.43
               Mean episode length: 112.98
    Episode_Reward/reaching_object: 0.3644
     Episode_Reward/lifting_object: 21.8546
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 1.99s
                      Time elapsed: 00:19:02
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 48522 steps/s (collection: 1.907s, learning 0.119s)
             Mean action noise std: 1.86
          Mean value_function loss: 106.9489
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.4725
                       Mean reward: 115.07
               Mean episode length: 113.79
    Episode_Reward/reaching_object: 0.3571
     Episode_Reward/lifting_object: 21.5337
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.03s
                      Time elapsed: 00:19:04
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 50285 steps/s (collection: 1.833s, learning 0.122s)
             Mean action noise std: 1.86
          Mean value_function loss: 110.4413
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 36.4728
                       Mean reward: 101.14
               Mean episode length: 106.92
    Episode_Reward/reaching_object: 0.3531
     Episode_Reward/lifting_object: 21.2644
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 1.95s
                      Time elapsed: 00:19:06
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 48811 steps/s (collection: 1.900s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 103.1480
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.4728
                       Mean reward: 116.72
               Mean episode length: 114.12
    Episode_Reward/reaching_object: 0.3647
     Episode_Reward/lifting_object: 22.3184
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.01s
                      Time elapsed: 00:19:08
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 49425 steps/s (collection: 1.872s, learning 0.117s)
             Mean action noise std: 1.86
          Mean value_function loss: 103.6752
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.4729
                       Mean reward: 118.56
               Mean episode length: 112.18
    Episode_Reward/reaching_object: 0.3789
     Episode_Reward/lifting_object: 23.1539
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 1.99s
                      Time elapsed: 00:19:10
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 50341 steps/s (collection: 1.854s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 109.0887
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.4732
                       Mean reward: 110.38
               Mean episode length: 120.75
    Episode_Reward/reaching_object: 0.3795
     Episode_Reward/lifting_object: 23.0796
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 1.95s
                      Time elapsed: 00:19:12
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 49704 steps/s (collection: 1.881s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 100.8364
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.4746
                       Mean reward: 110.23
               Mean episode length: 114.33
    Episode_Reward/reaching_object: 0.3781
     Episode_Reward/lifting_object: 22.9703
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.98s
                      Time elapsed: 00:19:14
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 50195 steps/s (collection: 1.855s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 103.2474
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.4750
                       Mean reward: 115.44
               Mean episode length: 123.10
    Episode_Reward/reaching_object: 0.3852
     Episode_Reward/lifting_object: 23.3784
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 1.96s
                      Time elapsed: 00:19:16
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 50401 steps/s (collection: 1.843s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 100.1116
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 36.4741
                       Mean reward: 132.25
               Mean episode length: 126.92
    Episode_Reward/reaching_object: 0.3921
     Episode_Reward/lifting_object: 24.7337
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 1.95s
                      Time elapsed: 00:19:18
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 49915 steps/s (collection: 1.870s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.2757
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.4735
                       Mean reward: 136.31
               Mean episode length: 135.20
    Episode_Reward/reaching_object: 0.4069
     Episode_Reward/lifting_object: 25.3225
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 1.97s
                      Time elapsed: 00:19:20
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 50228 steps/s (collection: 1.861s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 105.7989
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.4738
                       Mean reward: 124.50
               Mean episode length: 122.07
    Episode_Reward/reaching_object: 0.4154
     Episode_Reward/lifting_object: 25.7393
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 1.96s
                      Time elapsed: 00:19:22
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 50016 steps/s (collection: 1.867s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 105.3891
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.4745
                       Mean reward: 126.82
               Mean episode length: 129.51
    Episode_Reward/reaching_object: 0.4235
     Episode_Reward/lifting_object: 25.8353
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 1.97s
                      Time elapsed: 00:19:24
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 50593 steps/s (collection: 1.849s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 109.7422
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4749
                       Mean reward: 127.48
               Mean episode length: 123.89
    Episode_Reward/reaching_object: 0.4200
     Episode_Reward/lifting_object: 26.5262
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 1.94s
                      Time elapsed: 00:19:26
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 49858 steps/s (collection: 1.866s, learning 0.106s)
             Mean action noise std: 1.86
          Mean value_function loss: 117.5991
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 36.4751
                       Mean reward: 135.46
               Mean episode length: 127.99
    Episode_Reward/reaching_object: 0.4225
     Episode_Reward/lifting_object: 26.2895
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 1.97s
                      Time elapsed: 00:19:28
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 50027 steps/s (collection: 1.861s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 109.8911
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 36.4756
                       Mean reward: 132.70
               Mean episode length: 118.83
    Episode_Reward/reaching_object: 0.4156
     Episode_Reward/lifting_object: 26.4201
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 1.96s
                      Time elapsed: 00:19:29
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 49337 steps/s (collection: 1.885s, learning 0.107s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.6099
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 36.4758
                       Mean reward: 140.95
               Mean episode length: 133.21
    Episode_Reward/reaching_object: 0.4215
     Episode_Reward/lifting_object: 26.5257
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 1.99s
                      Time elapsed: 00:19:31
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 48254 steps/s (collection: 1.934s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 115.3212
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.4763
                       Mean reward: 126.28
               Mean episode length: 118.24
    Episode_Reward/reaching_object: 0.4125
     Episode_Reward/lifting_object: 26.0903
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.04s
                      Time elapsed: 00:19:34
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 49720 steps/s (collection: 1.882s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 117.5340
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.4757
                       Mean reward: 121.94
               Mean episode length: 120.69
    Episode_Reward/reaching_object: 0.4089
     Episode_Reward/lifting_object: 25.8186
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 1.98s
                      Time elapsed: 00:19:35
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 48525 steps/s (collection: 1.929s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 115.6698
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.4749
                       Mean reward: 142.91
               Mean episode length: 131.34
    Episode_Reward/reaching_object: 0.4426
     Episode_Reward/lifting_object: 28.5193
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.03s
                      Time elapsed: 00:19:38
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 49134 steps/s (collection: 1.887s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 120.5451
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.4752
                       Mean reward: 136.90
               Mean episode length: 125.30
    Episode_Reward/reaching_object: 0.4163
     Episode_Reward/lifting_object: 26.2071
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.00s
                      Time elapsed: 00:19:40
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 49443 steps/s (collection: 1.876s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 132.4806
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.4751
                       Mean reward: 147.64
               Mean episode length: 127.04
    Episode_Reward/reaching_object: 0.4360
     Episode_Reward/lifting_object: 27.7779
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 1.99s
                      Time elapsed: 00:19:41
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 48816 steps/s (collection: 1.901s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 123.3881
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.4745
                       Mean reward: 147.06
               Mean episode length: 130.06
    Episode_Reward/reaching_object: 0.4328
     Episode_Reward/lifting_object: 27.9158
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.01s
                      Time elapsed: 00:19:44
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 49821 steps/s (collection: 1.877s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 141.3902
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 36.4737
                       Mean reward: 137.28
               Mean episode length: 126.64
    Episode_Reward/reaching_object: 0.4146
     Episode_Reward/lifting_object: 26.0984
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.97s
                      Time elapsed: 00:19:45
                               ETA: 00:48:18

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 49381 steps/s (collection: 1.878s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 127.9072
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.4736
                       Mean reward: 130.92
               Mean episode length: 121.51
    Episode_Reward/reaching_object: 0.4267
     Episode_Reward/lifting_object: 27.7182
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 1.99s
                      Time elapsed: 00:19:47
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 50158 steps/s (collection: 1.852s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 128.5165
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.4740
                       Mean reward: 137.95
               Mean episode length: 127.29
    Episode_Reward/reaching_object: 0.4410
     Episode_Reward/lifting_object: 28.2974
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 1.96s
                      Time elapsed: 00:19:49
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 50047 steps/s (collection: 1.858s, learning 0.107s)
             Mean action noise std: 1.86
          Mean value_function loss: 128.9085
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.4738
                       Mean reward: 139.35
               Mean episode length: 132.51
    Episode_Reward/reaching_object: 0.4200
     Episode_Reward/lifting_object: 27.3481
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 1.96s
                      Time elapsed: 00:19:51
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 48994 steps/s (collection: 1.909s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 131.2361
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.4728
                       Mean reward: 130.95
               Mean episode length: 119.95
    Episode_Reward/reaching_object: 0.4142
     Episode_Reward/lifting_object: 27.3947
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.01s
                      Time elapsed: 00:19:53
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 49165 steps/s (collection: 1.895s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 119.6720
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.4727
                       Mean reward: 156.35
               Mean episode length: 132.50
    Episode_Reward/reaching_object: 0.4251
     Episode_Reward/lifting_object: 28.4076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.00s
                      Time elapsed: 00:19:55
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 46952 steps/s (collection: 1.979s, learning 0.115s)
             Mean action noise std: 1.86
          Mean value_function loss: 128.0556
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.4718
                       Mean reward: 125.20
               Mean episode length: 118.51
    Episode_Reward/reaching_object: 0.4077
     Episode_Reward/lifting_object: 26.7188
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.09s
                      Time elapsed: 00:19:57
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 47686 steps/s (collection: 1.950s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 120.9348
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 36.4709
                       Mean reward: 142.43
               Mean episode length: 130.71
    Episode_Reward/reaching_object: 0.3970
     Episode_Reward/lifting_object: 26.3232
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.06s
                      Time elapsed: 00:20:00
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 42779 steps/s (collection: 2.106s, learning 0.192s)
             Mean action noise std: 1.86
          Mean value_function loss: 120.8853
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.4711
                       Mean reward: 149.65
               Mean episode length: 127.08
    Episode_Reward/reaching_object: 0.4156
     Episode_Reward/lifting_object: 27.7094
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.30s
                      Time elapsed: 00:20:02
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 41306 steps/s (collection: 2.186s, learning 0.194s)
             Mean action noise std: 1.86
          Mean value_function loss: 123.0612
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.4718
                       Mean reward: 140.52
               Mean episode length: 128.38
    Episode_Reward/reaching_object: 0.4113
     Episode_Reward/lifting_object: 27.3437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.38s
                      Time elapsed: 00:20:04
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 42864 steps/s (collection: 2.066s, learning 0.227s)
             Mean action noise std: 1.86
          Mean value_function loss: 124.6663
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.4734
                       Mean reward: 146.34
               Mean episode length: 132.48
    Episode_Reward/reaching_object: 0.4234
     Episode_Reward/lifting_object: 27.8492
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.29s
                      Time elapsed: 00:20:07
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 34645 steps/s (collection: 2.712s, learning 0.126s)
             Mean action noise std: 1.86
          Mean value_function loss: 119.4254
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 36.4746
                       Mean reward: 156.71
               Mean episode length: 137.41
    Episode_Reward/reaching_object: 0.4345
     Episode_Reward/lifting_object: 28.2310
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.84s
                      Time elapsed: 00:20:09
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 45836 steps/s (collection: 2.050s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 132.0633
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.4749
                       Mean reward: 147.46
               Mean episode length: 129.67
    Episode_Reward/reaching_object: 0.4192
     Episode_Reward/lifting_object: 27.5173
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.14s
                      Time elapsed: 00:20:12
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 47213 steps/s (collection: 1.990s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 123.6771
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.4764
                       Mean reward: 125.37
               Mean episode length: 112.99
    Episode_Reward/reaching_object: 0.4165
     Episode_Reward/lifting_object: 27.3238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.08s
                      Time elapsed: 00:20:14
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 44344 steps/s (collection: 2.099s, learning 0.118s)
             Mean action noise std: 1.86
          Mean value_function loss: 122.9959
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.4788
                       Mean reward: 138.33
               Mean episode length: 128.75
    Episode_Reward/reaching_object: 0.4139
     Episode_Reward/lifting_object: 27.3789
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.22s
                      Time elapsed: 00:20:16
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 47554 steps/s (collection: 1.973s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 125.8215
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.4798
                       Mean reward: 143.02
               Mean episode length: 127.28
    Episode_Reward/reaching_object: 0.4141
     Episode_Reward/lifting_object: 27.7992
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.07s
                      Time elapsed: 00:20:18
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 49324 steps/s (collection: 1.898s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.0827
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.4819
                       Mean reward: 152.35
               Mean episode length: 124.46
    Episode_Reward/reaching_object: 0.4313
     Episode_Reward/lifting_object: 29.8162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 1.99s
                      Time elapsed: 00:20:20
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 49834 steps/s (collection: 1.881s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 131.5395
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.4850
                       Mean reward: 142.76
               Mean episode length: 125.81
    Episode_Reward/reaching_object: 0.4272
     Episode_Reward/lifting_object: 28.8897
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 1.97s
                      Time elapsed: 00:20:22
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 49360 steps/s (collection: 1.891s, learning 0.101s)
             Mean action noise std: 1.86
          Mean value_function loss: 133.0320
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.4879
                       Mean reward: 135.09
               Mean episode length: 117.48
    Episode_Reward/reaching_object: 0.4217
     Episode_Reward/lifting_object: 29.0006
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 1.99s
                      Time elapsed: 00:20:24
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 46835 steps/s (collection: 1.986s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 131.8516
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.4900
                       Mean reward: 148.42
               Mean episode length: 126.56
    Episode_Reward/reaching_object: 0.4115
     Episode_Reward/lifting_object: 28.6269
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.10s
                      Time elapsed: 00:20:26
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 47353 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.5839
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.4925
                       Mean reward: 143.15
               Mean episode length: 119.59
    Episode_Reward/reaching_object: 0.4093
     Episode_Reward/lifting_object: 28.0537
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.08s
                      Time elapsed: 00:20:28
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 46512 steps/s (collection: 1.989s, learning 0.125s)
             Mean action noise std: 1.86
          Mean value_function loss: 126.6262
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.4939
                       Mean reward: 136.13
               Mean episode length: 125.04
    Episode_Reward/reaching_object: 0.3999
     Episode_Reward/lifting_object: 27.1932
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.11s
                      Time elapsed: 00:20:30
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 42946 steps/s (collection: 2.164s, learning 0.125s)
             Mean action noise std: 1.86
          Mean value_function loss: 131.0470
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.4940
                       Mean reward: 137.33
               Mean episode length: 116.10
    Episode_Reward/reaching_object: 0.4114
     Episode_Reward/lifting_object: 28.1589
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.29s
                      Time elapsed: 00:20:32
                               ETA: 00:47:38

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 46377 steps/s (collection: 2.028s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 126.7792
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.4941
                       Mean reward: 136.75
               Mean episode length: 121.56
    Episode_Reward/reaching_object: 0.3984
     Episode_Reward/lifting_object: 27.3927
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.12s
                      Time elapsed: 00:20:35
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 45962 steps/s (collection: 2.029s, learning 0.110s)
             Mean action noise std: 1.86
          Mean value_function loss: 129.4780
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.4952
                       Mean reward: 143.67
               Mean episode length: 123.64
    Episode_Reward/reaching_object: 0.4033
     Episode_Reward/lifting_object: 27.9372
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.14s
                      Time elapsed: 00:20:37
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 45379 steps/s (collection: 2.062s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 135.6379
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.4979
                       Mean reward: 135.03
               Mean episode length: 124.83
    Episode_Reward/reaching_object: 0.4060
     Episode_Reward/lifting_object: 28.1464
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.17s
                      Time elapsed: 00:20:39
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 46786 steps/s (collection: 2.000s, learning 0.101s)
             Mean action noise std: 1.86
          Mean value_function loss: 128.3334
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.4999
                       Mean reward: 160.95
               Mean episode length: 131.87
    Episode_Reward/reaching_object: 0.4127
     Episode_Reward/lifting_object: 29.6972
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.10s
                      Time elapsed: 00:20:41
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 46972 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 130.6484
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.4999
                       Mean reward: 139.72
               Mean episode length: 123.60
    Episode_Reward/reaching_object: 0.4197
     Episode_Reward/lifting_object: 29.3695
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.09s
                      Time elapsed: 00:20:43
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 49268 steps/s (collection: 1.900s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.5996
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 36.5007
                       Mean reward: 150.32
               Mean episode length: 134.06
    Episode_Reward/reaching_object: 0.4197
     Episode_Reward/lifting_object: 29.3663
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.00s
                      Time elapsed: 00:20:45
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 49354 steps/s (collection: 1.890s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.1838
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.5024
                       Mean reward: 126.54
               Mean episode length: 114.29
    Episode_Reward/reaching_object: 0.4201
     Episode_Reward/lifting_object: 29.7255
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 1.99s
                      Time elapsed: 00:20:47
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 49505 steps/s (collection: 1.871s, learning 0.115s)
             Mean action noise std: 1.86
          Mean value_function loss: 133.9984
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5035
                       Mean reward: 143.98
               Mean episode length: 127.42
    Episode_Reward/reaching_object: 0.4192
     Episode_Reward/lifting_object: 29.6056
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 1.99s
                      Time elapsed: 00:20:49
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 48815 steps/s (collection: 1.903s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 125.0596
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5049
                       Mean reward: 143.42
               Mean episode length: 121.23
    Episode_Reward/reaching_object: 0.4003
     Episode_Reward/lifting_object: 28.3434
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.01s
                      Time elapsed: 00:20:51
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 47452 steps/s (collection: 1.967s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 128.6927
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.5066
                       Mean reward: 160.19
               Mean episode length: 137.99
    Episode_Reward/reaching_object: 0.4425
     Episode_Reward/lifting_object: 32.0189
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.07s
                      Time elapsed: 00:20:53
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 50028 steps/s (collection: 1.871s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 132.7483
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.5064
                       Mean reward: 157.01
               Mean episode length: 129.32
    Episode_Reward/reaching_object: 0.4354
     Episode_Reward/lifting_object: 30.6787
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 1.96s
                      Time elapsed: 00:20:55
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 49019 steps/s (collection: 1.902s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 139.5219
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.5073
                       Mean reward: 152.38
               Mean episode length: 128.19
    Episode_Reward/reaching_object: 0.4122
     Episode_Reward/lifting_object: 29.3969
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.01s
                      Time elapsed: 00:20:57
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 49613 steps/s (collection: 1.872s, learning 0.110s)
             Mean action noise std: 1.86
          Mean value_function loss: 126.6972
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.5076
                       Mean reward: 153.62
               Mean episode length: 123.37
    Episode_Reward/reaching_object: 0.4330
     Episode_Reward/lifting_object: 31.5881
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 1.98s
                      Time elapsed: 00:20:59
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 49214 steps/s (collection: 1.886s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.9099
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 36.5065
                       Mean reward: 166.56
               Mean episode length: 133.33
    Episode_Reward/reaching_object: 0.4423
     Episode_Reward/lifting_object: 31.7757
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.00s
                      Time elapsed: 00:21:01
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 48975 steps/s (collection: 1.896s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 137.6594
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 36.5059
                       Mean reward: 161.15
               Mean episode length: 133.62
    Episode_Reward/reaching_object: 0.4306
     Episode_Reward/lifting_object: 31.3020
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.01s
                      Time elapsed: 00:21:03
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 49067 steps/s (collection: 1.895s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.2149
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.5052
                       Mean reward: 160.96
               Mean episode length: 126.94
    Episode_Reward/reaching_object: 0.4198
     Episode_Reward/lifting_object: 29.7604
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.00s
                      Time elapsed: 00:21:05
                               ETA: 00:47:05

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 48977 steps/s (collection: 1.903s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 134.0089
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.5031
                       Mean reward: 151.00
               Mean episode length: 120.58
    Episode_Reward/reaching_object: 0.4315
     Episode_Reward/lifting_object: 30.9829
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.01s
                      Time elapsed: 00:21:07
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 48968 steps/s (collection: 1.914s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 126.8644
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.5027
                       Mean reward: 162.03
               Mean episode length: 134.66
    Episode_Reward/reaching_object: 0.4357
     Episode_Reward/lifting_object: 31.6275
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.01s
                      Time elapsed: 00:21:09
                               ETA: 00:47:01

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 49869 steps/s (collection: 1.881s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 129.4860
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.5035
                       Mean reward: 166.86
               Mean episode length: 140.10
    Episode_Reward/reaching_object: 0.4384
     Episode_Reward/lifting_object: 31.5930
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 1.97s
                      Time elapsed: 00:21:11
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 49472 steps/s (collection: 1.888s, learning 0.100s)
             Mean action noise std: 1.86
          Mean value_function loss: 142.6899
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.5032
                       Mean reward: 156.64
               Mean episode length: 134.79
    Episode_Reward/reaching_object: 0.4429
     Episode_Reward/lifting_object: 31.6974
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 1.99s
                      Time elapsed: 00:21:13
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 49385 steps/s (collection: 1.893s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 136.7218
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.5023
                       Mean reward: 162.52
               Mean episode length: 131.12
    Episode_Reward/reaching_object: 0.4481
     Episode_Reward/lifting_object: 33.0400
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 1.99s
                      Time elapsed: 00:21:15
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 48601 steps/s (collection: 1.924s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 129.2040
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.5032
                       Mean reward: 162.82
               Mean episode length: 134.08
    Episode_Reward/reaching_object: 0.4493
     Episode_Reward/lifting_object: 32.5628
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.02s
                      Time elapsed: 00:21:17
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 49411 steps/s (collection: 1.890s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 124.6597
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.5021
                       Mean reward: 174.41
               Mean episode length: 138.98
    Episode_Reward/reaching_object: 0.4586
     Episode_Reward/lifting_object: 34.2393
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 1.99s
                      Time elapsed: 00:21:19
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 48774 steps/s (collection: 1.911s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 130.2509
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 36.5014
                       Mean reward: 192.92
               Mean episode length: 154.45
    Episode_Reward/reaching_object: 0.4850
     Episode_Reward/lifting_object: 35.8922
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.02s
                      Time elapsed: 00:21:21
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 49445 steps/s (collection: 1.893s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 132.5695
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.5012
                       Mean reward: 179.64
               Mean episode length: 142.19
    Episode_Reward/reaching_object: 0.4894
     Episode_Reward/lifting_object: 36.2977
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 1.99s
                      Time elapsed: 00:21:23
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 49114 steps/s (collection: 1.907s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 127.7581
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.5014
                       Mean reward: 200.62
               Mean episode length: 156.20
    Episode_Reward/reaching_object: 0.5070
     Episode_Reward/lifting_object: 38.1579
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.00s
                      Time elapsed: 00:21:25
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 48538 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 130.3822
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 36.5015
                       Mean reward: 191.19
               Mean episode length: 146.62
    Episode_Reward/reaching_object: 0.4910
     Episode_Reward/lifting_object: 35.9333
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.03s
                      Time elapsed: 00:21:27
                               ETA: 00:46:41

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 49601 steps/s (collection: 1.887s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 121.5881
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.5013
                       Mean reward: 178.84
               Mean episode length: 136.88
    Episode_Reward/reaching_object: 0.4882
     Episode_Reward/lifting_object: 36.3893
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 1.98s
                      Time elapsed: 00:21:29
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 49196 steps/s (collection: 1.886s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 136.1974
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.4999
                       Mean reward: 179.98
               Mean episode length: 143.12
    Episode_Reward/reaching_object: 0.4813
     Episode_Reward/lifting_object: 35.7205
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.00s
                      Time elapsed: 00:21:31
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 48416 steps/s (collection: 1.921s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 129.3686
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 36.4996
                       Mean reward: 206.87
               Mean episode length: 157.48
    Episode_Reward/reaching_object: 0.5084
     Episode_Reward/lifting_object: 38.4016
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.03s
                      Time elapsed: 00:21:33
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 48949 steps/s (collection: 1.906s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 144.0554
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.5003
                       Mean reward: 180.98
               Mean episode length: 139.56
    Episode_Reward/reaching_object: 0.4720
     Episode_Reward/lifting_object: 35.6200
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.01s
                      Time elapsed: 00:21:35
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 47922 steps/s (collection: 1.941s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 136.6977
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.5001
                       Mean reward: 186.95
               Mean episode length: 146.14
    Episode_Reward/reaching_object: 0.4743
     Episode_Reward/lifting_object: 35.2799
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.05s
                      Time elapsed: 00:21:37
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 49124 steps/s (collection: 1.896s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 142.4527
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.4994
                       Mean reward: 180.59
               Mean episode length: 144.94
    Episode_Reward/reaching_object: 0.4837
     Episode_Reward/lifting_object: 36.7606
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.00s
                      Time elapsed: 00:21:39
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 48806 steps/s (collection: 1.898s, learning 0.116s)
             Mean action noise std: 1.87
          Mean value_function loss: 144.6059
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.4998
                       Mean reward: 187.12
               Mean episode length: 148.25
    Episode_Reward/reaching_object: 0.4681
     Episode_Reward/lifting_object: 35.2734
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.01s
                      Time elapsed: 00:21:41
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 48812 steps/s (collection: 1.910s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 144.8498
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 36.5006
                       Mean reward: 207.04
               Mean episode length: 155.61
    Episode_Reward/reaching_object: 0.4765
     Episode_Reward/lifting_object: 36.1267
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.01s
                      Time elapsed: 00:21:43
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 49120 steps/s (collection: 1.900s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 146.1904
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 36.5008
                       Mean reward: 183.76
               Mean episode length: 140.99
    Episode_Reward/reaching_object: 0.4672
     Episode_Reward/lifting_object: 35.8981
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.00s
                      Time elapsed: 00:21:45
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 49421 steps/s (collection: 1.887s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 150.9576
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.5009
                       Mean reward: 177.00
               Mean episode length: 135.05
    Episode_Reward/reaching_object: 0.4487
     Episode_Reward/lifting_object: 33.9801
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 1.99s
                      Time elapsed: 00:21:47
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 49334 steps/s (collection: 1.903s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 151.4418
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.5009
                       Mean reward: 171.25
               Mean episode length: 137.04
    Episode_Reward/reaching_object: 0.4423
     Episode_Reward/lifting_object: 34.0765
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 1.99s
                      Time elapsed: 00:21:49
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 48872 steps/s (collection: 1.914s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 149.7388
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.5006
                       Mean reward: 177.60
               Mean episode length: 137.01
    Episode_Reward/reaching_object: 0.4508
     Episode_Reward/lifting_object: 34.2222
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.01s
                      Time elapsed: 00:21:51
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 48525 steps/s (collection: 1.928s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 146.6576
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.5005
                       Mean reward: 178.17
               Mean episode length: 137.42
    Episode_Reward/reaching_object: 0.4518
     Episode_Reward/lifting_object: 35.0075
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.03s
                      Time elapsed: 00:21:53
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 49390 steps/s (collection: 1.896s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 141.4657
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.5011
                       Mean reward: 188.09
               Mean episode length: 144.58
    Episode_Reward/reaching_object: 0.4520
     Episode_Reward/lifting_object: 35.4395
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 1.99s
                      Time elapsed: 00:21:55
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 49295 steps/s (collection: 1.900s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 145.9271
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.5019
                       Mean reward: 191.80
               Mean episode length: 139.41
    Episode_Reward/reaching_object: 0.4882
     Episode_Reward/lifting_object: 38.5911
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 1.99s
                      Time elapsed: 00:21:57
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 48479 steps/s (collection: 1.930s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 142.1489
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 36.5016
                       Mean reward: 187.52
               Mean episode length: 141.23
    Episode_Reward/reaching_object: 0.4760
     Episode_Reward/lifting_object: 37.0410
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.03s
                      Time elapsed: 00:21:59
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 48916 steps/s (collection: 1.904s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 141.9529
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 36.5008
                       Mean reward: 173.50
               Mean episode length: 134.17
    Episode_Reward/reaching_object: 0.4632
     Episode_Reward/lifting_object: 36.4937
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.01s
                      Time elapsed: 00:22:01
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 49184 steps/s (collection: 1.883s, learning 0.116s)
             Mean action noise std: 1.87
          Mean value_function loss: 143.8011
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.5008
                       Mean reward: 190.06
               Mean episode length: 142.00
    Episode_Reward/reaching_object: 0.4785
     Episode_Reward/lifting_object: 37.0734
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.00s
                      Time elapsed: 00:22:03
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 49194 steps/s (collection: 1.893s, learning 0.106s)
             Mean action noise std: 1.87
          Mean value_function loss: 136.2715
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.5002
                       Mean reward: 198.73
               Mean episode length: 148.65
    Episode_Reward/reaching_object: 0.4729
     Episode_Reward/lifting_object: 37.0223
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.00s
                      Time elapsed: 00:22:05
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 49584 steps/s (collection: 1.891s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 139.6386
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.4995
                       Mean reward: 193.83
               Mean episode length: 147.21
    Episode_Reward/reaching_object: 0.4850
     Episode_Reward/lifting_object: 37.8703
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 1.98s
                      Time elapsed: 00:22:07
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 49711 steps/s (collection: 1.887s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 136.7043
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.4999
                       Mean reward: 196.80
               Mean episode length: 148.76
    Episode_Reward/reaching_object: 0.4978
     Episode_Reward/lifting_object: 39.3204
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 1.98s
                      Time elapsed: 00:22:09
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 48615 steps/s (collection: 1.921s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 136.1932
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.4994
                       Mean reward: 195.81
               Mean episode length: 149.98
    Episode_Reward/reaching_object: 0.5062
     Episode_Reward/lifting_object: 39.9702
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.02s
                      Time elapsed: 00:22:11
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 49632 steps/s (collection: 1.877s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 142.5372
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.4983
                       Mean reward: 203.49
               Mean episode length: 149.72
    Episode_Reward/reaching_object: 0.5159
     Episode_Reward/lifting_object: 40.5126
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 1.98s
                      Time elapsed: 00:22:13
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 49325 steps/s (collection: 1.899s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 143.5536
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.4993
                       Mean reward: 196.10
               Mean episode length: 146.86
    Episode_Reward/reaching_object: 0.5114
     Episode_Reward/lifting_object: 40.4748
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 1.99s
                      Time elapsed: 00:22:15
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 49906 steps/s (collection: 1.868s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 139.9531
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.4989
                       Mean reward: 202.66
               Mean episode length: 147.37
    Episode_Reward/reaching_object: 0.4921
     Episode_Reward/lifting_object: 39.3119
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 1.97s
                      Time elapsed: 00:22:17
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 49027 steps/s (collection: 1.906s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 140.7786
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.4987
                       Mean reward: 221.10
               Mean episode length: 155.54
    Episode_Reward/reaching_object: 0.5244
     Episode_Reward/lifting_object: 42.1594
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.01s
                      Time elapsed: 00:22:19
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 49315 steps/s (collection: 1.893s, learning 0.101s)
             Mean action noise std: 1.87
          Mean value_function loss: 146.3592
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.4993
                       Mean reward: 207.19
               Mean episode length: 152.12
    Episode_Reward/reaching_object: 0.5178
     Episode_Reward/lifting_object: 41.6274
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 1.99s
                      Time elapsed: 00:22:21
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 50018 steps/s (collection: 1.872s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 137.2309
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 36.5004
                       Mean reward: 203.14
               Mean episode length: 142.37
    Episode_Reward/reaching_object: 0.4947
     Episode_Reward/lifting_object: 39.9939
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 1.97s
                      Time elapsed: 00:22:23
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 49555 steps/s (collection: 1.883s, learning 0.101s)
             Mean action noise std: 1.87
          Mean value_function loss: 144.7851
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 36.5007
                       Mean reward: 188.72
               Mean episode length: 145.74
    Episode_Reward/reaching_object: 0.5037
     Episode_Reward/lifting_object: 40.2032
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 1.98s
                      Time elapsed: 00:22:25
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 49686 steps/s (collection: 1.884s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 136.1109
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.5009
                       Mean reward: 199.15
               Mean episode length: 147.11
    Episode_Reward/reaching_object: 0.4999
     Episode_Reward/lifting_object: 40.8352
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 1.98s
                      Time elapsed: 00:22:27
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 49367 steps/s (collection: 1.895s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 137.1413
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.5009
                       Mean reward: 187.99
               Mean episode length: 138.45
    Episode_Reward/reaching_object: 0.4999
     Episode_Reward/lifting_object: 40.4428
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 1.99s
                      Time elapsed: 00:22:29
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 49758 steps/s (collection: 1.877s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 137.2628
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.5008
                       Mean reward: 221.37
               Mean episode length: 161.66
    Episode_Reward/reaching_object: 0.5056
     Episode_Reward/lifting_object: 40.5844
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 1.98s
                      Time elapsed: 00:22:31
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 48617 steps/s (collection: 1.908s, learning 0.114s)
             Mean action noise std: 1.87
          Mean value_function loss: 130.5750
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.5008
                       Mean reward: 202.30
               Mean episode length: 149.31
    Episode_Reward/reaching_object: 0.5007
     Episode_Reward/lifting_object: 39.9585
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.02s
                      Time elapsed: 00:22:33
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 49769 steps/s (collection: 1.873s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 134.9901
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 36.5008
                       Mean reward: 203.99
               Mean episode length: 145.71
    Episode_Reward/reaching_object: 0.5345
     Episode_Reward/lifting_object: 43.1927
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 1.98s
                      Time elapsed: 00:22:35
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 49251 steps/s (collection: 1.892s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 138.9247
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.5008
                       Mean reward: 224.48
               Mean episode length: 166.61
    Episode_Reward/reaching_object: 0.5353
     Episode_Reward/lifting_object: 43.4783
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.00s
                      Time elapsed: 00:22:37
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 48758 steps/s (collection: 1.917s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 131.9550
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.5007
                       Mean reward: 206.65
               Mean episode length: 150.71
    Episode_Reward/reaching_object: 0.5327
     Episode_Reward/lifting_object: 42.9584
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.02s
                      Time elapsed: 00:22:39
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 29081 steps/s (collection: 3.282s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 134.0192
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.5001
                       Mean reward: 224.60
               Mean episode length: 159.50
    Episode_Reward/reaching_object: 0.5450
     Episode_Reward/lifting_object: 43.0130
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.38s
                      Time elapsed: 00:22:42
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14648 steps/s (collection: 6.595s, learning 0.115s)
             Mean action noise std: 1.87
          Mean value_function loss: 140.4795
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.4996
                       Mean reward: 217.60
               Mean episode length: 153.84
    Episode_Reward/reaching_object: 0.5271
     Episode_Reward/lifting_object: 42.3876
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.71s
                      Time elapsed: 00:22:49
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14338 steps/s (collection: 6.726s, learning 0.130s)
             Mean action noise std: 1.87
          Mean value_function loss: 133.6480
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.4998
                       Mean reward: 221.35
               Mean episode length: 162.12
    Episode_Reward/reaching_object: 0.5412
     Episode_Reward/lifting_object: 43.5147
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.86s
                      Time elapsed: 00:22:56
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 13974 steps/s (collection: 6.908s, learning 0.126s)
             Mean action noise std: 1.87
          Mean value_function loss: 143.5538
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.5014
                       Mean reward: 209.62
               Mean episode length: 152.93
    Episode_Reward/reaching_object: 0.5361
     Episode_Reward/lifting_object: 42.0427
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.03s
                      Time elapsed: 00:23:03
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14542 steps/s (collection: 6.635s, learning 0.125s)
             Mean action noise std: 1.87
          Mean value_function loss: 141.5438
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.5015
                       Mean reward: 173.97
               Mean episode length: 134.31
    Episode_Reward/reaching_object: 0.5266
     Episode_Reward/lifting_object: 41.9074
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.76s
                      Time elapsed: 00:23:10
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14471 steps/s (collection: 6.673s, learning 0.120s)
             Mean action noise std: 1.87
          Mean value_function loss: 144.7901
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 36.5021
                       Mean reward: 231.26
               Mean episode length: 165.87
    Episode_Reward/reaching_object: 0.5347
     Episode_Reward/lifting_object: 41.9312
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.79s
                      Time elapsed: 00:23:17
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14440 steps/s (collection: 6.692s, learning 0.116s)
             Mean action noise std: 1.87
          Mean value_function loss: 139.2996
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 36.5034
                       Mean reward: 208.99
               Mean episode length: 153.00
    Episode_Reward/reaching_object: 0.5061
     Episode_Reward/lifting_object: 39.6661
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.81s
                      Time elapsed: 00:23:23
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14555 steps/s (collection: 6.637s, learning 0.117s)
             Mean action noise std: 1.87
          Mean value_function loss: 141.1611
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.5045
                       Mean reward: 195.19
               Mean episode length: 144.39
    Episode_Reward/reaching_object: 0.5368
     Episode_Reward/lifting_object: 42.7187
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.75s
                      Time elapsed: 00:23:30
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13822 steps/s (collection: 6.918s, learning 0.194s)
             Mean action noise std: 1.87
          Mean value_function loss: 147.3022
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.5052
                       Mean reward: 211.33
               Mean episode length: 150.31
    Episode_Reward/reaching_object: 0.5280
     Episode_Reward/lifting_object: 42.9103
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.11s
                      Time elapsed: 00:23:37
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 24421 steps/s (collection: 3.923s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 156.8155
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.5046
                       Mean reward: 209.87
               Mean episode length: 150.68
    Episode_Reward/reaching_object: 0.5119
     Episode_Reward/lifting_object: 40.7211
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.03s
                      Time elapsed: 00:23:41
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 48587 steps/s (collection: 1.894s, learning 0.129s)
             Mean action noise std: 1.87
          Mean value_function loss: 161.1075
               Mean surrogate loss: 0.0169
                 Mean entropy loss: 36.5043
                       Mean reward: 180.84
               Mean episode length: 132.09
    Episode_Reward/reaching_object: 0.4879
     Episode_Reward/lifting_object: 39.2446
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.02s
                      Time elapsed: 00:23:43
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 50180 steps/s (collection: 1.847s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 148.2450
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.5046
                       Mean reward: 213.94
               Mean episode length: 151.65
    Episode_Reward/reaching_object: 0.5268
     Episode_Reward/lifting_object: 43.0889
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 1.96s
                      Time elapsed: 00:23:45
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 45952 steps/s (collection: 2.028s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 147.8315
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 36.5048
                       Mean reward: 219.88
               Mean episode length: 154.91
    Episode_Reward/reaching_object: 0.5060
     Episode_Reward/lifting_object: 41.1771
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.14s
                      Time elapsed: 00:23:47
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 51614 steps/s (collection: 1.802s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 150.9348
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 36.5051
                       Mean reward: 206.41
               Mean episode length: 145.47
    Episode_Reward/reaching_object: 0.4987
     Episode_Reward/lifting_object: 41.1590
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 1.90s
                      Time elapsed: 00:23:49
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 50834 steps/s (collection: 1.841s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.5860
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 36.5058
                       Mean reward: 238.16
               Mean episode length: 164.52
    Episode_Reward/reaching_object: 0.5185
     Episode_Reward/lifting_object: 43.1179
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 1.93s
                      Time elapsed: 00:23:51
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 48318 steps/s (collection: 1.929s, learning 0.106s)
             Mean action noise std: 1.87
          Mean value_function loss: 161.2371
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.5063
                       Mean reward: 209.67
               Mean episode length: 145.49
    Episode_Reward/reaching_object: 0.4936
     Episode_Reward/lifting_object: 41.4910
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.03s
                      Time elapsed: 00:23:53
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 51388 steps/s (collection: 1.812s, learning 0.101s)
             Mean action noise std: 1.87
          Mean value_function loss: 154.7923
               Mean surrogate loss: 0.0140
                 Mean entropy loss: 36.5065
                       Mean reward: 214.30
               Mean episode length: 149.14
    Episode_Reward/reaching_object: 0.4951
     Episode_Reward/lifting_object: 41.9868
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 1.91s
                      Time elapsed: 00:23:55
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 52045 steps/s (collection: 1.790s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 149.9925
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.5065
                       Mean reward: 225.72
               Mean episode length: 154.34
    Episode_Reward/reaching_object: 0.5006
     Episode_Reward/lifting_object: 42.3363
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 1.89s
                      Time elapsed: 00:23:57
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 52567 steps/s (collection: 1.784s, learning 0.087s)
             Mean action noise std: 1.87
          Mean value_function loss: 158.6523
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 36.5065
                       Mean reward: 239.25
               Mean episode length: 160.78
    Episode_Reward/reaching_object: 0.4932
     Episode_Reward/lifting_object: 42.3303
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 1.87s
                      Time elapsed: 00:23:59
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 52383 steps/s (collection: 1.787s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 151.5694
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5070
                       Mean reward: 222.41
               Mean episode length: 150.89
    Episode_Reward/reaching_object: 0.4997
     Episode_Reward/lifting_object: 43.0765
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 1.88s
                      Time elapsed: 00:24:01
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 51904 steps/s (collection: 1.794s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.4093
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 36.5080
                       Mean reward: 233.74
               Mean episode length: 155.13
    Episode_Reward/reaching_object: 0.4958
     Episode_Reward/lifting_object: 42.1494
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 1.89s
                      Time elapsed: 00:24:03
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 50179 steps/s (collection: 1.871s, learning 0.088s)
             Mean action noise std: 1.87
          Mean value_function loss: 155.0778
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5090
                       Mean reward: 226.40
               Mean episode length: 153.07
    Episode_Reward/reaching_object: 0.5034
     Episode_Reward/lifting_object: 43.3071
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 1.96s
                      Time elapsed: 00:24:05
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 50389 steps/s (collection: 1.852s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 150.7855
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 36.5102
                       Mean reward: 216.65
               Mean episode length: 149.24
    Episode_Reward/reaching_object: 0.4908
     Episode_Reward/lifting_object: 42.2414
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 1.95s
                      Time elapsed: 00:24:07
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 51478 steps/s (collection: 1.820s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 165.0172
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.5105
                       Mean reward: 241.31
               Mean episode length: 158.86
    Episode_Reward/reaching_object: 0.5105
     Episode_Reward/lifting_object: 44.1818
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 1.91s
                      Time elapsed: 00:24:09
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 46716 steps/s (collection: 1.970s, learning 0.134s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.2455
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 36.5130
                       Mean reward: 240.10
               Mean episode length: 160.34
    Episode_Reward/reaching_object: 0.4984
     Episode_Reward/lifting_object: 42.8867
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.10s
                      Time elapsed: 00:24:11
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 47947 steps/s (collection: 1.963s, learning 0.087s)
             Mean action noise std: 1.87
          Mean value_function loss: 151.8308
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.5140
                       Mean reward: 204.15
               Mean episode length: 146.32
    Episode_Reward/reaching_object: 0.4957
     Episode_Reward/lifting_object: 42.4791
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.05s
                      Time elapsed: 00:24:13
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 49880 steps/s (collection: 1.883s, learning 0.088s)
             Mean action noise std: 1.87
          Mean value_function loss: 163.1705
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.5123
                       Mean reward: 211.06
               Mean episode length: 144.46
    Episode_Reward/reaching_object: 0.4991
     Episode_Reward/lifting_object: 43.3814
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 1.97s
                      Time elapsed: 00:24:15
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 51533 steps/s (collection: 1.809s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.5846
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.5117
                       Mean reward: 241.64
               Mean episode length: 159.53
    Episode_Reward/reaching_object: 0.5069
     Episode_Reward/lifting_object: 44.3598
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 1.91s
                      Time elapsed: 00:24:17
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 46793 steps/s (collection: 1.989s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 149.6425
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.5133
                       Mean reward: 244.45
               Mean episode length: 166.57
    Episode_Reward/reaching_object: 0.5121
     Episode_Reward/lifting_object: 44.3250
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.10s
                      Time elapsed: 00:24:19
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 47854 steps/s (collection: 1.952s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 149.9054
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 36.5142
                       Mean reward: 222.80
               Mean episode length: 156.65
    Episode_Reward/reaching_object: 0.5184
     Episode_Reward/lifting_object: 45.1543
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.05s
                      Time elapsed: 00:24:21
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 48744 steps/s (collection: 1.903s, learning 0.114s)
             Mean action noise std: 1.87
          Mean value_function loss: 159.2439
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 36.5144
                       Mean reward: 223.42
               Mean episode length: 152.45
    Episode_Reward/reaching_object: 0.5080
     Episode_Reward/lifting_object: 43.5452
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.02s
                      Time elapsed: 00:24:23
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 47087 steps/s (collection: 1.977s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 153.3999
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 36.5146
                       Mean reward: 214.90
               Mean episode length: 144.95
    Episode_Reward/reaching_object: 0.5060
     Episode_Reward/lifting_object: 44.7716
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.09s
                      Time elapsed: 00:24:25
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 47447 steps/s (collection: 1.969s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.0842
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.5147
                       Mean reward: 239.71
               Mean episode length: 157.42
    Episode_Reward/reaching_object: 0.5165
     Episode_Reward/lifting_object: 45.0352
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.07s
                      Time elapsed: 00:24:27
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 43042 steps/s (collection: 2.147s, learning 0.137s)
             Mean action noise std: 1.87
          Mean value_function loss: 158.8757
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.5152
                       Mean reward: 224.00
               Mean episode length: 151.70
    Episode_Reward/reaching_object: 0.5023
     Episode_Reward/lifting_object: 44.3463
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.28s
                      Time elapsed: 00:24:29
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47338 steps/s (collection: 1.963s, learning 0.114s)
             Mean action noise std: 1.87
          Mean value_function loss: 153.1993
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.5159
                       Mean reward: 227.96
               Mean episode length: 149.07
    Episode_Reward/reaching_object: 0.5098
     Episode_Reward/lifting_object: 44.9251
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.08s
                      Time elapsed: 00:24:31
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 49683 steps/s (collection: 1.874s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 164.7668
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.5174
                       Mean reward: 216.83
               Mean episode length: 143.59
    Episode_Reward/reaching_object: 0.5143
     Episode_Reward/lifting_object: 45.6021
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 1.98s
                      Time elapsed: 00:24:33
                               ETA: 00:45:26

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 50596 steps/s (collection: 1.835s, learning 0.108s)
             Mean action noise std: 1.87
          Mean value_function loss: 163.4144
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.5174
                       Mean reward: 224.24
               Mean episode length: 148.43
    Episode_Reward/reaching_object: 0.5059
     Episode_Reward/lifting_object: 45.3016
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 1.94s
                      Time elapsed: 00:24:35
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 51167 steps/s (collection: 1.825s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 150.0490
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5169
                       Mean reward: 225.57
               Mean episode length: 151.84
    Episode_Reward/reaching_object: 0.5108
     Episode_Reward/lifting_object: 44.8324
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 1.92s
                      Time elapsed: 00:24:37
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 48692 steps/s (collection: 1.917s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 150.6715
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 36.5175
                       Mean reward: 243.82
               Mean episode length: 159.25
    Episode_Reward/reaching_object: 0.4986
     Episode_Reward/lifting_object: 44.3859
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.02s
                      Time elapsed: 00:24:39
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 45456 steps/s (collection: 2.046s, learning 0.117s)
             Mean action noise std: 1.87
          Mean value_function loss: 161.3799
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.5177
                       Mean reward: 214.22
               Mean episode length: 143.36
    Episode_Reward/reaching_object: 0.5137
     Episode_Reward/lifting_object: 45.1463
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.16s
                      Time elapsed: 00:24:41
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 48010 steps/s (collection: 1.955s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 160.2266
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.5176
                       Mean reward: 222.79
               Mean episode length: 147.29
    Episode_Reward/reaching_object: 0.4966
     Episode_Reward/lifting_object: 44.4662
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.05s
                      Time elapsed: 00:24:43
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 49601 steps/s (collection: 1.872s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 162.6013
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.5177
                       Mean reward: 227.85
               Mean episode length: 151.59
    Episode_Reward/reaching_object: 0.5051
     Episode_Reward/lifting_object: 45.1199
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.98s
                      Time elapsed: 00:24:45
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 49128 steps/s (collection: 1.856s, learning 0.145s)
             Mean action noise std: 1.87
          Mean value_function loss: 163.4710
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 36.5184
                       Mean reward: 213.19
               Mean episode length: 148.15
    Episode_Reward/reaching_object: 0.5116
     Episode_Reward/lifting_object: 46.5680
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.00s
                      Time elapsed: 00:24:47
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 44104 steps/s (collection: 2.109s, learning 0.120s)
             Mean action noise std: 1.87
          Mean value_function loss: 163.3244
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 36.5192
                       Mean reward: 256.22
               Mean episode length: 161.08
    Episode_Reward/reaching_object: 0.4934
     Episode_Reward/lifting_object: 44.8844
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.23s
                      Time elapsed: 00:24:50
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 47467 steps/s (collection: 1.975s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 178.9160
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.5207
                       Mean reward: 219.51
               Mean episode length: 143.75
    Episode_Reward/reaching_object: 0.5019
     Episode_Reward/lifting_object: 45.4113
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.07s
                      Time elapsed: 00:24:52
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 51096 steps/s (collection: 1.812s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 178.1951
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.5226
                       Mean reward: 205.61
               Mean episode length: 137.43
    Episode_Reward/reaching_object: 0.5022
     Episode_Reward/lifting_object: 45.8346
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 1.92s
                      Time elapsed: 00:24:54
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 50107 steps/s (collection: 1.847s, learning 0.115s)
             Mean action noise std: 1.87
          Mean value_function loss: 174.5515
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.5233
                       Mean reward: 226.87
               Mean episode length: 148.14
    Episode_Reward/reaching_object: 0.4869
     Episode_Reward/lifting_object: 44.1796
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 1.96s
                      Time elapsed: 00:24:55
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 49153 steps/s (collection: 1.875s, learning 0.125s)
             Mean action noise std: 1.87
          Mean value_function loss: 170.6369
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.5233
                       Mean reward: 225.03
               Mean episode length: 141.99
    Episode_Reward/reaching_object: 0.4820
     Episode_Reward/lifting_object: 44.4298
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.00s
                      Time elapsed: 00:24:57
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 51071 steps/s (collection: 1.834s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 175.1556
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.5226
                       Mean reward: 231.09
               Mean episode length: 150.69
    Episode_Reward/reaching_object: 0.4807
     Episode_Reward/lifting_object: 43.9054
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 1.92s
                      Time elapsed: 00:24:59
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 51715 steps/s (collection: 1.807s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 182.0923
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.5227
                       Mean reward: 221.79
               Mean episode length: 139.68
    Episode_Reward/reaching_object: 0.4722
     Episode_Reward/lifting_object: 43.7160
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 1.90s
                      Time elapsed: 00:25:01
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 50907 steps/s (collection: 1.833s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 179.5277
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 36.5236
                       Mean reward: 218.77
               Mean episode length: 143.22
    Episode_Reward/reaching_object: 0.4803
     Episode_Reward/lifting_object: 44.1520
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 1.93s
                      Time elapsed: 00:25:03
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 49316 steps/s (collection: 1.897s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 191.6967
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 36.5244
                       Mean reward: 234.86
               Mean episode length: 149.68
    Episode_Reward/reaching_object: 0.4864
     Episode_Reward/lifting_object: 44.6796
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.99s
                      Time elapsed: 00:25:05
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 50465 steps/s (collection: 1.841s, learning 0.107s)
             Mean action noise std: 1.87
          Mean value_function loss: 200.6792
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 36.5248
                       Mean reward: 200.39
               Mean episode length: 122.81
    Episode_Reward/reaching_object: 0.4552
     Episode_Reward/lifting_object: 42.0936
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 1.95s
                      Time elapsed: 00:25:07
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 50763 steps/s (collection: 1.845s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 194.1152
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.5259
                       Mean reward: 217.28
               Mean episode length: 140.35
    Episode_Reward/reaching_object: 0.4546
     Episode_Reward/lifting_object: 42.1893
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.94s
                      Time elapsed: 00:25:09
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 51175 steps/s (collection: 1.825s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 197.8063
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.5276
                       Mean reward: 225.37
               Mean episode length: 140.80
    Episode_Reward/reaching_object: 0.4610
     Episode_Reward/lifting_object: 42.9307
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 1.92s
                      Time elapsed: 00:25:11
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 50175 steps/s (collection: 1.847s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 196.5676
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.5289
                       Mean reward: 225.09
               Mean episode length: 140.13
    Episode_Reward/reaching_object: 0.4700
     Episode_Reward/lifting_object: 43.7646
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 1.96s
                      Time elapsed: 00:25:13
                               ETA: 00:44:41

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 51733 steps/s (collection: 1.806s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 193.0815
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 36.5298
                       Mean reward: 210.46
               Mean episode length: 132.15
    Episode_Reward/reaching_object: 0.4514
     Episode_Reward/lifting_object: 42.3244
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 1.90s
                      Time elapsed: 00:25:15
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 51252 steps/s (collection: 1.818s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 190.7500
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.5304
                       Mean reward: 191.01
               Mean episode length: 123.67
    Episode_Reward/reaching_object: 0.4463
     Episode_Reward/lifting_object: 41.3656
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 1.92s
                      Time elapsed: 00:25:17
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 50134 steps/s (collection: 1.851s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 193.6620
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.5305
                       Mean reward: 207.26
               Mean episode length: 133.44
    Episode_Reward/reaching_object: 0.4500
     Episode_Reward/lifting_object: 42.2729
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 1.96s
                      Time elapsed: 00:25:19
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 51295 steps/s (collection: 1.826s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 186.0821
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.5323
                       Mean reward: 248.86
               Mean episode length: 158.13
    Episode_Reward/reaching_object: 0.4884
     Episode_Reward/lifting_object: 45.3411
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 1.92s
                      Time elapsed: 00:25:21
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 51131 steps/s (collection: 1.823s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 214.8085
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.5340
                       Mean reward: 224.81
               Mean episode length: 139.02
    Episode_Reward/reaching_object: 0.4762
     Episode_Reward/lifting_object: 44.6290
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 1.92s
                      Time elapsed: 00:25:23
                               ETA: 00:44:29

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 50981 steps/s (collection: 1.838s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 192.8912
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.5349
                       Mean reward: 211.58
               Mean episode length: 137.12
    Episode_Reward/reaching_object: 0.4744
     Episode_Reward/lifting_object: 43.4035
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 1.93s
                      Time elapsed: 00:25:25
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 50710 steps/s (collection: 1.851s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 183.1638
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 36.5361
                       Mean reward: 233.77
               Mean episode length: 143.59
    Episode_Reward/reaching_object: 0.5029
     Episode_Reward/lifting_object: 47.0760
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.94s
                      Time elapsed: 00:25:26
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 50487 steps/s (collection: 1.841s, learning 0.107s)
             Mean action noise std: 1.88
          Mean value_function loss: 191.4050
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.5364
                       Mean reward: 207.62
               Mean episode length: 132.60
    Episode_Reward/reaching_object: 0.5041
     Episode_Reward/lifting_object: 47.4171
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 1.95s
                      Time elapsed: 00:25:28
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 50333 steps/s (collection: 1.840s, learning 0.114s)
             Mean action noise std: 1.88
          Mean value_function loss: 200.5679
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.5369
                       Mean reward: 243.55
               Mean episode length: 147.97
    Episode_Reward/reaching_object: 0.4906
     Episode_Reward/lifting_object: 45.9617
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 1.95s
                      Time elapsed: 00:25:30
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 50828 steps/s (collection: 1.835s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 195.5639
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 36.5376
                       Mean reward: 214.96
               Mean episode length: 139.97
    Episode_Reward/reaching_object: 0.4958
     Episode_Reward/lifting_object: 46.3529
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 1.93s
                      Time elapsed: 00:25:32
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 50654 steps/s (collection: 1.851s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 180.8156
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.5378
                       Mean reward: 245.41
               Mean episode length: 148.69
    Episode_Reward/reaching_object: 0.4912
     Episode_Reward/lifting_object: 46.2228
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 1.94s
                      Time elapsed: 00:25:34
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 50993 steps/s (collection: 1.824s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 188.0715
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.5373
                       Mean reward: 215.14
               Mean episode length: 134.65
    Episode_Reward/reaching_object: 0.4901
     Episode_Reward/lifting_object: 45.4841
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 1.93s
                      Time elapsed: 00:25:36
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 51215 steps/s (collection: 1.818s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 185.6686
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.5383
                       Mean reward: 235.71
               Mean episode length: 144.73
    Episode_Reward/reaching_object: 0.4889
     Episode_Reward/lifting_object: 45.9380
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 1.92s
                      Time elapsed: 00:25:38
                               ETA: 00:44:10

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 51059 steps/s (collection: 1.835s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 184.7096
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.5408
                       Mean reward: 252.03
               Mean episode length: 153.62
    Episode_Reward/reaching_object: 0.4972
     Episode_Reward/lifting_object: 46.3749
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 1.93s
                      Time elapsed: 00:25:40
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 50925 steps/s (collection: 1.843s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 192.4546
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.5444
                       Mean reward: 243.04
               Mean episode length: 150.33
    Episode_Reward/reaching_object: 0.5051
     Episode_Reward/lifting_object: 47.0485
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 1.93s
                      Time elapsed: 00:25:42
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 49761 steps/s (collection: 1.887s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 185.4958
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 36.5476
                       Mean reward: 239.80
               Mean episode length: 142.56
    Episode_Reward/reaching_object: 0.5217
     Episode_Reward/lifting_object: 48.7068
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 1.98s
                      Time elapsed: 00:25:44
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 50483 steps/s (collection: 1.853s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 203.0098
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.5481
                       Mean reward: 215.44
               Mean episode length: 136.24
    Episode_Reward/reaching_object: 0.4920
     Episode_Reward/lifting_object: 45.3712
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 1.95s
                      Time elapsed: 00:25:46
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 51243 steps/s (collection: 1.830s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 201.7516
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5485
                       Mean reward: 217.40
               Mean episode length: 142.07
    Episode_Reward/reaching_object: 0.4829
     Episode_Reward/lifting_object: 44.9217
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 1.92s
                      Time elapsed: 00:25:48
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 50003 steps/s (collection: 1.862s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 191.0697
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5489
                       Mean reward: 263.71
               Mean episode length: 158.90
    Episode_Reward/reaching_object: 0.5291
     Episode_Reward/lifting_object: 49.7211
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 1.97s
                      Time elapsed: 00:25:50
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 49193 steps/s (collection: 1.894s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 207.6742
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.5497
                       Mean reward: 209.74
               Mean episode length: 134.48
    Episode_Reward/reaching_object: 0.4923
     Episode_Reward/lifting_object: 45.8952
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.00s
                      Time elapsed: 00:25:52
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 50092 steps/s (collection: 1.874s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 191.2109
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.5497
                       Mean reward: 241.18
               Mean episode length: 143.68
    Episode_Reward/reaching_object: 0.5144
     Episode_Reward/lifting_object: 49.0675
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 1.96s
                      Time elapsed: 00:25:54
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 50115 steps/s (collection: 1.859s, learning 0.102s)
             Mean action noise std: 1.88
          Mean value_function loss: 201.8791
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.5501
                       Mean reward: 251.97
               Mean episode length: 153.21
    Episode_Reward/reaching_object: 0.5069
     Episode_Reward/lifting_object: 48.2140
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 1.96s
                      Time elapsed: 00:25:56
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 50464 steps/s (collection: 1.849s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 208.9899
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.5519
                       Mean reward: 228.71
               Mean episode length: 138.13
    Episode_Reward/reaching_object: 0.5080
     Episode_Reward/lifting_object: 48.9629
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 1.95s
                      Time elapsed: 00:25:58
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 49262 steps/s (collection: 1.894s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 199.3995
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.5536
                       Mean reward: 240.39
               Mean episode length: 148.15
    Episode_Reward/reaching_object: 0.5087
     Episode_Reward/lifting_object: 48.7952
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.00s
                      Time elapsed: 00:26:00
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 50623 steps/s (collection: 1.855s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 199.8590
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 36.5565
                       Mean reward: 239.26
               Mean episode length: 145.87
    Episode_Reward/reaching_object: 0.5098
     Episode_Reward/lifting_object: 48.9696
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 1.94s
                      Time elapsed: 00:26:02
                               ETA: 00:43:42

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 50725 steps/s (collection: 1.849s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 198.7836
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.5587
                       Mean reward: 258.39
               Mean episode length: 146.48
    Episode_Reward/reaching_object: 0.5227
     Episode_Reward/lifting_object: 51.1179
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 1.94s
                      Time elapsed: 00:26:04
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 50368 steps/s (collection: 1.843s, learning 0.108s)
             Mean action noise std: 1.88
          Mean value_function loss: 191.1173
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 36.5580
                       Mean reward: 271.17
               Mean episode length: 161.85
    Episode_Reward/reaching_object: 0.5055
     Episode_Reward/lifting_object: 49.3669
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 1.95s
                      Time elapsed: 00:26:05
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 51074 steps/s (collection: 1.838s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 202.9015
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.5586
                       Mean reward: 241.41
               Mean episode length: 141.84
    Episode_Reward/reaching_object: 0.5100
     Episode_Reward/lifting_object: 49.9522
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 1.92s
                      Time elapsed: 00:26:07
                               ETA: 00:43:35

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 49111 steps/s (collection: 1.896s, learning 0.106s)
             Mean action noise std: 1.88
          Mean value_function loss: 215.3723
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.5596
                       Mean reward: 243.08
               Mean episode length: 138.70
    Episode_Reward/reaching_object: 0.5029
     Episode_Reward/lifting_object: 49.8452
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.00s
                      Time elapsed: 00:26:09
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 50622 steps/s (collection: 1.851s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 212.9736
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5588
                       Mean reward: 238.35
               Mean episode length: 138.83
    Episode_Reward/reaching_object: 0.4978
     Episode_Reward/lifting_object: 49.2706
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 1.94s
                      Time elapsed: 00:26:11
                               ETA: 00:43:30

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 51042 steps/s (collection: 1.842s, learning 0.084s)
             Mean action noise std: 1.88
          Mean value_function loss: 219.6252
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.5599
                       Mean reward: 234.59
               Mean episode length: 132.85
    Episode_Reward/reaching_object: 0.4901
     Episode_Reward/lifting_object: 49.0921
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 1.93s
                      Time elapsed: 00:26:13
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 49224 steps/s (collection: 1.906s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 225.9612
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.5636
                       Mean reward: 221.33
               Mean episode length: 136.62
    Episode_Reward/reaching_object: 0.4968
     Episode_Reward/lifting_object: 49.8539
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.00s
                      Time elapsed: 00:26:15
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 49530 steps/s (collection: 1.880s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 241.4225
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.5649
                       Mean reward: 246.56
               Mean episode length: 139.74
    Episode_Reward/reaching_object: 0.4873
     Episode_Reward/lifting_object: 48.2804
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 1.98s
                      Time elapsed: 00:26:17
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 48021 steps/s (collection: 1.930s, learning 0.118s)
             Mean action noise std: 1.88
          Mean value_function loss: 226.2898
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.5658
                       Mean reward: 240.06
               Mean episode length: 135.45
    Episode_Reward/reaching_object: 0.4847
     Episode_Reward/lifting_object: 49.3734
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.05s
                      Time elapsed: 00:26:19
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 49826 steps/s (collection: 1.873s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 229.1264
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.5675
                       Mean reward: 244.07
               Mean episode length: 135.30
    Episode_Reward/reaching_object: 0.4565
     Episode_Reward/lifting_object: 46.7289
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 1.97s
                      Time elapsed: 00:26:21
                               ETA: 00:43:19

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 50860 steps/s (collection: 1.846s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 244.5823
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5687
                       Mean reward: 239.68
               Mean episode length: 138.61
    Episode_Reward/reaching_object: 0.4541
     Episode_Reward/lifting_object: 46.6560
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 1.93s
                      Time elapsed: 00:26:23
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 50741 steps/s (collection: 1.847s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 249.3354
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.5670
                       Mean reward: 230.40
               Mean episode length: 130.31
    Episode_Reward/reaching_object: 0.4471
     Episode_Reward/lifting_object: 45.9125
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 1.94s
                      Time elapsed: 00:26:25
                               ETA: 00:43:14

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 50574 steps/s (collection: 1.857s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 242.5637
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 36.5671
                       Mean reward: 229.21
               Mean episode length: 127.38
    Episode_Reward/reaching_object: 0.4395
     Episode_Reward/lifting_object: 45.0390
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 1.94s
                      Time elapsed: 00:26:27
                               ETA: 00:43:12

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 49234 steps/s (collection: 1.893s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 259.7408
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.5687
                       Mean reward: 252.67
               Mean episode length: 141.28
    Episode_Reward/reaching_object: 0.4679
     Episode_Reward/lifting_object: 48.5221
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.00s
                      Time elapsed: 00:26:29
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 50498 steps/s (collection: 1.840s, learning 0.107s)
             Mean action noise std: 1.88
          Mean value_function loss: 242.0860
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.5697
                       Mean reward: 250.69
               Mean episode length: 136.88
    Episode_Reward/reaching_object: 0.4613
     Episode_Reward/lifting_object: 47.7731
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 1.95s
                      Time elapsed: 00:26:31
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 50287 steps/s (collection: 1.849s, learning 0.106s)
             Mean action noise std: 1.88
          Mean value_function loss: 255.0602
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.5695
                       Mean reward: 238.87
               Mean episode length: 132.50
    Episode_Reward/reaching_object: 0.4484
     Episode_Reward/lifting_object: 46.8576
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 1.95s
                      Time elapsed: 00:26:33
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 50175 steps/s (collection: 1.865s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 263.3167
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5706
                       Mean reward: 228.66
               Mean episode length: 126.70
    Episode_Reward/reaching_object: 0.4315
     Episode_Reward/lifting_object: 44.9218
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 1.96s
                      Time elapsed: 00:26:35
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 48000 steps/s (collection: 1.939s, learning 0.109s)
             Mean action noise std: 1.88
          Mean value_function loss: 261.4664
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.5708
                       Mean reward: 227.53
               Mean episode length: 126.06
    Episode_Reward/reaching_object: 0.4344
     Episode_Reward/lifting_object: 45.6746
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.05s
                      Time elapsed: 00:26:37
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 50943 steps/s (collection: 1.840s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 268.4625
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.5720
                       Mean reward: 225.43
               Mean episode length: 125.45
    Episode_Reward/reaching_object: 0.4358
     Episode_Reward/lifting_object: 45.8742
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 1.93s
                      Time elapsed: 00:26:39
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 50196 steps/s (collection: 1.867s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 249.1774
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.5705
                       Mean reward: 231.45
               Mean episode length: 128.78
    Episode_Reward/reaching_object: 0.4250
     Episode_Reward/lifting_object: 45.0850
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 1.96s
                      Time elapsed: 00:26:41
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 49661 steps/s (collection: 1.880s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 250.2921
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.5696
                       Mean reward: 225.08
               Mean episode length: 123.06
    Episode_Reward/reaching_object: 0.4306
     Episode_Reward/lifting_object: 46.0211
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 1.98s
                      Time elapsed: 00:26:43
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 49667 steps/s (collection: 1.890s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 252.6488
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.5691
                       Mean reward: 216.11
               Mean episode length: 124.77
    Episode_Reward/reaching_object: 0.4301
     Episode_Reward/lifting_object: 45.1455
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 1.98s
                      Time elapsed: 00:26:45
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 49054 steps/s (collection: 1.884s, learning 0.120s)
             Mean action noise std: 1.88
          Mean value_function loss: 264.8521
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.5712
                       Mean reward: 219.04
               Mean episode length: 118.13
    Episode_Reward/reaching_object: 0.4346
     Episode_Reward/lifting_object: 46.6971
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.00s
                      Time elapsed: 00:26:47
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 50159 steps/s (collection: 1.856s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 263.1241
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.5729
                       Mean reward: 233.71
               Mean episode length: 126.42
    Episode_Reward/reaching_object: 0.4467
     Episode_Reward/lifting_object: 48.3274
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 1.96s
                      Time elapsed: 00:26:49
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 49605 steps/s (collection: 1.872s, learning 0.110s)
             Mean action noise std: 1.89
          Mean value_function loss: 285.7944
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.5753
                       Mean reward: 242.32
               Mean episode length: 126.74
    Episode_Reward/reaching_object: 0.4411
     Episode_Reward/lifting_object: 47.9166
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 1.98s
                      Time elapsed: 00:26:51
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 49638 steps/s (collection: 1.880s, learning 0.100s)
             Mean action noise std: 1.89
          Mean value_function loss: 276.9613
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.5782
                       Mean reward: 242.89
               Mean episode length: 131.79
    Episode_Reward/reaching_object: 0.4485
     Episode_Reward/lifting_object: 48.9220
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 1.98s
                      Time elapsed: 00:26:53
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 48744 steps/s (collection: 1.914s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 271.6187
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.5806
                       Mean reward: 247.26
               Mean episode length: 128.02
    Episode_Reward/reaching_object: 0.4539
     Episode_Reward/lifting_object: 49.5421
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.02s
                      Time elapsed: 00:26:55
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 50190 steps/s (collection: 1.872s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 276.8491
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.5828
                       Mean reward: 259.94
               Mean episode length: 127.49
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 49.0681
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 1.96s
                      Time elapsed: 00:26:57
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 49180 steps/s (collection: 1.914s, learning 0.085s)
             Mean action noise std: 1.89
          Mean value_function loss: 288.6624
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 36.5831
                       Mean reward: 234.69
               Mean episode length: 124.69
    Episode_Reward/reaching_object: 0.4485
     Episode_Reward/lifting_object: 50.0432
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.00s
                      Time elapsed: 00:26:59
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 48591 steps/s (collection: 1.937s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 290.7328
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 36.5836
                       Mean reward: 243.60
               Mean episode length: 121.67
    Episode_Reward/reaching_object: 0.4470
     Episode_Reward/lifting_object: 50.1643
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.02s
                      Time elapsed: 00:27:01
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 49494 steps/s (collection: 1.890s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 278.9580
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.5861
                       Mean reward: 237.02
               Mean episode length: 122.56
    Episode_Reward/reaching_object: 0.4336
     Episode_Reward/lifting_object: 49.2267
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 1.99s
                      Time elapsed: 00:27:03
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 50071 steps/s (collection: 1.865s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 287.9105
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.5910
                       Mean reward: 260.40
               Mean episode length: 127.46
    Episode_Reward/reaching_object: 0.4484
     Episode_Reward/lifting_object: 51.5823
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 1.96s
                      Time elapsed: 00:27:05
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 50073 steps/s (collection: 1.864s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 269.6597
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.5932
                       Mean reward: 253.50
               Mean episode length: 134.14
    Episode_Reward/reaching_object: 0.4433
     Episode_Reward/lifting_object: 49.9992
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 1.96s
                      Time elapsed: 00:27:07
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 50723 steps/s (collection: 1.849s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 281.2802
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.5947
                       Mean reward: 250.59
               Mean episode length: 128.05
    Episode_Reward/reaching_object: 0.4414
     Episode_Reward/lifting_object: 50.4999
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 1.94s
                      Time elapsed: 00:27:09
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 51101 steps/s (collection: 1.838s, learning 0.085s)
             Mean action noise std: 1.89
          Mean value_function loss: 294.4785
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.5940
                       Mean reward: 273.08
               Mean episode length: 136.08
    Episode_Reward/reaching_object: 0.4452
     Episode_Reward/lifting_object: 51.5198
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 1.92s
                      Time elapsed: 00:27:11
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 50840 steps/s (collection: 1.848s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 295.3784
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.5928
                       Mean reward: 251.49
               Mean episode length: 135.39
    Episode_Reward/reaching_object: 0.4613
     Episode_Reward/lifting_object: 53.3708
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 1.93s
                      Time elapsed: 00:27:12
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 50688 steps/s (collection: 1.852s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 295.8538
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 36.5933
                       Mean reward: 272.83
               Mean episode length: 139.35
    Episode_Reward/reaching_object: 0.4369
     Episode_Reward/lifting_object: 50.1530
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 1.94s
                      Time elapsed: 00:27:14
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 50639 steps/s (collection: 1.850s, learning 0.092s)
             Mean action noise std: 1.89
          Mean value_function loss: 304.6707
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.5936
                       Mean reward: 280.84
               Mean episode length: 140.70
    Episode_Reward/reaching_object: 0.4413
     Episode_Reward/lifting_object: 51.3001
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 1.94s
                      Time elapsed: 00:27:16
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 50095 steps/s (collection: 1.862s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 295.6060
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.5920
                       Mean reward: 268.69
               Mean episode length: 134.70
    Episode_Reward/reaching_object: 0.4480
     Episode_Reward/lifting_object: 52.9687
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 1.96s
                      Time elapsed: 00:27:18
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 50084 steps/s (collection: 1.870s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 311.6517
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.5912
                       Mean reward: 248.95
               Mean episode length: 127.93
    Episode_Reward/reaching_object: 0.4349
     Episode_Reward/lifting_object: 51.2649
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 1.96s
                      Time elapsed: 00:27:20
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 49806 steps/s (collection: 1.880s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 317.8443
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.5922
                       Mean reward: 284.08
               Mean episode length: 137.24
    Episode_Reward/reaching_object: 0.4515
     Episode_Reward/lifting_object: 53.9014
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 1.97s
                      Time elapsed: 00:27:22
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 50766 steps/s (collection: 1.842s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 311.2113
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.5924
                       Mean reward: 280.47
               Mean episode length: 132.52
    Episode_Reward/reaching_object: 0.4532
     Episode_Reward/lifting_object: 54.5472
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 1.94s
                      Time elapsed: 00:27:24
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 50210 steps/s (collection: 1.863s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 328.1558
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.5925
                       Mean reward: 257.16
               Mean episode length: 127.29
    Episode_Reward/reaching_object: 0.4367
     Episode_Reward/lifting_object: 52.6480
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 1.96s
                      Time elapsed: 00:27:26
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 50960 steps/s (collection: 1.836s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 331.6588
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.5913
                       Mean reward: 288.17
               Mean episode length: 132.86
    Episode_Reward/reaching_object: 0.4433
     Episode_Reward/lifting_object: 54.0639
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 1.93s
                      Time elapsed: 00:27:28
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 50157 steps/s (collection: 1.865s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 331.3596
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.5900
                       Mean reward: 245.95
               Mean episode length: 121.86
    Episode_Reward/reaching_object: 0.4328
     Episode_Reward/lifting_object: 53.5389
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 1.96s
                      Time elapsed: 00:27:30
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 49890 steps/s (collection: 1.884s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 357.1610
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.5899
                       Mean reward: 247.78
               Mean episode length: 124.34
    Episode_Reward/reaching_object: 0.4229
     Episode_Reward/lifting_object: 51.5517
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 1.97s
                      Time elapsed: 00:27:32
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 50588 steps/s (collection: 1.846s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 348.9506
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.5915
                       Mean reward: 274.83
               Mean episode length: 134.23
    Episode_Reward/reaching_object: 0.4415
     Episode_Reward/lifting_object: 54.7342
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 1.94s
                      Time elapsed: 00:27:34
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 50558 steps/s (collection: 1.852s, learning 0.092s)
             Mean action noise std: 1.89
          Mean value_function loss: 351.0594
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.5909
                       Mean reward: 263.69
               Mean episode length: 126.71
    Episode_Reward/reaching_object: 0.4093
     Episode_Reward/lifting_object: 50.8641
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 1.94s
                      Time elapsed: 00:27:36
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 50857 steps/s (collection: 1.846s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 340.0878
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.5898
                       Mean reward: 280.88
               Mean episode length: 124.67
    Episode_Reward/reaching_object: 0.4123
     Episode_Reward/lifting_object: 51.7800
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 1.93s
                      Time elapsed: 00:27:38
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 50665 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 356.8951
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5905
                       Mean reward: 248.33
               Mean episode length: 116.64
    Episode_Reward/reaching_object: 0.3983
     Episode_Reward/lifting_object: 49.8975
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 1.94s
                      Time elapsed: 00:27:40
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 48895 steps/s (collection: 1.889s, learning 0.121s)
             Mean action noise std: 1.89
          Mean value_function loss: 366.7198
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 36.5892
                       Mean reward: 231.63
               Mean episode length: 113.84
    Episode_Reward/reaching_object: 0.3828
     Episode_Reward/lifting_object: 47.4191
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.01s
                      Time elapsed: 00:27:42
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 50226 steps/s (collection: 1.853s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 367.1832
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.5879
                       Mean reward: 249.08
               Mean episode length: 117.29
    Episode_Reward/reaching_object: 0.3908
     Episode_Reward/lifting_object: 48.5887
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 1.96s
                      Time elapsed: 00:27:44
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 50611 steps/s (collection: 1.842s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 351.4037
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 36.5874
                       Mean reward: 260.26
               Mean episode length: 113.67
    Episode_Reward/reaching_object: 0.3901
     Episode_Reward/lifting_object: 50.3047
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 1.94s
                      Time elapsed: 00:27:46
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 50493 steps/s (collection: 1.840s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 376.3962
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 36.5877
                       Mean reward: 259.66
               Mean episode length: 113.86
    Episode_Reward/reaching_object: 0.3808
     Episode_Reward/lifting_object: 48.8796
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 1.95s
                      Time elapsed: 00:27:48
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 49427 steps/s (collection: 1.880s, learning 0.109s)
             Mean action noise std: 1.89
          Mean value_function loss: 372.5390
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.5876
                       Mean reward: 245.52
               Mean episode length: 108.00
    Episode_Reward/reaching_object: 0.3768
     Episode_Reward/lifting_object: 49.3715
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 1.99s
                      Time elapsed: 00:27:50
                               ETA: 00:41:36

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 49824 steps/s (collection: 1.862s, learning 0.111s)
             Mean action noise std: 1.89
          Mean value_function loss: 381.6778
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.5865
                       Mean reward: 249.28
               Mean episode length: 108.95
    Episode_Reward/reaching_object: 0.3899
     Episode_Reward/lifting_object: 50.9613
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 1.97s
                      Time elapsed: 00:27:52
                               ETA: 00:41:34

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 49820 steps/s (collection: 1.875s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 375.6312
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5834
                       Mean reward: 256.44
               Mean episode length: 111.48
    Episode_Reward/reaching_object: 0.3830
     Episode_Reward/lifting_object: 50.2041
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 1.97s
                      Time elapsed: 00:27:54
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 48567 steps/s (collection: 1.906s, learning 0.119s)
             Mean action noise std: 1.89
          Mean value_function loss: 388.5070
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.5813
                       Mean reward: 238.61
               Mean episode length: 109.40
    Episode_Reward/reaching_object: 0.3990
     Episode_Reward/lifting_object: 52.6338
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.02s
                      Time elapsed: 00:27:56
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 46500 steps/s (collection: 2.018s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 412.4988
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.5824
                       Mean reward: 271.22
               Mean episode length: 116.00
    Episode_Reward/reaching_object: 0.4110
     Episode_Reward/lifting_object: 54.6191
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.11s
                      Time elapsed: 00:27:58
                               ETA: 00:41:28

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 50054 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 393.9935
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.5827
                       Mean reward: 253.52
               Mean episode length: 111.29
    Episode_Reward/reaching_object: 0.3868
     Episode_Reward/lifting_object: 51.7013
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 1.96s
                      Time elapsed: 00:28:00
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 49994 steps/s (collection: 1.880s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 409.1622
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.5808
                       Mean reward: 285.43
               Mean episode length: 120.31
    Episode_Reward/reaching_object: 0.4022
     Episode_Reward/lifting_object: 54.2479
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 1.97s
                      Time elapsed: 00:28:02
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 50289 steps/s (collection: 1.868s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 435.4453
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.5801
                       Mean reward: 257.70
               Mean episode length: 111.89
    Episode_Reward/reaching_object: 0.3995
     Episode_Reward/lifting_object: 53.8052
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 1.95s
                      Time elapsed: 00:28:04
                               ETA: 00:41:21

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 49062 steps/s (collection: 1.913s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 404.5777
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.5804
                       Mean reward: 297.22
               Mean episode length: 119.65
    Episode_Reward/reaching_object: 0.4038
     Episode_Reward/lifting_object: 55.4266
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.00s
                      Time elapsed: 00:28:06
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 50067 steps/s (collection: 1.871s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 446.5969
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.5798
                       Mean reward: 280.23
               Mean episode length: 120.83
    Episode_Reward/reaching_object: 0.4005
     Episode_Reward/lifting_object: 54.6984
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 1.96s
                      Time elapsed: 00:28:08
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 49606 steps/s (collection: 1.873s, learning 0.109s)
             Mean action noise std: 1.89
          Mean value_function loss: 418.6093
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 36.5774
                       Mean reward: 251.36
               Mean episode length: 111.82
    Episode_Reward/reaching_object: 0.3858
     Episode_Reward/lifting_object: 53.2130
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 1.98s
                      Time elapsed: 00:28:10
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 50392 steps/s (collection: 1.848s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 431.6388
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.5760
                       Mean reward: 289.58
               Mean episode length: 118.12
    Episode_Reward/reaching_object: 0.4119
     Episode_Reward/lifting_object: 57.7037
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 1.95s
                      Time elapsed: 00:28:11
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 50029 steps/s (collection: 1.872s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 442.6019
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.5770
                       Mean reward: 276.87
               Mean episode length: 112.61
    Episode_Reward/reaching_object: 0.4035
     Episode_Reward/lifting_object: 57.3029
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.96s
                      Time elapsed: 00:28:13
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 50355 steps/s (collection: 1.867s, learning 0.085s)
             Mean action noise std: 1.89
          Mean value_function loss: 433.3743
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5795
                       Mean reward: 271.18
               Mean episode length: 105.78
    Episode_Reward/reaching_object: 0.3936
     Episode_Reward/lifting_object: 55.2344
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 1.95s
                      Time elapsed: 00:28:15
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 50010 steps/s (collection: 1.869s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 455.9283
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.5808
                       Mean reward: 278.11
               Mean episode length: 112.68
    Episode_Reward/reaching_object: 0.3885
     Episode_Reward/lifting_object: 53.8441
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 1.97s
                      Time elapsed: 00:28:17
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 48546 steps/s (collection: 1.917s, learning 0.108s)
             Mean action noise std: 1.89
          Mean value_function loss: 451.2436
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.5787
                       Mean reward: 274.04
               Mean episode length: 113.84
    Episode_Reward/reaching_object: 0.4020
     Episode_Reward/lifting_object: 56.7785
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.02s
                      Time elapsed: 00:28:19
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 49151 steps/s (collection: 1.891s, learning 0.109s)
             Mean action noise std: 1.89
          Mean value_function loss: 481.3054
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.5767
                       Mean reward: 274.96
               Mean episode length: 112.99
    Episode_Reward/reaching_object: 0.3971
     Episode_Reward/lifting_object: 56.3862
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.00s
                      Time elapsed: 00:28:21
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 49513 steps/s (collection: 1.895s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 509.4097
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.5772
                       Mean reward: 303.96
               Mean episode length: 122.58
    Episode_Reward/reaching_object: 0.4022
     Episode_Reward/lifting_object: 56.9680
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 1.99s
                      Time elapsed: 00:28:23
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 49667 steps/s (collection: 1.880s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 483.4814
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 36.5786
                       Mean reward: 276.69
               Mean episode length: 110.44
    Episode_Reward/reaching_object: 0.3888
     Episode_Reward/lifting_object: 54.9897
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 1.98s
                      Time elapsed: 00:28:25
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 49637 steps/s (collection: 1.884s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 492.5644
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.5804
                       Mean reward: 269.39
               Mean episode length: 111.78
    Episode_Reward/reaching_object: 0.4025
     Episode_Reward/lifting_object: 57.8058
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 1.98s
                      Time elapsed: 00:28:27
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 50663 steps/s (collection: 1.850s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 479.7152
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.5806
                       Mean reward: 299.97
               Mean episode length: 111.94
    Episode_Reward/reaching_object: 0.4077
     Episode_Reward/lifting_object: 59.5095
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 1.94s
                      Time elapsed: 00:28:29
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 49661 steps/s (collection: 1.889s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 599.5089
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.5791
                       Mean reward: 293.52
               Mean episode length: 110.82
    Episode_Reward/reaching_object: 0.4018
     Episode_Reward/lifting_object: 58.9705
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 1.98s
                      Time elapsed: 00:28:31
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 50324 steps/s (collection: 1.852s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 757.3981
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.5778
                       Mean reward: 302.58
               Mean episode length: 114.39
    Episode_Reward/reaching_object: 0.4098
     Episode_Reward/lifting_object: 61.6199
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 1.95s
                      Time elapsed: 00:28:33
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 49247 steps/s (collection: 1.893s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 524.5228
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.5793
                       Mean reward: 327.02
               Mean episode length: 121.95
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: 62.9840
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.00s
                      Time elapsed: 00:28:35
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 50299 steps/s (collection: 1.855s, learning 0.100s)
             Mean action noise std: 1.89
          Mean value_function loss: 524.2217
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.5824
                       Mean reward: 340.65
               Mean episode length: 122.31
    Episode_Reward/reaching_object: 0.4220
     Episode_Reward/lifting_object: 65.0335
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 1.95s
                      Time elapsed: 00:28:37
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 50343 steps/s (collection: 1.854s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 530.2745
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.5831
                       Mean reward: 316.02
               Mean episode length: 115.29
    Episode_Reward/reaching_object: 0.4204
     Episode_Reward/lifting_object: 65.0787
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 1.95s
                      Time elapsed: 00:28:39
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 49590 steps/s (collection: 1.880s, learning 0.103s)
             Mean action noise std: 1.90
          Mean value_function loss: 527.2022
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.5829
                       Mean reward: 338.25
               Mean episode length: 124.86
    Episode_Reward/reaching_object: 0.4168
     Episode_Reward/lifting_object: 64.9962
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.98s
                      Time elapsed: 00:28:41
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 50434 steps/s (collection: 1.860s, learning 0.089s)
             Mean action noise std: 1.90
          Mean value_function loss: 534.1551
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.5860
                       Mean reward: 346.06
               Mean episode length: 119.44
    Episode_Reward/reaching_object: 0.4156
     Episode_Reward/lifting_object: 65.7029
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 1.95s
                      Time elapsed: 00:28:43
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 49583 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 532.5875
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 36.5878
                       Mean reward: 352.44
               Mean episode length: 117.20
    Episode_Reward/reaching_object: 0.4285
     Episode_Reward/lifting_object: 68.9904
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 1.98s
                      Time elapsed: 00:28:45
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 49094 steps/s (collection: 1.888s, learning 0.114s)
             Mean action noise std: 1.90
          Mean value_function loss: 537.6724
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.5902
                       Mean reward: 369.09
               Mean episode length: 125.78
    Episode_Reward/reaching_object: 0.4230
     Episode_Reward/lifting_object: 67.3038
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.00s
                      Time elapsed: 00:28:47
                               ETA: 00:40:32

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 46877 steps/s (collection: 1.974s, learning 0.123s)
             Mean action noise std: 1.90
          Mean value_function loss: 519.8930
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.5948
                       Mean reward: 361.32
               Mean episode length: 125.71
    Episode_Reward/reaching_object: 0.4303
     Episode_Reward/lifting_object: 68.8647
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.10s
                      Time elapsed: 00:28:49
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 46183 steps/s (collection: 2.021s, learning 0.107s)
             Mean action noise std: 1.90
          Mean value_function loss: 536.6174
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 36.5996
                       Mean reward: 348.13
               Mean episode length: 119.74
    Episode_Reward/reaching_object: 0.4390
     Episode_Reward/lifting_object: 71.3200
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.13s
                      Time elapsed: 00:28:51
                               ETA: 00:40:28

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 49057 steps/s (collection: 1.909s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 522.3569
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.6011
                       Mean reward: 351.92
               Mean episode length: 118.07
    Episode_Reward/reaching_object: 0.4234
     Episode_Reward/lifting_object: 68.2126
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.00s
                      Time elapsed: 00:28:53
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 41711 steps/s (collection: 2.168s, learning 0.189s)
             Mean action noise std: 1.90
          Mean value_function loss: 519.0179
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 36.6004
                       Mean reward: 366.35
               Mean episode length: 124.39
    Episode_Reward/reaching_object: 0.4225
     Episode_Reward/lifting_object: 68.5003
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.36s
                      Time elapsed: 00:28:56
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 42020 steps/s (collection: 2.175s, learning 0.165s)
             Mean action noise std: 1.90
          Mean value_function loss: 524.0924
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 36.6012
                       Mean reward: 353.26
               Mean episode length: 122.18
    Episode_Reward/reaching_object: 0.4341
     Episode_Reward/lifting_object: 70.1037
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.34s
                      Time elapsed: 00:28:58
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 42659 steps/s (collection: 2.190s, learning 0.115s)
             Mean action noise std: 1.90
          Mean value_function loss: 502.3996
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.6028
                       Mean reward: 364.83
               Mean episode length: 120.68
    Episode_Reward/reaching_object: 0.4316
     Episode_Reward/lifting_object: 70.5622
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.30s
                      Time elapsed: 00:29:00
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 47916 steps/s (collection: 1.961s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 530.8270
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 36.6033
                       Mean reward: 373.29
               Mean episode length: 125.24
    Episode_Reward/reaching_object: 0.4241
     Episode_Reward/lifting_object: 68.4303
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.05s
                      Time elapsed: 00:29:02
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 46978 steps/s (collection: 1.980s, learning 0.113s)
             Mean action noise std: 1.90
          Mean value_function loss: 525.9965
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.6036
                       Mean reward: 362.23
               Mean episode length: 125.00
    Episode_Reward/reaching_object: 0.4302
     Episode_Reward/lifting_object: 69.2411
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.09s
                      Time elapsed: 00:29:04
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 46435 steps/s (collection: 2.026s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 568.9530
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.6068
                       Mean reward: 375.63
               Mean episode length: 130.31
    Episode_Reward/reaching_object: 0.4393
     Episode_Reward/lifting_object: 70.9080
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.12s
                      Time elapsed: 00:29:06
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 48743 steps/s (collection: 1.925s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 520.2892
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.6101
                       Mean reward: 374.16
               Mean episode length: 127.15
    Episode_Reward/reaching_object: 0.4341
     Episode_Reward/lifting_object: 70.7600
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.02s
                      Time elapsed: 00:29:09
                               ETA: 00:40:12

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 49019 steps/s (collection: 1.914s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 538.2411
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.6111
                       Mean reward: 349.15
               Mean episode length: 116.81
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 71.5068
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.01s
                      Time elapsed: 00:29:11
                               ETA: 00:40:10

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 48558 steps/s (collection: 1.930s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 519.0716
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.6125
                       Mean reward: 358.69
               Mean episode length: 120.76
    Episode_Reward/reaching_object: 0.4411
     Episode_Reward/lifting_object: 72.7018
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.02s
                      Time elapsed: 00:29:13
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 50328 steps/s (collection: 1.867s, learning 0.086s)
             Mean action noise std: 1.90
          Mean value_function loss: 502.2569
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.6124
                       Mean reward: 386.87
               Mean episode length: 131.45
    Episode_Reward/reaching_object: 0.4640
     Episode_Reward/lifting_object: 76.7499
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 1.95s
                      Time elapsed: 00:29:14
                               ETA: 00:40:05

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 49794 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 1.90
          Mean value_function loss: 526.4183
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.6122
                       Mean reward: 368.40
               Mean episode length: 127.85
    Episode_Reward/reaching_object: 0.4691
     Episode_Reward/lifting_object: 77.9186
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 1.97s
                      Time elapsed: 00:29:16
                               ETA: 00:40:03

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 49343 steps/s (collection: 1.907s, learning 0.085s)
             Mean action noise std: 1.90
          Mean value_function loss: 518.3362
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.6149
                       Mean reward: 378.38
               Mean episode length: 124.33
    Episode_Reward/reaching_object: 0.4631
     Episode_Reward/lifting_object: 76.8189
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 1.99s
                      Time elapsed: 00:29:18
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 49570 steps/s (collection: 1.892s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 516.7461
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.6163
                       Mean reward: 378.83
               Mean episode length: 126.91
    Episode_Reward/reaching_object: 0.4691
     Episode_Reward/lifting_object: 78.0031
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 1.98s
                      Time elapsed: 00:29:20
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 49477 steps/s (collection: 1.896s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 535.3808
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.6161
                       Mean reward: 411.37
               Mean episode length: 136.90
    Episode_Reward/reaching_object: 0.4851
     Episode_Reward/lifting_object: 80.5745
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 1.99s
                      Time elapsed: 00:29:22
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 49237 steps/s (collection: 1.906s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 530.2987
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.6162
                       Mean reward: 393.47
               Mean episode length: 127.18
    Episode_Reward/reaching_object: 0.4838
     Episode_Reward/lifting_object: 82.0732
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.00s
                      Time elapsed: 00:29:24
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 43829 steps/s (collection: 2.110s, learning 0.132s)
             Mean action noise std: 1.90
          Mean value_function loss: 506.7431
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.6210
                       Mean reward: 434.79
               Mean episode length: 138.71
    Episode_Reward/reaching_object: 0.4817
     Episode_Reward/lifting_object: 80.6640
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.24s
                      Time elapsed: 00:29:27
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 47123 steps/s (collection: 1.965s, learning 0.121s)
             Mean action noise std: 1.90
          Mean value_function loss: 512.1173
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.6243
                       Mean reward: 454.98
               Mean episode length: 142.81
    Episode_Reward/reaching_object: 0.4935
     Episode_Reward/lifting_object: 83.2754
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.09s
                      Time elapsed: 00:29:29
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 46253 steps/s (collection: 2.013s, learning 0.112s)
             Mean action noise std: 1.90
          Mean value_function loss: 531.6026
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.6280
                       Mean reward: 383.84
               Mean episode length: 125.76
    Episode_Reward/reaching_object: 0.4650
     Episode_Reward/lifting_object: 77.8932
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.13s
                      Time elapsed: 00:29:31
                               ETA: 00:39:48

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 48051 steps/s (collection: 1.952s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 514.4116
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.6329
                       Mean reward: 438.71
               Mean episode length: 138.22
    Episode_Reward/reaching_object: 0.5111
     Episode_Reward/lifting_object: 87.3548
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.05s
                      Time elapsed: 00:29:33
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 49979 steps/s (collection: 1.882s, learning 0.085s)
             Mean action noise std: 1.90
          Mean value_function loss: 506.5083
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.6361
                       Mean reward: 441.35
               Mean episode length: 140.00
    Episode_Reward/reaching_object: 0.5066
     Episode_Reward/lifting_object: 86.2697
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 1.97s
                      Time elapsed: 00:29:35
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 46394 steps/s (collection: 2.022s, learning 0.096s)
             Mean action noise std: 1.90
          Mean value_function loss: 495.1732
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.6376
                       Mean reward: 387.11
               Mean episode length: 128.55
    Episode_Reward/reaching_object: 0.5114
     Episode_Reward/lifting_object: 87.9995
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.12s
                      Time elapsed: 00:29:37
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 45461 steps/s (collection: 2.060s, learning 0.103s)
             Mean action noise std: 1.90
          Mean value_function loss: 504.0526
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.6381
                       Mean reward: 452.43
               Mean episode length: 141.65
    Episode_Reward/reaching_object: 0.4922
     Episode_Reward/lifting_object: 84.2732
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.16s
                      Time elapsed: 00:29:39
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 48196 steps/s (collection: 1.945s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 502.1105
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.6380
                       Mean reward: 440.07
               Mean episode length: 138.87
    Episode_Reward/reaching_object: 0.5081
     Episode_Reward/lifting_object: 86.8529
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.04s
                      Time elapsed: 00:29:41
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 50198 steps/s (collection: 1.872s, learning 0.087s)
             Mean action noise std: 1.90
          Mean value_function loss: 493.0057
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.6385
                       Mean reward: 433.79
               Mean episode length: 139.88
    Episode_Reward/reaching_object: 0.4980
     Episode_Reward/lifting_object: 83.3624
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 1.96s
                      Time elapsed: 00:29:43
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 49690 steps/s (collection: 1.892s, learning 0.087s)
             Mean action noise std: 1.90
          Mean value_function loss: 515.5247
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6415
                       Mean reward: 469.86
               Mean episode length: 146.92
    Episode_Reward/reaching_object: 0.4968
     Episode_Reward/lifting_object: 84.1216
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 1.98s
                      Time elapsed: 00:29:45
                               ETA: 00:39:33

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 50463 steps/s (collection: 1.842s, learning 0.107s)
             Mean action noise std: 1.91
          Mean value_function loss: 512.7728
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6434
                       Mean reward: 437.11
               Mean episode length: 142.12
    Episode_Reward/reaching_object: 0.4895
     Episode_Reward/lifting_object: 82.8074
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 1.95s
                      Time elapsed: 00:29:47
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 50279 steps/s (collection: 1.848s, learning 0.108s)
             Mean action noise std: 1.91
          Mean value_function loss: 510.5968
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.6447
                       Mean reward: 391.50
               Mean episode length: 128.79
    Episode_Reward/reaching_object: 0.4977
     Episode_Reward/lifting_object: 84.2527
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 1.96s
                      Time elapsed: 00:29:49
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 50448 steps/s (collection: 1.852s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 506.8646
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.6476
                       Mean reward: 411.95
               Mean episode length: 136.87
    Episode_Reward/reaching_object: 0.4803
     Episode_Reward/lifting_object: 80.3553
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 1.95s
                      Time elapsed: 00:29:51
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 50428 steps/s (collection: 1.860s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 501.0574
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.6484
                       Mean reward: 369.39
               Mean episode length: 124.39
    Episode_Reward/reaching_object: 0.4814
     Episode_Reward/lifting_object: 80.8837
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 1.95s
                      Time elapsed: 00:29:53
                               ETA: 00:39:24

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 49837 steps/s (collection: 1.879s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 510.3471
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.6474
                       Mean reward: 412.23
               Mean episode length: 136.02
    Episode_Reward/reaching_object: 0.4771
     Episode_Reward/lifting_object: 79.7779
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 1.97s
                      Time elapsed: 00:29:55
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 49964 steps/s (collection: 1.872s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 481.2910
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.6483
                       Mean reward: 397.63
               Mean episode length: 133.24
    Episode_Reward/reaching_object: 0.4651
     Episode_Reward/lifting_object: 77.0963
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 1.97s
                      Time elapsed: 00:29:57
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 50396 steps/s (collection: 1.861s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 504.4229
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.6487
                       Mean reward: 392.87
               Mean episode length: 133.85
    Episode_Reward/reaching_object: 0.4685
     Episode_Reward/lifting_object: 77.7399
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 1.95s
                      Time elapsed: 00:29:59
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 50304 steps/s (collection: 1.863s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 500.6638
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.6496
                       Mean reward: 378.54
               Mean episode length: 127.54
    Episode_Reward/reaching_object: 0.4759
     Episode_Reward/lifting_object: 79.1968
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 1.95s
                      Time elapsed: 00:30:01
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 50967 steps/s (collection: 1.841s, learning 0.088s)
             Mean action noise std: 1.91
          Mean value_function loss: 484.5657
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 36.6515
                       Mean reward: 387.34
               Mean episode length: 129.37
    Episode_Reward/reaching_object: 0.4850
     Episode_Reward/lifting_object: 80.7930
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 1.93s
                      Time elapsed: 00:30:03
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 50227 steps/s (collection: 1.865s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 493.1858
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 36.6524
                       Mean reward: 464.02
               Mean episode length: 148.86
    Episode_Reward/reaching_object: 0.4975
     Episode_Reward/lifting_object: 82.9442
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 1.96s
                      Time elapsed: 00:30:05
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 49485 steps/s (collection: 1.891s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 525.7081
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.6519
                       Mean reward: 451.67
               Mean episode length: 144.68
    Episode_Reward/reaching_object: 0.5137
     Episode_Reward/lifting_object: 87.2220
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 1.99s
                      Time elapsed: 00:30:07
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 49600 steps/s (collection: 1.891s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 486.5688
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.6511
                       Mean reward: 461.40
               Mean episode length: 145.66
    Episode_Reward/reaching_object: 0.5421
     Episode_Reward/lifting_object: 93.3163
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 1.98s
                      Time elapsed: 00:30:09
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 50135 steps/s (collection: 1.856s, learning 0.105s)
             Mean action noise std: 1.91
          Mean value_function loss: 480.9355
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.6495
                       Mean reward: 500.28
               Mean episode length: 156.95
    Episode_Reward/reaching_object: 0.5375
     Episode_Reward/lifting_object: 92.2056
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 1.96s
                      Time elapsed: 00:30:11
                               ETA: 00:39:04

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 50124 steps/s (collection: 1.869s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 506.9121
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.6470
                       Mean reward: 480.33
               Mean episode length: 151.76
    Episode_Reward/reaching_object: 0.5561
     Episode_Reward/lifting_object: 96.1730
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 1.96s
                      Time elapsed: 00:30:13
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 50178 steps/s (collection: 1.865s, learning 0.095s)
             Mean action noise std: 1.91
          Mean value_function loss: 485.0266
               Mean surrogate loss: 0.0180
                 Mean entropy loss: 36.6540
                       Mean reward: 477.66
               Mean episode length: 149.93
    Episode_Reward/reaching_object: 0.5857
     Episode_Reward/lifting_object: 101.8186
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 1.96s
                      Time elapsed: 00:30:15
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 49836 steps/s (collection: 1.871s, learning 0.101s)
             Mean action noise std: 1.91
          Mean value_function loss: 454.9833
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.6564
                       Mean reward: 506.41
               Mean episode length: 154.49
    Episode_Reward/reaching_object: 0.5738
     Episode_Reward/lifting_object: 100.4873
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 1.97s
                      Time elapsed: 00:30:17
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 50181 steps/s (collection: 1.856s, learning 0.103s)
             Mean action noise std: 1.91
          Mean value_function loss: 480.5251
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.6570
                       Mean reward: 553.92
               Mean episode length: 163.84
    Episode_Reward/reaching_object: 0.5697
     Episode_Reward/lifting_object: 99.3861
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 1.96s
                      Time elapsed: 00:30:18
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 49269 steps/s (collection: 1.885s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 456.4818
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.6568
                       Mean reward: 545.87
               Mean episode length: 165.70
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 105.5757
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.00s
                      Time elapsed: 00:30:20
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 50088 steps/s (collection: 1.861s, learning 0.102s)
             Mean action noise std: 1.91
          Mean value_function loss: 449.1219
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.6595
                       Mean reward: 525.01
               Mean episode length: 160.03
    Episode_Reward/reaching_object: 0.6067
     Episode_Reward/lifting_object: 107.0938
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 1.96s
                      Time elapsed: 00:30:22
                               ETA: 00:38:51

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 50169 steps/s (collection: 1.869s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 466.1425
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.6635
                       Mean reward: 507.18
               Mean episode length: 153.39
    Episode_Reward/reaching_object: 0.5963
     Episode_Reward/lifting_object: 105.3714
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 1.96s
                      Time elapsed: 00:30:24
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 50398 steps/s (collection: 1.858s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 465.2057
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.6679
                       Mean reward: 506.67
               Mean episode length: 154.50
    Episode_Reward/reaching_object: 0.6045
     Episode_Reward/lifting_object: 107.1723
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 1.95s
                      Time elapsed: 00:30:26
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 50698 steps/s (collection: 1.851s, learning 0.088s)
             Mean action noise std: 1.91
          Mean value_function loss: 438.4476
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.6694
                       Mean reward: 576.75
               Mean episode length: 169.17
    Episode_Reward/reaching_object: 0.6268
     Episode_Reward/lifting_object: 112.0003
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 1.94s
                      Time elapsed: 00:30:28
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 50973 steps/s (collection: 1.843s, learning 0.086s)
             Mean action noise std: 1.91
          Mean value_function loss: 419.6821
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.6702
                       Mean reward: 605.21
               Mean episode length: 176.01
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 112.3926
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 1.93s
                      Time elapsed: 00:30:30
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 50629 steps/s (collection: 1.850s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 431.1677
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.6731
                       Mean reward: 650.86
               Mean episode length: 186.68
    Episode_Reward/reaching_object: 0.6759
     Episode_Reward/lifting_object: 122.4327
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 1.94s
                      Time elapsed: 00:30:32
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 50145 steps/s (collection: 1.861s, learning 0.099s)
             Mean action noise std: 1.91
          Mean value_function loss: 415.7831
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.6755
                       Mean reward: 574.19
               Mean episode length: 168.81
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 118.0601
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 1.96s
                      Time elapsed: 00:30:34
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 50077 steps/s (collection: 1.875s, learning 0.088s)
             Mean action noise std: 1.91
          Mean value_function loss: 390.3819
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.6772
                       Mean reward: 573.04
               Mean episode length: 170.03
    Episode_Reward/reaching_object: 0.6629
     Episode_Reward/lifting_object: 119.4135
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 1.96s
                      Time elapsed: 00:30:36
                               ETA: 00:38:35

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 49742 steps/s (collection: 1.861s, learning 0.115s)
             Mean action noise std: 1.91
          Mean value_function loss: 438.4047
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.6798
                       Mean reward: 544.05
               Mean episode length: 163.78
    Episode_Reward/reaching_object: 0.6252
     Episode_Reward/lifting_object: 111.8506
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 1.98s
                      Time elapsed: 00:30:38
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 49523 steps/s (collection: 1.873s, learning 0.112s)
             Mean action noise std: 1.92
          Mean value_function loss: 396.0262
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.6813
                       Mean reward: 627.96
               Mean episode length: 183.02
    Episode_Reward/reaching_object: 0.6809
     Episode_Reward/lifting_object: 123.6337
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 1.98s
                      Time elapsed: 00:30:40
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 49300 steps/s (collection: 1.894s, learning 0.100s)
             Mean action noise std: 1.92
          Mean value_function loss: 409.9600
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.6821
                       Mean reward: 523.08
               Mean episode length: 158.25
    Episode_Reward/reaching_object: 0.6480
     Episode_Reward/lifting_object: 116.8969
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 1.99s
                      Time elapsed: 00:30:42
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 49042 steps/s (collection: 1.907s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 401.0876
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.6817
                       Mean reward: 620.19
               Mean episode length: 178.62
    Episode_Reward/reaching_object: 0.6617
     Episode_Reward/lifting_object: 119.9760
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.00s
                      Time elapsed: 00:30:44
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 49363 steps/s (collection: 1.893s, learning 0.099s)
             Mean action noise std: 1.92
          Mean value_function loss: 376.7654
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.6805
                       Mean reward: 658.84
               Mean episode length: 190.94
    Episode_Reward/reaching_object: 0.6869
     Episode_Reward/lifting_object: 125.0781
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 1.99s
                      Time elapsed: 00:30:46
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 49738 steps/s (collection: 1.880s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 358.4346
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 36.6785
                       Mean reward: 643.10
               Mean episode length: 185.92
    Episode_Reward/reaching_object: 0.6990
     Episode_Reward/lifting_object: 127.2864
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 1.98s
                      Time elapsed: 00:30:48
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 50145 steps/s (collection: 1.857s, learning 0.104s)
             Mean action noise std: 1.92
          Mean value_function loss: 355.8602
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.6797
                       Mean reward: 635.45
               Mean episode length: 182.15
    Episode_Reward/reaching_object: 0.6927
     Episode_Reward/lifting_object: 125.7315
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 1.96s
                      Time elapsed: 00:30:50
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 49563 steps/s (collection: 1.884s, learning 0.099s)
             Mean action noise std: 1.92
          Mean value_function loss: 363.3165
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.6862
                       Mean reward: 616.08
               Mean episode length: 178.60
    Episode_Reward/reaching_object: 0.6683
     Episode_Reward/lifting_object: 121.2329
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 1.98s
                      Time elapsed: 00:30:52
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 48908 steps/s (collection: 1.914s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 344.0198
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 36.6889
                       Mean reward: 691.02
               Mean episode length: 197.13
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 135.2572
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.01s
                      Time elapsed: 00:30:54
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 50504 steps/s (collection: 1.851s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 374.4279
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.6885
                       Mean reward: 672.75
               Mean episode length: 192.07
    Episode_Reward/reaching_object: 0.6856
     Episode_Reward/lifting_object: 124.7751
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 1.95s
                      Time elapsed: 00:30:56
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 50106 steps/s (collection: 1.861s, learning 0.101s)
             Mean action noise std: 1.92
          Mean value_function loss: 360.0826
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 36.6881
                       Mean reward: 553.82
               Mean episode length: 166.53
    Episode_Reward/reaching_object: 0.6890
     Episode_Reward/lifting_object: 124.8470
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 1.96s
                      Time elapsed: 00:30:58
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 49927 steps/s (collection: 1.868s, learning 0.101s)
             Mean action noise std: 1.92
          Mean value_function loss: 371.0381
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.6885
                       Mean reward: 660.13
               Mean episode length: 188.37
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 130.3361
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 1.97s
                      Time elapsed: 00:31:00
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 49215 steps/s (collection: 1.890s, learning 0.107s)
             Mean action noise std: 1.92
          Mean value_function loss: 343.8688
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 36.6886
                       Mean reward: 641.31
               Mean episode length: 183.97
    Episode_Reward/reaching_object: 0.7044
     Episode_Reward/lifting_object: 128.2053
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.00s
                      Time elapsed: 00:31:02
                               ETA: 00:38:07

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 49378 steps/s (collection: 1.884s, learning 0.107s)
             Mean action noise std: 1.92
          Mean value_function loss: 356.3412
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.6887
                       Mean reward: 713.26
               Mean episode length: 202.11
    Episode_Reward/reaching_object: 0.7175
     Episode_Reward/lifting_object: 131.2182
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 1.99s
                      Time elapsed: 00:31:04
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 48738 steps/s (collection: 1.909s, learning 0.108s)
             Mean action noise std: 1.92
          Mean value_function loss: 354.7248
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 36.6888
                       Mean reward: 617.47
               Mean episode length: 178.77
    Episode_Reward/reaching_object: 0.6887
     Episode_Reward/lifting_object: 125.2083
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.02s
                      Time elapsed: 00:31:06
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 49538 steps/s (collection: 1.875s, learning 0.109s)
             Mean action noise std: 1.92
          Mean value_function loss: 353.7359
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 36.6889
                       Mean reward: 647.85
               Mean episode length: 186.47
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: 125.2527
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 1.98s
                      Time elapsed: 00:31:08
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 48921 steps/s (collection: 1.899s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 332.9782
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.6894
                       Mean reward: 673.52
               Mean episode length: 192.39
    Episode_Reward/reaching_object: 0.7214
     Episode_Reward/lifting_object: 131.9948
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.01s
                      Time elapsed: 00:31:10
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 49408 steps/s (collection: 1.897s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 383.1495
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 36.6897
                       Mean reward: 600.77
               Mean episode length: 174.70
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 126.4025
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 1.99s
                      Time elapsed: 00:31:12
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 49368 steps/s (collection: 1.890s, learning 0.102s)
             Mean action noise std: 1.92
          Mean value_function loss: 366.7449
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.6900
                       Mean reward: 673.33
               Mean episode length: 192.12
    Episode_Reward/reaching_object: 0.7191
     Episode_Reward/lifting_object: 132.1794
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 1.99s
                      Time elapsed: 00:31:14
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 48953 steps/s (collection: 1.909s, learning 0.099s)
             Mean action noise std: 1.92
          Mean value_function loss: 371.8492
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 36.6905
                       Mean reward: 673.36
               Mean episode length: 191.96
    Episode_Reward/reaching_object: 0.6960
     Episode_Reward/lifting_object: 127.5641
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.01s
                      Time elapsed: 00:31:16
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 48567 steps/s (collection: 1.911s, learning 0.114s)
             Mean action noise std: 1.92
          Mean value_function loss: 383.7536
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.6907
                       Mean reward: 634.19
               Mean episode length: 181.47
    Episode_Reward/reaching_object: 0.6946
     Episode_Reward/lifting_object: 127.7129
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.02s
                      Time elapsed: 00:31:18
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 48906 steps/s (collection: 1.899s, learning 0.111s)
             Mean action noise std: 1.92
          Mean value_function loss: 416.8921
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.6906
                       Mean reward: 635.85
               Mean episode length: 182.50
    Episode_Reward/reaching_object: 0.6866
     Episode_Reward/lifting_object: 126.3607
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.01s
                      Time elapsed: 00:31:20
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 48334 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 397.9212
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6934
                       Mean reward: 622.61
               Mean episode length: 178.04
    Episode_Reward/reaching_object: 0.6869
     Episode_Reward/lifting_object: 126.5598
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.03s
                      Time elapsed: 00:31:22
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 48577 steps/s (collection: 1.934s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 423.3277
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.6950
                       Mean reward: 591.02
               Mean episode length: 171.42
    Episode_Reward/reaching_object: 0.6742
     Episode_Reward/lifting_object: 123.9865
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.02s
                      Time elapsed: 00:31:24
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 48954 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 1.92
          Mean value_function loss: 401.6328
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.6960
                       Mean reward: 608.97
               Mean episode length: 174.22
    Episode_Reward/reaching_object: 0.6603
     Episode_Reward/lifting_object: 121.2636
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.01s
                      Time elapsed: 00:31:26
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 48840 steps/s (collection: 1.921s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 419.9266
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.6980
                       Mean reward: 606.47
               Mean episode length: 173.52
    Episode_Reward/reaching_object: 0.6409
     Episode_Reward/lifting_object: 117.3596
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.01s
                      Time elapsed: 00:31:28
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 48811 steps/s (collection: 1.925s, learning 0.089s)
             Mean action noise std: 1.92
          Mean value_function loss: 403.8392
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 36.7003
                       Mean reward: 561.80
               Mean episode length: 165.23
    Episode_Reward/reaching_object: 0.6524
     Episode_Reward/lifting_object: 119.8640
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.01s
                      Time elapsed: 00:31:30
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 48961 steps/s (collection: 1.915s, learning 0.093s)
             Mean action noise std: 1.92
          Mean value_function loss: 416.4347
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.7011
                       Mean reward: 564.98
               Mean episode length: 165.81
    Episode_Reward/reaching_object: 0.6489
     Episode_Reward/lifting_object: 118.8617
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.01s
                      Time elapsed: 00:31:32
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 49127 steps/s (collection: 1.903s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 376.2185
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.7013
                       Mean reward: 656.37
               Mean episode length: 187.38
    Episode_Reward/reaching_object: 0.6828
     Episode_Reward/lifting_object: 125.8792
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.00s
                      Time elapsed: 00:31:34
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 49434 steps/s (collection: 1.895s, learning 0.094s)
             Mean action noise std: 1.92
          Mean value_function loss: 361.3168
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 36.7017
                       Mean reward: 668.46
               Mean episode length: 188.79
    Episode_Reward/reaching_object: 0.6714
     Episode_Reward/lifting_object: 123.8022
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 1.99s
                      Time elapsed: 00:31:36
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 48534 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 342.2403
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.7017
                       Mean reward: 661.53
               Mean episode length: 188.55
    Episode_Reward/reaching_object: 0.6936
     Episode_Reward/lifting_object: 127.8488
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.03s
                      Time elapsed: 00:31:38
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 48682 steps/s (collection: 1.929s, learning 0.091s)
             Mean action noise std: 1.92
          Mean value_function loss: 353.4496
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 36.7016
                       Mean reward: 648.75
               Mean episode length: 186.18
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 132.6717
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.02s
                      Time elapsed: 00:31:40
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 48774 steps/s (collection: 1.919s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 331.1251
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.7016
                       Mean reward: 601.59
               Mean episode length: 175.80
    Episode_Reward/reaching_object: 0.7260
     Episode_Reward/lifting_object: 134.2632
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.02s
                      Time elapsed: 00:31:42
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 49090 steps/s (collection: 1.901s, learning 0.102s)
             Mean action noise std: 1.92
          Mean value_function loss: 348.0475
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 36.7035
                       Mean reward: 669.67
               Mean episode length: 193.43
    Episode_Reward/reaching_object: 0.7167
     Episode_Reward/lifting_object: 131.9311
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.00s
                      Time elapsed: 00:31:44
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 48621 steps/s (collection: 1.911s, learning 0.111s)
             Mean action noise std: 1.92
          Mean value_function loss: 354.6231
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.7047
                       Mean reward: 660.10
               Mean episode length: 189.34
    Episode_Reward/reaching_object: 0.7075
     Episode_Reward/lifting_object: 130.2266
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.02s
                      Time elapsed: 00:31:46
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 49404 steps/s (collection: 1.896s, learning 0.094s)
             Mean action noise std: 1.92
          Mean value_function loss: 339.3485
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.7045
                       Mean reward: 642.04
               Mean episode length: 186.65
    Episode_Reward/reaching_object: 0.7087
     Episode_Reward/lifting_object: 129.8336
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 1.99s
                      Time elapsed: 00:31:48
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 49055 steps/s (collection: 1.906s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 338.8024
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.7061
                       Mean reward: 599.99
               Mean episode length: 175.20
    Episode_Reward/reaching_object: 0.6833
     Episode_Reward/lifting_object: 124.1350
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.00s
                      Time elapsed: 00:31:50
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 49381 steps/s (collection: 1.894s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 347.6199
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 36.7063
                       Mean reward: 665.72
               Mean episode length: 191.75
    Episode_Reward/reaching_object: 0.7002
     Episode_Reward/lifting_object: 128.4837
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 1.99s
                      Time elapsed: 00:31:52
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 48347 steps/s (collection: 1.928s, learning 0.105s)
             Mean action noise std: 1.92
          Mean value_function loss: 315.2154
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.7067
                       Mean reward: 567.60
               Mean episode length: 170.81
    Episode_Reward/reaching_object: 0.6900
     Episode_Reward/lifting_object: 126.0044
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.03s
                      Time elapsed: 00:31:54
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 49149 steps/s (collection: 1.898s, learning 0.102s)
             Mean action noise std: 1.92
          Mean value_function loss: 313.8962
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.7067
                       Mean reward: 672.51
               Mean episode length: 195.99
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 129.5728
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.00s
                      Time elapsed: 00:31:56
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 47925 steps/s (collection: 1.942s, learning 0.109s)
             Mean action noise std: 1.92
          Mean value_function loss: 300.1632
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.7097
                       Mean reward: 612.15
               Mean episode length: 180.92
    Episode_Reward/reaching_object: 0.7098
     Episode_Reward/lifting_object: 130.1151
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.05s
                      Time elapsed: 00:31:58
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 49571 steps/s (collection: 1.894s, learning 0.090s)
             Mean action noise std: 1.93
          Mean value_function loss: 306.2417
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.7184
                       Mean reward: 676.57
               Mean episode length: 194.38
    Episode_Reward/reaching_object: 0.6997
     Episode_Reward/lifting_object: 127.7710
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 1.98s
                      Time elapsed: 00:32:00
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 49135 steps/s (collection: 1.910s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 292.3810
               Mean surrogate loss: 0.0135
                 Mean entropy loss: 36.7215
                       Mean reward: 714.29
               Mean episode length: 203.96
    Episode_Reward/reaching_object: 0.7266
     Episode_Reward/lifting_object: 133.3391
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.00s
                      Time elapsed: 00:32:02
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 48723 steps/s (collection: 1.923s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 315.2802
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 36.7214
                       Mean reward: 694.50
               Mean episode length: 197.95
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 131.1610
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.02s
                      Time elapsed: 00:32:04
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 49181 steps/s (collection: 1.903s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 319.8747
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 36.7214
                       Mean reward: 646.94
               Mean episode length: 190.56
    Episode_Reward/reaching_object: 0.7132
     Episode_Reward/lifting_object: 129.6879
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.00s
                      Time elapsed: 00:32:06
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 46829 steps/s (collection: 1.994s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 294.1928
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 36.7217
                       Mean reward: 601.77
               Mean episode length: 177.63
    Episode_Reward/reaching_object: 0.7338
     Episode_Reward/lifting_object: 134.0036
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.10s
                      Time elapsed: 00:32:08
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 48313 steps/s (collection: 1.942s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 292.0364
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.7220
                       Mean reward: 695.06
               Mean episode length: 199.65
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 137.1964
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.03s
                      Time elapsed: 00:32:10
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 48922 steps/s (collection: 1.914s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 297.2355
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.7221
                       Mean reward: 677.74
               Mean episode length: 193.02
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 142.8377
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.01s
                      Time elapsed: 00:32:12
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 49186 steps/s (collection: 1.903s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 293.9951
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.7222
                       Mean reward: 640.67
               Mean episode length: 185.84
    Episode_Reward/reaching_object: 0.7245
     Episode_Reward/lifting_object: 133.1617
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.00s
                      Time elapsed: 00:32:14
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 48848 steps/s (collection: 1.916s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 296.3340
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 36.7221
                       Mean reward: 706.83
               Mean episode length: 200.81
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 141.3636
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.01s
                      Time elapsed: 00:32:16
                               ETA: 00:36:48

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 48575 steps/s (collection: 1.916s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 316.0423
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.7220
                       Mean reward: 719.01
               Mean episode length: 201.50
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 138.7019
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.02s
                      Time elapsed: 00:32:18
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 48517 steps/s (collection: 1.917s, learning 0.109s)
             Mean action noise std: 1.93
          Mean value_function loss: 319.5085
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 36.7220
                       Mean reward: 710.30
               Mean episode length: 201.29
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 137.8333
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.03s
                      Time elapsed: 00:32:20
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 48319 steps/s (collection: 1.928s, learning 0.107s)
             Mean action noise std: 1.93
          Mean value_function loss: 313.5415
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.7218
                       Mean reward: 694.09
               Mean episode length: 197.43
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 136.7185
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.03s
                      Time elapsed: 00:32:22
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 48980 steps/s (collection: 1.914s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 320.2141
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.7216
                       Mean reward: 661.02
               Mean episode length: 187.77
    Episode_Reward/reaching_object: 0.7367
     Episode_Reward/lifting_object: 136.5444
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.01s
                      Time elapsed: 00:32:24
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 49216 steps/s (collection: 1.908s, learning 0.090s)
             Mean action noise std: 1.93
          Mean value_function loss: 317.9912
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.7216
                       Mean reward: 766.69
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 144.1331
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.00s
                      Time elapsed: 00:32:26
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 48910 steps/s (collection: 1.919s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 326.2019
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 36.7215
                       Mean reward: 689.09
               Mean episode length: 194.41
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 139.5430
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.01s
                      Time elapsed: 00:32:28
                               ETA: 00:36:35

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 49056 steps/s (collection: 1.911s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 333.6183
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.7218
                       Mean reward: 706.87
               Mean episode length: 199.71
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 138.4966
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.00s
                      Time elapsed: 00:32:30
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 48830 steps/s (collection: 1.918s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 356.0499
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.7220
                       Mean reward: 623.43
               Mean episode length: 179.12
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 135.4784
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.01s
                      Time elapsed: 00:32:32
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 48443 steps/s (collection: 1.932s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 338.9627
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.7224
                       Mean reward: 669.26
               Mean episode length: 188.80
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 138.5761
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.03s
                      Time elapsed: 00:32:34
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 48920 steps/s (collection: 1.911s, learning 0.099s)
             Mean action noise std: 1.93
          Mean value_function loss: 337.7825
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 36.7236
                       Mean reward: 692.41
               Mean episode length: 195.69
    Episode_Reward/reaching_object: 0.7004
     Episode_Reward/lifting_object: 130.1535
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.01s
                      Time elapsed: 00:32:36
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 48508 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 1.93
          Mean value_function loss: 328.5269
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 36.7239
                       Mean reward: 761.49
               Mean episode length: 210.12
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 141.4913
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.03s
                      Time elapsed: 00:32:38
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 48268 steps/s (collection: 1.943s, learning 0.094s)
             Mean action noise std: 1.93
          Mean value_function loss: 311.4339
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 36.7243
                       Mean reward: 694.17
               Mean episode length: 194.44
    Episode_Reward/reaching_object: 0.7336
     Episode_Reward/lifting_object: 137.0391
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.04s
                      Time elapsed: 00:32:40
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 48167 steps/s (collection: 1.943s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 305.4894
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 36.7245
                       Mean reward: 723.45
               Mean episode length: 202.04
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 142.1689
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.04s
                      Time elapsed: 00:32:42
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 48216 steps/s (collection: 1.936s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 306.5387
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 36.7248
                       Mean reward: 711.66
               Mean episode length: 198.57
    Episode_Reward/reaching_object: 0.7353
     Episode_Reward/lifting_object: 137.3356
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.04s
                      Time elapsed: 00:32:45
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 48531 steps/s (collection: 1.922s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 275.8784
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 36.7251
                       Mean reward: 717.93
               Mean episode length: 201.06
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 140.5297
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.03s
                      Time elapsed: 00:32:47
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 46903 steps/s (collection: 1.983s, learning 0.113s)
             Mean action noise std: 1.93
          Mean value_function loss: 295.8813
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.7255
                       Mean reward: 764.04
               Mean episode length: 212.14
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 140.5622
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.10s
                      Time elapsed: 00:32:49
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 47187 steps/s (collection: 1.973s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 294.7604
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.7258
                       Mean reward: 682.86
               Mean episode length: 193.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 144.7062
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.08s
                      Time elapsed: 00:32:51
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 46817 steps/s (collection: 1.984s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 284.8922
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.7260
                       Mean reward: 713.83
               Mean episode length: 201.22
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 135.7560
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.10s
                      Time elapsed: 00:32:53
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 46855 steps/s (collection: 2.006s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 306.8734
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.7262
                       Mean reward: 673.83
               Mean episode length: 190.30
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 140.8397
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.10s
                      Time elapsed: 00:32:55
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 47741 steps/s (collection: 1.963s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 274.6886
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.7261
                       Mean reward: 730.88
               Mean episode length: 204.07
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 143.8186
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.06s
                      Time elapsed: 00:32:57
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 48870 steps/s (collection: 1.916s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 256.9754
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.7260
                       Mean reward: 701.30
               Mean episode length: 197.53
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 140.1074
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.01s
                      Time elapsed: 00:32:59
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 48107 steps/s (collection: 1.925s, learning 0.118s)
             Mean action noise std: 1.93
          Mean value_function loss: 261.8700
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.7260
                       Mean reward: 713.56
               Mean episode length: 200.18
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 144.1286
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.04s
                      Time elapsed: 00:33:01
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 48300 steps/s (collection: 1.936s, learning 0.099s)
             Mean action noise std: 1.93
          Mean value_function loss: 266.5759
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.7264
                       Mean reward: 735.78
               Mean episode length: 204.25
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 144.8087
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.04s
                      Time elapsed: 00:33:03
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 48605 steps/s (collection: 1.926s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 236.0241
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.7261
                       Mean reward: 745.34
               Mean episode length: 207.40
    Episode_Reward/reaching_object: 0.8034
     Episode_Reward/lifting_object: 150.4151
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.02s
                      Time elapsed: 00:33:05
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 47381 steps/s (collection: 1.972s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 242.6071
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.7230
                       Mean reward: 764.51
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 145.2216
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.07s
                      Time elapsed: 00:33:07
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 48164 steps/s (collection: 1.943s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 258.2968
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.7242
                       Mean reward: 732.57
               Mean episode length: 204.07
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 146.7994
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.04s
                      Time elapsed: 00:33:09
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 48257 steps/s (collection: 1.931s, learning 0.106s)
             Mean action noise std: 1.93
          Mean value_function loss: 251.0121
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 36.7252
                       Mean reward: 745.61
               Mean episode length: 206.45
    Episode_Reward/reaching_object: 0.8028
     Episode_Reward/lifting_object: 150.8289
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.04s
                      Time elapsed: 00:33:11
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 48625 steps/s (collection: 1.927s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 240.5128
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.7255
                       Mean reward: 753.10
               Mean episode length: 209.15
    Episode_Reward/reaching_object: 0.7931
     Episode_Reward/lifting_object: 148.1534
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.02s
                      Time elapsed: 00:33:13
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 48539 steps/s (collection: 1.931s, learning 0.094s)
             Mean action noise std: 1.93
          Mean value_function loss: 280.0175
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.7251
                       Mean reward: 780.22
               Mean episode length: 213.70
    Episode_Reward/reaching_object: 0.7998
     Episode_Reward/lifting_object: 150.2554
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.03s
                      Time elapsed: 00:33:15
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 48418 steps/s (collection: 1.935s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 262.7217
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 36.7252
                       Mean reward: 763.07
               Mean episode length: 210.10
    Episode_Reward/reaching_object: 0.8024
     Episode_Reward/lifting_object: 151.0746
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.03s
                      Time elapsed: 00:33:17
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 48294 steps/s (collection: 1.926s, learning 0.109s)
             Mean action noise std: 1.93
          Mean value_function loss: 275.7474
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.7253
                       Mean reward: 726.80
               Mean episode length: 202.11
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 147.9006
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.04s
                      Time elapsed: 00:33:19
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 48965 steps/s (collection: 1.904s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 263.2073
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.7254
                       Mean reward: 765.97
               Mean episode length: 211.93
    Episode_Reward/reaching_object: 0.7930
     Episode_Reward/lifting_object: 148.9397
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.01s
                      Time elapsed: 00:33:21
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 47966 steps/s (collection: 1.941s, learning 0.109s)
             Mean action noise std: 1.93
          Mean value_function loss: 253.8490
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.7253
                       Mean reward: 726.59
               Mean episode length: 202.46
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 146.0871
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.05s
                      Time elapsed: 00:33:23
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 47748 steps/s (collection: 1.960s, learning 0.099s)
             Mean action noise std: 1.93
          Mean value_function loss: 237.6580
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 36.7253
                       Mean reward: 817.59
               Mean episode length: 224.18
    Episode_Reward/reaching_object: 0.8167
     Episode_Reward/lifting_object: 153.7277
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.06s
                      Time elapsed: 00:33:25
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 47835 steps/s (collection: 1.951s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 240.9711
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 36.7254
                       Mean reward: 802.87
               Mean episode length: 220.97
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 145.8433
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.06s
                      Time elapsed: 00:33:28
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 45291 steps/s (collection: 2.060s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 227.2578
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 36.7256
                       Mean reward: 803.15
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 0.8155
     Episode_Reward/lifting_object: 153.4800
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.17s
                      Time elapsed: 00:33:30
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 45013 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 1.93
          Mean value_function loss: 227.3855
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.7259
                       Mean reward: 718.75
               Mean episode length: 201.81
    Episode_Reward/reaching_object: 0.7977
     Episode_Reward/lifting_object: 149.6751
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.18s
                      Time elapsed: 00:33:32
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 39951 steps/s (collection: 2.306s, learning 0.155s)
             Mean action noise std: 1.93
          Mean value_function loss: 247.5120
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 36.7264
                       Mean reward: 674.97
               Mean episode length: 192.52
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 142.3562
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.46s
                      Time elapsed: 00:33:34
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 44251 steps/s (collection: 2.105s, learning 0.117s)
             Mean action noise std: 1.93
          Mean value_function loss: 243.9783
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 36.7271
                       Mean reward: 691.30
               Mean episode length: 194.88
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 147.6938
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.22s
                      Time elapsed: 00:33:37
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 41964 steps/s (collection: 2.144s, learning 0.199s)
             Mean action noise std: 1.93
          Mean value_function loss: 243.2151
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.7263
                       Mean reward: 684.35
               Mean episode length: 192.46
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 146.8886
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.34s
                      Time elapsed: 00:33:39
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 45101 steps/s (collection: 2.079s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 235.3827
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.7222
                       Mean reward: 722.25
               Mean episode length: 200.32
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 142.3702
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.18s
                      Time elapsed: 00:33:41
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 43778 steps/s (collection: 2.135s, learning 0.111s)
             Mean action noise std: 1.93
          Mean value_function loss: 219.7663
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 36.7222
                       Mean reward: 789.79
               Mean episode length: 216.79
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 146.3435
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.25s
                      Time elapsed: 00:33:43
                               ETA: 00:35:21

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 44536 steps/s (collection: 2.079s, learning 0.128s)
             Mean action noise std: 1.93
          Mean value_function loss: 218.5403
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 36.7227
                       Mean reward: 744.23
               Mean episode length: 207.86
    Episode_Reward/reaching_object: 0.8093
     Episode_Reward/lifting_object: 151.3044
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.21s
                      Time elapsed: 00:33:46
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 46021 steps/s (collection: 2.009s, learning 0.128s)
             Mean action noise std: 1.93
          Mean value_function loss: 219.8554
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.7226
                       Mean reward: 748.15
               Mean episode length: 207.58
    Episode_Reward/reaching_object: 0.8308
     Episode_Reward/lifting_object: 155.8990
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.14s
                      Time elapsed: 00:33:48
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 46897 steps/s (collection: 1.982s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 247.0420
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.7235
                       Mean reward: 680.57
               Mean episode length: 193.14
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 140.7694
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.10s
                      Time elapsed: 00:33:50
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 44590 steps/s (collection: 2.075s, learning 0.130s)
             Mean action noise std: 1.93
          Mean value_function loss: 213.1321
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.7227
                       Mean reward: 756.53
               Mean episode length: 210.54
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 147.2669
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.20s
                      Time elapsed: 00:33:52
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 44547 steps/s (collection: 2.086s, learning 0.121s)
             Mean action noise std: 1.93
          Mean value_function loss: 225.6219
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 36.7223
                       Mean reward: 725.06
               Mean episode length: 202.57
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 146.6800
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.21s
                      Time elapsed: 00:33:54
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 42821 steps/s (collection: 2.127s, learning 0.169s)
             Mean action noise std: 1.93
          Mean value_function loss: 229.1290
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.7222
                       Mean reward: 796.08
               Mean episode length: 218.86
    Episode_Reward/reaching_object: 0.8092
     Episode_Reward/lifting_object: 151.3041
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.30s
                      Time elapsed: 00:33:56
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 40348 steps/s (collection: 2.293s, learning 0.144s)
             Mean action noise std: 1.93
          Mean value_function loss: 222.1760
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.7233
                       Mean reward: 767.62
               Mean episode length: 212.62
    Episode_Reward/reaching_object: 0.8170
     Episode_Reward/lifting_object: 153.1006
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.44s
                      Time elapsed: 00:33:59
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 43135 steps/s (collection: 2.176s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 216.1516
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.7240
                       Mean reward: 771.95
               Mean episode length: 212.97
    Episode_Reward/reaching_object: 0.8181
     Episode_Reward/lifting_object: 153.2501
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.28s
                      Time elapsed: 00:34:01
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 42505 steps/s (collection: 2.105s, learning 0.208s)
             Mean action noise std: 1.93
          Mean value_function loss: 248.9766
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.7232
                       Mean reward: 707.18
               Mean episode length: 199.27
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 146.1248
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.31s
                      Time elapsed: 00:34:04
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 34597 steps/s (collection: 2.744s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 236.0692
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.7219
                       Mean reward: 760.38
               Mean episode length: 210.20
    Episode_Reward/reaching_object: 0.7985
     Episode_Reward/lifting_object: 149.2307
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.84s
                      Time elapsed: 00:34:06
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 47199 steps/s (collection: 1.986s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 276.6623
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.7241
                       Mean reward: 705.43
               Mean episode length: 198.26
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 144.4562
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.08s
                      Time elapsed: 00:34:08
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 48227 steps/s (collection: 1.942s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 243.9063
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.7296
                       Mean reward: 738.12
               Mean episode length: 206.07
    Episode_Reward/reaching_object: 0.7871
     Episode_Reward/lifting_object: 146.4866
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.04s
                      Time elapsed: 00:34:10
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 46634 steps/s (collection: 2.011s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 247.4693
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.7351
                       Mean reward: 717.77
               Mean episode length: 201.15
    Episode_Reward/reaching_object: 0.7943
     Episode_Reward/lifting_object: 148.6688
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.11s
                      Time elapsed: 00:34:13
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 46076 steps/s (collection: 2.026s, learning 0.107s)
             Mean action noise std: 1.94
          Mean value_function loss: 250.0267
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.7394
                       Mean reward: 755.24
               Mean episode length: 209.20
    Episode_Reward/reaching_object: 0.7968
     Episode_Reward/lifting_object: 148.7386
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.13s
                      Time elapsed: 00:34:15
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 39295 steps/s (collection: 2.341s, learning 0.161s)
             Mean action noise std: 1.94
          Mean value_function loss: 236.5503
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.7396
                       Mean reward: 778.75
               Mean episode length: 215.36
    Episode_Reward/reaching_object: 0.7851
     Episode_Reward/lifting_object: 146.6968
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.50s
                      Time elapsed: 00:34:17
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 43467 steps/s (collection: 2.119s, learning 0.143s)
             Mean action noise std: 1.94
          Mean value_function loss: 227.9478
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.7398
                       Mean reward: 792.34
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 0.8195
     Episode_Reward/lifting_object: 154.2273
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.26s
                      Time elapsed: 00:34:19
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 48356 steps/s (collection: 1.929s, learning 0.104s)
             Mean action noise std: 1.94
          Mean value_function loss: 232.8539
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.7400
                       Mean reward: 714.46
               Mean episode length: 199.45
    Episode_Reward/reaching_object: 0.8211
     Episode_Reward/lifting_object: 154.3543
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.03s
                      Time elapsed: 00:34:22
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.944s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 217.8201
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.7401
                       Mean reward: 780.26
               Mean episode length: 214.80
    Episode_Reward/reaching_object: 0.8146
     Episode_Reward/lifting_object: 153.0688
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.04s
                      Time elapsed: 00:34:24
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 48338 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 239.3303
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.7384
                       Mean reward: 747.00
               Mean episode length: 208.31
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 144.6632
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.03s
                      Time elapsed: 00:34:26
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 48873 steps/s (collection: 1.902s, learning 0.109s)
             Mean action noise std: 1.94
          Mean value_function loss: 212.7538
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.7388
                       Mean reward: 735.82
               Mean episode length: 204.70
    Episode_Reward/reaching_object: 0.8080
     Episode_Reward/lifting_object: 151.3354
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.01s
                      Time elapsed: 00:34:28
                               ETA: 00:34:42

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 48876 steps/s (collection: 1.907s, learning 0.104s)
             Mean action noise std: 1.94
          Mean value_function loss: 219.9053
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 36.7404
                       Mean reward: 788.22
               Mean episode length: 217.56
    Episode_Reward/reaching_object: 0.8113
     Episode_Reward/lifting_object: 152.1806
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.01s
                      Time elapsed: 00:34:30
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 49404 steps/s (collection: 1.895s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 232.9360
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.7406
                       Mean reward: 753.01
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 0.8027
     Episode_Reward/lifting_object: 150.7346
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 1.99s
                      Time elapsed: 00:34:32
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 49281 steps/s (collection: 1.893s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 204.9277
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 36.7407
                       Mean reward: 784.14
               Mean episode length: 215.55
    Episode_Reward/reaching_object: 0.8114
     Episode_Reward/lifting_object: 152.2301
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 1.99s
                      Time elapsed: 00:34:34
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14917 steps/s (collection: 6.464s, learning 0.126s)
             Mean action noise std: 1.94
          Mean value_function loss: 217.0457
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.7415
                       Mean reward: 815.08
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 0.8237
     Episode_Reward/lifting_object: 155.0582
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.59s
                      Time elapsed: 00:34:40
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14595 steps/s (collection: 6.622s, learning 0.114s)
             Mean action noise std: 1.94
          Mean value_function loss: 239.4172
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.7426
                       Mean reward: 769.96
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 0.8026
     Episode_Reward/lifting_object: 150.7522
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.74s
                      Time elapsed: 00:34:47
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14743 steps/s (collection: 6.556s, learning 0.112s)
             Mean action noise std: 1.94
          Mean value_function loss: 216.7723
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.7426
                       Mean reward: 786.70
               Mean episode length: 216.25
    Episode_Reward/reaching_object: 0.8220
     Episode_Reward/lifting_object: 154.4949
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.67s
                      Time elapsed: 00:34:54
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14669 steps/s (collection: 6.576s, learning 0.126s)
             Mean action noise std: 1.94
          Mean value_function loss: 226.1295
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.7414
                       Mean reward: 796.23
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 0.8233
     Episode_Reward/lifting_object: 155.1053
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.70s
                      Time elapsed: 00:35:00
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14821 steps/s (collection: 6.522s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 211.9532
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 36.7409
                       Mean reward: 801.44
               Mean episode length: 219.27
    Episode_Reward/reaching_object: 0.8289
     Episode_Reward/lifting_object: 155.9041
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.63s
                      Time elapsed: 00:35:07
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14799 steps/s (collection: 6.522s, learning 0.120s)
             Mean action noise std: 1.94
          Mean value_function loss: 201.5163
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 36.7410
                       Mean reward: 787.87
               Mean episode length: 216.65
    Episode_Reward/reaching_object: 0.8249
     Episode_Reward/lifting_object: 155.0777
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.64s
                      Time elapsed: 00:35:14
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14593 steps/s (collection: 6.620s, learning 0.117s)
             Mean action noise std: 1.94
          Mean value_function loss: 222.7242
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.7413
                       Mean reward: 782.56
               Mean episode length: 214.57
    Episode_Reward/reaching_object: 0.8305
     Episode_Reward/lifting_object: 157.0673
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.74s
                      Time elapsed: 00:35:20
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14678 steps/s (collection: 6.571s, learning 0.127s)
             Mean action noise std: 1.94
          Mean value_function loss: 208.2774
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 36.7417
                       Mean reward: 773.21
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 158.1351
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.70s
                      Time elapsed: 00:35:27
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 18290 steps/s (collection: 5.282s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 239.7283
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.7420
                       Mean reward: 865.71
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.8319
     Episode_Reward/lifting_object: 157.2941
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.37s
                      Time elapsed: 00:35:32
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 51594 steps/s (collection: 1.813s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 240.0163
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.7423
                       Mean reward: 785.59
               Mean episode length: 214.77
    Episode_Reward/reaching_object: 0.8162
     Episode_Reward/lifting_object: 153.7707
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 1.91s
                      Time elapsed: 00:35:34
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 50698 steps/s (collection: 1.834s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 271.4841
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.7426
                       Mean reward: 746.94
               Mean episode length: 207.13
    Episode_Reward/reaching_object: 0.8196
     Episode_Reward/lifting_object: 154.3602
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.94s
                      Time elapsed: 00:35:36
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 51039 steps/s (collection: 1.817s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 269.8648
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.7434
                       Mean reward: 793.06
               Mean episode length: 219.68
    Episode_Reward/reaching_object: 0.8054
     Episode_Reward/lifting_object: 151.5611
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.93s
                      Time elapsed: 00:35:38
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 51725 steps/s (collection: 1.802s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 254.1870
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.7436
                       Mean reward: 751.14
               Mean episode length: 207.12
    Episode_Reward/reaching_object: 0.8045
     Episode_Reward/lifting_object: 151.8640
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 1.90s
                      Time elapsed: 00:35:40
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 51973 steps/s (collection: 1.797s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 285.2139
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.7427
                       Mean reward: 741.04
               Mean episode length: 206.09
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 146.5766
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.89s
                      Time elapsed: 00:35:42
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 52024 steps/s (collection: 1.805s, learning 0.084s)
             Mean action noise std: 1.94
          Mean value_function loss: 239.4121
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.7447
                       Mean reward: 795.53
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 0.8133
     Episode_Reward/lifting_object: 153.7091
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.89s
                      Time elapsed: 00:35:44
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 51027 steps/s (collection: 1.827s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 210.6520
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.7474
                       Mean reward: 717.80
               Mean episode length: 200.90
    Episode_Reward/reaching_object: 0.7940
     Episode_Reward/lifting_object: 149.4185
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.93s
                      Time elapsed: 00:35:46
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 50659 steps/s (collection: 1.838s, learning 0.103s)
             Mean action noise std: 1.94
          Mean value_function loss: 223.3102
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7485
                       Mean reward: 748.77
               Mean episode length: 209.53
    Episode_Reward/reaching_object: 0.8090
     Episode_Reward/lifting_object: 152.6301
      Episode_Reward/object_height: 0.0304
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.94s
                      Time elapsed: 00:35:48
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 51743 steps/s (collection: 1.796s, learning 0.104s)
             Mean action noise std: 1.94
          Mean value_function loss: 213.7907
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.7514
                       Mean reward: 794.97
               Mean episode length: 218.04
    Episode_Reward/reaching_object: 0.8270
     Episode_Reward/lifting_object: 156.5517
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.90s
                      Time elapsed: 00:35:50
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 51253 steps/s (collection: 1.832s, learning 0.087s)
             Mean action noise std: 1.94
          Mean value_function loss: 238.9426
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.7540
                       Mean reward: 775.12
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 0.8166
     Episode_Reward/lifting_object: 154.6281
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.92s
                      Time elapsed: 00:35:52
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 50980 steps/s (collection: 1.843s, learning 0.086s)
             Mean action noise std: 1.95
          Mean value_function loss: 223.7203
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.7581
                       Mean reward: 735.14
               Mean episode length: 206.44
    Episode_Reward/reaching_object: 0.8174
     Episode_Reward/lifting_object: 154.7052
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.93s
                      Time elapsed: 00:35:53
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 51522 steps/s (collection: 1.815s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 207.9676
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.7645
                       Mean reward: 846.44
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 0.8291
     Episode_Reward/lifting_object: 157.2279
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 1.91s
                      Time elapsed: 00:35:55
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 52028 steps/s (collection: 1.801s, learning 0.088s)
             Mean action noise std: 1.95
          Mean value_function loss: 221.2354
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.7708
                       Mean reward: 820.37
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 0.8345
     Episode_Reward/lifting_object: 157.9007
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 1.89s
                      Time elapsed: 00:35:57
                               ETA: 00:34:26

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 51722 steps/s (collection: 1.806s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 207.2176
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.7757
                       Mean reward: 790.71
               Mean episode length: 217.77
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 157.9662
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.90s
                      Time elapsed: 00:35:59
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 52001 steps/s (collection: 1.798s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 186.4186
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7757
                       Mean reward: 760.36
               Mean episode length: 210.18
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 158.6571
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 1.89s
                      Time elapsed: 00:36:01
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 51821 steps/s (collection: 1.812s, learning 0.085s)
             Mean action noise std: 1.95
          Mean value_function loss: 201.9376
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.7750
                       Mean reward: 794.90
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 158.3000
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 1.90s
                      Time elapsed: 00:36:03
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 49768 steps/s (collection: 1.867s, learning 0.108s)
             Mean action noise std: 1.95
          Mean value_function loss: 210.1732
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7771
                       Mean reward: 808.70
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 0.8205
     Episode_Reward/lifting_object: 155.0410
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.98s
                      Time elapsed: 00:36:05
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 50516 steps/s (collection: 1.856s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 197.7910
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.7797
                       Mean reward: 814.05
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 0.8269
     Episode_Reward/lifting_object: 156.6795
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.95s
                      Time elapsed: 00:36:07
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 51528 steps/s (collection: 1.821s, learning 0.087s)
             Mean action noise std: 1.95
          Mean value_function loss: 200.0126
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.7772
                       Mean reward: 764.74
               Mean episode length: 211.97
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 159.8781
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.91s
                      Time elapsed: 00:36:09
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 51915 steps/s (collection: 1.806s, learning 0.088s)
             Mean action noise std: 1.95
          Mean value_function loss: 204.6107
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 36.7761
                       Mean reward: 823.98
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 0.8356
     Episode_Reward/lifting_object: 158.6017
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.89s
                      Time elapsed: 00:36:11
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 50957 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 214.8864
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.7782
                       Mean reward: 775.84
               Mean episode length: 213.83
    Episode_Reward/reaching_object: 0.8098
     Episode_Reward/lifting_object: 153.4263
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.93s
                      Time elapsed: 00:36:13
                               ETA: 00:34:08

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 51256 steps/s (collection: 1.820s, learning 0.098s)
             Mean action noise std: 1.95
          Mean value_function loss: 218.8329
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.7811
                       Mean reward: 790.23
               Mean episode length: 217.48
    Episode_Reward/reaching_object: 0.8246
     Episode_Reward/lifting_object: 156.5114
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.92s
                      Time elapsed: 00:36:15
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 52066 steps/s (collection: 1.801s, learning 0.088s)
             Mean action noise std: 1.95
          Mean value_function loss: 201.8699
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.7817
                       Mean reward: 754.60
               Mean episode length: 210.19
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 160.7727
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.89s
                      Time elapsed: 00:36:16
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 51524 steps/s (collection: 1.820s, learning 0.088s)
             Mean action noise std: 1.95
          Mean value_function loss: 187.3469
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.7819
                       Mean reward: 786.55
               Mean episode length: 215.73
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 157.9734
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.91s
                      Time elapsed: 00:36:18
                               ETA: 00:34:01

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 51196 steps/s (collection: 1.816s, learning 0.104s)
             Mean action noise std: 1.96
          Mean value_function loss: 208.6057
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.7861
                       Mean reward: 845.52
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.8283
     Episode_Reward/lifting_object: 157.4555
      Episode_Reward/object_height: 0.0306
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 1.92s
                      Time elapsed: 00:36:20
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 51582 steps/s (collection: 1.818s, learning 0.088s)
             Mean action noise std: 1.96
          Mean value_function loss: 207.0082
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 36.7934
                       Mean reward: 782.43
               Mean episode length: 214.83
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 157.7610
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 1.91s
                      Time elapsed: 00:36:22
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 51511 steps/s (collection: 1.812s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 188.1154
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.7949
                       Mean reward: 817.71
               Mean episode length: 222.37
    Episode_Reward/reaching_object: 0.8530
     Episode_Reward/lifting_object: 162.8549
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.91s
                      Time elapsed: 00:36:24
                               ETA: 00:33:54

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 51413 steps/s (collection: 1.822s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 211.0227
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.7998
                       Mean reward: 739.08
               Mean episode length: 204.31
    Episode_Reward/reaching_object: 0.8102
     Episode_Reward/lifting_object: 153.7384
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 1.91s
                      Time elapsed: 00:36:26
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 50979 steps/s (collection: 1.839s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 235.4931
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.8077
                       Mean reward: 784.37
               Mean episode length: 214.18
    Episode_Reward/reaching_object: 0.8052
     Episode_Reward/lifting_object: 152.9459
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.93s
                      Time elapsed: 00:36:28
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 51186 steps/s (collection: 1.831s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 220.3912
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.8105
                       Mean reward: 797.52
               Mean episode length: 217.95
    Episode_Reward/reaching_object: 0.8112
     Episode_Reward/lifting_object: 153.6671
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 1.92s
                      Time elapsed: 00:36:30
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 50790 steps/s (collection: 1.843s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 235.0566
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.8106
                       Mean reward: 771.88
               Mean episode length: 211.87
    Episode_Reward/reaching_object: 0.8005
     Episode_Reward/lifting_object: 151.9227
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 1.94s
                      Time elapsed: 00:36:32
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 50075 steps/s (collection: 1.843s, learning 0.121s)
             Mean action noise std: 1.96
          Mean value_function loss: 191.6942
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.8145
                       Mean reward: 770.14
               Mean episode length: 213.61
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 158.8263
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 1.96s
                      Time elapsed: 00:36:34
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 50395 steps/s (collection: 1.839s, learning 0.112s)
             Mean action noise std: 1.96
          Mean value_function loss: 204.5249
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.8179
                       Mean reward: 835.02
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 0.7992
     Episode_Reward/lifting_object: 151.2744
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.95s
                      Time elapsed: 00:36:36
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 51414 steps/s (collection: 1.820s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 174.0524
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.8239
                       Mean reward: 845.51
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 0.8657
     Episode_Reward/lifting_object: 164.8959
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.91s
                      Time elapsed: 00:36:38
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 50685 steps/s (collection: 1.850s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 207.6747
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.8276
                       Mean reward: 836.94
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 0.8435
     Episode_Reward/lifting_object: 160.5856
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.94s
                      Time elapsed: 00:36:39
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 50456 steps/s (collection: 1.839s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 176.3273
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.8303
                       Mean reward: 831.75
               Mean episode length: 225.75
    Episode_Reward/reaching_object: 0.8551
     Episode_Reward/lifting_object: 162.9855
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 1.95s
                      Time elapsed: 00:36:41
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 49336 steps/s (collection: 1.903s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 216.1153
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 36.8322
                       Mean reward: 751.75
               Mean episode length: 205.90
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 161.0667
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.99s
                      Time elapsed: 00:36:43
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 51135 steps/s (collection: 1.830s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 234.9506
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.8334
                       Mean reward: 744.77
               Mean episode length: 204.50
    Episode_Reward/reaching_object: 0.7967
     Episode_Reward/lifting_object: 151.1808
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.92s
                      Time elapsed: 00:36:45
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 49602 steps/s (collection: 1.871s, learning 0.111s)
             Mean action noise std: 1.97
          Mean value_function loss: 251.2415
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.8378
                       Mean reward: 799.50
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 0.8178
     Episode_Reward/lifting_object: 155.9493
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 1.98s
                      Time elapsed: 00:36:47
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 50506 steps/s (collection: 1.840s, learning 0.106s)
             Mean action noise std: 1.97
          Mean value_function loss: 226.1069
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.8442
                       Mean reward: 737.81
               Mean episode length: 203.38
    Episode_Reward/reaching_object: 0.8235
     Episode_Reward/lifting_object: 156.8497
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 1.95s
                      Time elapsed: 00:36:49
                               ETA: 00:33:25

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 48857 steps/s (collection: 1.903s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 241.8849
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.8551
                       Mean reward: 772.53
               Mean episode length: 212.30
    Episode_Reward/reaching_object: 0.7992
     Episode_Reward/lifting_object: 151.7732
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.01s
                      Time elapsed: 00:36:51
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 49940 steps/s (collection: 1.882s, learning 0.086s)
             Mean action noise std: 1.97
          Mean value_function loss: 231.3579
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.8640
                       Mean reward: 786.96
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 0.8149
     Episode_Reward/lifting_object: 155.1416
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.97s
                      Time elapsed: 00:36:53
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 50861 steps/s (collection: 1.849s, learning 0.084s)
             Mean action noise std: 1.97
          Mean value_function loss: 239.9812
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.8725
                       Mean reward: 765.48
               Mean episode length: 209.18
    Episode_Reward/reaching_object: 0.8045
     Episode_Reward/lifting_object: 153.0013
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 1.93s
                      Time elapsed: 00:36:55
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 50309 steps/s (collection: 1.861s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 247.6838
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.8907
                       Mean reward: 763.94
               Mean episode length: 208.49
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 153.6393
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 1.95s
                      Time elapsed: 00:36:57
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 50005 steps/s (collection: 1.862s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 223.7186
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.9082
                       Mean reward: 773.72
               Mean episode length: 211.30
    Episode_Reward/reaching_object: 0.8304
     Episode_Reward/lifting_object: 158.8159
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 1.97s
                      Time elapsed: 00:36:59
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 50261 steps/s (collection: 1.859s, learning 0.097s)
             Mean action noise std: 1.98
          Mean value_function loss: 239.4047
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.9145
                       Mean reward: 760.57
               Mean episode length: 208.56
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 148.4427
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 1.96s
                      Time elapsed: 00:37:01
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 49191 steps/s (collection: 1.895s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 208.7392
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.9197
                       Mean reward: 792.86
               Mean episode length: 215.09
    Episode_Reward/reaching_object: 0.8197
     Episode_Reward/lifting_object: 156.5002
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.00s
                      Time elapsed: 00:37:03
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 50081 steps/s (collection: 1.869s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 199.9132
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.9218
                       Mean reward: 792.30
               Mean episode length: 215.76
    Episode_Reward/reaching_object: 0.8096
     Episode_Reward/lifting_object: 154.7427
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 1.96s
                      Time elapsed: 00:37:05
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 49549 steps/s (collection: 1.875s, learning 0.109s)
             Mean action noise std: 1.98
          Mean value_function loss: 197.5803
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.9296
                       Mean reward: 790.62
               Mean episode length: 215.02
    Episode_Reward/reaching_object: 0.8336
     Episode_Reward/lifting_object: 159.3855
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 1.98s
                      Time elapsed: 00:37:07
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 49173 steps/s (collection: 1.905s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 209.0116
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.9407
                       Mean reward: 815.70
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 161.1184
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.00s
                      Time elapsed: 00:37:09
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 49658 steps/s (collection: 1.893s, learning 0.087s)
             Mean action noise std: 1.98
          Mean value_function loss: 175.0662
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.9499
                       Mean reward: 810.95
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 159.9706
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 1.98s
                      Time elapsed: 00:37:11
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 48815 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 159.8935
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.9526
                       Mean reward: 809.18
               Mean episode length: 219.38
    Episode_Reward/reaching_object: 0.8639
     Episode_Reward/lifting_object: 165.5213
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.01s
                      Time elapsed: 00:37:13
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 49520 steps/s (collection: 1.895s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 210.1754
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.9582
                       Mean reward: 812.92
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 0.8329
     Episode_Reward/lifting_object: 159.5720
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 1.99s
                      Time elapsed: 00:37:15
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 50217 steps/s (collection: 1.870s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 194.7928
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.9639
                       Mean reward: 806.38
               Mean episode length: 218.82
    Episode_Reward/reaching_object: 0.8194
     Episode_Reward/lifting_object: 156.6013
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 1.96s
                      Time elapsed: 00:37:17
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 48759 steps/s (collection: 1.909s, learning 0.107s)
             Mean action noise std: 1.99
          Mean value_function loss: 194.5950
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.9673
                       Mean reward: 787.70
               Mean episode length: 215.96
    Episode_Reward/reaching_object: 0.8332
     Episode_Reward/lifting_object: 159.1351
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.02s
                      Time elapsed: 00:37:19
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 48657 steps/s (collection: 1.926s, learning 0.094s)
             Mean action noise std: 1.99
          Mean value_function loss: 182.4964
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.9747
                       Mean reward: 828.72
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 0.8319
     Episode_Reward/lifting_object: 158.9400
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.02s
                      Time elapsed: 00:37:21
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 48892 steps/s (collection: 1.906s, learning 0.105s)
             Mean action noise std: 1.99
          Mean value_function loss: 178.6422
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.9839
                       Mean reward: 793.97
               Mean episode length: 216.60
    Episode_Reward/reaching_object: 0.8317
     Episode_Reward/lifting_object: 158.8470
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.01s
                      Time elapsed: 00:37:23
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 48910 steps/s (collection: 1.917s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 148.0875
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.9951
                       Mean reward: 815.84
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 159.3320
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.01s
                      Time elapsed: 00:37:25
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 49344 steps/s (collection: 1.898s, learning 0.094s)
             Mean action noise std: 1.99
          Mean value_function loss: 158.8215
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.0023
                       Mean reward: 775.72
               Mean episode length: 212.92
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 160.3196
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.99s
                      Time elapsed: 00:37:27
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 50283 steps/s (collection: 1.866s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 169.0798
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.0207
                       Mean reward: 810.13
               Mean episode length: 220.38
    Episode_Reward/reaching_object: 0.8528
     Episode_Reward/lifting_object: 163.0722
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 1.96s
                      Time elapsed: 00:37:29
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 50486 steps/s (collection: 1.853s, learning 0.095s)
             Mean action noise std: 2.00
          Mean value_function loss: 188.6342
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.0332
                       Mean reward: 771.81
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 159.6056
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 1.95s
                      Time elapsed: 00:37:31
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 50981 steps/s (collection: 1.841s, learning 0.087s)
             Mean action noise std: 2.00
          Mean value_function loss: 172.6409
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 37.0435
                       Mean reward: 812.89
               Mean episode length: 220.27
    Episode_Reward/reaching_object: 0.8324
     Episode_Reward/lifting_object: 159.2681
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 1.93s
                      Time elapsed: 00:37:33
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 49331 steps/s (collection: 1.880s, learning 0.113s)
             Mean action noise std: 2.00
          Mean value_function loss: 164.5263
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.0510
                       Mean reward: 862.71
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 0.8796
     Episode_Reward/lifting_object: 168.7049
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 1.99s
                      Time elapsed: 00:37:35
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 49320 steps/s (collection: 1.879s, learning 0.114s)
             Mean action noise std: 2.00
          Mean value_function loss: 152.8603
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.0530
                       Mean reward: 824.49
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 160.7012
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.99s
                      Time elapsed: 00:37:37
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 50147 steps/s (collection: 1.855s, learning 0.105s)
             Mean action noise std: 2.00
          Mean value_function loss: 183.2720
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.0549
                       Mean reward: 833.47
               Mean episode length: 225.47
    Episode_Reward/reaching_object: 0.8539
     Episode_Reward/lifting_object: 163.5288
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 1.96s
                      Time elapsed: 00:37:39
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 50833 steps/s (collection: 1.848s, learning 0.086s)
             Mean action noise std: 2.00
          Mean value_function loss: 163.5401
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.0575
                       Mean reward: 795.53
               Mean episode length: 216.55
    Episode_Reward/reaching_object: 0.8436
     Episode_Reward/lifting_object: 161.4185
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 1.93s
                      Time elapsed: 00:37:41
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 50677 steps/s (collection: 1.850s, learning 0.090s)
             Mean action noise std: 2.00
          Mean value_function loss: 149.1272
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.0622
                       Mean reward: 893.36
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.8769
     Episode_Reward/lifting_object: 168.4618
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 1.94s
                      Time elapsed: 00:37:43
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 50634 steps/s (collection: 1.850s, learning 0.091s)
             Mean action noise std: 2.00
          Mean value_function loss: 150.0485
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 37.0693
                       Mean reward: 820.73
               Mean episode length: 222.65
    Episode_Reward/reaching_object: 0.8606
     Episode_Reward/lifting_object: 165.3689
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 1.94s
                      Time elapsed: 00:37:45
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 50044 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 2.00
          Mean value_function loss: 155.9998
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.0738
                       Mean reward: 826.64
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 0.8538
     Episode_Reward/lifting_object: 163.8593
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 1.96s
                      Time elapsed: 00:37:47
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 50263 steps/s (collection: 1.851s, learning 0.105s)
             Mean action noise std: 2.01
          Mean value_function loss: 140.9317
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.0789
                       Mean reward: 872.37
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 0.8816
     Episode_Reward/lifting_object: 169.7272
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 1.96s
                      Time elapsed: 00:37:49
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 50196 steps/s (collection: 1.859s, learning 0.100s)
             Mean action noise std: 2.01
          Mean value_function loss: 192.3867
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.0787
                       Mean reward: 795.75
               Mean episode length: 216.03
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 161.7413
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 1.96s
                      Time elapsed: 00:37:50
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 50325 steps/s (collection: 1.857s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 189.9947
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.0814
                       Mean reward: 783.68
               Mean episode length: 214.69
    Episode_Reward/reaching_object: 0.8597
     Episode_Reward/lifting_object: 165.2606
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 1.95s
                      Time elapsed: 00:37:52
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 50742 steps/s (collection: 1.845s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 206.6619
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.0875
                       Mean reward: 769.73
               Mean episode length: 209.29
    Episode_Reward/reaching_object: 0.8248
     Episode_Reward/lifting_object: 158.2841
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 1.94s
                      Time elapsed: 00:37:54
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 50733 steps/s (collection: 1.850s, learning 0.087s)
             Mean action noise std: 2.01
          Mean value_function loss: 176.6403
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.0946
                       Mean reward: 828.15
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 0.8781
     Episode_Reward/lifting_object: 169.8788
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 1.94s
                      Time elapsed: 00:37:56
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 49739 steps/s (collection: 1.884s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 211.0164
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.0974
                       Mean reward: 823.52
               Mean episode length: 222.83
    Episode_Reward/reaching_object: 0.8414
     Episode_Reward/lifting_object: 162.3090
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 1.98s
                      Time elapsed: 00:37:58
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 50443 steps/s (collection: 1.863s, learning 0.086s)
             Mean action noise std: 2.01
          Mean value_function loss: 179.6196
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.1003
                       Mean reward: 831.95
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 0.8603
     Episode_Reward/lifting_object: 166.2999
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 1.95s
                      Time elapsed: 00:38:00
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 50212 steps/s (collection: 1.868s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 198.5763
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.1046
                       Mean reward: 846.42
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 0.8456
     Episode_Reward/lifting_object: 163.3564
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 1.96s
                      Time elapsed: 00:38:02
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 50454 steps/s (collection: 1.858s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 192.7135
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1065
                       Mean reward: 800.84
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 0.8346
     Episode_Reward/lifting_object: 161.1553
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 1.95s
                      Time elapsed: 00:38:04
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 50437 steps/s (collection: 1.840s, learning 0.110s)
             Mean action noise std: 2.01
          Mean value_function loss: 171.1287
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.1115
                       Mean reward: 831.70
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 0.8353
     Episode_Reward/lifting_object: 161.5026
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.95s
                      Time elapsed: 00:38:06
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 50116 steps/s (collection: 1.857s, learning 0.104s)
             Mean action noise std: 2.01
          Mean value_function loss: 205.2985
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.1162
                       Mean reward: 805.09
               Mean episode length: 219.06
    Episode_Reward/reaching_object: 0.8449
     Episode_Reward/lifting_object: 163.6260
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 1.96s
                      Time elapsed: 00:38:08
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 50242 steps/s (collection: 1.866s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 195.2239
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.1280
                       Mean reward: 785.45
               Mean episode length: 214.78
    Episode_Reward/reaching_object: 0.8512
     Episode_Reward/lifting_object: 164.8973
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.96s
                      Time elapsed: 00:38:10
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 50632 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.02
          Mean value_function loss: 225.7702
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.1487
                       Mean reward: 779.97
               Mean episode length: 213.00
    Episode_Reward/reaching_object: 0.8332
     Episode_Reward/lifting_object: 161.4681
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.94s
                      Time elapsed: 00:38:12
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 50292 steps/s (collection: 1.865s, learning 0.089s)
             Mean action noise std: 2.02
          Mean value_function loss: 212.9819
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.1572
                       Mean reward: 831.36
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 0.8346
     Episode_Reward/lifting_object: 162.1083
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.95s
                      Time elapsed: 00:38:14
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 50504 steps/s (collection: 1.847s, learning 0.100s)
             Mean action noise std: 2.02
          Mean value_function loss: 194.4587
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.1607
                       Mean reward: 808.93
               Mean episode length: 219.91
    Episode_Reward/reaching_object: 0.8357
     Episode_Reward/lifting_object: 162.5245
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.95s
                      Time elapsed: 00:38:16
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 50926 steps/s (collection: 1.839s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 219.2541
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.1636
                       Mean reward: 798.53
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 0.8454
     Episode_Reward/lifting_object: 164.4130
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 1.93s
                      Time elapsed: 00:38:18
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 50395 steps/s (collection: 1.853s, learning 0.098s)
             Mean action noise std: 2.02
          Mean value_function loss: 180.7873
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.1656
                       Mean reward: 829.29
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 0.8529
     Episode_Reward/lifting_object: 166.1473
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 1.95s
                      Time elapsed: 00:38:20
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 50286 steps/s (collection: 1.862s, learning 0.093s)
             Mean action noise std: 2.02
          Mean value_function loss: 212.0428
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.1718
                       Mean reward: 825.36
               Mean episode length: 224.15
    Episode_Reward/reaching_object: 0.8537
     Episode_Reward/lifting_object: 166.4370
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 1.95s
                      Time elapsed: 00:38:22
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 50842 steps/s (collection: 1.840s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 188.3725
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.1790
                       Mean reward: 806.19
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 162.7166
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 1.93s
                      Time elapsed: 00:38:24
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 50564 steps/s (collection: 1.842s, learning 0.102s)
             Mean action noise std: 2.02
          Mean value_function loss: 183.6962
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.1834
                       Mean reward: 817.40
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 0.8350
     Episode_Reward/lifting_object: 162.3648
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 1.94s
                      Time elapsed: 00:38:26
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 50179 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 189.5921
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.1939
                       Mean reward: 799.23
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 162.5594
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 1.96s
                      Time elapsed: 00:38:28
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 50047 steps/s (collection: 1.876s, learning 0.088s)
             Mean action noise std: 2.03
          Mean value_function loss: 197.0060
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 37.2076
                       Mean reward: 874.70
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.8240
     Episode_Reward/lifting_object: 160.7887
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 1.96s
                      Time elapsed: 00:38:29
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 50180 steps/s (collection: 1.871s, learning 0.088s)
             Mean action noise std: 2.03
          Mean value_function loss: 191.0813
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.2114
                       Mean reward: 840.74
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 0.8446
     Episode_Reward/lifting_object: 164.7087
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 1.96s
                      Time elapsed: 00:38:31
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 49425 steps/s (collection: 1.888s, learning 0.101s)
             Mean action noise std: 2.03
          Mean value_function loss: 188.3320
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.2193
                       Mean reward: 824.90
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 0.8414
     Episode_Reward/lifting_object: 164.1416
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 1.99s
                      Time elapsed: 00:38:33
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 49550 steps/s (collection: 1.858s, learning 0.126s)
             Mean action noise std: 2.03
          Mean value_function loss: 213.5775
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.2238
                       Mean reward: 810.95
               Mean episode length: 219.73
    Episode_Reward/reaching_object: 0.8455
     Episode_Reward/lifting_object: 165.2759
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.98s
                      Time elapsed: 00:38:35
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 49322 steps/s (collection: 1.892s, learning 0.101s)
             Mean action noise std: 2.03
          Mean value_function loss: 198.3439
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.2273
                       Mean reward: 790.36
               Mean episode length: 215.66
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 163.1399
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 1.99s
                      Time elapsed: 00:38:37
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 48595 steps/s (collection: 1.910s, learning 0.113s)
             Mean action noise std: 2.03
          Mean value_function loss: 172.4237
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.2271
                       Mean reward: 848.70
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 0.8348
     Episode_Reward/lifting_object: 162.8033
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.02s
                      Time elapsed: 00:38:39
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 49100 steps/s (collection: 1.905s, learning 0.097s)
             Mean action noise std: 2.03
          Mean value_function loss: 185.4047
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.2326
                       Mean reward: 852.10
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 0.8296
     Episode_Reward/lifting_object: 162.0847
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.00s
                      Time elapsed: 00:38:41
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 49424 steps/s (collection: 1.892s, learning 0.097s)
             Mean action noise std: 2.03
          Mean value_function loss: 179.0331
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.2391
                       Mean reward: 802.63
               Mean episode length: 217.42
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 162.5296
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.99s
                      Time elapsed: 00:38:43
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 49311 steps/s (collection: 1.896s, learning 0.098s)
             Mean action noise std: 2.03
          Mean value_function loss: 142.4644
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.2448
                       Mean reward: 871.53
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 0.8661
     Episode_Reward/lifting_object: 169.3693
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 1.99s
                      Time elapsed: 00:38:45
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 49930 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 2.04
          Mean value_function loss: 136.8745
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.2513
                       Mean reward: 847.65
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 0.8678
     Episode_Reward/lifting_object: 169.6414
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.97s
                      Time elapsed: 00:38:47
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 50146 steps/s (collection: 1.863s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 135.5780
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.2624
                       Mean reward: 843.30
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 0.8673
     Episode_Reward/lifting_object: 169.9435
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 1.96s
                      Time elapsed: 00:38:49
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 49494 steps/s (collection: 1.878s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 154.2973
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 37.2691
                       Mean reward: 876.60
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.8719
     Episode_Reward/lifting_object: 170.7378
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.99s
                      Time elapsed: 00:38:51
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 50294 steps/s (collection: 1.861s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 150.2090
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.2763
                       Mean reward: 851.35
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 167.9606
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 1.95s
                      Time elapsed: 00:38:53
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 49712 steps/s (collection: 1.876s, learning 0.102s)
             Mean action noise std: 2.04
          Mean value_function loss: 143.8765
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.2853
                       Mean reward: 860.06
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 0.8572
     Episode_Reward/lifting_object: 167.9327
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.98s
                      Time elapsed: 00:38:55
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 50044 steps/s (collection: 1.865s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 168.6187
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 37.2997
                       Mean reward: 829.06
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 0.8580
     Episode_Reward/lifting_object: 167.8171
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 1.96s
                      Time elapsed: 00:38:57
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 49769 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 163.1619
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.3096
                       Mean reward: 827.99
               Mean episode length: 222.57
    Episode_Reward/reaching_object: 0.8600
     Episode_Reward/lifting_object: 168.9061
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 1.98s
                      Time elapsed: 00:38:59
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 47737 steps/s (collection: 1.944s, learning 0.116s)
             Mean action noise std: 2.05
          Mean value_function loss: 168.9148
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.3142
                       Mean reward: 813.46
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 165.9619
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.06s
                      Time elapsed: 00:39:01
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 44120 steps/s (collection: 2.111s, learning 0.117s)
             Mean action noise std: 2.05
          Mean value_function loss: 166.7714
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.3211
                       Mean reward: 757.28
               Mean episode length: 208.50
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 165.5651
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.23s
                      Time elapsed: 00:39:03
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 45123 steps/s (collection: 2.061s, learning 0.117s)
             Mean action noise std: 2.05
          Mean value_function loss: 162.4167
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.3281
                       Mean reward: 844.40
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 0.8462
     Episode_Reward/lifting_object: 165.6391
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.18s
                      Time elapsed: 00:39:06
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 47014 steps/s (collection: 2.000s, learning 0.091s)
             Mean action noise std: 2.05
          Mean value_function loss: 153.0958
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.3325
                       Mean reward: 826.99
               Mean episode length: 224.92
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 165.4820
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.09s
                      Time elapsed: 00:39:08
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 48385 steps/s (collection: 1.927s, learning 0.104s)
             Mean action noise std: 2.05
          Mean value_function loss: 182.4476
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.3391
                       Mean reward: 812.43
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 164.3816
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.03s
                      Time elapsed: 00:39:10
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.920s, learning 0.121s)
             Mean action noise std: 2.05
          Mean value_function loss: 152.9206
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 37.3442
                       Mean reward: 811.26
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 0.8493
     Episode_Reward/lifting_object: 166.9455
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.04s
                      Time elapsed: 00:39:12
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 48084 steps/s (collection: 1.926s, learning 0.119s)
             Mean action noise std: 2.05
          Mean value_function loss: 143.3784
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.3472
                       Mean reward: 894.39
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.8496
     Episode_Reward/lifting_object: 167.4020
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.04s
                      Time elapsed: 00:39:14
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 47488 steps/s (collection: 1.976s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 172.9280
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.3523
                       Mean reward: 829.48
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 0.8573
     Episode_Reward/lifting_object: 168.8584
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.07s
                      Time elapsed: 00:39:16
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 47893 steps/s (collection: 1.921s, learning 0.132s)
             Mean action noise std: 2.05
          Mean value_function loss: 146.4624
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.3574
                       Mean reward: 890.23
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 0.8562
     Episode_Reward/lifting_object: 168.6445
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.05s
                      Time elapsed: 00:39:18
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 49194 steps/s (collection: 1.867s, learning 0.131s)
             Mean action noise std: 2.05
          Mean value_function loss: 136.3847
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.3587
                       Mean reward: 877.63
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.8611
     Episode_Reward/lifting_object: 169.7351
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.00s
                      Time elapsed: 00:39:20
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 48135 steps/s (collection: 1.923s, learning 0.120s)
             Mean action noise std: 2.05
          Mean value_function loss: 155.0227
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3672
                       Mean reward: 841.41
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: 167.8709
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.04s
                      Time elapsed: 00:39:22
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 47643 steps/s (collection: 1.926s, learning 0.137s)
             Mean action noise std: 2.06
          Mean value_function loss: 141.8490
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 37.3767
                       Mean reward: 868.82
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 0.8417
     Episode_Reward/lifting_object: 165.8020
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.06s
                      Time elapsed: 00:39:24
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 49424 steps/s (collection: 1.900s, learning 0.089s)
             Mean action noise std: 2.06
          Mean value_function loss: 129.8732
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.3856
                       Mean reward: 859.78
               Mean episode length: 231.68
    Episode_Reward/reaching_object: 0.8603
     Episode_Reward/lifting_object: 170.0648
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 1.99s
                      Time elapsed: 00:39:26
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 47731 steps/s (collection: 1.941s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 150.3186
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.3957
                       Mean reward: 823.91
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 0.8407
     Episode_Reward/lifting_object: 165.9494
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.06s
                      Time elapsed: 00:39:28
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 49551 steps/s (collection: 1.871s, learning 0.113s)
             Mean action noise std: 2.06
          Mean value_function loss: 163.1301
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.4025
                       Mean reward: 815.57
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 0.8442
     Episode_Reward/lifting_object: 166.4808
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.98s
                      Time elapsed: 00:39:30
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 49032 steps/s (collection: 1.916s, learning 0.089s)
             Mean action noise std: 2.06
          Mean value_function loss: 140.4099
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.4113
                       Mean reward: 861.63
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 167.5501
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.00s
                      Time elapsed: 00:39:32
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 48231 steps/s (collection: 1.927s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 152.7346
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.4197
                       Mean reward: 860.82
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 0.8509
     Episode_Reward/lifting_object: 168.0508
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.04s
                      Time elapsed: 00:39:34
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 49540 steps/s (collection: 1.899s, learning 0.085s)
             Mean action noise std: 2.06
          Mean value_function loss: 137.5687
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.4279
                       Mean reward: 877.31
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.8573
     Episode_Reward/lifting_object: 169.3559
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 1.98s
                      Time elapsed: 00:39:36
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 48348 steps/s (collection: 1.927s, learning 0.106s)
             Mean action noise std: 2.07
          Mean value_function loss: 164.3272
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.4492
                       Mean reward: 835.71
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 0.8532
     Episode_Reward/lifting_object: 168.6827
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.03s
                      Time elapsed: 00:39:38
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 49798 steps/s (collection: 1.875s, learning 0.099s)
             Mean action noise std: 2.07
          Mean value_function loss: 142.0015
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.4622
                       Mean reward: 830.20
               Mean episode length: 224.70
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 165.9167
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 1.97s
                      Time elapsed: 00:39:40
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 49400 steps/s (collection: 1.903s, learning 0.087s)
             Mean action noise std: 2.07
          Mean value_function loss: 131.8641
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 37.4676
                       Mean reward: 819.79
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 0.8634
     Episode_Reward/lifting_object: 171.3269
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.99s
                      Time elapsed: 00:39:42
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 49561 steps/s (collection: 1.883s, learning 0.101s)
             Mean action noise std: 2.07
          Mean value_function loss: 157.3907
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.4741
                       Mean reward: 843.31
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 0.8421
     Episode_Reward/lifting_object: 166.7143
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.98s
                      Time elapsed: 00:39:44
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 48560 steps/s (collection: 1.901s, learning 0.124s)
             Mean action noise std: 2.07
          Mean value_function loss: 138.4590
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.4773
                       Mean reward: 846.83
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 0.8550
     Episode_Reward/lifting_object: 169.6252
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.02s
                      Time elapsed: 00:39:46
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 48995 steps/s (collection: 1.893s, learning 0.114s)
             Mean action noise std: 2.07
          Mean value_function loss: 125.2813
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.4817
                       Mean reward: 871.27
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 0.8529
     Episode_Reward/lifting_object: 169.0611
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.01s
                      Time elapsed: 00:39:48
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 49165 steps/s (collection: 1.914s, learning 0.086s)
             Mean action noise std: 2.07
          Mean value_function loss: 129.8533
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.4849
                       Mean reward: 858.43
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 170.7856
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.00s
                      Time elapsed: 00:39:50
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 49742 steps/s (collection: 1.883s, learning 0.094s)
             Mean action noise std: 2.07
          Mean value_function loss: 107.5333
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.4865
                       Mean reward: 871.75
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 0.8707
     Episode_Reward/lifting_object: 172.9586
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.98s
                      Time elapsed: 00:39:52
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 48931 steps/s (collection: 1.893s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 136.3428
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.4883
                       Mean reward: 871.99
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 0.8780
     Episode_Reward/lifting_object: 174.9484
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.01s
                      Time elapsed: 00:39:54
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 49119 steps/s (collection: 1.880s, learning 0.121s)
             Mean action noise std: 2.07
          Mean value_function loss: 135.2096
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.4899
                       Mean reward: 854.18
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 0.8581
     Episode_Reward/lifting_object: 170.4711
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.00s
                      Time elapsed: 00:39:56
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 49231 steps/s (collection: 1.879s, learning 0.118s)
             Mean action noise std: 2.07
          Mean value_function loss: 130.0561
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.4906
                       Mean reward: 831.12
               Mean episode length: 224.42
    Episode_Reward/reaching_object: 0.8531
     Episode_Reward/lifting_object: 169.3922
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.00s
                      Time elapsed: 00:39:58
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 48731 steps/s (collection: 1.917s, learning 0.100s)
             Mean action noise std: 2.07
          Mean value_function loss: 128.0348
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.4928
                       Mean reward: 803.75
               Mean episode length: 218.51
    Episode_Reward/reaching_object: 0.8582
     Episode_Reward/lifting_object: 170.1443
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.02s
                      Time elapsed: 00:40:00
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 48913 steps/s (collection: 1.887s, learning 0.123s)
             Mean action noise std: 2.07
          Mean value_function loss: 123.6701
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.4998
                       Mean reward: 868.15
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 0.8515
     Episode_Reward/lifting_object: 169.2648
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.01s
                      Time elapsed: 00:40:02
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 49132 steps/s (collection: 1.869s, learning 0.132s)
             Mean action noise std: 2.08
          Mean value_function loss: 144.7700
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.5072
                       Mean reward: 858.85
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 0.8497
     Episode_Reward/lifting_object: 168.3498
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.00s
                      Time elapsed: 00:40:04
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 48578 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 2.08
          Mean value_function loss: 142.4951
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 37.5170
                       Mean reward: 824.35
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 168.9169
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.02s
                      Time elapsed: 00:40:06
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 49188 steps/s (collection: 1.883s, learning 0.116s)
             Mean action noise std: 2.08
          Mean value_function loss: 138.1831
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.5240
                       Mean reward: 840.87
               Mean episode length: 226.10
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 168.1376
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.00s
                      Time elapsed: 00:40:08
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 48257 steps/s (collection: 1.918s, learning 0.119s)
             Mean action noise std: 2.08
          Mean value_function loss: 126.8654
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.5275
                       Mean reward: 847.97
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 168.9258
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.04s
                      Time elapsed: 00:40:10
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 48737 steps/s (collection: 1.897s, learning 0.120s)
             Mean action noise std: 2.08
          Mean value_function loss: 157.6792
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.5318
                       Mean reward: 840.70
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 166.3230
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.02s
                      Time elapsed: 00:40:12
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 48026 steps/s (collection: 1.923s, learning 0.124s)
             Mean action noise std: 2.08
          Mean value_function loss: 148.0624
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.5360
                       Mean reward: 840.68
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 0.8614
     Episode_Reward/lifting_object: 172.0385
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.05s
                      Time elapsed: 00:40:14
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 48004 steps/s (collection: 1.938s, learning 0.110s)
             Mean action noise std: 2.08
          Mean value_function loss: 152.9391
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.5422
                       Mean reward: 856.44
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 0.8524
     Episode_Reward/lifting_object: 169.8379
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.05s
                      Time elapsed: 00:40:16
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 49207 steps/s (collection: 1.881s, learning 0.117s)
             Mean action noise std: 2.08
          Mean value_function loss: 133.1663
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.5510
                       Mean reward: 851.26
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 0.8694
     Episode_Reward/lifting_object: 173.6574
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.00s
                      Time elapsed: 00:40:18
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 48420 steps/s (collection: 1.917s, learning 0.113s)
             Mean action noise std: 2.08
          Mean value_function loss: 171.0069
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.5666
                       Mean reward: 871.26
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 0.8460
     Episode_Reward/lifting_object: 168.9276
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.03s
                      Time elapsed: 00:40:20
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 48930 steps/s (collection: 1.896s, learning 0.113s)
             Mean action noise std: 2.08
          Mean value_function loss: 187.7607
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.5739
                       Mean reward: 803.10
               Mean episode length: 217.39
    Episode_Reward/reaching_object: 0.8386
     Episode_Reward/lifting_object: 167.4528
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.01s
                      Time elapsed: 00:40:22
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 48293 steps/s (collection: 1.924s, learning 0.112s)
             Mean action noise std: 2.08
          Mean value_function loss: 157.3697
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.5794
                       Mean reward: 887.37
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: 168.9530
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.04s
                      Time elapsed: 00:40:24
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 47873 steps/s (collection: 1.954s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 176.6935
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.5893
                       Mean reward: 830.54
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 0.8367
     Episode_Reward/lifting_object: 167.2322
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.05s
                      Time elapsed: 00:40:26
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 48376 steps/s (collection: 1.920s, learning 0.113s)
             Mean action noise std: 2.09
          Mean value_function loss: 143.5556
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.5978
                       Mean reward: 824.44
               Mean episode length: 223.31
    Episode_Reward/reaching_object: 0.8275
     Episode_Reward/lifting_object: 165.1064
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.03s
                      Time elapsed: 00:40:29
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 50299 steps/s (collection: 1.861s, learning 0.093s)
             Mean action noise std: 2.09
          Mean value_function loss: 118.9058
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.6057
                       Mean reward: 878.98
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 0.8517
     Episode_Reward/lifting_object: 170.4320
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 1.95s
                      Time elapsed: 00:40:30
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 49514 steps/s (collection: 1.875s, learning 0.111s)
             Mean action noise std: 2.09
          Mean value_function loss: 127.7431
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.6149
                       Mean reward: 915.90
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.8590
     Episode_Reward/lifting_object: 172.0282
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.99s
                      Time elapsed: 00:40:32
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.873s, learning 0.122s)
             Mean action noise std: 2.09
          Mean value_function loss: 143.9772
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.6226
                       Mean reward: 859.86
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 0.8366
     Episode_Reward/lifting_object: 166.9661
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 1.99s
                      Time elapsed: 00:40:34
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 50198 steps/s (collection: 1.852s, learning 0.106s)
             Mean action noise std: 2.09
          Mean value_function loss: 119.0878
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.6287
                       Mean reward: 861.25
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 0.8523
     Episode_Reward/lifting_object: 170.4562
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.96s
                      Time elapsed: 00:40:36
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 48829 steps/s (collection: 1.889s, learning 0.124s)
             Mean action noise std: 2.09
          Mean value_function loss: 139.0962
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.6326
                       Mean reward: 872.39
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 169.0066
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.01s
                      Time elapsed: 00:40:38
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 48135 steps/s (collection: 1.912s, learning 0.130s)
             Mean action noise std: 2.09
          Mean value_function loss: 120.7482
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.6430
                       Mean reward: 898.70
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.8680
     Episode_Reward/lifting_object: 173.7731
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.04s
                      Time elapsed: 00:40:40
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 50055 steps/s (collection: 1.862s, learning 0.102s)
             Mean action noise std: 2.10
          Mean value_function loss: 122.8288
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.6566
                       Mean reward: 811.06
               Mean episode length: 219.01
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: 171.5690
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.96s
                      Time elapsed: 00:40:42
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 49134 steps/s (collection: 1.910s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 115.9397
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.6625
                       Mean reward: 881.72
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 0.8713
     Episode_Reward/lifting_object: 174.0010
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.00s
                      Time elapsed: 00:40:44
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 49636 steps/s (collection: 1.877s, learning 0.103s)
             Mean action noise std: 2.10
          Mean value_function loss: 126.8316
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.6665
                       Mean reward: 873.15
               Mean episode length: 234.32
    Episode_Reward/reaching_object: 0.8738
     Episode_Reward/lifting_object: 174.8957
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 1.98s
                      Time elapsed: 00:40:46
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 49713 steps/s (collection: 1.885s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 150.1823
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.6690
                       Mean reward: 867.63
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 0.8498
     Episode_Reward/lifting_object: 169.7403
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.98s
                      Time elapsed: 00:40:48
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 49971 steps/s (collection: 1.858s, learning 0.110s)
             Mean action noise std: 2.10
          Mean value_function loss: 128.7896
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.6734
                       Mean reward: 850.89
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 0.8591
     Episode_Reward/lifting_object: 171.9225
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.97s
                      Time elapsed: 00:40:50
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 49408 steps/s (collection: 1.904s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 116.2167
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.6795
                       Mean reward: 886.79
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 170.1843
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.99s
                      Time elapsed: 00:40:52
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 49929 steps/s (collection: 1.884s, learning 0.085s)
             Mean action noise std: 2.10
          Mean value_function loss: 141.6128
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.6855
                       Mean reward: 858.53
               Mean episode length: 230.33
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 172.0957
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.97s
                      Time elapsed: 00:40:54
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 49549 steps/s (collection: 1.891s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 145.1649
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.6935
                       Mean reward: 869.49
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 0.8536
     Episode_Reward/lifting_object: 171.8392
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.98s
                      Time elapsed: 00:40:56
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 49742 steps/s (collection: 1.864s, learning 0.113s)
             Mean action noise std: 2.10
          Mean value_function loss: 154.5029
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.6997
                       Mean reward: 839.28
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 167.3759
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.98s
                      Time elapsed: 00:40:58
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 50067 steps/s (collection: 1.862s, learning 0.102s)
             Mean action noise std: 2.10
          Mean value_function loss: 138.1335
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 37.7054
                       Mean reward: 869.68
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 0.8552
     Episode_Reward/lifting_object: 172.8184
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.96s
                      Time elapsed: 00:41:00
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 48192 steps/s (collection: 1.915s, learning 0.125s)
             Mean action noise std: 2.10
          Mean value_function loss: 140.9714
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 37.7078
                       Mean reward: 828.18
               Mean episode length: 223.74
    Episode_Reward/reaching_object: 0.8362
     Episode_Reward/lifting_object: 168.7704
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.04s
                      Time elapsed: 00:41:02
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.887s, learning 0.128s)
             Mean action noise std: 2.10
          Mean value_function loss: 147.0900
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.7082
                       Mean reward: 889.09
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.8516
     Episode_Reward/lifting_object: 172.1497
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.02s
                      Time elapsed: 00:41:04
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 47788 steps/s (collection: 1.944s, learning 0.113s)
             Mean action noise std: 2.10
          Mean value_function loss: 188.9931
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.7090
                       Mean reward: 808.60
               Mean episode length: 219.03
    Episode_Reward/reaching_object: 0.8296
     Episode_Reward/lifting_object: 167.8158
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.06s
                      Time elapsed: 00:41:06
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 47760 steps/s (collection: 1.935s, learning 0.124s)
             Mean action noise std: 2.10
          Mean value_function loss: 148.1908
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.7097
                       Mean reward: 876.53
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 0.8411
     Episode_Reward/lifting_object: 170.7825
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.06s
                      Time elapsed: 00:41:08
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 48994 steps/s (collection: 1.895s, learning 0.111s)
             Mean action noise std: 2.10
          Mean value_function loss: 132.0669
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.7136
                       Mean reward: 859.56
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 172.3014
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.01s
                      Time elapsed: 00:41:10
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 49156 steps/s (collection: 1.905s, learning 0.095s)
             Mean action noise std: 2.10
          Mean value_function loss: 136.6371
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.7182
                       Mean reward: 887.65
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 0.8529
     Episode_Reward/lifting_object: 173.8586
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.00s
                      Time elapsed: 00:41:12
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 48886 steps/s (collection: 1.900s, learning 0.110s)
             Mean action noise std: 2.11
          Mean value_function loss: 128.7183
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.7250
                       Mean reward: 875.17
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 170.2495
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.01s
                      Time elapsed: 00:41:14
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 48659 steps/s (collection: 1.923s, learning 0.097s)
             Mean action noise std: 2.11
          Mean value_function loss: 142.6750
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.7372
                       Mean reward: 869.61
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.8549
     Episode_Reward/lifting_object: 174.3819
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.02s
                      Time elapsed: 00:41:16
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 49479 steps/s (collection: 1.903s, learning 0.084s)
             Mean action noise std: 2.11
          Mean value_function loss: 140.7443
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.7500
                       Mean reward: 872.02
               Mean episode length: 234.76
    Episode_Reward/reaching_object: 0.8402
     Episode_Reward/lifting_object: 171.3398
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 1.99s
                      Time elapsed: 00:41:18
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 49712 steps/s (collection: 1.879s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 160.2811
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.7600
                       Mean reward: 821.90
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 0.8239
     Episode_Reward/lifting_object: 167.4958
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.98s
                      Time elapsed: 00:41:20
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 49464 steps/s (collection: 1.854s, learning 0.133s)
             Mean action noise std: 2.11
          Mean value_function loss: 147.2651
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.7674
                       Mean reward: 856.00
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 169.4846
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 1.99s
                      Time elapsed: 00:41:22
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 49807 steps/s (collection: 1.885s, learning 0.089s)
             Mean action noise std: 2.11
          Mean value_function loss: 117.9373
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.7761
                       Mean reward: 885.11
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 172.8902
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 1.97s
                      Time elapsed: 00:41:24
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 49043 steps/s (collection: 1.873s, learning 0.131s)
             Mean action noise std: 2.11
          Mean value_function loss: 132.9053
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.7906
                       Mean reward: 881.98
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.8373
     Episode_Reward/lifting_object: 170.8047
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.00s
                      Time elapsed: 00:41:26
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 48147 steps/s (collection: 1.928s, learning 0.113s)
             Mean action noise std: 2.12
          Mean value_function loss: 115.5598
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 37.8001
                       Mean reward: 901.02
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.8604
     Episode_Reward/lifting_object: 175.8440
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.04s
                      Time elapsed: 00:41:28
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 49756 steps/s (collection: 1.865s, learning 0.111s)
             Mean action noise std: 2.12
          Mean value_function loss: 109.8803
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.8090
                       Mean reward: 858.67
               Mean episode length: 230.58
    Episode_Reward/reaching_object: 0.8659
     Episode_Reward/lifting_object: 177.0690
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 1.98s
                      Time elapsed: 00:41:30
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 49588 steps/s (collection: 1.874s, learning 0.109s)
             Mean action noise std: 2.12
          Mean value_function loss: 123.8500
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.8160
                       Mean reward: 853.17
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 0.8222
     Episode_Reward/lifting_object: 167.4201
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.98s
                      Time elapsed: 00:41:32
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 49709 steps/s (collection: 1.888s, learning 0.090s)
             Mean action noise std: 2.12
          Mean value_function loss: 127.9679
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.8214
                       Mean reward: 882.17
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.8382
     Episode_Reward/lifting_object: 171.2497
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 1.98s
                      Time elapsed: 00:41:34
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 50449 steps/s (collection: 1.850s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 105.7447
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.8287
                       Mean reward: 909.46
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 0.8608
     Episode_Reward/lifting_object: 175.9658
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 1.95s
                      Time elapsed: 00:41:36
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 49351 steps/s (collection: 1.896s, learning 0.096s)
             Mean action noise std: 2.12
          Mean value_function loss: 126.4999
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.8382
                       Mean reward: 867.04
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 0.8469
     Episode_Reward/lifting_object: 172.9392
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.99s
                      Time elapsed: 00:41:38
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 48879 steps/s (collection: 1.905s, learning 0.106s)
             Mean action noise std: 2.12
          Mean value_function loss: 126.8312
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.8455
                       Mean reward: 857.71
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 0.8496
     Episode_Reward/lifting_object: 173.6952
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.01s
                      Time elapsed: 00:41:40
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 48685 steps/s (collection: 1.926s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 129.8923
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.8509
                       Mean reward: 862.62
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 0.8335
     Episode_Reward/lifting_object: 169.9140
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.02s
                      Time elapsed: 00:41:42
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 50192 steps/s (collection: 1.872s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 112.6121
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.8588
                       Mean reward: 832.10
               Mean episode length: 224.04
    Episode_Reward/reaching_object: 0.8362
     Episode_Reward/lifting_object: 170.9839
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.96s
                      Time elapsed: 00:41:44
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 49665 steps/s (collection: 1.859s, learning 0.120s)
             Mean action noise std: 2.12
          Mean value_function loss: 151.9339
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.8639
                       Mean reward: 820.54
               Mean episode length: 222.10
    Episode_Reward/reaching_object: 0.8135
     Episode_Reward/lifting_object: 166.1827
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.98s
                      Time elapsed: 00:41:46
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 49555 steps/s (collection: 1.867s, learning 0.117s)
             Mean action noise std: 2.12
          Mean value_function loss: 170.3985
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.8689
                       Mean reward: 865.32
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 0.8230
     Episode_Reward/lifting_object: 168.6229
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 1.98s
                      Time elapsed: 00:41:48
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 49676 steps/s (collection: 1.862s, learning 0.117s)
             Mean action noise std: 2.13
          Mean value_function loss: 133.3414
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.8747
                       Mean reward: 882.77
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.8417
     Episode_Reward/lifting_object: 172.6534
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 1.98s
                      Time elapsed: 00:41:50
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 49314 steps/s (collection: 1.878s, learning 0.115s)
             Mean action noise std: 2.13
          Mean value_function loss: 145.7625
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.8833
                       Mean reward: 878.74
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 170.7247
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 1.99s
                      Time elapsed: 00:41:52
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 46659 steps/s (collection: 1.946s, learning 0.161s)
             Mean action noise std: 2.13
          Mean value_function loss: 133.2194
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.8926
                       Mean reward: 878.24
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 0.8256
     Episode_Reward/lifting_object: 169.2449
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.11s
                      Time elapsed: 00:41:54
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 47202 steps/s (collection: 1.956s, learning 0.127s)
             Mean action noise std: 2.13
          Mean value_function loss: 122.8516
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.9010
                       Mean reward: 888.54
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.8365
     Episode_Reward/lifting_object: 172.1277
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.08s
                      Time elapsed: 00:41:56
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 48102 steps/s (collection: 1.914s, learning 0.130s)
             Mean action noise std: 2.13
          Mean value_function loss: 105.8488
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.9081
                       Mean reward: 862.34
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 174.5310
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.04s
                      Time elapsed: 00:41:58
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 48417 steps/s (collection: 1.880s, learning 0.150s)
             Mean action noise std: 2.13
          Mean value_function loss: 136.5049
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.9128
                       Mean reward: 850.14
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 171.4266
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.03s
                      Time elapsed: 00:42:00
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 48577 steps/s (collection: 1.923s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 134.9215
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.9180
                       Mean reward: 841.84
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 170.1859
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.02s
                      Time elapsed: 00:42:03
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 48607 steps/s (collection: 1.917s, learning 0.106s)
             Mean action noise std: 2.13
          Mean value_function loss: 155.6700
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.9290
                       Mean reward: 867.17
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 172.8214
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.02s
                      Time elapsed: 00:42:05
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 48727 steps/s (collection: 1.922s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 150.0084
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.9375
                       Mean reward: 864.51
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.8230
     Episode_Reward/lifting_object: 169.1548
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.02s
                      Time elapsed: 00:42:07
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 46975 steps/s (collection: 1.984s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 122.1196
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9421
                       Mean reward: 886.38
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 173.0338
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.09s
                      Time elapsed: 00:42:09
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 45622 steps/s (collection: 2.052s, learning 0.103s)
             Mean action noise std: 2.14
          Mean value_function loss: 111.9670
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9535
                       Mean reward: 849.37
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 173.5142
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.15s
                      Time elapsed: 00:42:11
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 46018 steps/s (collection: 2.033s, learning 0.103s)
             Mean action noise std: 2.14
          Mean value_function loss: 134.2233
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.9641
                       Mean reward: 867.53
               Mean episode length: 233.39
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 173.5484
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.14s
                      Time elapsed: 00:42:13
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 48342 steps/s (collection: 1.930s, learning 0.103s)
             Mean action noise std: 2.14
          Mean value_function loss: 141.1210
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9687
                       Mean reward: 870.10
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 0.8439
     Episode_Reward/lifting_object: 173.2778
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.03s
                      Time elapsed: 00:42:15
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 47867 steps/s (collection: 1.927s, learning 0.127s)
             Mean action noise std: 2.14
          Mean value_function loss: 113.5990
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.9775
                       Mean reward: 820.09
               Mean episode length: 221.93
    Episode_Reward/reaching_object: 0.8366
     Episode_Reward/lifting_object: 171.6376
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.05s
                      Time elapsed: 00:42:17
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 49071 steps/s (collection: 1.895s, learning 0.108s)
             Mean action noise std: 2.14
          Mean value_function loss: 103.5729
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9883
                       Mean reward: 861.43
               Mean episode length: 231.45
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 171.6447
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.00s
                      Time elapsed: 00:42:19
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 49289 steps/s (collection: 1.893s, learning 0.102s)
             Mean action noise std: 2.14
          Mean value_function loss: 106.3433
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.9966
                       Mean reward: 863.43
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 172.6067
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 1.99s
                      Time elapsed: 00:42:21
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 49155 steps/s (collection: 1.896s, learning 0.104s)
             Mean action noise std: 2.14
          Mean value_function loss: 96.0875
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.0022
                       Mean reward: 908.08
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.8754
     Episode_Reward/lifting_object: 180.8781
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.00s
                      Time elapsed: 00:42:23
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 49119 steps/s (collection: 1.909s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 126.7128
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.0092
                       Mean reward: 844.75
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 0.8311
     Episode_Reward/lifting_object: 170.8600
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.00s
                      Time elapsed: 00:42:25
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 47755 steps/s (collection: 1.956s, learning 0.102s)
             Mean action noise std: 2.14
          Mean value_function loss: 110.6984
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.0163
                       Mean reward: 882.04
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 172.0610
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.06s
                      Time elapsed: 00:42:27
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 48816 steps/s (collection: 1.921s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 126.3121
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.0254
                       Mean reward: 842.49
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 172.4720
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.01s
                      Time elapsed: 00:42:29
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 48356 steps/s (collection: 1.935s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 117.9224
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.0326
                       Mean reward: 861.88
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 0.8527
     Episode_Reward/lifting_object: 175.4039
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.03s
                      Time elapsed: 00:42:31
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 49087 steps/s (collection: 1.904s, learning 0.099s)
             Mean action noise std: 2.15
          Mean value_function loss: 116.3361
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.0394
                       Mean reward: 906.13
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.8544
     Episode_Reward/lifting_object: 176.4745
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.00s
                      Time elapsed: 00:42:33
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 49154 steps/s (collection: 1.907s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 133.0213
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.0434
                       Mean reward: 835.06
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 0.8379
     Episode_Reward/lifting_object: 172.9951
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.00s
                      Time elapsed: 00:42:35
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 47082 steps/s (collection: 1.953s, learning 0.135s)
             Mean action noise std: 2.15
          Mean value_function loss: 124.9867
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.0473
                       Mean reward: 842.49
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.8317
     Episode_Reward/lifting_object: 170.9273
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.09s
                      Time elapsed: 00:42:37
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 46313 steps/s (collection: 2.033s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 128.3574
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.0563
                       Mean reward: 829.27
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 0.8345
     Episode_Reward/lifting_object: 171.8597
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.12s
                      Time elapsed: 00:42:39
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 47207 steps/s (collection: 1.982s, learning 0.100s)
             Mean action noise std: 2.15
          Mean value_function loss: 145.1702
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.0723
                       Mean reward: 857.80
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 0.8372
     Episode_Reward/lifting_object: 172.7243
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.08s
                      Time elapsed: 00:42:41
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 47794 steps/s (collection: 1.942s, learning 0.115s)
             Mean action noise std: 2.15
          Mean value_function loss: 116.9355
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.0855
                       Mean reward: 884.90
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 171.6122
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.06s
                      Time elapsed: 00:42:43
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 46671 steps/s (collection: 2.005s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 135.7564
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.0923
                       Mean reward: 852.40
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 174.1124
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.11s
                      Time elapsed: 00:42:46
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 46579 steps/s (collection: 1.962s, learning 0.149s)
             Mean action noise std: 2.15
          Mean value_function loss: 121.7163
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.1017
                       Mean reward: 846.58
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 0.8206
     Episode_Reward/lifting_object: 168.8668
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.11s
                      Time elapsed: 00:42:48
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 46557 steps/s (collection: 1.996s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 97.6252
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.1089
                       Mean reward: 861.94
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 0.8501
     Episode_Reward/lifting_object: 175.6685
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.11s
                      Time elapsed: 00:42:50
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 46725 steps/s (collection: 2.004s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 119.8299
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.1134
                       Mean reward: 876.11
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 0.8422
     Episode_Reward/lifting_object: 174.0006
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.10s
                      Time elapsed: 00:42:52
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 47287 steps/s (collection: 1.976s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 113.7730
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.1207
                       Mean reward: 855.98
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 0.8438
     Episode_Reward/lifting_object: 174.6654
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.08s
                      Time elapsed: 00:42:54
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 47422 steps/s (collection: 1.963s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 109.1182
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.1312
                       Mean reward: 849.76
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 0.8467
     Episode_Reward/lifting_object: 175.1590
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.07s
                      Time elapsed: 00:42:56
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 49066 steps/s (collection: 1.914s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 106.1609
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.1475
                       Mean reward: 870.31
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 0.8336
     Episode_Reward/lifting_object: 172.4680
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.00s
                      Time elapsed: 00:42:58
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 48283 steps/s (collection: 1.947s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 106.2509
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.1633
                       Mean reward: 870.01
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 174.9036
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.04s
                      Time elapsed: 00:43:00
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 48569 steps/s (collection: 1.914s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 131.2100
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 38.1733
                       Mean reward: 868.91
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 0.8298
     Episode_Reward/lifting_object: 171.3770
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.02s
                      Time elapsed: 00:43:02
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 48055 steps/s (collection: 1.950s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 124.0191
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.1781
                       Mean reward: 878.75
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 0.8380
     Episode_Reward/lifting_object: 172.7976
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.05s
                      Time elapsed: 00:43:04
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 48105 steps/s (collection: 1.943s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 130.0664
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.1864
                       Mean reward: 870.56
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.8302
     Episode_Reward/lifting_object: 172.0049
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.04s
                      Time elapsed: 00:43:06
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 48270 steps/s (collection: 1.941s, learning 0.096s)
             Mean action noise std: 2.17
          Mean value_function loss: 157.7043
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.1982
                       Mean reward: 844.99
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 0.8220
     Episode_Reward/lifting_object: 170.0973
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.04s
                      Time elapsed: 00:43:08
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 48036 steps/s (collection: 1.955s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 140.7998
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.2088
                       Mean reward: 887.16
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.8375
     Episode_Reward/lifting_object: 173.9295
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.05s
                      Time elapsed: 00:43:10
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 47893 steps/s (collection: 1.957s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 151.6131
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.2186
                       Mean reward: 883.40
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.8236
     Episode_Reward/lifting_object: 170.7238
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.05s
                      Time elapsed: 00:43:12
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 48248 steps/s (collection: 1.943s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 206.1946
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 38.2251
                       Mean reward: 838.75
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 166.7043
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.04s
                      Time elapsed: 00:43:14
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 47842 steps/s (collection: 1.961s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 150.6686
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.2286
                       Mean reward: 807.63
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.8129
     Episode_Reward/lifting_object: 168.3056
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.05s
                      Time elapsed: 00:43:16
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 48604 steps/s (collection: 1.931s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 142.5263
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.2355
                       Mean reward: 838.75
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 168.6693
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.02s
                      Time elapsed: 00:43:18
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 48327 steps/s (collection: 1.937s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 117.3958
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.2428
                       Mean reward: 887.93
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.8224
     Episode_Reward/lifting_object: 171.1423
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.03s
                      Time elapsed: 00:43:20
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 48520 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 169.0479
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.2456
                       Mean reward: 844.33
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 0.7922
     Episode_Reward/lifting_object: 163.8426
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.03s
                      Time elapsed: 00:43:23
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 47819 steps/s (collection: 1.949s, learning 0.107s)
             Mean action noise std: 2.17
          Mean value_function loss: 126.3942
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 38.2488
                       Mean reward: 820.16
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 0.8203
     Episode_Reward/lifting_object: 170.1978
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.06s
                      Time elapsed: 00:43:25
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 47680 steps/s (collection: 1.961s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 110.7954
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.2499
                       Mean reward: 882.62
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.8337
     Episode_Reward/lifting_object: 173.7174
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.06s
                      Time elapsed: 00:43:27
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 47836 steps/s (collection: 1.940s, learning 0.115s)
             Mean action noise std: 2.17
          Mean value_function loss: 97.0974
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.2536
                       Mean reward: 904.89
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 176.9640
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.06s
                      Time elapsed: 00:43:29
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 48585 steps/s (collection: 1.928s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 119.1051
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.2669
                       Mean reward: 842.33
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 175.4346
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.02s
                      Time elapsed: 00:43:31
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 46571 steps/s (collection: 1.970s, learning 0.141s)
             Mean action noise std: 2.18
          Mean value_function loss: 127.4523
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.2819
                       Mean reward: 853.69
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 0.8249
     Episode_Reward/lifting_object: 171.1908
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.11s
                      Time elapsed: 00:43:33
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 47373 steps/s (collection: 1.939s, learning 0.137s)
             Mean action noise std: 2.18
          Mean value_function loss: 100.5344
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 38.2941
                       Mean reward: 856.70
               Mean episode length: 229.89
    Episode_Reward/reaching_object: 0.8455
     Episode_Reward/lifting_object: 175.9332
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.08s
                      Time elapsed: 00:43:35
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 46167 steps/s (collection: 2.024s, learning 0.105s)
             Mean action noise std: 2.18
          Mean value_function loss: 104.3765
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.2984
                       Mean reward: 866.18
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 174.9985
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.13s
                      Time elapsed: 00:43:37
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 48642 steps/s (collection: 1.925s, learning 0.096s)
             Mean action noise std: 2.18
          Mean value_function loss: 121.3152
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.3049
                       Mean reward: 848.00
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 173.6174
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.02s
                      Time elapsed: 00:43:39
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 47105 steps/s (collection: 1.984s, learning 0.103s)
             Mean action noise std: 2.18
          Mean value_function loss: 112.2201
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.3084
                       Mean reward: 879.79
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 0.8272
     Episode_Reward/lifting_object: 172.3955
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.09s
                      Time elapsed: 00:43:41
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 47140 steps/s (collection: 1.987s, learning 0.098s)
             Mean action noise std: 2.18
          Mean value_function loss: 122.2492
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 38.3113
                       Mean reward: 875.96
               Mean episode length: 235.23
    Episode_Reward/reaching_object: 0.8125
     Episode_Reward/lifting_object: 168.2543
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.09s
                      Time elapsed: 00:43:43
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 47657 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 113.5185
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.3164
                       Mean reward: 862.80
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 176.6759
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.06s
                      Time elapsed: 00:43:45
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 48475 steps/s (collection: 1.934s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 106.4250
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.3222
                       Mean reward: 865.05
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 174.6158
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.03s
                      Time elapsed: 00:43:47
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 48417 steps/s (collection: 1.937s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 96.6853
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.3264
                       Mean reward: 882.62
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 176.0842
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.03s
                      Time elapsed: 00:43:49
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 47638 steps/s (collection: 1.967s, learning 0.097s)
             Mean action noise std: 2.18
          Mean value_function loss: 86.1892
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.3321
                       Mean reward: 896.22
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.8495
     Episode_Reward/lifting_object: 176.6078
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.06s
                      Time elapsed: 00:43:51
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 48061 steps/s (collection: 1.947s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 102.7747
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.3417
                       Mean reward: 852.56
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 0.8279
     Episode_Reward/lifting_object: 171.9168
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.05s
                      Time elapsed: 00:43:53
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 47993 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 2.19
          Mean value_function loss: 112.2523
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.3488
                       Mean reward: 893.07
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 0.8494
     Episode_Reward/lifting_object: 176.5823
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.05s
                      Time elapsed: 00:43:55
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 48546 steps/s (collection: 1.926s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 101.2973
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.3555
                       Mean reward: 856.34
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 0.8444
     Episode_Reward/lifting_object: 175.5014
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.02s
                      Time elapsed: 00:43:58
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 47471 steps/s (collection: 1.955s, learning 0.116s)
             Mean action noise std: 2.19
          Mean value_function loss: 132.2254
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.3606
                       Mean reward: 879.44
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 0.8197
     Episode_Reward/lifting_object: 170.0189
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.07s
                      Time elapsed: 00:44:00
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 48271 steps/s (collection: 1.937s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 112.6573
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.3648
                       Mean reward: 849.44
               Mean episode length: 229.26
    Episode_Reward/reaching_object: 0.8257
     Episode_Reward/lifting_object: 171.6147
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.04s
                      Time elapsed: 00:44:02
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 47224 steps/s (collection: 1.969s, learning 0.113s)
             Mean action noise std: 2.19
          Mean value_function loss: 106.7841
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.3723
                       Mean reward: 898.68
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 0.8330
     Episode_Reward/lifting_object: 173.8834
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.08s
                      Time elapsed: 00:44:04
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 47676 steps/s (collection: 1.935s, learning 0.127s)
             Mean action noise std: 2.19
          Mean value_function loss: 104.7289
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.3797
                       Mean reward: 867.41
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 175.6126
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.06s
                      Time elapsed: 00:44:06
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 48170 steps/s (collection: 1.934s, learning 0.107s)
             Mean action noise std: 2.19
          Mean value_function loss: 100.2017
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.3921
                       Mean reward: 870.27
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.8304
     Episode_Reward/lifting_object: 172.6545
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.04s
                      Time elapsed: 00:44:08
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 47080 steps/s (collection: 1.964s, learning 0.124s)
             Mean action noise std: 2.19
          Mean value_function loss: 112.0212
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.4031
                       Mean reward: 868.94
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.8433
     Episode_Reward/lifting_object: 176.0063
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.09s
                      Time elapsed: 00:44:10
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 47149 steps/s (collection: 1.958s, learning 0.127s)
             Mean action noise std: 2.19
          Mean value_function loss: 102.6388
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.4117
                       Mean reward: 889.79
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 0.8396
     Episode_Reward/lifting_object: 175.3315
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.08s
                      Time elapsed: 00:44:12
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 48219 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 2.20
          Mean value_function loss: 108.7399
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.4199
                       Mean reward: 865.46
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 172.2171
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.04s
                      Time elapsed: 00:44:14
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 47945 steps/s (collection: 1.943s, learning 0.108s)
             Mean action noise std: 2.20
          Mean value_function loss: 101.3996
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.4302
                       Mean reward: 861.44
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 0.8426
     Episode_Reward/lifting_object: 175.7159
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.05s
                      Time elapsed: 00:44:16
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 46843 steps/s (collection: 1.988s, learning 0.111s)
             Mean action noise std: 2.20
          Mean value_function loss: 97.2743
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 38.4382
                       Mean reward: 876.55
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 0.8376
     Episode_Reward/lifting_object: 174.1946
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.10s
                      Time elapsed: 00:44:18
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 47535 steps/s (collection: 1.954s, learning 0.114s)
             Mean action noise std: 2.20
          Mean value_function loss: 79.4036
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.4444
                       Mean reward: 872.13
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 174.1736
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.07s
                      Time elapsed: 00:44:20
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 47803 steps/s (collection: 1.966s, learning 0.091s)
             Mean action noise std: 2.20
          Mean value_function loss: 110.6093
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.4534
                       Mean reward: 910.62
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.8490
     Episode_Reward/lifting_object: 176.9127
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.06s
                      Time elapsed: 00:44:22
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 47356 steps/s (collection: 1.977s, learning 0.099s)
             Mean action noise std: 2.20
          Mean value_function loss: 106.1328
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.4668
                       Mean reward: 860.08
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 174.0418
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.08s
                      Time elapsed: 00:44:24
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 47957 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 80.8885
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.4789
                       Mean reward: 870.34
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 0.8520
     Episode_Reward/lifting_object: 177.5375
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.05s
                      Time elapsed: 00:44:26
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 46982 steps/s (collection: 1.987s, learning 0.106s)
             Mean action noise std: 2.20
          Mean value_function loss: 99.4116
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.4964
                       Mean reward: 878.45
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 173.7297
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.09s
                      Time elapsed: 00:44:29
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 45588 steps/s (collection: 2.046s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 57.6483
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.5104
                       Mean reward: 903.16
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.8499
     Episode_Reward/lifting_object: 176.7493
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.16s
                      Time elapsed: 00:44:31
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 46927 steps/s (collection: 1.969s, learning 0.126s)
             Mean action noise std: 2.21
          Mean value_function loss: 75.0884
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.5174
                       Mean reward: 889.75
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 177.9070
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.09s
                      Time elapsed: 00:44:33
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 45357 steps/s (collection: 2.005s, learning 0.163s)
             Mean action noise std: 2.21
          Mean value_function loss: 111.5705
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.5245
                       Mean reward: 888.66
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 0.8434
     Episode_Reward/lifting_object: 174.9193
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.17s
                      Time elapsed: 00:44:35
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 45800 steps/s (collection: 2.045s, learning 0.101s)
             Mean action noise std: 2.21
          Mean value_function loss: 102.2861
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.5306
                       Mean reward: 868.40
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.8353
     Episode_Reward/lifting_object: 173.2219
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.15s
                      Time elapsed: 00:44:37
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 47366 steps/s (collection: 1.954s, learning 0.122s)
             Mean action noise std: 2.21
          Mean value_function loss: 98.4757
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.5357
                       Mean reward: 899.23
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 176.0926
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.08s
                      Time elapsed: 00:44:39
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 47514 steps/s (collection: 1.957s, learning 0.112s)
             Mean action noise std: 2.21
          Mean value_function loss: 87.9070
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.5436
                       Mean reward: 893.89
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 174.9957
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.07s
                      Time elapsed: 00:44:41
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 46077 steps/s (collection: 2.030s, learning 0.103s)
             Mean action noise std: 2.21
          Mean value_function loss: 89.5516
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.5530
                       Mean reward: 916.50
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 176.2103
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.13s
                      Time elapsed: 00:44:43
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 46166 steps/s (collection: 2.016s, learning 0.114s)
             Mean action noise std: 2.21
          Mean value_function loss: 80.8230
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.5683
                       Mean reward: 855.38
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 0.8509
     Episode_Reward/lifting_object: 176.5787
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.13s
                      Time elapsed: 00:44:45
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 46004 steps/s (collection: 2.005s, learning 0.132s)
             Mean action noise std: 2.21
          Mean value_function loss: 84.4371
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.5862
                       Mean reward: 889.58
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 0.8482
     Episode_Reward/lifting_object: 176.4913
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.14s
                      Time elapsed: 00:44:48
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 47356 steps/s (collection: 1.972s, learning 0.104s)
             Mean action noise std: 2.22
          Mean value_function loss: 88.0236
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.5967
                       Mean reward: 900.44
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.8604
     Episode_Reward/lifting_object: 178.6405
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.08s
                      Time elapsed: 00:44:50
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 46660 steps/s (collection: 1.971s, learning 0.136s)
             Mean action noise std: 2.22
          Mean value_function loss: 106.2062
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.6014
                       Mean reward: 872.02
               Mean episode length: 233.56
    Episode_Reward/reaching_object: 0.8426
     Episode_Reward/lifting_object: 175.1848
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.11s
                      Time elapsed: 00:44:52
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 45344 steps/s (collection: 1.985s, learning 0.183s)
             Mean action noise std: 2.22
          Mean value_function loss: 102.4123
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.6093
                       Mean reward: 896.78
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 0.8487
     Episode_Reward/lifting_object: 176.2366
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.17s
                      Time elapsed: 00:44:54
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 45062 steps/s (collection: 2.063s, learning 0.118s)
             Mean action noise std: 2.22
          Mean value_function loss: 106.8138
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.6185
                       Mean reward: 860.18
               Mean episode length: 230.91
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 172.5136
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.18s
                      Time elapsed: 00:44:56
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 47491 steps/s (collection: 1.977s, learning 0.093s)
             Mean action noise std: 2.22
          Mean value_function loss: 87.9032
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.6250
                       Mean reward: 891.65
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.8564
     Episode_Reward/lifting_object: 178.1418
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.07s
                      Time elapsed: 00:44:58
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 47854 steps/s (collection: 1.960s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 80.8765
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.6330
                       Mean reward: 884.87
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: 178.1821
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.05s
                      Time elapsed: 00:45:00
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 46809 steps/s (collection: 2.004s, learning 0.097s)
             Mean action noise std: 2.22
          Mean value_function loss: 109.0750
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.6438
                       Mean reward: 873.76
               Mean episode length: 233.56
    Episode_Reward/reaching_object: 0.8533
     Episode_Reward/lifting_object: 176.6261
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.10s
                      Time elapsed: 00:45:02
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 46976 steps/s (collection: 1.979s, learning 0.114s)
             Mean action noise std: 2.22
          Mean value_function loss: 93.1174
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.6603
                       Mean reward: 882.44
               Mean episode length: 236.12
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 173.1796
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.09s
                      Time elapsed: 00:45:04
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 46743 steps/s (collection: 1.985s, learning 0.118s)
             Mean action noise std: 2.22
          Mean value_function loss: 92.0372
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.6681
                       Mean reward: 908.25
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.8560
     Episode_Reward/lifting_object: 177.0028
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.10s
                      Time elapsed: 00:45:07
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 47247 steps/s (collection: 1.986s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 96.4430
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.6753
                       Mean reward: 905.91
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.8438
     Episode_Reward/lifting_object: 174.6258
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.08s
                      Time elapsed: 00:45:09
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 47140 steps/s (collection: 1.974s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 103.1947
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 38.6862
                       Mean reward: 892.00
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.8404
     Episode_Reward/lifting_object: 173.9216
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.09s
                      Time elapsed: 00:45:11
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 46875 steps/s (collection: 1.972s, learning 0.126s)
             Mean action noise std: 2.23
          Mean value_function loss: 91.1670
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.6940
                       Mean reward: 910.90
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 0.8477
     Episode_Reward/lifting_object: 175.3111
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.10s
                      Time elapsed: 00:45:13
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 46788 steps/s (collection: 1.971s, learning 0.131s)
             Mean action noise std: 2.23
          Mean value_function loss: 84.7631
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.7092
                       Mean reward: 903.38
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.8546
     Episode_Reward/lifting_object: 177.4174
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.10s
                      Time elapsed: 00:45:15
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 47387 steps/s (collection: 1.961s, learning 0.113s)
             Mean action noise std: 2.23
          Mean value_function loss: 90.2229
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.7269
                       Mean reward: 885.78
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 176.7359
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.07s
                      Time elapsed: 00:45:17
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 46766 steps/s (collection: 1.997s, learning 0.105s)
             Mean action noise std: 2.23
          Mean value_function loss: 95.8107
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.7393
                       Mean reward: 875.96
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 0.8427
     Episode_Reward/lifting_object: 174.9350
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.10s
                      Time elapsed: 00:45:19
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 46954 steps/s (collection: 1.999s, learning 0.095s)
             Mean action noise std: 2.24
          Mean value_function loss: 108.5573
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.7523
                       Mean reward: 888.49
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.8509
     Episode_Reward/lifting_object: 177.0892
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.09s
                      Time elapsed: 00:45:21
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 46713 steps/s (collection: 2.007s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 116.9705
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.7670
                       Mean reward: 849.08
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 0.8328
     Episode_Reward/lifting_object: 173.1145
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.10s
                      Time elapsed: 00:45:23
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 47017 steps/s (collection: 1.993s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 108.1486
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 38.7789
                       Mean reward: 891.89
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 174.9244
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.09s
                      Time elapsed: 00:45:25
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 47922 steps/s (collection: 1.953s, learning 0.099s)
             Mean action noise std: 2.24
          Mean value_function loss: 107.0692
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.7866
                       Mean reward: 871.57
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 0.8374
     Episode_Reward/lifting_object: 174.3793
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.05s
                      Time elapsed: 00:45:27
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 46555 steps/s (collection: 2.004s, learning 0.107s)
             Mean action noise std: 2.24
          Mean value_function loss: 104.6387
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.7954
                       Mean reward: 873.66
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 0.8327
     Episode_Reward/lifting_object: 173.3935
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.11s
                      Time elapsed: 00:45:30
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 47404 steps/s (collection: 1.976s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 119.9663
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 38.7989
                       Mean reward: 841.43
               Mean episode length: 226.79
    Episode_Reward/reaching_object: 0.8173
     Episode_Reward/lifting_object: 169.7692
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.07s
                      Time elapsed: 00:45:32
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 47564 steps/s (collection: 1.965s, learning 0.101s)
             Mean action noise std: 2.24
          Mean value_function loss: 110.8326
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.8013
                       Mean reward: 899.24
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.8350
     Episode_Reward/lifting_object: 174.0525
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.07s
                      Time elapsed: 00:45:34
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 44739 steps/s (collection: 2.013s, learning 0.185s)
             Mean action noise std: 2.24
          Mean value_function loss: 105.4795
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.8077
                       Mean reward: 825.76
               Mean episode length: 223.13
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 173.6813
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.20s
                      Time elapsed: 00:45:36
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 46391 steps/s (collection: 2.003s, learning 0.116s)
             Mean action noise std: 2.24
          Mean value_function loss: 118.4549
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.8197
                       Mean reward: 836.49
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 0.8288
     Episode_Reward/lifting_object: 172.4528
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.12s
                      Time elapsed: 00:45:38
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 46627 steps/s (collection: 1.978s, learning 0.130s)
             Mean action noise std: 2.24
          Mean value_function loss: 87.4710
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.8270
                       Mean reward: 931.41
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.8487
     Episode_Reward/lifting_object: 177.3753
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.11s
                      Time elapsed: 00:45:40
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 46829 steps/s (collection: 1.976s, learning 0.124s)
             Mean action noise std: 2.25
          Mean value_function loss: 112.0464
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 38.8316
                       Mean reward: 899.27
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 0.8302
     Episode_Reward/lifting_object: 173.1056
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.10s
                      Time elapsed: 00:45:42
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 47435 steps/s (collection: 1.949s, learning 0.123s)
             Mean action noise std: 2.25
          Mean value_function loss: 92.7701
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.8408
                       Mean reward: 888.54
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.8268
     Episode_Reward/lifting_object: 172.3505
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.07s
                      Time elapsed: 00:45:44
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 48012 steps/s (collection: 1.935s, learning 0.113s)
             Mean action noise std: 2.25
          Mean value_function loss: 106.1205
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8475
                       Mean reward: 911.12
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.8425
     Episode_Reward/lifting_object: 176.0518
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.05s
                      Time elapsed: 00:45:46
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 48075 steps/s (collection: 1.927s, learning 0.118s)
             Mean action noise std: 2.25
          Mean value_function loss: 96.3735
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.8585
                       Mean reward: 884.57
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 0.8305
     Episode_Reward/lifting_object: 173.1329
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.04s
                      Time elapsed: 00:45:48
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 47566 steps/s (collection: 1.941s, learning 0.125s)
             Mean action noise std: 2.25
          Mean value_function loss: 86.8176
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.8752
                       Mean reward: 883.77
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 176.4581
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.07s
                      Time elapsed: 00:45:50
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 47971 steps/s (collection: 1.934s, learning 0.115s)
             Mean action noise std: 2.25
          Mean value_function loss: 109.7924
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.8863
                       Mean reward: 868.34
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 173.9087
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.05s
                      Time elapsed: 00:45:53
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 47957 steps/s (collection: 1.928s, learning 0.122s)
             Mean action noise std: 2.25
          Mean value_function loss: 96.0800
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.8920
                       Mean reward: 891.40
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.8303
     Episode_Reward/lifting_object: 172.6584
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.05s
                      Time elapsed: 00:45:55
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 47659 steps/s (collection: 1.943s, learning 0.120s)
             Mean action noise std: 2.25
          Mean value_function loss: 112.4179
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8960
                       Mean reward: 879.35
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.8318
     Episode_Reward/lifting_object: 173.4535
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.06s
                      Time elapsed: 00:45:57
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 48265 steps/s (collection: 1.923s, learning 0.114s)
             Mean action noise std: 2.25
          Mean value_function loss: 122.0503
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.9020
                       Mean reward: 861.27
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.8326
     Episode_Reward/lifting_object: 173.4867
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.04s
                      Time elapsed: 00:45:59
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 47523 steps/s (collection: 1.951s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 69.9799
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.9098
                       Mean reward: 913.90
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.8577
     Episode_Reward/lifting_object: 179.1158
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.07s
                      Time elapsed: 00:46:01
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 48140 steps/s (collection: 1.928s, learning 0.114s)
             Mean action noise std: 2.26
          Mean value_function loss: 91.2779
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.9188
                       Mean reward: 898.00
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: 177.0168
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.04s
                      Time elapsed: 00:46:03
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 48159 steps/s (collection: 1.933s, learning 0.108s)
             Mean action noise std: 2.26
          Mean value_function loss: 129.5372
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.9253
                       Mean reward: 909.17
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.8310
     Episode_Reward/lifting_object: 173.2697
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.04s
                      Time elapsed: 00:46:05
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 47891 steps/s (collection: 1.939s, learning 0.113s)
             Mean action noise std: 2.26
          Mean value_function loss: 74.7026
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.9336
                       Mean reward: 869.69
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 0.8505
     Episode_Reward/lifting_object: 177.6037
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.05s
                      Time elapsed: 00:46:07
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 47627 steps/s (collection: 1.948s, learning 0.116s)
             Mean action noise std: 2.26
          Mean value_function loss: 108.0098
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.9406
                       Mean reward: 868.56
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 174.8502
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.06s
                      Time elapsed: 00:46:09
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 48077 steps/s (collection: 1.930s, learning 0.115s)
             Mean action noise std: 2.26
          Mean value_function loss: 88.9922
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 38.9459
                       Mean reward: 861.64
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 175.4036
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.04s
                      Time elapsed: 00:46:11
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 48215 steps/s (collection: 1.919s, learning 0.120s)
             Mean action noise std: 2.26
          Mean value_function loss: 115.9648
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.9535
                       Mean reward: 853.81
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 0.8168
     Episode_Reward/lifting_object: 169.8909
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.04s
                      Time elapsed: 00:46:13
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 48091 steps/s (collection: 1.918s, learning 0.126s)
             Mean action noise std: 2.26
          Mean value_function loss: 104.9101
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.9607
                       Mean reward: 850.89
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 0.8280
     Episode_Reward/lifting_object: 172.2251
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.04s
                      Time elapsed: 00:46:15
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 47767 steps/s (collection: 1.929s, learning 0.129s)
             Mean action noise std: 2.26
          Mean value_function loss: 106.3096
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.9769
                       Mean reward: 900.29
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 177.3882
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.06s
                      Time elapsed: 00:46:17
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 48447 steps/s (collection: 1.920s, learning 0.110s)
             Mean action noise std: 2.27
          Mean value_function loss: 110.5090
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.9905
                       Mean reward: 902.10
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 0.8426
     Episode_Reward/lifting_object: 175.6859
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.03s
                      Time elapsed: 00:46:19
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 47373 steps/s (collection: 1.955s, learning 0.121s)
             Mean action noise std: 2.27
          Mean value_function loss: 85.0546
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.9973
                       Mean reward: 924.11
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.8468
     Episode_Reward/lifting_object: 176.7521
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.08s
                      Time elapsed: 00:46:21
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 47753 steps/s (collection: 1.932s, learning 0.127s)
             Mean action noise std: 2.27
          Mean value_function loss: 115.5461
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.0090
                       Mean reward: 870.99
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 0.8302
     Episode_Reward/lifting_object: 172.9160
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.06s
                      Time elapsed: 00:46:23
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 47798 steps/s (collection: 1.933s, learning 0.124s)
             Mean action noise std: 2.27
          Mean value_function loss: 108.2619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.0148
                       Mean reward: 876.29
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 174.2202
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.06s
                      Time elapsed: 00:46:25
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19035 steps/s (collection: 5.044s, learning 0.121s)
             Mean action noise std: 2.27
          Mean value_function loss: 91.1987
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.0185
                       Mean reward: 907.14
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.8444
     Episode_Reward/lifting_object: 176.3681
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.16s
                      Time elapsed: 00:46:31
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14651 steps/s (collection: 6.594s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.5699
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.0223
                       Mean reward: 867.73
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 176.0610
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.71s
                      Time elapsed: 00:46:37
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14448 steps/s (collection: 6.692s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 110.6250
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.0251
                       Mean reward: 900.32
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 173.8790
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.80s
                      Time elapsed: 00:46:44
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14293 steps/s (collection: 6.754s, learning 0.124s)
             Mean action noise std: 2.27
          Mean value_function loss: 110.9244
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.0312
                       Mean reward: 872.23
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.8299
     Episode_Reward/lifting_object: 173.2303
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.88s
                      Time elapsed: 00:46:51
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14281 steps/s (collection: 6.759s, learning 0.124s)
             Mean action noise std: 2.27
          Mean value_function loss: 104.9872
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.0392
                       Mean reward: 903.64
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.8297
     Episode_Reward/lifting_object: 173.6675
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.88s
                      Time elapsed: 00:46:58
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 13990 steps/s (collection: 6.907s, learning 0.119s)
             Mean action noise std: 2.27
          Mean value_function loss: 112.2641
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 39.0515
                       Mean reward: 908.81
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 0.8422
     Episode_Reward/lifting_object: 176.7056
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.03s
                      Time elapsed: 00:47:05
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14391 steps/s (collection: 6.687s, learning 0.144s)
             Mean action noise std: 2.27
          Mean value_function loss: 105.1844
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.0566
                       Mean reward: 879.37
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.8427
     Episode_Reward/lifting_object: 176.6112
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.83s
                      Time elapsed: 00:47:12
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14371 steps/s (collection: 6.711s, learning 0.129s)
             Mean action noise std: 2.28
          Mean value_function loss: 125.7101
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.0617
                       Mean reward: 865.06
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 0.8288
     Episode_Reward/lifting_object: 173.4994
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.84s
                      Time elapsed: 00:47:18
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13960 steps/s (collection: 6.920s, learning 0.122s)
             Mean action noise std: 2.28
          Mean value_function loss: 121.1878
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.0701
                       Mean reward: 861.60
               Mean episode length: 230.70
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: 171.5997
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.04s
                      Time elapsed: 00:47:26
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 50877 steps/s (collection: 1.833s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 118.3315
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.0825
                       Mean reward: 888.40
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 0.8213
     Episode_Reward/lifting_object: 172.1519
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.93s
                      Time elapsed: 00:47:27
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 50496 steps/s (collection: 1.833s, learning 0.114s)
             Mean action noise std: 2.28
          Mean value_function loss: 103.6258
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.0930
                       Mean reward: 876.32
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 174.8365
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 1.95s
                      Time elapsed: 00:47:29
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 51268 steps/s (collection: 1.816s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 105.3327
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.0991
                       Mean reward: 890.40
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.8310
     Episode_Reward/lifting_object: 173.9390
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 1.92s
                      Time elapsed: 00:47:31
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 50631 steps/s (collection: 1.825s, learning 0.117s)
             Mean action noise std: 2.28
          Mean value_function loss: 118.0480
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.1082
                       Mean reward: 889.96
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 0.8375
     Episode_Reward/lifting_object: 175.7213
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 1.94s
                      Time elapsed: 00:47:33
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 51033 steps/s (collection: 1.821s, learning 0.105s)
             Mean action noise std: 2.28
          Mean value_function loss: 115.9545
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.1186
                       Mean reward: 886.90
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.8434
     Episode_Reward/lifting_object: 177.0284
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 1.93s
                      Time elapsed: 00:47:35
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 50239 steps/s (collection: 1.848s, learning 0.109s)
             Mean action noise std: 2.28
          Mean value_function loss: 124.8038
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 39.1295
                       Mean reward: 857.84
               Mean episode length: 228.80
    Episode_Reward/reaching_object: 0.8324
     Episode_Reward/lifting_object: 174.6149
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.96s
                      Time elapsed: 00:47:37
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 51097 steps/s (collection: 1.822s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 98.5569
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.1341
                       Mean reward: 910.89
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 174.8009
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.92s
                      Time elapsed: 00:47:39
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 50931 steps/s (collection: 1.838s, learning 0.092s)
             Mean action noise std: 2.28
          Mean value_function loss: 106.9228
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.1384
                       Mean reward: 870.23
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 0.8373
     Episode_Reward/lifting_object: 175.3215
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.93s
                      Time elapsed: 00:47:41
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 51036 steps/s (collection: 1.824s, learning 0.102s)
             Mean action noise std: 2.28
          Mean value_function loss: 127.0104
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 39.1404
                       Mean reward: 873.89
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 0.8346
     Episode_Reward/lifting_object: 175.1313
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.93s
                      Time elapsed: 00:47:43
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 50106 steps/s (collection: 1.840s, learning 0.122s)
             Mean action noise std: 2.28
          Mean value_function loss: 97.3692
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 39.1412
                       Mean reward: 882.09
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 0.8306
     Episode_Reward/lifting_object: 173.8972
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 1.96s
                      Time elapsed: 00:47:45
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 51008 steps/s (collection: 1.819s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 99.5981
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.1423
                       Mean reward: 861.64
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 0.8413
     Episode_Reward/lifting_object: 176.2648
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.93s
                      Time elapsed: 00:47:47
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 51757 steps/s (collection: 1.795s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 84.8204
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.1445
                       Mean reward: 900.49
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 175.5249
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.90s
                      Time elapsed: 00:47:49
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 51268 steps/s (collection: 1.817s, learning 0.100s)
             Mean action noise std: 2.29
          Mean value_function loss: 133.3272
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1487
                       Mean reward: 874.57
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 0.8399
     Episode_Reward/lifting_object: 175.9737
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.92s
                      Time elapsed: 00:47:51
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 51861 steps/s (collection: 1.795s, learning 0.100s)
             Mean action noise std: 2.29
          Mean value_function loss: 111.5354
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.1569
                       Mean reward: 910.69
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.8470
     Episode_Reward/lifting_object: 177.5941
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.90s
                      Time elapsed: 00:47:53
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 51264 steps/s (collection: 1.802s, learning 0.116s)
             Mean action noise std: 2.29
          Mean value_function loss: 104.6745
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.1634
                       Mean reward: 858.02
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 0.8329
     Episode_Reward/lifting_object: 174.3940
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.92s
                      Time elapsed: 00:47:54
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 51590 steps/s (collection: 1.807s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 150.1239
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.1673
                       Mean reward: 822.22
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 171.5217
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.91s
                      Time elapsed: 00:47:56
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 50936 steps/s (collection: 1.816s, learning 0.114s)
             Mean action noise std: 2.29
          Mean value_function loss: 114.4558
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.1758
                       Mean reward: 894.27
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 177.7045
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.93s
                      Time elapsed: 00:47:58
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 51415 steps/s (collection: 1.811s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 112.2898
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.1893
                       Mean reward: 894.13
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 178.1033
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 1.91s
                      Time elapsed: 00:48:00
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 51341 steps/s (collection: 1.819s, learning 0.096s)
             Mean action noise std: 2.29
          Mean value_function loss: 95.0587
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1975
                       Mean reward: 904.70
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 174.2745
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.91s
                      Time elapsed: 00:48:02
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 51262 steps/s (collection: 1.812s, learning 0.106s)
             Mean action noise std: 2.29
          Mean value_function loss: 96.9526
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.2035
                       Mean reward: 898.55
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.8538
     Episode_Reward/lifting_object: 179.3530
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.92s
                      Time elapsed: 00:48:04
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 51227 steps/s (collection: 1.820s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 94.1147
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.2114
                       Mean reward: 864.45
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 0.8367
     Episode_Reward/lifting_object: 175.0154
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 1.92s
                      Time elapsed: 00:48:06
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 49644 steps/s (collection: 1.882s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 107.8561
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.2217
                       Mean reward: 902.62
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.8541
     Episode_Reward/lifting_object: 178.5970
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.98s
                      Time elapsed: 00:48:08
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 50695 steps/s (collection: 1.831s, learning 0.108s)
             Mean action noise std: 2.30
          Mean value_function loss: 118.1972
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.2288
                       Mean reward: 902.57
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 176.8871
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.94s
                      Time elapsed: 00:48:10
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 51189 steps/s (collection: 1.813s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 115.4521
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.2336
                       Mean reward: 893.38
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 0.8317
     Episode_Reward/lifting_object: 173.9200
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.92s
                      Time elapsed: 00:48:12
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 51762 steps/s (collection: 1.805s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 114.0332
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.2405
                       Mean reward: 868.23
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 175.1595
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.90s
                      Time elapsed: 00:48:14
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 51624 steps/s (collection: 1.808s, learning 0.096s)
             Mean action noise std: 2.30
          Mean value_function loss: 106.4764
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.2490
                       Mean reward: 865.98
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 174.8541
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 1.90s
                      Time elapsed: 00:48:16
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 50844 steps/s (collection: 1.836s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 99.0520
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.2561
                       Mean reward: 876.39
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 175.7314
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 1.93s
                      Time elapsed: 00:48:18
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 50761 steps/s (collection: 1.844s, learning 0.093s)
             Mean action noise std: 2.30
          Mean value_function loss: 99.7485
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.2620
                       Mean reward: 886.76
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.8380
     Episode_Reward/lifting_object: 174.9975
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.94s
                      Time elapsed: 00:48:19
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 50983 steps/s (collection: 1.829s, learning 0.099s)
             Mean action noise std: 2.30
          Mean value_function loss: 114.6132
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.2678
                       Mean reward: 893.51
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.8296
     Episode_Reward/lifting_object: 173.4839
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.93s
                      Time elapsed: 00:48:21
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 51053 steps/s (collection: 1.827s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 101.7932
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.2729
                       Mean reward: 904.82
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 174.8701
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.93s
                      Time elapsed: 00:48:23
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 50995 steps/s (collection: 1.817s, learning 0.111s)
             Mean action noise std: 2.30
          Mean value_function loss: 73.7397
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.2782
                       Mean reward: 886.72
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 0.8374
     Episode_Reward/lifting_object: 174.9540
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 1.93s
                      Time elapsed: 00:48:25
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 51011 steps/s (collection: 1.821s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 112.6047
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.2853
                       Mean reward: 901.06
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 174.8042
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.93s
                      Time elapsed: 00:48:27
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 50972 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 76.0290
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.2876
                       Mean reward: 897.81
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 176.5848
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 1.93s
                      Time elapsed: 00:48:29
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 50684 steps/s (collection: 1.846s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 87.1047
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.2905
                       Mean reward: 897.56
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 0.8433
     Episode_Reward/lifting_object: 176.6099
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.94s
                      Time elapsed: 00:48:31
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 50567 steps/s (collection: 1.833s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 95.6750
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.3015
                       Mean reward: 888.86
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 0.8583
     Episode_Reward/lifting_object: 179.8485
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 1.94s
                      Time elapsed: 00:48:33
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 50884 steps/s (collection: 1.825s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 98.5221
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.3138
                       Mean reward: 893.15
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 0.8399
     Episode_Reward/lifting_object: 175.5831
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.93s
                      Time elapsed: 00:48:35
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 49858 steps/s (collection: 1.859s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 106.6407
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.3215
                       Mean reward: 882.62
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.8434
     Episode_Reward/lifting_object: 176.3509
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.97s
                      Time elapsed: 00:48:37
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 50836 steps/s (collection: 1.833s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 90.1075
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.3315
                       Mean reward: 903.18
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 0.8500
     Episode_Reward/lifting_object: 178.0424
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 1.93s
                      Time elapsed: 00:48:39
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 50262 steps/s (collection: 1.853s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 94.8608
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.3423
                       Mean reward: 881.65
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 0.8355
     Episode_Reward/lifting_object: 175.2794
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.96s
                      Time elapsed: 00:48:41
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 49638 steps/s (collection: 1.889s, learning 0.092s)
             Mean action noise std: 2.31
          Mean value_function loss: 89.1485
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.3487
                       Mean reward: 875.27
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.8406
     Episode_Reward/lifting_object: 175.8047
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.98s
                      Time elapsed: 00:48:43
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 50700 steps/s (collection: 1.838s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 93.3870
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.3542
                       Mean reward: 897.96
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.8507
     Episode_Reward/lifting_object: 178.1001
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.94s
                      Time elapsed: 00:48:45
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 50596 steps/s (collection: 1.848s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 89.9465
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.3643
                       Mean reward: 876.71
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: 176.9869
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.94s
                      Time elapsed: 00:48:47
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 50737 steps/s (collection: 1.845s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 86.1292
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.3762
                       Mean reward: 900.44
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 179.9764
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 1.94s
                      Time elapsed: 00:48:49
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 50133 steps/s (collection: 1.861s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 96.9347
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.3873
                       Mean reward: 885.21
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 0.8376
     Episode_Reward/lifting_object: 176.2198
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 1.96s
                      Time elapsed: 00:48:51
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 50819 steps/s (collection: 1.849s, learning 0.086s)
             Mean action noise std: 2.32
          Mean value_function loss: 94.6556
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.3969
                       Mean reward: 885.24
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 176.1182
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 1.93s
                      Time elapsed: 00:48:52
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 50531 steps/s (collection: 1.852s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 123.9866
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.4108
                       Mean reward: 919.48
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.8297
     Episode_Reward/lifting_object: 173.6654
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.95s
                      Time elapsed: 00:48:54
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 50857 steps/s (collection: 1.844s, learning 0.089s)
             Mean action noise std: 2.32
          Mean value_function loss: 119.0528
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.4198
                       Mean reward: 865.53
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 0.8409
     Episode_Reward/lifting_object: 176.2425
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 1.93s
                      Time elapsed: 00:48:56
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 50182 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 101.2781
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.4270
                       Mean reward: 860.09
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 0.8371
     Episode_Reward/lifting_object: 175.7067
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.96s
                      Time elapsed: 00:48:58
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 51004 steps/s (collection: 1.837s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 109.3059
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.4335
                       Mean reward: 845.80
               Mean episode length: 227.35
    Episode_Reward/reaching_object: 0.8164
     Episode_Reward/lifting_object: 170.9458
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 1.93s
                      Time elapsed: 00:49:00
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 49980 steps/s (collection: 1.850s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 108.6010
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.4414
                       Mean reward: 858.81
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 175.6380
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 1.97s
                      Time elapsed: 00:49:02
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 50189 steps/s (collection: 1.850s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 103.1908
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.4531
                       Mean reward: 875.21
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 176.2176
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 1.96s
                      Time elapsed: 00:49:04
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 51082 steps/s (collection: 1.818s, learning 0.107s)
             Mean action noise std: 2.33
          Mean value_function loss: 100.0636
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 39.4581
                       Mean reward: 878.82
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.8319
     Episode_Reward/lifting_object: 174.7057
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.92s
                      Time elapsed: 00:49:06
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 49895 steps/s (collection: 1.854s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 121.8057
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 39.4593
                       Mean reward: 899.70
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.8214
     Episode_Reward/lifting_object: 172.3403
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.97s
                      Time elapsed: 00:49:08
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 50174 steps/s (collection: 1.849s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 115.7453
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 39.4600
                       Mean reward: 833.78
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 0.8258
     Episode_Reward/lifting_object: 173.5968
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.96s
                      Time elapsed: 00:49:10
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 49695 steps/s (collection: 1.860s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 121.0018
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.4617
                       Mean reward: 863.64
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 0.8421
     Episode_Reward/lifting_object: 176.5849
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.98s
                      Time elapsed: 00:49:12
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 50110 steps/s (collection: 1.850s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 111.4784
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.4649
                       Mean reward: 883.22
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 0.8418
     Episode_Reward/lifting_object: 177.3664
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.96s
                      Time elapsed: 00:49:14
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 50336 steps/s (collection: 1.843s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 126.2183
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.4680
                       Mean reward: 879.88
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 0.8239
     Episode_Reward/lifting_object: 173.3478
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.95s
                      Time elapsed: 00:49:16
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 49870 steps/s (collection: 1.873s, learning 0.098s)
             Mean action noise std: 2.33
          Mean value_function loss: 134.8635
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.4729
                       Mean reward: 838.74
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 0.8243
     Episode_Reward/lifting_object: 173.8682
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.97s
                      Time elapsed: 00:49:18
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 49864 steps/s (collection: 1.860s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 133.0309
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.4784
                       Mean reward: 879.59
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 0.8127
     Episode_Reward/lifting_object: 171.3018
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.97s
                      Time elapsed: 00:49:20
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 48896 steps/s (collection: 1.912s, learning 0.099s)
             Mean action noise std: 2.33
          Mean value_function loss: 127.8784
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.4859
                       Mean reward: 877.12
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 0.8215
     Episode_Reward/lifting_object: 173.0352
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.01s
                      Time elapsed: 00:49:22
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 50389 steps/s (collection: 1.860s, learning 0.091s)
             Mean action noise std: 2.33
          Mean value_function loss: 127.9617
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4933
                       Mean reward: 892.85
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.8342
     Episode_Reward/lifting_object: 176.3875
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.95s
                      Time elapsed: 00:49:24
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 50249 steps/s (collection: 1.846s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 108.4040
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.5005
                       Mean reward: 879.81
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.8355
     Episode_Reward/lifting_object: 176.4043
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 1.96s
                      Time elapsed: 00:49:26
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 49818 steps/s (collection: 1.854s, learning 0.119s)
             Mean action noise std: 2.33
          Mean value_function loss: 126.7601
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.5074
                       Mean reward: 900.35
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 0.8171
     Episode_Reward/lifting_object: 172.4136
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 1.97s
                      Time elapsed: 00:49:28
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 50566 steps/s (collection: 1.833s, learning 0.112s)
             Mean action noise std: 2.34
          Mean value_function loss: 134.1941
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5129
                       Mean reward: 885.25
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.8196
     Episode_Reward/lifting_object: 172.9596
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.94s
                      Time elapsed: 00:49:30
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 50043 steps/s (collection: 1.851s, learning 0.114s)
             Mean action noise std: 2.34
          Mean value_function loss: 112.4890
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.5183
                       Mean reward: 894.91
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.8351
     Episode_Reward/lifting_object: 176.0990
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.96s
                      Time elapsed: 00:49:32
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 50714 steps/s (collection: 1.844s, learning 0.095s)
             Mean action noise std: 2.34
          Mean value_function loss: 110.1310
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.5214
                       Mean reward: 895.47
               Mean episode length: 238.13
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 177.6339
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.94s
                      Time elapsed: 00:49:34
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 49967 steps/s (collection: 1.865s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 150.4968
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.5280
                       Mean reward: 887.37
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 174.5289
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.97s
                      Time elapsed: 00:49:36
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 50178 steps/s (collection: 1.857s, learning 0.102s)
             Mean action noise std: 2.34
          Mean value_function loss: 132.9013
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.5393
                       Mean reward: 880.78
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 0.8193
     Episode_Reward/lifting_object: 173.0678
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 1.96s
                      Time elapsed: 00:49:37
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 49304 steps/s (collection: 1.895s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 101.8064
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.5553
                       Mean reward: 859.23
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 0.8160
     Episode_Reward/lifting_object: 172.1930
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 1.99s
                      Time elapsed: 00:49:39
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 49762 steps/s (collection: 1.873s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 112.6646
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.5611
                       Mean reward: 888.58
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 0.8306
     Episode_Reward/lifting_object: 175.0883
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.98s
                      Time elapsed: 00:49:41
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 49976 steps/s (collection: 1.865s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 110.4353
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.5627
                       Mean reward: 889.58
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 0.8293
     Episode_Reward/lifting_object: 174.8647
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.97s
                      Time elapsed: 00:49:43
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 50017 steps/s (collection: 1.867s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 123.4696
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5655
                       Mean reward: 872.00
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 175.4049
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 1.97s
                      Time elapsed: 00:49:45
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 48772 steps/s (collection: 1.914s, learning 0.102s)
             Mean action noise std: 2.34
          Mean value_function loss: 139.6983
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5706
                       Mean reward: 892.68
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.8203
     Episode_Reward/lifting_object: 172.8767
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.02s
                      Time elapsed: 00:49:47
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 49717 steps/s (collection: 1.868s, learning 0.109s)
             Mean action noise std: 2.34
          Mean value_function loss: 139.7720
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5776
                       Mean reward: 871.41
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 0.8122
     Episode_Reward/lifting_object: 170.9432
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 1.98s
                      Time elapsed: 00:49:49
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 49867 steps/s (collection: 1.852s, learning 0.120s)
             Mean action noise std: 2.35
          Mean value_function loss: 157.1948
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.5870
                       Mean reward: 841.06
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.7989
     Episode_Reward/lifting_object: 167.9010
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 1.97s
                      Time elapsed: 00:49:51
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 50410 steps/s (collection: 1.855s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 107.4807
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.6000
                       Mean reward: 867.91
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.8327
     Episode_Reward/lifting_object: 176.4908
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.95s
                      Time elapsed: 00:49:53
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 50260 steps/s (collection: 1.862s, learning 0.094s)
             Mean action noise std: 2.35
          Mean value_function loss: 106.2687
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.6090
                       Mean reward: 913.95
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.8411
     Episode_Reward/lifting_object: 178.0239
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 1.96s
                      Time elapsed: 00:49:55
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 49784 steps/s (collection: 1.871s, learning 0.103s)
             Mean action noise std: 2.35
          Mean value_function loss: 143.7173
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.6153
                       Mean reward: 872.09
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 0.8124
     Episode_Reward/lifting_object: 171.1721
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.97s
                      Time elapsed: 00:49:57
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 48830 steps/s (collection: 1.890s, learning 0.123s)
             Mean action noise std: 2.35
          Mean value_function loss: 101.9913
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.6246
                       Mean reward: 905.88
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.8421
     Episode_Reward/lifting_object: 178.1806
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.01s
                      Time elapsed: 00:49:59
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 49542 steps/s (collection: 1.874s, learning 0.110s)
             Mean action noise std: 2.35
          Mean value_function loss: 95.6179
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.6321
                       Mean reward: 850.17
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 0.8294
     Episode_Reward/lifting_object: 175.1023
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.98s
                      Time elapsed: 00:50:01
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 49899 steps/s (collection: 1.864s, learning 0.106s)
             Mean action noise std: 2.35
          Mean value_function loss: 129.4246
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 39.6361
                       Mean reward: 839.19
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 0.8237
     Episode_Reward/lifting_object: 173.9724
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.97s
                      Time elapsed: 00:50:03
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 50038 steps/s (collection: 1.854s, learning 0.111s)
             Mean action noise std: 2.35
          Mean value_function loss: 116.9493
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.6376
                       Mean reward: 902.42
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.8233
     Episode_Reward/lifting_object: 173.9423
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.96s
                      Time elapsed: 00:50:05
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 50123 steps/s (collection: 1.869s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 115.6419
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.6397
                       Mean reward: 887.60
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 0.8387
     Episode_Reward/lifting_object: 177.3818
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.96s
                      Time elapsed: 00:50:07
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 50404 steps/s (collection: 1.846s, learning 0.104s)
             Mean action noise std: 2.35
          Mean value_function loss: 88.1172
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.6462
                       Mean reward: 905.91
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.8443
     Episode_Reward/lifting_object: 178.7015
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 1.95s
                      Time elapsed: 00:50:09
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 50378 steps/s (collection: 1.853s, learning 0.098s)
             Mean action noise std: 2.35
          Mean value_function loss: 81.8664
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.6535
                       Mean reward: 919.78
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.8400
     Episode_Reward/lifting_object: 177.9555
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 1.95s
                      Time elapsed: 00:50:11
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 50224 steps/s (collection: 1.840s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 129.5386
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.6604
                       Mean reward: 861.97
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 0.8202
     Episode_Reward/lifting_object: 173.1979
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.96s
                      Time elapsed: 00:50:13
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 50381 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 2.35
          Mean value_function loss: 120.4804
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.6634
                       Mean reward: 860.78
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 0.8260
     Episode_Reward/lifting_object: 174.4342
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.95s
                      Time elapsed: 00:50:15
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 49822 steps/s (collection: 1.878s, learning 0.096s)
             Mean action noise std: 2.36
          Mean value_function loss: 142.1236
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.6688
                       Mean reward: 862.31
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 0.8371
     Episode_Reward/lifting_object: 176.9788
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.97s
                      Time elapsed: 00:50:17
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 51018 steps/s (collection: 1.834s, learning 0.093s)
             Mean action noise std: 2.36
          Mean value_function loss: 99.9775
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.6772
                       Mean reward: 878.66
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 0.8368
     Episode_Reward/lifting_object: 176.0999
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.93s
                      Time elapsed: 00:50:19
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 50796 steps/s (collection: 1.851s, learning 0.084s)
             Mean action noise std: 2.36
          Mean value_function loss: 110.1186
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.6888
                       Mean reward: 894.64
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 0.8288
     Episode_Reward/lifting_object: 174.8990
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.94s
                      Time elapsed: 00:50:21
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 50644 steps/s (collection: 1.856s, learning 0.086s)
             Mean action noise std: 2.36
          Mean value_function loss: 101.2370
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.6974
                       Mean reward: 880.40
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 0.8344
     Episode_Reward/lifting_object: 175.7569
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 1.94s
                      Time elapsed: 00:50:23
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 50594 steps/s (collection: 1.855s, learning 0.088s)
             Mean action noise std: 2.36
          Mean value_function loss: 76.1140
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.7053
                       Mean reward: 909.80
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 0.8525
     Episode_Reward/lifting_object: 179.8202
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.94s
                      Time elapsed: 00:50:25
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 50189 steps/s (collection: 1.866s, learning 0.092s)
             Mean action noise std: 2.36
          Mean value_function loss: 109.8507
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.7116
                       Mean reward: 893.78
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 176.2850
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 1.96s
                      Time elapsed: 00:50:27
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 50046 steps/s (collection: 1.863s, learning 0.102s)
             Mean action noise std: 2.36
          Mean value_function loss: 115.9681
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.7178
                       Mean reward: 869.02
               Mean episode length: 232.84
    Episode_Reward/reaching_object: 0.8260
     Episode_Reward/lifting_object: 173.5276
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.96s
                      Time elapsed: 00:50:29
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 50758 steps/s (collection: 1.839s, learning 0.098s)
             Mean action noise std: 2.36
          Mean value_function loss: 105.6803
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.7260
                       Mean reward: 894.94
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.8230
     Episode_Reward/lifting_object: 172.6765
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 1.94s
                      Time elapsed: 00:50:31
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 50651 steps/s (collection: 1.836s, learning 0.105s)
             Mean action noise std: 2.36
          Mean value_function loss: 114.1303
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.7292
                       Mean reward: 904.13
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 0.8308
     Episode_Reward/lifting_object: 174.7001
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 1.94s
                      Time elapsed: 00:50:32
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 49002 steps/s (collection: 1.916s, learning 0.090s)
             Mean action noise std: 2.36
          Mean value_function loss: 109.8911
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.7347
                       Mean reward: 882.13
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 0.8382
     Episode_Reward/lifting_object: 176.1346
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.01s
                      Time elapsed: 00:50:34
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 49641 steps/s (collection: 1.878s, learning 0.102s)
             Mean action noise std: 2.37
          Mean value_function loss: 87.6344
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.7457
                       Mean reward: 880.81
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 176.4824
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.98s
                      Time elapsed: 00:50:36
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 49919 steps/s (collection: 1.883s, learning 0.087s)
             Mean action noise std: 2.37
          Mean value_function loss: 114.3172
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.7579
                       Mean reward: 889.77
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.8268
     Episode_Reward/lifting_object: 173.2873
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.97s
                      Time elapsed: 00:50:38
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 49615 steps/s (collection: 1.894s, learning 0.087s)
             Mean action noise std: 2.37
          Mean value_function loss: 97.5776
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.7710
                       Mean reward: 906.88
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.8364
     Episode_Reward/lifting_object: 175.4160
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.98s
                      Time elapsed: 00:50:40
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 49112 steps/s (collection: 1.900s, learning 0.102s)
             Mean action noise std: 2.37
          Mean value_function loss: 118.4646
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.7810
                       Mean reward: 820.19
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 0.8221
     Episode_Reward/lifting_object: 172.3163
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.00s
                      Time elapsed: 00:50:42
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 48387 steps/s (collection: 1.930s, learning 0.101s)
             Mean action noise std: 2.37
          Mean value_function loss: 102.5148
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.7890
                       Mean reward: 875.99
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 175.4905
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.03s
                      Time elapsed: 00:50:44
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 50518 steps/s (collection: 1.858s, learning 0.088s)
             Mean action noise std: 2.37
          Mean value_function loss: 113.3778
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.8025
                       Mean reward: 835.89
               Mean episode length: 224.67
    Episode_Reward/reaching_object: 0.8305
     Episode_Reward/lifting_object: 173.8810
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.95s
                      Time elapsed: 00:50:46
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 49731 steps/s (collection: 1.891s, learning 0.086s)
             Mean action noise std: 2.37
          Mean value_function loss: 119.5502
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.8147
                       Mean reward: 905.19
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.8346
     Episode_Reward/lifting_object: 174.9184
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.98s
                      Time elapsed: 00:50:48
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 49160 steps/s (collection: 1.889s, learning 0.111s)
             Mean action noise std: 2.38
          Mean value_function loss: 117.2705
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.8218
                       Mean reward: 881.28
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 0.8281
     Episode_Reward/lifting_object: 173.3867
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.00s
                      Time elapsed: 00:50:50
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 50243 steps/s (collection: 1.866s, learning 0.091s)
             Mean action noise std: 2.38
          Mean value_function loss: 128.7627
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.8278
                       Mean reward: 876.29
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: 172.6011
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 1.96s
                      Time elapsed: 00:50:52
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 50168 steps/s (collection: 1.862s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 115.6732
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.8379
                       Mean reward: 854.38
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 0.8351
     Episode_Reward/lifting_object: 175.1474
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.96s
                      Time elapsed: 00:50:54
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 50498 steps/s (collection: 1.852s, learning 0.094s)
             Mean action noise std: 2.38
          Mean value_function loss: 111.4603
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.8484
                       Mean reward: 852.88
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 171.5889
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 1.95s
                      Time elapsed: 00:50:56
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 49180 steps/s (collection: 1.896s, learning 0.102s)
             Mean action noise std: 2.38
          Mean value_function loss: 150.1390
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.8569
                       Mean reward: 875.46
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 0.8188
     Episode_Reward/lifting_object: 171.1911
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.00s
                      Time elapsed: 00:50:58
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 47733 steps/s (collection: 1.966s, learning 0.093s)
             Mean action noise std: 2.38
          Mean value_function loss: 137.2885
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8643
                       Mean reward: 865.05
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 0.8210
     Episode_Reward/lifting_object: 171.8946
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.06s
                      Time elapsed: 00:51:00
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 49615 steps/s (collection: 1.866s, learning 0.116s)
             Mean action noise std: 2.38
          Mean value_function loss: 115.9707
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.8709
                       Mean reward: 853.73
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 0.8234
     Episode_Reward/lifting_object: 172.4708
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 1.98s
                      Time elapsed: 00:51:02
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 49000 steps/s (collection: 1.909s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 88.6884
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.8815
                       Mean reward: 896.98
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 175.7395
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.01s
                      Time elapsed: 00:51:04
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 49818 steps/s (collection: 1.881s, learning 0.092s)
             Mean action noise std: 2.39
          Mean value_function loss: 101.2316
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.8921
                       Mean reward: 873.77
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 0.8390
     Episode_Reward/lifting_object: 176.1108
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 1.97s
                      Time elapsed: 00:51:06
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 50361 steps/s (collection: 1.858s, learning 0.094s)
             Mean action noise std: 2.39
          Mean value_function loss: 108.5364
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.9029
                       Mean reward: 846.76
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 0.8396
     Episode_Reward/lifting_object: 176.2902
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 1.95s
                      Time elapsed: 00:51:08
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 49443 steps/s (collection: 1.891s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 96.8356
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.9113
                       Mean reward: 876.60
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 0.8454
     Episode_Reward/lifting_object: 177.6310
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.99s
                      Time elapsed: 00:51:10
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 49623 steps/s (collection: 1.885s, learning 0.096s)
             Mean action noise std: 2.39
          Mean value_function loss: 97.0410
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.9208
                       Mean reward: 899.96
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.8454
     Episode_Reward/lifting_object: 178.1589
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 1.98s
                      Time elapsed: 00:51:12
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 49938 steps/s (collection: 1.875s, learning 0.094s)
             Mean action noise std: 2.39
          Mean value_function loss: 110.4122
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.9292
                       Mean reward: 883.42
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 174.3347
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 1.97s
                      Time elapsed: 00:51:14
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 50753 steps/s (collection: 1.849s, learning 0.088s)
             Mean action noise std: 2.39
          Mean value_function loss: 125.1480
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9438
                       Mean reward: 834.99
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: 172.0032
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.94s
                      Time elapsed: 00:51:16
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 50465 steps/s (collection: 1.855s, learning 0.093s)
             Mean action noise std: 2.39
          Mean value_function loss: 123.0181
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.9599
                       Mean reward: 866.18
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 0.8311
     Episode_Reward/lifting_object: 174.6855
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 1.95s
                      Time elapsed: 00:51:18
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 50554 steps/s (collection: 1.857s, learning 0.088s)
             Mean action noise std: 2.39
          Mean value_function loss: 105.6877
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.9696
                       Mean reward: 896.41
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.8441
     Episode_Reward/lifting_object: 177.7686
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 1.94s
                      Time elapsed: 00:51:20
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 49993 steps/s (collection: 1.879s, learning 0.088s)
             Mean action noise std: 2.40
          Mean value_function loss: 115.8000
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.9766
                       Mean reward: 857.86
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 0.8209
     Episode_Reward/lifting_object: 172.3997
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.97s
                      Time elapsed: 00:51:22
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 49723 steps/s (collection: 1.882s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 70.9832
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.9856
                       Mean reward: 899.93
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 0.8424
     Episode_Reward/lifting_object: 177.0699
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.98s
                      Time elapsed: 00:51:24
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 47863 steps/s (collection: 1.881s, learning 0.173s)
             Mean action noise std: 2.40
          Mean value_function loss: 119.3359
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.9914
                       Mean reward: 878.87
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 0.8253
     Episode_Reward/lifting_object: 172.7984
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.05s
                      Time elapsed: 00:51:26
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 46298 steps/s (collection: 2.033s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 96.7152
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.9977
                       Mean reward: 906.55
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 0.8421
     Episode_Reward/lifting_object: 177.0124
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.12s
                      Time elapsed: 00:51:28
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 49003 steps/s (collection: 1.900s, learning 0.106s)
             Mean action noise std: 2.40
          Mean value_function loss: 127.1877
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.0022
                       Mean reward: 862.36
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 0.8350
     Episode_Reward/lifting_object: 175.0597
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.01s
                      Time elapsed: 00:51:30
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 46003 steps/s (collection: 2.012s, learning 0.125s)
             Mean action noise std: 2.40
          Mean value_function loss: 143.8707
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0085
                       Mean reward: 854.75
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 0.8192
     Episode_Reward/lifting_object: 171.3651
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.14s
                      Time elapsed: 00:51:32
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 44880 steps/s (collection: 2.091s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 87.5080
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.0169
                       Mean reward: 878.47
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 175.8904
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.19s
                      Time elapsed: 00:51:34
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 47504 steps/s (collection: 1.952s, learning 0.118s)
             Mean action noise std: 2.40
          Mean value_function loss: 105.2774
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.0212
                       Mean reward: 858.36
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 0.8409
     Episode_Reward/lifting_object: 175.8177
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.07s
                      Time elapsed: 00:51:36
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 48495 steps/s (collection: 1.915s, learning 0.112s)
             Mean action noise std: 2.40
          Mean value_function loss: 117.4228
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.0316
                       Mean reward: 906.72
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.8320
     Episode_Reward/lifting_object: 174.0336
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.03s
                      Time elapsed: 00:51:39
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 47804 steps/s (collection: 1.963s, learning 0.093s)
             Mean action noise std: 2.40
          Mean value_function loss: 86.5238
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.0385
                       Mean reward: 921.97
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 177.5567
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.06s
                      Time elapsed: 00:51:41
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 50276 steps/s (collection: 1.868s, learning 0.087s)
             Mean action noise std: 2.41
          Mean value_function loss: 101.5728
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.0461
                       Mean reward: 880.31
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.8403
     Episode_Reward/lifting_object: 176.2094
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 1.96s
                      Time elapsed: 00:51:43
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 50416 steps/s (collection: 1.854s, learning 0.096s)
             Mean action noise std: 2.41
          Mean value_function loss: 114.4160
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0553
                       Mean reward: 871.29
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 174.0959
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 1.95s
                      Time elapsed: 00:51:44
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 49836 steps/s (collection: 1.868s, learning 0.105s)
             Mean action noise std: 2.41
          Mean value_function loss: 113.3783
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.0686
                       Mean reward: 882.04
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 0.8405
     Episode_Reward/lifting_object: 175.9209
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.97s
                      Time elapsed: 00:51:46
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 48887 steps/s (collection: 1.912s, learning 0.099s)
             Mean action noise std: 2.41
          Mean value_function loss: 118.5880
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.0798
                       Mean reward: 888.59
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 0.8449
     Episode_Reward/lifting_object: 177.1534
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.01s
                      Time elapsed: 00:51:48
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 48782 steps/s (collection: 1.913s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 114.8567
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.0908
                       Mean reward: 870.33
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 0.8259
     Episode_Reward/lifting_object: 172.6352
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.02s
                      Time elapsed: 00:51:50
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 50255 steps/s (collection: 1.868s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 94.1578
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.1019
                       Mean reward: 879.32
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 174.9375
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 1.96s
                      Time elapsed: 00:51:52
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 48992 steps/s (collection: 1.915s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 96.1283
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.1069
                       Mean reward: 861.20
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.8386
     Episode_Reward/lifting_object: 175.2771
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.01s
                      Time elapsed: 00:51:54
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 49362 steps/s (collection: 1.900s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 84.0861
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.1122
                       Mean reward: 891.50
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 0.8508
     Episode_Reward/lifting_object: 178.2900
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.99s
                      Time elapsed: 00:51:56
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 48632 steps/s (collection: 1.914s, learning 0.107s)
             Mean action noise std: 2.41
          Mean value_function loss: 120.9616
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.1165
                       Mean reward: 881.30
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.8289
     Episode_Reward/lifting_object: 173.0515
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.02s
                      Time elapsed: 00:51:58
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 48713 steps/s (collection: 1.912s, learning 0.106s)
             Mean action noise std: 2.42
          Mean value_function loss: 84.0602
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.1253
                       Mean reward: 906.49
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.8637
     Episode_Reward/lifting_object: 180.2540
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.02s
                      Time elapsed: 00:52:00
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.880s, learning 0.103s)
             Mean action noise std: 2.42
          Mean value_function loss: 93.4468
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.1359
                       Mean reward: 890.54
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.8493
     Episode_Reward/lifting_object: 177.3037
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 1.98s
                      Time elapsed: 00:52:02
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 50060 steps/s (collection: 1.869s, learning 0.095s)
             Mean action noise std: 2.42
          Mean value_function loss: 111.7005
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.1444
                       Mean reward: 830.73
               Mean episode length: 223.12
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 173.8503
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 1.96s
                      Time elapsed: 00:52:04
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 49890 steps/s (collection: 1.879s, learning 0.091s)
             Mean action noise std: 2.42
          Mean value_function loss: 101.2290
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.1552
                       Mean reward: 877.45
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.8402
     Episode_Reward/lifting_object: 175.0552
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.97s
                      Time elapsed: 00:52:06
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 49243 steps/s (collection: 1.904s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 105.7360
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.1669
                       Mean reward: 889.78
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 0.8404
     Episode_Reward/lifting_object: 175.2151
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.00s
                      Time elapsed: 00:52:08
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 48920 steps/s (collection: 1.917s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 129.4446
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.1766
                       Mean reward: 897.13
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 173.9090
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.01s
                      Time elapsed: 00:52:10
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 49908 steps/s (collection: 1.877s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 91.6829
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.1857
                       Mean reward: 824.26
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 175.1093
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 1.97s
                      Time elapsed: 00:52:12
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 49448 steps/s (collection: 1.880s, learning 0.108s)
             Mean action noise std: 2.43
          Mean value_function loss: 103.0646
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1999
                       Mean reward: 844.87
               Mean episode length: 226.98
    Episode_Reward/reaching_object: 0.8299
     Episode_Reward/lifting_object: 172.6920
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 1.99s
                      Time elapsed: 00:52:14
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 49982 steps/s (collection: 1.868s, learning 0.099s)
             Mean action noise std: 2.43
          Mean value_function loss: 98.3071
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.2163
                       Mean reward: 862.11
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 174.3346
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 1.97s
                      Time elapsed: 00:52:16
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 49334 steps/s (collection: 1.892s, learning 0.101s)
             Mean action noise std: 2.43
          Mean value_function loss: 105.2783
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.2274
                       Mean reward: 904.68
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 0.8446
     Episode_Reward/lifting_object: 176.5067
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 1.99s
                      Time elapsed: 00:52:18
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 48466 steps/s (collection: 1.933s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 142.1134
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.2374
                       Mean reward: 909.17
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.8290
     Episode_Reward/lifting_object: 173.0368
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.03s
                      Time elapsed: 00:52:20
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 49619 steps/s (collection: 1.878s, learning 0.103s)
             Mean action noise std: 2.43
          Mean value_function loss: 124.9662
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.2517
                       Mean reward: 910.27
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.8269
     Episode_Reward/lifting_object: 172.7128
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.98s
                      Time elapsed: 00:52:22
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 49533 steps/s (collection: 1.889s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 103.5088
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2615
                       Mean reward: 903.28
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 174.2701
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 1.98s
                      Time elapsed: 00:52:24
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 49349 steps/s (collection: 1.897s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 99.7971
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2708
                       Mean reward: 844.42
               Mean episode length: 226.10
    Episode_Reward/reaching_object: 0.8310
     Episode_Reward/lifting_object: 173.0890
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 1.99s
                      Time elapsed: 00:52:26
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 50259 steps/s (collection: 1.856s, learning 0.100s)
             Mean action noise std: 2.44
          Mean value_function loss: 132.3971
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.2818
                       Mean reward: 849.35
               Mean episode length: 226.78
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: 170.9002
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 1.96s
                      Time elapsed: 00:52:28
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 50163 steps/s (collection: 1.852s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 103.7977
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.2907
                       Mean reward: 919.92
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.8477
     Episode_Reward/lifting_object: 177.2983
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 1.96s
                      Time elapsed: 00:52:30
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 49524 steps/s (collection: 1.889s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 108.4492
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.2962
                       Mean reward: 885.17
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 0.8412
     Episode_Reward/lifting_object: 175.2792
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 1.98s
                      Time elapsed: 00:52:32
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 49580 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 2.44
          Mean value_function loss: 108.6887
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.3032
                       Mean reward: 891.06
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.8451
     Episode_Reward/lifting_object: 176.1954
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 1.98s
                      Time elapsed: 00:52:34
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 49360 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 135.3149
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.3149
                       Mean reward: 873.91
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.8353
     Episode_Reward/lifting_object: 174.0120
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.99s
                      Time elapsed: 00:52:36
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 50041 steps/s (collection: 1.875s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 127.0964
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.3257
                       Mean reward: 840.77
               Mean episode length: 225.53
    Episode_Reward/reaching_object: 0.8379
     Episode_Reward/lifting_object: 174.3586
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 1.96s
                      Time elapsed: 00:52:38
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 49033 steps/s (collection: 1.914s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 102.5182
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.3352
                       Mean reward: 900.87
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 176.4109
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.00s
                      Time elapsed: 00:52:40
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 49895 steps/s (collection: 1.883s, learning 0.087s)
             Mean action noise std: 2.45
          Mean value_function loss: 103.4235
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.3413
                       Mean reward: 875.64
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 178.7036
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 1.97s
                      Time elapsed: 00:52:42
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 47964 steps/s (collection: 1.958s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 123.1915
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.3503
                       Mean reward: 841.46
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 0.8458
     Episode_Reward/lifting_object: 175.6933
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.05s
                      Time elapsed: 00:52:44
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 48963 steps/s (collection: 1.917s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 132.3972
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.3582
                       Mean reward: 848.13
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 0.8158
     Episode_Reward/lifting_object: 168.5546
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.01s
                      Time elapsed: 00:52:46
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 48773 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 130.6367
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.3714
                       Mean reward: 881.85
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.8275
     Episode_Reward/lifting_object: 171.2719
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.02s
                      Time elapsed: 00:52:48
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 49432 steps/s (collection: 1.892s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 102.5075
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.3779
                       Mean reward: 843.84
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 177.7362
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 1.99s
                      Time elapsed: 00:52:50
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 49879 steps/s (collection: 1.883s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 90.6270
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.3890
                       Mean reward: 896.37
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 0.8583
     Episode_Reward/lifting_object: 177.4870
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 1.97s
                      Time elapsed: 00:52:52
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 50233 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 104.2470
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 40.3975
                       Mean reward: 894.36
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.8364
     Episode_Reward/lifting_object: 172.6943
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 1.96s
                      Time elapsed: 00:52:54
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 48473 steps/s (collection: 1.936s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 103.2403
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.3998
                       Mean reward: 849.92
               Mean episode length: 227.51
    Episode_Reward/reaching_object: 0.8296
     Episode_Reward/lifting_object: 171.2426
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.03s
                      Time elapsed: 00:52:56
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 48972 steps/s (collection: 1.889s, learning 0.118s)
             Mean action noise std: 2.45
          Mean value_function loss: 111.3953
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.4036
                       Mean reward: 833.30
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 0.8458
     Episode_Reward/lifting_object: 174.6081
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.01s
                      Time elapsed: 00:52:58
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 48642 steps/s (collection: 1.899s, learning 0.122s)
             Mean action noise std: 2.45
          Mean value_function loss: 105.3928
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.4061
                       Mean reward: 869.22
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 0.8329
     Episode_Reward/lifting_object: 171.6392
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.02s
                      Time elapsed: 00:53:00
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 49482 steps/s (collection: 1.887s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 90.4366
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.4084
                       Mean reward: 900.27
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 177.2784
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 1.99s
                      Time elapsed: 00:53:02
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 49347 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 108.9985
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.4118
                       Mean reward: 896.33
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.8462
     Episode_Reward/lifting_object: 174.7558
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 1.99s
                      Time elapsed: 00:53:04
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 50449 steps/s (collection: 1.857s, learning 0.092s)
             Mean action noise std: 2.46
          Mean value_function loss: 86.8505
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.4167
                       Mean reward: 895.48
               Mean episode length: 238.33
    Episode_Reward/reaching_object: 0.8612
     Episode_Reward/lifting_object: 178.7507
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 1.95s
                      Time elapsed: 00:53:06
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 49244 steps/s (collection: 1.906s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 108.4008
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.4223
                       Mean reward: 886.45
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 0.8394
     Episode_Reward/lifting_object: 173.6458
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.00s
                      Time elapsed: 00:53:08
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 49715 steps/s (collection: 1.888s, learning 0.089s)
             Mean action noise std: 2.46
          Mean value_function loss: 89.6065
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.4295
                       Mean reward: 852.29
               Mean episode length: 229.51
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 175.0147
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 1.98s
                      Time elapsed: 00:53:10
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 49674 steps/s (collection: 1.893s, learning 0.086s)
             Mean action noise std: 2.46
          Mean value_function loss: 82.9965
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.4338
                       Mean reward: 894.42
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.8507
     Episode_Reward/lifting_object: 176.8629
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 1.98s
                      Time elapsed: 00:53:12
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 49003 steps/s (collection: 1.916s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 89.3805
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4405
                       Mean reward: 917.05
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.8368
     Episode_Reward/lifting_object: 173.5465
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.01s
                      Time elapsed: 00:53:14
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 49494 steps/s (collection: 1.898s, learning 0.089s)
             Mean action noise std: 2.46
          Mean value_function loss: 89.3871
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.4521
                       Mean reward: 874.10
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 0.8458
     Episode_Reward/lifting_object: 176.0271
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 1.99s
                      Time elapsed: 00:53:16
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 49937 steps/s (collection: 1.875s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 96.6831
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.4579
                       Mean reward: 884.22
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.8457
     Episode_Reward/lifting_object: 176.0652
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 1.97s
                      Time elapsed: 00:53:18
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 48934 steps/s (collection: 1.912s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 88.8280
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.4648
                       Mean reward: 893.33
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 177.1106
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.01s
                      Time elapsed: 00:53:20
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 48482 steps/s (collection: 1.930s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 105.7635
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.4715
                       Mean reward: 899.59
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 174.1410
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.03s
                      Time elapsed: 00:53:22
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 49424 steps/s (collection: 1.899s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 124.4837
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.4830
                       Mean reward: 870.46
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 174.0882
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 1.99s
                      Time elapsed: 00:53:24
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 48535 steps/s (collection: 1.931s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 115.2529
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.4920
                       Mean reward: 847.20
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.8187
     Episode_Reward/lifting_object: 170.8425
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.03s
                      Time elapsed: 00:53:26
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 48885 steps/s (collection: 1.903s, learning 0.108s)
             Mean action noise std: 2.47
          Mean value_function loss: 107.9815
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.5016
                       Mean reward: 843.17
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 0.8320
     Episode_Reward/lifting_object: 173.4239
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.01s
                      Time elapsed: 00:53:28
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 49564 steps/s (collection: 1.865s, learning 0.119s)
             Mean action noise std: 2.47
          Mean value_function loss: 97.3938
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.5085
                       Mean reward: 904.27
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.8407
     Episode_Reward/lifting_object: 175.8584
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.98s
                      Time elapsed: 00:53:30
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 50242 steps/s (collection: 1.860s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 93.4489
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.5174
                       Mean reward: 898.25
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 174.5299
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 1.96s
                      Time elapsed: 00:53:32
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 49784 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 98.4379
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.5252
                       Mean reward: 894.65
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 173.9553
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 1.97s
                      Time elapsed: 00:53:34
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 47631 steps/s (collection: 1.974s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 105.9095
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.5297
                       Mean reward: 884.68
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 175.8474
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.06s
                      Time elapsed: 00:53:36
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 49237 steps/s (collection: 1.908s, learning 0.089s)
             Mean action noise std: 2.47
          Mean value_function loss: 80.9610
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.5380
                       Mean reward: 892.72
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 0.8507
     Episode_Reward/lifting_object: 177.8658
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.00s
                      Time elapsed: 00:53:38
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 49090 steps/s (collection: 1.916s, learning 0.087s)
             Mean action noise std: 2.47
          Mean value_function loss: 105.3930
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.5459
                       Mean reward: 857.68
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 0.8404
     Episode_Reward/lifting_object: 175.5717
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.00s
                      Time elapsed: 00:53:40
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 50487 steps/s (collection: 1.859s, learning 0.088s)
             Mean action noise std: 2.47
          Mean value_function loss: 106.9789
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.5487
                       Mean reward: 890.34
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 177.3876
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 1.95s
                      Time elapsed: 00:53:42
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 50000 steps/s (collection: 1.881s, learning 0.086s)
             Mean action noise std: 2.47
          Mean value_function loss: 91.9251
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.5533
                       Mean reward: 888.29
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.8277
     Episode_Reward/lifting_object: 172.5688
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 1.97s
                      Time elapsed: 00:53:44
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 49716 steps/s (collection: 1.881s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 95.4229
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.5588
                       Mean reward: 891.36
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 174.3988
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 1.98s
                      Time elapsed: 00:53:46
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 48627 steps/s (collection: 1.924s, learning 0.098s)
             Mean action noise std: 2.48
          Mean value_function loss: 125.0046
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.5643
                       Mean reward: 875.71
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 174.5450
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.02s
                      Time elapsed: 00:53:48
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 49827 steps/s (collection: 1.883s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 100.8443
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 40.5712
                       Mean reward: 830.16
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 0.8261
     Episode_Reward/lifting_object: 172.4942
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.97s
                      Time elapsed: 00:53:50
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 48222 steps/s (collection: 1.948s, learning 0.091s)
             Mean action noise std: 2.48
          Mean value_function loss: 119.0550
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.5776
                       Mean reward: 874.99
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 0.8321
     Episode_Reward/lifting_object: 173.9294
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.04s
                      Time elapsed: 00:53:52
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 50199 steps/s (collection: 1.864s, learning 0.094s)
             Mean action noise std: 2.48
          Mean value_function loss: 112.2724
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.5854
                       Mean reward: 893.27
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.8189
     Episode_Reward/lifting_object: 171.0086
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 1.96s
                      Time elapsed: 00:53:54
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 50062 steps/s (collection: 1.867s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 94.4335
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.5978
                       Mean reward: 902.98
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.8439
     Episode_Reward/lifting_object: 176.6888
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 1.96s
                      Time elapsed: 00:53:56
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 49739 steps/s (collection: 1.863s, learning 0.114s)
             Mean action noise std: 2.48
          Mean value_function loss: 100.0462
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.6100
                       Mean reward: 909.29
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.8381
     Episode_Reward/lifting_object: 175.4104
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 1.98s
                      Time elapsed: 00:53:58
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 49338 steps/s (collection: 1.888s, learning 0.105s)
             Mean action noise std: 2.48
          Mean value_function loss: 94.6681
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.6224
                       Mean reward: 882.99
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.8467
     Episode_Reward/lifting_object: 177.1974
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 1.99s
                      Time elapsed: 00:54:00
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 49466 steps/s (collection: 1.877s, learning 0.110s)
             Mean action noise std: 2.48
          Mean value_function loss: 94.1771
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.6300
                       Mean reward: 856.86
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 0.8451
     Episode_Reward/lifting_object: 177.0957
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 1.99s
                      Time elapsed: 00:54:02
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.913s, learning 0.091s)
             Mean action noise std: 2.48
          Mean value_function loss: 103.3059
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.6343
                       Mean reward: 917.05
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.8461
     Episode_Reward/lifting_object: 177.3564
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.00s
                      Time elapsed: 00:54:04
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 49702 steps/s (collection: 1.886s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 118.9183
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.6419
                       Mean reward: 889.44
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 174.6971
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 1.98s
                      Time elapsed: 00:54:06
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 49921 steps/s (collection: 1.875s, learning 0.095s)
             Mean action noise std: 2.49
          Mean value_function loss: 114.2150
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.6491
                       Mean reward: 882.10
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 175.4902
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 1.97s
                      Time elapsed: 00:54:08
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 49758 steps/s (collection: 1.878s, learning 0.098s)
             Mean action noise std: 2.49
          Mean value_function loss: 115.7056
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.6555
                       Mean reward: 867.73
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 176.7453
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 1.98s
                      Time elapsed: 00:54:10
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 49755 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 124.3898
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.6670
                       Mean reward: 856.45
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 0.8093
     Episode_Reward/lifting_object: 169.0527
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 1.98s
                      Time elapsed: 00:54:12
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 49667 steps/s (collection: 1.886s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 118.7829
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.6788
                       Mean reward: 866.36
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 0.8374
     Episode_Reward/lifting_object: 175.4367
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 1.98s
                      Time elapsed: 00:54:14
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 50358 steps/s (collection: 1.864s, learning 0.088s)
             Mean action noise std: 2.49
          Mean value_function loss: 105.4636
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.6828
                       Mean reward: 878.40
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 0.8372
     Episode_Reward/lifting_object: 175.1097
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 1.95s
                      Time elapsed: 00:54:16
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 49542 steps/s (collection: 1.896s, learning 0.089s)
             Mean action noise std: 2.49
          Mean value_function loss: 99.4873
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.6901
                       Mean reward: 898.46
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.8302
     Episode_Reward/lifting_object: 173.3418
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 1.98s
                      Time elapsed: 00:54:18
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 49301 steps/s (collection: 1.905s, learning 0.089s)
             Mean action noise std: 2.49
          Mean value_function loss: 132.2576
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.6993
                       Mean reward: 836.63
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 0.8344
     Episode_Reward/lifting_object: 174.1344
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 1.99s
                      Time elapsed: 00:54:20
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 49985 steps/s (collection: 1.874s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 93.8722
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.7071
                       Mean reward: 880.85
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 175.8744
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 1.97s
                      Time elapsed: 00:54:22
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 50300 steps/s (collection: 1.869s, learning 0.086s)
             Mean action noise std: 2.49
          Mean value_function loss: 112.4821
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.7140
                       Mean reward: 879.28
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 0.8468
     Episode_Reward/lifting_object: 177.8050
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 1.95s
                      Time elapsed: 00:54:24
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 49461 steps/s (collection: 1.897s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 96.3751
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.7171
                       Mean reward: 882.18
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.8484
     Episode_Reward/lifting_object: 177.9306
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 1.99s
                      Time elapsed: 00:54:26
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 48832 steps/s (collection: 1.916s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 117.3969
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.7220
                       Mean reward: 907.84
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 176.0308
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.01s
                      Time elapsed: 00:54:28
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 49480 steps/s (collection: 1.867s, learning 0.120s)
             Mean action noise std: 2.50
          Mean value_function loss: 106.3133
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.7294
                       Mean reward: 872.57
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.8409
     Episode_Reward/lifting_object: 176.7858
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 1.99s
                      Time elapsed: 00:54:30
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 48062 steps/s (collection: 1.926s, learning 0.119s)
             Mean action noise std: 2.50
          Mean value_function loss: 86.4822
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.7338
                       Mean reward: 919.90
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.8469
     Episode_Reward/lifting_object: 177.5491
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.05s
                      Time elapsed: 00:54:32
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 47295 steps/s (collection: 1.950s, learning 0.128s)
             Mean action noise std: 2.50
          Mean value_function loss: 139.4569
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.7398
                       Mean reward: 855.49
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 0.8077
     Episode_Reward/lifting_object: 169.1605
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.08s
                      Time elapsed: 00:54:34
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 47978 steps/s (collection: 1.941s, learning 0.108s)
             Mean action noise std: 2.50
          Mean value_function loss: 117.4542
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.7463
                       Mean reward: 828.42
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 0.8265
     Episode_Reward/lifting_object: 173.4049
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.05s
                      Time elapsed: 00:54:36
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 48619 steps/s (collection: 1.908s, learning 0.114s)
             Mean action noise std: 2.50
          Mean value_function loss: 111.0401
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.7525
                       Mean reward: 898.55
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 177.4159
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.02s
                      Time elapsed: 00:54:38
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 48981 steps/s (collection: 1.904s, learning 0.103s)
             Mean action noise std: 2.50
          Mean value_function loss: 100.7623
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.7579
                       Mean reward: 906.58
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 0.8446
     Episode_Reward/lifting_object: 177.8958
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.01s
                      Time elapsed: 00:54:40
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 48464 steps/s (collection: 1.923s, learning 0.105s)
             Mean action noise std: 2.50
          Mean value_function loss: 106.8757
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.7655
                       Mean reward: 912.77
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 0.8399
     Episode_Reward/lifting_object: 176.8583
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.03s
                      Time elapsed: 00:54:42
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 48704 steps/s (collection: 1.908s, learning 0.111s)
             Mean action noise std: 2.51
          Mean value_function loss: 110.7972
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.7795
                       Mean reward: 853.37
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 175.7497
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.02s
                      Time elapsed: 00:54:44
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 48716 steps/s (collection: 1.915s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 111.8600
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 40.7882
                       Mean reward: 887.08
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 0.8299
     Episode_Reward/lifting_object: 173.8163
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.02s
                      Time elapsed: 00:54:46
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 48129 steps/s (collection: 1.940s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 86.5680
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.7940
                       Mean reward: 906.69
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.8534
     Episode_Reward/lifting_object: 179.3175
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.04s
                      Time elapsed: 00:54:48
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 48451 steps/s (collection: 1.933s, learning 0.096s)
             Mean action noise std: 2.51
          Mean value_function loss: 103.7470
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.7996
                       Mean reward: 848.93
               Mean episode length: 226.85
    Episode_Reward/reaching_object: 0.8287
     Episode_Reward/lifting_object: 174.4513
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.03s
                      Time elapsed: 00:54:50
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 46491 steps/s (collection: 2.013s, learning 0.102s)
             Mean action noise std: 2.51
          Mean value_function loss: 118.4845
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.8031
                       Mean reward: 908.50
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 0.8238
     Episode_Reward/lifting_object: 172.8172
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.11s
                      Time elapsed: 00:54:52
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 48595 steps/s (collection: 1.932s, learning 0.091s)
             Mean action noise std: 2.51
          Mean value_function loss: 96.1730
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.8085
                       Mean reward: 898.64
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.8367
     Episode_Reward/lifting_object: 175.9753
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.02s
                      Time elapsed: 00:54:54
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 48522 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 82.2803
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.8149
                       Mean reward: 875.03
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 176.3143
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.03s
                      Time elapsed: 00:54:56
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 48766 steps/s (collection: 1.899s, learning 0.117s)
             Mean action noise std: 2.51
          Mean value_function loss: 97.1372
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.8266
                       Mean reward: 913.86
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.8400
     Episode_Reward/lifting_object: 176.5427
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.02s
                      Time elapsed: 00:54:58
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 48786 steps/s (collection: 1.896s, learning 0.119s)
             Mean action noise std: 2.51
          Mean value_function loss: 105.2829
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.8355
                       Mean reward: 875.21
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.8213
     Episode_Reward/lifting_object: 172.7020
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.01s
                      Time elapsed: 00:55:00
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 48077 steps/s (collection: 1.940s, learning 0.105s)
             Mean action noise std: 2.51
          Mean value_function loss: 116.2396
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.8442
                       Mean reward: 873.32
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 170.3064
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.04s
                      Time elapsed: 00:55:02
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 43604 steps/s (collection: 2.135s, learning 0.119s)
             Mean action noise std: 2.52
          Mean value_function loss: 96.6625
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.8512
                       Mean reward: 884.09
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 0.8368
     Episode_Reward/lifting_object: 175.8665
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.25s
                      Time elapsed: 00:55:04
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 42906 steps/s (collection: 2.178s, learning 0.114s)
             Mean action noise std: 2.52
          Mean value_function loss: 98.1478
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.8604
                       Mean reward: 888.87
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 0.8462
     Episode_Reward/lifting_object: 178.0718
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.29s
                      Time elapsed: 00:55:07
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 46571 steps/s (collection: 1.993s, learning 0.118s)
             Mean action noise std: 2.52
          Mean value_function loss: 89.2216
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.8662
                       Mean reward: 872.69
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 0.8286
     Episode_Reward/lifting_object: 173.7130
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.11s
                      Time elapsed: 00:55:09
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 47809 steps/s (collection: 1.947s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 86.6301
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.8781
                       Mean reward: 878.20
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 0.8372
     Episode_Reward/lifting_object: 175.7004
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.06s
                      Time elapsed: 00:55:11
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 47668 steps/s (collection: 1.947s, learning 0.115s)
             Mean action noise std: 2.52
          Mean value_function loss: 82.4914
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.8884
                       Mean reward: 875.06
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 0.8506
     Episode_Reward/lifting_object: 178.4396
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.06s
                      Time elapsed: 00:55:13
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 47018 steps/s (collection: 1.981s, learning 0.110s)
             Mean action noise std: 2.52
          Mean value_function loss: 98.2382
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.8924
                       Mean reward: 913.86
               Mean episode length: 242.31
    Episode_Reward/reaching_object: 0.8377
     Episode_Reward/lifting_object: 175.6084
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.09s
                      Time elapsed: 00:55:15
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 48282 steps/s (collection: 1.941s, learning 0.095s)
             Mean action noise std: 2.52
          Mean value_function loss: 87.2131
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.8979
                       Mean reward: 902.27
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 176.5597
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.04s
                      Time elapsed: 00:55:17
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 49532 steps/s (collection: 1.891s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 79.4937
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.9049
                       Mean reward: 911.89
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 179.9802
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 1.98s
                      Time elapsed: 00:55:19
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 48721 steps/s (collection: 1.907s, learning 0.111s)
             Mean action noise std: 2.52
          Mean value_function loss: 116.4299
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.9190
                       Mean reward: 876.34
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 0.8290
     Episode_Reward/lifting_object: 173.7369
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.02s
                      Time elapsed: 00:55:21
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 48364 steps/s (collection: 1.932s, learning 0.101s)
             Mean action noise std: 2.53
          Mean value_function loss: 110.7737
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.9320
                       Mean reward: 888.93
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 0.8305
     Episode_Reward/lifting_object: 173.8628
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.03s
                      Time elapsed: 00:55:23
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 47129 steps/s (collection: 1.973s, learning 0.113s)
             Mean action noise std: 2.53
          Mean value_function loss: 82.5593
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.9409
                       Mean reward: 874.30
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 0.8399
     Episode_Reward/lifting_object: 176.3431
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.09s
                      Time elapsed: 00:55:25
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 48662 steps/s (collection: 1.908s, learning 0.112s)
             Mean action noise std: 2.53
          Mean value_function loss: 90.9021
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.9516
                       Mean reward: 906.26
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.8470
     Episode_Reward/lifting_object: 177.5901
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.02s
                      Time elapsed: 00:55:27
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 48481 steps/s (collection: 1.914s, learning 0.114s)
             Mean action noise std: 2.53
          Mean value_function loss: 116.7315
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.9632
                       Mean reward: 869.45
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 0.8416
     Episode_Reward/lifting_object: 176.6121
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.03s
                      Time elapsed: 00:55:29
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 48698 steps/s (collection: 1.920s, learning 0.099s)
             Mean action noise std: 2.53
          Mean value_function loss: 113.5325
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.9699
                       Mean reward: 882.74
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 175.9700
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.02s
                      Time elapsed: 00:55:31
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 48706 steps/s (collection: 1.915s, learning 0.104s)
             Mean action noise std: 2.53
          Mean value_function loss: 124.0880
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.9764
                       Mean reward: 897.58
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.8386
     Episode_Reward/lifting_object: 176.0676
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.02s
                      Time elapsed: 00:55:33
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 46788 steps/s (collection: 1.982s, learning 0.120s)
             Mean action noise std: 2.53
          Mean value_function loss: 82.4459
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 40.9869
                       Mean reward: 864.35
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 0.8524
     Episode_Reward/lifting_object: 178.9553
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.10s
                      Time elapsed: 00:55:35
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 47218 steps/s (collection: 1.976s, learning 0.106s)
             Mean action noise std: 2.53
          Mean value_function loss: 83.4926
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 40.9945
                       Mean reward: 873.03
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 175.6583
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.08s
                      Time elapsed: 00:55:37
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 47997 steps/s (collection: 1.934s, learning 0.114s)
             Mean action noise std: 2.54
          Mean value_function loss: 83.2876
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.9993
                       Mean reward: 908.39
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 178.0595
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.05s
                      Time elapsed: 00:55:39
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 48413 steps/s (collection: 1.929s, learning 0.102s)
             Mean action noise std: 2.54
          Mean value_function loss: 97.3091
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 41.0069
                       Mean reward: 904.77
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 179.8578
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.03s
                      Time elapsed: 00:55:41
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 48054 steps/s (collection: 1.941s, learning 0.104s)
             Mean action noise std: 2.54
          Mean value_function loss: 70.9703
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.0108
                       Mean reward: 925.39
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.8613
     Episode_Reward/lifting_object: 180.7533
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.05s
                      Time elapsed: 00:55:44
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 48496 steps/s (collection: 1.929s, learning 0.098s)
             Mean action noise std: 2.54
          Mean value_function loss: 72.0642
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.0155
                       Mean reward: 894.64
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: 179.9924
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.03s
                      Time elapsed: 00:55:46
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 47274 steps/s (collection: 1.959s, learning 0.120s)
             Mean action noise std: 2.54
          Mean value_function loss: 98.2377
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.0220
                       Mean reward: 875.81
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 176.2330
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.08s
                      Time elapsed: 00:55:48
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 48420 steps/s (collection: 1.928s, learning 0.102s)
             Mean action noise std: 2.54
          Mean value_function loss: 88.3929
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.0304
                       Mean reward: 915.89
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.8572
     Episode_Reward/lifting_object: 180.1983
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.03s
                      Time elapsed: 00:55:50
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 48231 steps/s (collection: 1.935s, learning 0.104s)
             Mean action noise std: 2.54
          Mean value_function loss: 94.8967
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 41.0380
                       Mean reward: 899.98
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.8404
     Episode_Reward/lifting_object: 176.3323
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.04s
                      Time elapsed: 00:55:52
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 48268 steps/s (collection: 1.936s, learning 0.101s)
             Mean action noise std: 2.54
          Mean value_function loss: 127.1682
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.0397
                       Mean reward: 872.18
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 0.8234
     Episode_Reward/lifting_object: 172.1926
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.04s
                      Time elapsed: 00:55:54
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 48220 steps/s (collection: 1.946s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 97.5761
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.0415
                       Mean reward: 872.13
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 0.8520
     Episode_Reward/lifting_object: 179.1867
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.04s
                      Time elapsed: 00:55:56
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 46619 steps/s (collection: 1.988s, learning 0.121s)
             Mean action noise std: 2.54
          Mean value_function loss: 72.3835
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.0473
                       Mean reward: 867.23
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 178.2889
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.11s
                      Time elapsed: 00:55:58
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 45331 steps/s (collection: 2.059s, learning 0.110s)
             Mean action noise std: 2.54
          Mean value_function loss: 82.8139
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.0564
                       Mean reward: 912.43
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.8521
     Episode_Reward/lifting_object: 178.8941
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.17s
                      Time elapsed: 00:56:00
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 45457 steps/s (collection: 2.029s, learning 0.134s)
             Mean action noise std: 2.55
          Mean value_function loss: 97.7788
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.0646
                       Mean reward: 898.72
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.8410
     Episode_Reward/lifting_object: 177.0069
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.16s
                      Time elapsed: 00:56:02
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 45603 steps/s (collection: 2.039s, learning 0.117s)
             Mean action noise std: 2.55
          Mean value_function loss: 85.2137
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.0742
                       Mean reward: 908.29
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.8502
     Episode_Reward/lifting_object: 178.7762
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.16s
                      Time elapsed: 00:56:04
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 46373 steps/s (collection: 2.019s, learning 0.101s)
             Mean action noise std: 2.55
          Mean value_function loss: 115.5376
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.0830
                       Mean reward: 885.79
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 177.2503
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.12s
                      Time elapsed: 00:56:07
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 46955 steps/s (collection: 1.975s, learning 0.119s)
             Mean action noise std: 2.55
          Mean value_function loss: 104.7031
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.0934
                       Mean reward: 882.19
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.8345
     Episode_Reward/lifting_object: 174.6080
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.09s
                      Time elapsed: 00:56:09
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 45585 steps/s (collection: 2.044s, learning 0.113s)
             Mean action noise std: 2.55
          Mean value_function loss: 93.0670
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.1016
                       Mean reward: 893.16
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.8448
     Episode_Reward/lifting_object: 177.4681
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.16s
                      Time elapsed: 00:56:11
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 44913 steps/s (collection: 2.077s, learning 0.112s)
             Mean action noise std: 2.55
          Mean value_function loss: 97.4887
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.1143
                       Mean reward: 847.48
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 0.8321
     Episode_Reward/lifting_object: 174.5055
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.19s
                      Time elapsed: 00:56:13
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 47874 steps/s (collection: 1.946s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 98.2698
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.1297
                       Mean reward: 907.12
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.8549
     Episode_Reward/lifting_object: 179.8982
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.05s
                      Time elapsed: 00:56:15
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 47964 steps/s (collection: 1.953s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 115.9904
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 41.1404
                       Mean reward: 877.52
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 0.8197
     Episode_Reward/lifting_object: 172.0462
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.05s
                      Time elapsed: 00:56:17
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 48015 steps/s (collection: 1.943s, learning 0.104s)
             Mean action noise std: 2.56
          Mean value_function loss: 90.4914
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.1417
                       Mean reward: 884.90
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 0.8285
     Episode_Reward/lifting_object: 173.9299
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.05s
                      Time elapsed: 00:56:19
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 47305 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 91.5223
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.1452
                       Mean reward: 882.34
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 0.8354
     Episode_Reward/lifting_object: 175.2826
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.08s
                      Time elapsed: 00:56:21
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 48001 steps/s (collection: 1.945s, learning 0.103s)
             Mean action noise std: 2.56
          Mean value_function loss: 98.7578
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.1528
                       Mean reward: 877.02
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 0.8540
     Episode_Reward/lifting_object: 179.8734
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.05s
                      Time elapsed: 00:56:23
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 48108 steps/s (collection: 1.946s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 102.1042
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.1651
                       Mean reward: 893.58
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.8376
     Episode_Reward/lifting_object: 176.4775
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.04s
                      Time elapsed: 00:56:25
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 48362 steps/s (collection: 1.932s, learning 0.100s)
             Mean action noise std: 2.56
          Mean value_function loss: 103.5019
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.1769
                       Mean reward: 867.90
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 0.8390
     Episode_Reward/lifting_object: 176.0970
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.03s
                      Time elapsed: 00:56:27
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 47540 steps/s (collection: 1.955s, learning 0.113s)
             Mean action noise std: 2.56
          Mean value_function loss: 120.4419
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.1806
                       Mean reward: 874.61
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 0.8112
     Episode_Reward/lifting_object: 170.4371
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.07s
                      Time elapsed: 00:56:29
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 47534 steps/s (collection: 1.954s, learning 0.114s)
             Mean action noise std: 2.56
          Mean value_function loss: 120.2132
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.1832
                       Mean reward: 893.13
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 0.8400
     Episode_Reward/lifting_object: 177.0201
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.07s
                      Time elapsed: 00:56:31
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 46897 steps/s (collection: 1.983s, learning 0.114s)
             Mean action noise std: 2.56
          Mean value_function loss: 79.9678
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.1910
                       Mean reward: 906.10
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 179.9271
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.10s
                      Time elapsed: 00:56:34
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 46749 steps/s (collection: 1.995s, learning 0.107s)
             Mean action noise std: 2.56
          Mean value_function loss: 101.8619
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.2019
                       Mean reward: 906.54
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.8394
     Episode_Reward/lifting_object: 176.7000
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.10s
                      Time elapsed: 00:56:36
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 46717 steps/s (collection: 1.997s, learning 0.108s)
             Mean action noise std: 2.57
          Mean value_function loss: 123.0491
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.2097
                       Mean reward: 868.17
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.8222
     Episode_Reward/lifting_object: 173.2645
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.10s
                      Time elapsed: 00:56:38
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 47065 steps/s (collection: 1.977s, learning 0.112s)
             Mean action noise std: 2.57
          Mean value_function loss: 103.2271
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.2178
                       Mean reward: 923.85
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.8372
     Episode_Reward/lifting_object: 176.3805
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.09s
                      Time elapsed: 00:56:40
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 47757 steps/s (collection: 1.959s, learning 0.100s)
             Mean action noise std: 2.57
          Mean value_function loss: 94.8936
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.2247
                       Mean reward: 898.83
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.8429
     Episode_Reward/lifting_object: 177.5364
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.06s
                      Time elapsed: 00:56:42
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 46492 steps/s (collection: 2.011s, learning 0.104s)
             Mean action noise std: 2.57
          Mean value_function loss: 104.2001
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.2318
                       Mean reward: 875.83
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 0.8331
     Episode_Reward/lifting_object: 175.4104
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.11s
                      Time elapsed: 00:56:44
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 47419 steps/s (collection: 1.966s, learning 0.108s)
             Mean action noise std: 2.57
          Mean value_function loss: 97.3147
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.2418
                       Mean reward: 884.72
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 176.9111
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.07s
                      Time elapsed: 00:56:46
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 40779 steps/s (collection: 2.277s, learning 0.134s)
             Mean action noise std: 2.57
          Mean value_function loss: 89.3003
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.2501
                       Mean reward: 889.95
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 0.8543
     Episode_Reward/lifting_object: 179.8595
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.41s
                      Time elapsed: 00:56:48
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 43899 steps/s (collection: 2.113s, learning 0.126s)
             Mean action noise std: 2.57
          Mean value_function loss: 101.0727
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.2573
                       Mean reward: 883.76
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 0.8374
     Episode_Reward/lifting_object: 176.3668
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.24s
                      Time elapsed: 00:56:51
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 45968 steps/s (collection: 2.032s, learning 0.106s)
             Mean action noise std: 2.57
          Mean value_function loss: 74.5135
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.2664
                       Mean reward: 890.75
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.8502
     Episode_Reward/lifting_object: 179.3332
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.14s
                      Time elapsed: 00:56:53
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 42652 steps/s (collection: 2.135s, learning 0.170s)
             Mean action noise std: 2.58
          Mean value_function loss: 105.4576
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.2757
                       Mean reward: 839.28
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 0.8241
     Episode_Reward/lifting_object: 173.4067
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.30s
                      Time elapsed: 00:56:55
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 44309 steps/s (collection: 2.087s, learning 0.132s)
             Mean action noise std: 2.58
          Mean value_function loss: 102.7160
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.2862
                       Mean reward: 849.49
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 0.8315
     Episode_Reward/lifting_object: 174.7910
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.22s
                      Time elapsed: 00:56:57
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 46371 steps/s (collection: 1.993s, learning 0.127s)
             Mean action noise std: 2.58
          Mean value_function loss: 65.3977
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.2948
                       Mean reward: 917.30
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.8566
     Episode_Reward/lifting_object: 180.2957
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.12s
                      Time elapsed: 00:57:00
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 46523 steps/s (collection: 1.997s, learning 0.116s)
             Mean action noise std: 2.58
          Mean value_function loss: 86.5920
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.2988
                       Mean reward: 874.11
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 179.5198
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.11s
                      Time elapsed: 00:57:02
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 44491 steps/s (collection: 2.072s, learning 0.138s)
             Mean action noise std: 2.58
          Mean value_function loss: 105.9968
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.3051
                       Mean reward: 874.84
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 176.4413
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.21s
                      Time elapsed: 00:57:04
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 47306 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 114.8906
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3104
                       Mean reward: 850.10
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: 172.8966
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.08s
                      Time elapsed: 00:57:06
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 43987 steps/s (collection: 2.083s, learning 0.152s)
             Mean action noise std: 2.58
          Mean value_function loss: 100.5546
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.3163
                       Mean reward: 851.12
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 0.8342
     Episode_Reward/lifting_object: 175.9628
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.23s
                      Time elapsed: 00:57:08
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 42056 steps/s (collection: 2.107s, learning 0.230s)
             Mean action noise std: 2.58
          Mean value_function loss: 91.9560
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.3268
                       Mean reward: 908.18
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 0.8297
     Episode_Reward/lifting_object: 174.6129
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.34s
                      Time elapsed: 00:57:10
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 43695 steps/s (collection: 2.149s, learning 0.101s)
             Mean action noise std: 2.58
          Mean value_function loss: 72.1990
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.3346
                       Mean reward: 928.29
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.8585
     Episode_Reward/lifting_object: 181.0869
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.25s
                      Time elapsed: 00:57:13
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 46719 steps/s (collection: 1.994s, learning 0.110s)
             Mean action noise std: 2.58
          Mean value_function loss: 117.3531
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.3369
                       Mean reward: 887.68
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.8201
     Episode_Reward/lifting_object: 172.7607
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.10s
                      Time elapsed: 00:57:15
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 44521 steps/s (collection: 2.053s, learning 0.155s)
             Mean action noise std: 2.58
          Mean value_function loss: 86.6159
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.3412
                       Mean reward: 911.11
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.8467
     Episode_Reward/lifting_object: 178.3827
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.21s
                      Time elapsed: 00:57:17
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 46718 steps/s (collection: 2.008s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 113.2767
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.3488
                       Mean reward: 880.27
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.8243
     Episode_Reward/lifting_object: 173.0850
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.10s
                      Time elapsed: 00:57:19
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 47441 steps/s (collection: 1.966s, learning 0.107s)
             Mean action noise std: 2.59
          Mean value_function loss: 93.2110
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.3579
                       Mean reward: 907.09
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.8411
     Episode_Reward/lifting_object: 177.0697
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.07s
                      Time elapsed: 00:57:21
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 46597 steps/s (collection: 1.982s, learning 0.128s)
             Mean action noise std: 2.59
          Mean value_function loss: 104.9834
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.3662
                       Mean reward: 899.94
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 0.8378
     Episode_Reward/lifting_object: 176.1900
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.11s
                      Time elapsed: 00:57:23
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 45948 steps/s (collection: 2.000s, learning 0.139s)
             Mean action noise std: 2.59
          Mean value_function loss: 100.4074
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.3769
                       Mean reward: 873.33
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 0.8354
     Episode_Reward/lifting_object: 176.1504
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.14s
                      Time elapsed: 00:57:25
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 46404 steps/s (collection: 2.015s, learning 0.104s)
             Mean action noise std: 2.59
          Mean value_function loss: 97.3341
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3857
                       Mean reward: 879.82
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.8249
     Episode_Reward/lifting_object: 173.4909
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.12s
                      Time elapsed: 00:57:28
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 46044 steps/s (collection: 2.029s, learning 0.106s)
             Mean action noise std: 2.59
          Mean value_function loss: 109.8860
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.3920
                       Mean reward: 891.03
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 0.8330
     Episode_Reward/lifting_object: 175.5216
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.13s
                      Time elapsed: 00:57:30
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 42644 steps/s (collection: 2.136s, learning 0.170s)
             Mean action noise std: 2.59
          Mean value_function loss: 91.0700
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4000
                       Mean reward: 906.66
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 178.1696
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.31s
                      Time elapsed: 00:57:32
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 43329 steps/s (collection: 2.130s, learning 0.139s)
             Mean action noise std: 2.59
          Mean value_function loss: 104.1165
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.4056
                       Mean reward: 882.30
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 176.2467
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.27s
                      Time elapsed: 00:57:34
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 45148 steps/s (collection: 2.014s, learning 0.164s)
             Mean action noise std: 2.59
          Mean value_function loss: 95.6495
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4122
                       Mean reward: 879.42
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 176.8771
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.18s
                      Time elapsed: 00:57:36
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 45004 steps/s (collection: 2.079s, learning 0.106s)
             Mean action noise std: 2.60
          Mean value_function loss: 105.0472
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 41.4195
                       Mean reward: 878.78
               Mean episode length: 235.25
    Episode_Reward/reaching_object: 0.8401
     Episode_Reward/lifting_object: 176.7087
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.18s
                      Time elapsed: 00:57:39
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 47219 steps/s (collection: 1.988s, learning 0.094s)
             Mean action noise std: 2.60
          Mean value_function loss: 89.8926
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4241
                       Mean reward: 883.37
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.8324
     Episode_Reward/lifting_object: 175.3104
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.08s
                      Time elapsed: 00:57:41
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 43656 steps/s (collection: 2.106s, learning 0.146s)
             Mean action noise std: 2.60
          Mean value_function loss: 86.2182
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.4345
                       Mean reward: 917.52
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: 178.4771
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.25s
                      Time elapsed: 00:57:43
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 45163 steps/s (collection: 2.015s, learning 0.162s)
             Mean action noise std: 2.60
          Mean value_function loss: 87.8230
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.4481
                       Mean reward: 883.62
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.8493
     Episode_Reward/lifting_object: 179.2116
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.18s
                      Time elapsed: 00:57:45
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 47181 steps/s (collection: 1.985s, learning 0.099s)
             Mean action noise std: 2.60
          Mean value_function loss: 79.7627
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.4619
                       Mean reward: 912.40
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.8557
     Episode_Reward/lifting_object: 180.6154
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.08s
                      Time elapsed: 00:57:47
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 45192 steps/s (collection: 2.034s, learning 0.141s)
             Mean action noise std: 2.60
          Mean value_function loss: 96.1461
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.4720
                       Mean reward: 894.52
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 0.8458
     Episode_Reward/lifting_object: 178.2942
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.18s
                      Time elapsed: 00:57:49
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 45610 steps/s (collection: 2.040s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 103.1809
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.4823
                       Mean reward: 901.08
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.8252
     Episode_Reward/lifting_object: 173.5722
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.16s
                      Time elapsed: 00:57:52
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 46700 steps/s (collection: 2.007s, learning 0.098s)
             Mean action noise std: 2.61
          Mean value_function loss: 93.4118
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.4896
                       Mean reward: 888.57
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 178.7312
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.10s
                      Time elapsed: 00:57:54
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 48163 steps/s (collection: 1.949s, learning 0.092s)
             Mean action noise std: 2.61
          Mean value_function loss: 94.2010
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.4975
                       Mean reward: 903.76
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 178.7344
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.04s
                      Time elapsed: 00:57:56
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 46287 steps/s (collection: 2.032s, learning 0.092s)
             Mean action noise std: 2.61
          Mean value_function loss: 117.9494
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.5049
                       Mean reward: 866.43
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 0.8357
     Episode_Reward/lifting_object: 176.2567
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.12s
                      Time elapsed: 00:57:58
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 46006 steps/s (collection: 2.020s, learning 0.116s)
             Mean action noise std: 2.61
          Mean value_function loss: 81.3476
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.5150
                       Mean reward: 914.92
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 175.4123
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.14s
                      Time elapsed: 00:58:00
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 46284 steps/s (collection: 2.008s, learning 0.116s)
             Mean action noise std: 2.61
          Mean value_function loss: 101.4979
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5245
                       Mean reward: 899.06
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.8356
     Episode_Reward/lifting_object: 175.5222
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.12s
                      Time elapsed: 00:58:02
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 46697 steps/s (collection: 2.011s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 91.0949
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5309
                       Mean reward: 914.92
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 180.4510
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.11s
                      Time elapsed: 00:58:04
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 46610 steps/s (collection: 1.997s, learning 0.112s)
             Mean action noise std: 2.61
          Mean value_function loss: 115.5998
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5371
                       Mean reward: 912.87
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 0.8285
     Episode_Reward/lifting_object: 174.3381
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.11s
                      Time elapsed: 00:58:06
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 43514 steps/s (collection: 2.086s, learning 0.173s)
             Mean action noise std: 2.61
          Mean value_function loss: 78.1727
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.5465
                       Mean reward: 911.69
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 177.5229
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.26s
                      Time elapsed: 00:58:09
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 45618 steps/s (collection: 2.064s, learning 0.091s)
             Mean action noise std: 2.61
          Mean value_function loss: 111.3346
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.5539
                       Mean reward: 895.49
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 0.8303
     Episode_Reward/lifting_object: 174.4615
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.15s
                      Time elapsed: 00:58:11
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 45904 steps/s (collection: 2.050s, learning 0.092s)
             Mean action noise std: 2.62
          Mean value_function loss: 96.2722
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.5620
                       Mean reward: 878.69
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 0.8468
     Episode_Reward/lifting_object: 178.3702
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.14s
                      Time elapsed: 00:58:13
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 47096 steps/s (collection: 1.995s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 135.1378
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.5713
                       Mean reward: 904.15
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.8312
     Episode_Reward/lifting_object: 175.2670
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.09s
                      Time elapsed: 00:58:15
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 46992 steps/s (collection: 1.991s, learning 0.101s)
             Mean action noise std: 2.62
          Mean value_function loss: 66.2941
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5735
                       Mean reward: 885.20
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 0.8449
     Episode_Reward/lifting_object: 178.0414
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.09s
                      Time elapsed: 00:58:17
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 47473 steps/s (collection: 1.977s, learning 0.094s)
             Mean action noise std: 2.62
          Mean value_function loss: 79.2076
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5774
                       Mean reward: 920.67
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.8509
     Episode_Reward/lifting_object: 179.2853
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.07s
                      Time elapsed: 00:58:19
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 26535 steps/s (collection: 3.599s, learning 0.105s)
             Mean action noise std: 2.62
          Mean value_function loss: 94.1133
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5847
                       Mean reward: 898.37
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 179.8458
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.70s
                      Time elapsed: 00:58:23
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13936 steps/s (collection: 6.944s, learning 0.110s)
             Mean action noise std: 2.62
          Mean value_function loss: 106.2652
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.5954
                       Mean reward: 879.41
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 0.8220
     Episode_Reward/lifting_object: 173.3143
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.05s
                      Time elapsed: 00:58:30
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13973 steps/s (collection: 6.894s, learning 0.141s)
             Mean action noise std: 2.62
          Mean value_function loss: 124.6936
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.6002
                       Mean reward: 870.68
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 0.8132
     Episode_Reward/lifting_object: 171.4242
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.04s
                      Time elapsed: 00:58:37
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14117 steps/s (collection: 6.849s, learning 0.114s)
             Mean action noise std: 2.62
          Mean value_function loss: 100.3380
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.6051
                       Mean reward: 883.04
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.8362
     Episode_Reward/lifting_object: 176.1299
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.96s
                      Time elapsed: 00:58:44
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14093 steps/s (collection: 6.830s, learning 0.145s)
             Mean action noise std: 2.62
          Mean value_function loss: 115.4684
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.6131
                       Mean reward: 901.38
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.8318
     Episode_Reward/lifting_object: 176.0226
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.97s
                      Time elapsed: 00:58:51
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14355 steps/s (collection: 6.695s, learning 0.153s)
             Mean action noise std: 2.62
          Mean value_function loss: 96.3169
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.6218
                       Mean reward: 889.28
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 0.8235
     Episode_Reward/lifting_object: 173.4212
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.85s
                      Time elapsed: 00:58:58
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14069 steps/s (collection: 6.861s, learning 0.126s)
             Mean action noise std: 2.63
          Mean value_function loss: 92.7672
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.6317
                       Mean reward: 887.93
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 0.8291
     Episode_Reward/lifting_object: 175.2800
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.99s
                      Time elapsed: 00:59:05
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13857 steps/s (collection: 6.959s, learning 0.135s)
             Mean action noise std: 2.63
          Mean value_function loss: 81.8550
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.6405
                       Mean reward: 936.82
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 180.5781
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.09s
                      Time elapsed: 00:59:12
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14251 steps/s (collection: 6.759s, learning 0.139s)
             Mean action noise std: 2.63
          Mean value_function loss: 127.9640
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.6492
                       Mean reward: 825.74
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 0.8295
     Episode_Reward/lifting_object: 175.7249
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.90s
                      Time elapsed: 00:59:19
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 21599 steps/s (collection: 4.448s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 109.9598
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.6573
                       Mean reward: 912.06
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 0.8317
     Episode_Reward/lifting_object: 175.8731
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.55s
                      Time elapsed: 00:59:23
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 49534 steps/s (collection: 1.891s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 85.1496
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.6646
                       Mean reward: 882.74
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 178.1345
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.98s
                      Time elapsed: 00:59:25
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 50609 steps/s (collection: 1.850s, learning 0.092s)
             Mean action noise std: 2.63
          Mean value_function loss: 98.0720
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.6723
                       Mean reward: 897.93
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 0.8443
     Episode_Reward/lifting_object: 179.0031
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.94s
                      Time elapsed: 00:59:27
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 50847 steps/s (collection: 1.822s, learning 0.112s)
             Mean action noise std: 2.63
          Mean value_function loss: 91.6339
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.6781
                       Mean reward: 909.22
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: 178.9150
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 1.93s
                      Time elapsed: 00:59:29
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 49780 steps/s (collection: 1.878s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 103.5627
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.6827
                       Mean reward: 857.57
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 0.8329
     Episode_Reward/lifting_object: 176.3770
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 1.97s
                      Time elapsed: 00:59:31
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 48493 steps/s (collection: 1.882s, learning 0.145s)
             Mean action noise std: 2.63
          Mean value_function loss: 135.0287
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.6854
                       Mean reward: 869.53
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 176.6177
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.03s
                      Time elapsed: 00:59:33
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 49665 steps/s (collection: 1.868s, learning 0.111s)
             Mean action noise std: 2.63
          Mean value_function loss: 99.8039
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.6887
                       Mean reward: 887.60
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 177.1820
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.98s
                      Time elapsed: 00:59:35
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 48476 steps/s (collection: 1.916s, learning 0.112s)
             Mean action noise std: 2.64
          Mean value_function loss: 111.3937
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.6928
                       Mean reward: 888.30
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 0.8298
     Episode_Reward/lifting_object: 175.5798
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.03s
                      Time elapsed: 00:59:37
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 50649 steps/s (collection: 1.849s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 80.5370
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.6968
                       Mean reward: 879.38
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.8468
     Episode_Reward/lifting_object: 179.2911
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 1.94s
                      Time elapsed: 00:59:39
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 49588 steps/s (collection: 1.858s, learning 0.125s)
             Mean action noise std: 2.64
          Mean value_function loss: 105.7815
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.7038
                       Mean reward: 881.93
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.8463
     Episode_Reward/lifting_object: 178.7608
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.98s
                      Time elapsed: 00:59:41
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 49806 steps/s (collection: 1.848s, learning 0.126s)
             Mean action noise std: 2.64
          Mean value_function loss: 112.1466
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.7108
                       Mean reward: 866.80
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 0.8117
     Episode_Reward/lifting_object: 171.1470
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 1.97s
                      Time elapsed: 00:59:43
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 49937 steps/s (collection: 1.863s, learning 0.105s)
             Mean action noise std: 2.64
          Mean value_function loss: 91.1273
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.7222
                       Mean reward: 894.06
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 176.7464
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 1.97s
                      Time elapsed: 00:59:45
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 51884 steps/s (collection: 1.805s, learning 0.090s)
             Mean action noise std: 2.64
          Mean value_function loss: 71.6378
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.7368
                       Mean reward: 924.76
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.8535
     Episode_Reward/lifting_object: 179.5110
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.89s
                      Time elapsed: 00:59:47
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 50119 steps/s (collection: 1.841s, learning 0.121s)
             Mean action noise std: 2.64
          Mean value_function loss: 91.2700
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.7481
                       Mean reward: 922.39
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.8409
     Episode_Reward/lifting_object: 176.7276
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 1.96s
                      Time elapsed: 00:59:49
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 51149 steps/s (collection: 1.815s, learning 0.106s)
             Mean action noise std: 2.64
          Mean value_function loss: 71.4718
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.7513
                       Mean reward: 923.33
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.8628
     Episode_Reward/lifting_object: 181.4845
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.92s
                      Time elapsed: 00:59:51
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 50678 steps/s (collection: 1.845s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 65.9668
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.7564
                       Mean reward: 891.80
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.8616
     Episode_Reward/lifting_object: 180.9230
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 1.94s
                      Time elapsed: 00:59:53
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 49857 steps/s (collection: 1.863s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 50.3103
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.7608
                       Mean reward: 915.63
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.8616
     Episode_Reward/lifting_object: 180.6607
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 1.97s
                      Time elapsed: 00:59:55
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 49472 steps/s (collection: 1.897s, learning 0.091s)
             Mean action noise std: 2.65
          Mean value_function loss: 70.3642
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.7660
                       Mean reward: 896.27
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 178.3687
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 1.99s
                      Time elapsed: 00:59:57
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 49902 steps/s (collection: 1.886s, learning 0.084s)
             Mean action noise std: 2.65
          Mean value_function loss: 67.0171
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.7749
                       Mean reward: 903.72
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 0.8555
     Episode_Reward/lifting_object: 178.4690
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.97s
                      Time elapsed: 00:59:59
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 50766 steps/s (collection: 1.840s, learning 0.097s)
             Mean action noise std: 2.65
          Mean value_function loss: 52.6481
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.7803
                       Mean reward: 908.57
               Mean episode length: 241.24
    Episode_Reward/reaching_object: 0.8700
     Episode_Reward/lifting_object: 181.7787
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.94s
                      Time elapsed: 01:00:01
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 50392 steps/s (collection: 1.861s, learning 0.090s)
             Mean action noise std: 2.65
          Mean value_function loss: 88.4417
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.7883
                       Mean reward: 895.98
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.8645
     Episode_Reward/lifting_object: 180.5604
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.95s
                      Time elapsed: 01:00:03
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 46624 steps/s (collection: 1.982s, learning 0.126s)
             Mean action noise std: 2.65
          Mean value_function loss: 57.1057
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.7952
                       Mean reward: 904.27
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 0.8698
     Episode_Reward/lifting_object: 181.4010
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.11s
                      Time elapsed: 01:00:05
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 49435 steps/s (collection: 1.885s, learning 0.104s)
             Mean action noise std: 2.65
          Mean value_function loss: 73.0344
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.8018
                       Mean reward: 904.30
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.8581
     Episode_Reward/lifting_object: 178.5100
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.99s
                      Time elapsed: 01:00:07
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 47211 steps/s (collection: 1.964s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 90.6774
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 41.8085
                       Mean reward: 872.51
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 0.8469
     Episode_Reward/lifting_object: 176.2003
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.08s
                      Time elapsed: 01:00:09
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 47444 steps/s (collection: 1.956s, learning 0.116s)
             Mean action noise std: 2.65
          Mean value_function loss: 65.8966
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.8102
                       Mean reward: 896.27
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.8622
     Episode_Reward/lifting_object: 179.3165
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.07s
                      Time elapsed: 01:00:11
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 48364 steps/s (collection: 1.913s, learning 0.120s)
             Mean action noise std: 2.65
          Mean value_function loss: 93.0935
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.8132
                       Mean reward: 884.74
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 176.2349
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.03s
                      Time elapsed: 01:00:13
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 47104 steps/s (collection: 1.979s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 86.9300
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.8176
                       Mean reward: 850.35
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 177.9037
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.09s
                      Time elapsed: 01:00:15
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 48551 steps/s (collection: 1.914s, learning 0.111s)
             Mean action noise std: 2.65
          Mean value_function loss: 70.7317
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.8227
                       Mean reward: 902.09
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.8553
     Episode_Reward/lifting_object: 177.6953
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.02s
                      Time elapsed: 01:00:17
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 46150 steps/s (collection: 1.946s, learning 0.185s)
             Mean action noise std: 2.66
          Mean value_function loss: 95.2327
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.8296
                       Mean reward: 850.16
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 0.8472
     Episode_Reward/lifting_object: 175.7061
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.13s
                      Time elapsed: 01:00:19
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 48549 steps/s (collection: 1.887s, learning 0.138s)
             Mean action noise std: 2.66
          Mean value_function loss: 61.8481
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.8360
                       Mean reward: 907.92
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 0.8672
     Episode_Reward/lifting_object: 180.5635
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.02s
                      Time elapsed: 01:00:21
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 47515 steps/s (collection: 1.956s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 92.4342
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 41.8451
                       Mean reward: 911.33
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 176.1955
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.07s
                      Time elapsed: 01:00:23
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 49072 steps/s (collection: 1.890s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 77.5513
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.8544
                       Mean reward: 911.21
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 0.8661
     Episode_Reward/lifting_object: 179.8922
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.00s
                      Time elapsed: 01:00:25
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 50260 steps/s (collection: 1.826s, learning 0.130s)
             Mean action noise std: 2.66
          Mean value_function loss: 78.3362
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.8647
                       Mean reward: 898.98
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 0.8627
     Episode_Reward/lifting_object: 179.6605
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.96s
                      Time elapsed: 01:00:27
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 50398 steps/s (collection: 1.865s, learning 0.086s)
             Mean action noise std: 2.66
          Mean value_function loss: 69.5607
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.8726
                       Mean reward: 915.90
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.8666
     Episode_Reward/lifting_object: 180.8362
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.95s
                      Time elapsed: 01:00:29
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 48999 steps/s (collection: 1.838s, learning 0.168s)
             Mean action noise std: 2.66
          Mean value_function loss: 66.7664
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.8820
                       Mean reward: 903.29
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 0.8598
     Episode_Reward/lifting_object: 179.2984
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.01s
                      Time elapsed: 01:00:31
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 46801 steps/s (collection: 1.967s, learning 0.133s)
             Mean action noise std: 2.67
          Mean value_function loss: 101.2462
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.8945
                       Mean reward: 880.46
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 0.8428
     Episode_Reward/lifting_object: 175.5363
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.10s
                      Time elapsed: 01:00:33
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 48788 steps/s (collection: 1.883s, learning 0.132s)
             Mean action noise std: 2.67
          Mean value_function loss: 96.2622
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.9078
                       Mean reward: 870.66
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.8452
     Episode_Reward/lifting_object: 175.9095
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.01s
                      Time elapsed: 01:00:35
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 49953 steps/s (collection: 1.876s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 113.0699
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.9188
                       Mean reward: 851.91
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 0.8413
     Episode_Reward/lifting_object: 175.5813
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.97s
                      Time elapsed: 01:00:37
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 49602 steps/s (collection: 1.875s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 94.8401
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.9243
                       Mean reward: 859.40
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 175.3469
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 1.98s
                      Time elapsed: 01:00:39
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 48319 steps/s (collection: 1.890s, learning 0.145s)
             Mean action noise std: 2.67
          Mean value_function loss: 52.6643
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.9284
                       Mean reward: 914.59
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.8647
     Episode_Reward/lifting_object: 180.6103
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.03s
                      Time elapsed: 01:00:41
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 49107 steps/s (collection: 1.884s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 95.5216
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.9328
                       Mean reward: 900.78
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.8568
     Episode_Reward/lifting_object: 178.9946
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.00s
                      Time elapsed: 01:00:43
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 49942 steps/s (collection: 1.874s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 72.6238
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.9398
                       Mean reward: 904.79
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.8596
     Episode_Reward/lifting_object: 179.9233
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.97s
                      Time elapsed: 01:00:45
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 48740 steps/s (collection: 1.887s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 99.8248
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.9493
                       Mean reward: 847.94
               Mean episode length: 226.00
    Episode_Reward/reaching_object: 0.8381
     Episode_Reward/lifting_object: 174.9783
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.02s
                      Time elapsed: 01:00:47
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 49816 steps/s (collection: 1.871s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 93.4801
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.9537
                       Mean reward: 873.96
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 176.4688
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.97s
                      Time elapsed: 01:00:49
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 48716 steps/s (collection: 1.894s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 83.8368
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.9585
                       Mean reward: 904.93
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 176.6796
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.02s
                      Time elapsed: 01:00:51
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 49081 steps/s (collection: 1.914s, learning 0.089s)
             Mean action noise std: 2.68
          Mean value_function loss: 110.7157
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 41.9687
                       Mean reward: 869.49
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.8366
     Episode_Reward/lifting_object: 174.1265
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.00s
                      Time elapsed: 01:00:53
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 48166 steps/s (collection: 1.911s, learning 0.130s)
             Mean action noise std: 2.68
          Mean value_function loss: 96.9747
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.9791
                       Mean reward: 860.30
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 0.8363
     Episode_Reward/lifting_object: 174.0830
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.04s
                      Time elapsed: 01:00:55
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 49223 steps/s (collection: 1.898s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 113.5201
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.9852
                       Mean reward: 894.05
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 176.0878
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.00s
                      Time elapsed: 01:00:57
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 47560 steps/s (collection: 1.930s, learning 0.137s)
             Mean action noise std: 2.68
          Mean value_function loss: 120.1802
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.9931
                       Mean reward: 875.29
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 0.8351
     Episode_Reward/lifting_object: 173.3659
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.07s
                      Time elapsed: 01:00:59
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 49270 steps/s (collection: 1.898s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 102.6194
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.0048
                       Mean reward: 871.93
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 0.8483
     Episode_Reward/lifting_object: 175.8568
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.00s
                      Time elapsed: 01:01:01
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 48793 steps/s (collection: 1.878s, learning 0.137s)
             Mean action noise std: 2.68
          Mean value_function loss: 87.8807
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.0139
                       Mean reward: 894.85
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.8431
     Episode_Reward/lifting_object: 174.9537
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.01s
                      Time elapsed: 01:01:03
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 49031 steps/s (collection: 1.895s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 82.3524
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.0197
                       Mean reward: 914.69
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 0.8624
     Episode_Reward/lifting_object: 179.2809
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.00s
                      Time elapsed: 01:01:05
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 49633 steps/s (collection: 1.876s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 96.0428
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.0245
                       Mean reward: 878.37
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 0.8487
     Episode_Reward/lifting_object: 176.2960
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 1.98s
                      Time elapsed: 01:01:07
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 48969 steps/s (collection: 1.876s, learning 0.132s)
             Mean action noise std: 2.69
          Mean value_function loss: 72.0968
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.0304
                       Mean reward: 877.93
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 0.8608
     Episode_Reward/lifting_object: 178.7955
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.01s
                      Time elapsed: 01:01:09
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 49118 steps/s (collection: 1.872s, learning 0.129s)
             Mean action noise std: 2.69
          Mean value_function loss: 61.3221
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.0388
                       Mean reward: 908.14
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.8617
     Episode_Reward/lifting_object: 179.2811
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.00s
                      Time elapsed: 01:01:11
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 50008 steps/s (collection: 1.857s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 74.0709
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.0470
                       Mean reward: 904.95
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.8488
     Episode_Reward/lifting_object: 176.3864
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 1.97s
                      Time elapsed: 01:01:13
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 49540 steps/s (collection: 1.855s, learning 0.130s)
             Mean action noise std: 2.69
          Mean value_function loss: 102.7184
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.0519
                       Mean reward: 876.00
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 0.8258
     Episode_Reward/lifting_object: 170.7829
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 1.98s
                      Time elapsed: 01:01:15
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 48542 steps/s (collection: 1.890s, learning 0.135s)
             Mean action noise std: 2.69
          Mean value_function loss: 83.0745
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.0605
                       Mean reward: 901.30
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.8550
     Episode_Reward/lifting_object: 178.1299
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.03s
                      Time elapsed: 01:01:17
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 49059 steps/s (collection: 1.871s, learning 0.133s)
             Mean action noise std: 2.69
          Mean value_function loss: 75.7753
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.0708
                       Mean reward: 903.19
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 0.8463
     Episode_Reward/lifting_object: 176.0170
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.00s
                      Time elapsed: 01:01:19
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 48736 steps/s (collection: 1.884s, learning 0.133s)
             Mean action noise std: 2.69
          Mean value_function loss: 73.9120
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.0802
                       Mean reward: 867.92
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 177.8508
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.02s
                      Time elapsed: 01:01:21
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 49377 steps/s (collection: 1.860s, learning 0.131s)
             Mean action noise std: 2.69
          Mean value_function loss: 84.7844
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.0869
                       Mean reward: 872.17
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 176.7297
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 1.99s
                      Time elapsed: 01:01:23
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 49426 steps/s (collection: 1.885s, learning 0.104s)
             Mean action noise std: 2.70
          Mean value_function loss: 78.1759
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.0954
                       Mean reward: 918.81
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.8573
     Episode_Reward/lifting_object: 179.1148
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 1.99s
                      Time elapsed: 01:01:25
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 49137 steps/s (collection: 1.865s, learning 0.136s)
             Mean action noise std: 2.70
          Mean value_function loss: 62.1818
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.1008
                       Mean reward: 915.67
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 0.8600
     Episode_Reward/lifting_object: 179.8872
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.00s
                      Time elapsed: 01:01:27
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 50334 steps/s (collection: 1.865s, learning 0.088s)
             Mean action noise std: 2.70
          Mean value_function loss: 99.8298
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.1039
                       Mean reward: 884.05
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 0.8483
     Episode_Reward/lifting_object: 177.4665
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 1.95s
                      Time elapsed: 01:01:29
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 49292 steps/s (collection: 1.869s, learning 0.125s)
             Mean action noise std: 2.70
          Mean value_function loss: 98.1632
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1064
                       Mean reward: 875.70
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.8494
     Episode_Reward/lifting_object: 177.8860
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 1.99s
                      Time elapsed: 01:01:31
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 49175 steps/s (collection: 1.895s, learning 0.104s)
             Mean action noise std: 2.70
          Mean value_function loss: 94.2068
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1117
                       Mean reward: 893.86
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 0.8272
     Episode_Reward/lifting_object: 172.6825
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.00s
                      Time elapsed: 01:01:33
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 49268 steps/s (collection: 1.895s, learning 0.100s)
             Mean action noise std: 2.70
          Mean value_function loss: 87.2565
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1191
                       Mean reward: 892.74
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.8449
     Episode_Reward/lifting_object: 176.5285
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.00s
                      Time elapsed: 01:01:35
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 48854 steps/s (collection: 1.859s, learning 0.153s)
             Mean action noise std: 2.70
          Mean value_function loss: 96.8369
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.1282
                       Mean reward: 879.40
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 0.8423
     Episode_Reward/lifting_object: 176.5830
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.01s
                      Time elapsed: 01:01:37
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 50137 steps/s (collection: 1.859s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 96.5567
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1353
                       Mean reward: 871.70
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 0.8348
     Episode_Reward/lifting_object: 174.4540
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 1.96s
                      Time elapsed: 01:01:39
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 49292 steps/s (collection: 1.884s, learning 0.110s)
             Mean action noise std: 2.70
          Mean value_function loss: 110.9822
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.1427
                       Mean reward: 843.91
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 0.8293
     Episode_Reward/lifting_object: 173.6961
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 1.99s
                      Time elapsed: 01:01:41
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 49241 steps/s (collection: 1.878s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 78.8803
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1529
                       Mean reward: 907.03
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 0.8451
     Episode_Reward/lifting_object: 177.5378
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.00s
                      Time elapsed: 01:01:43
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 48413 steps/s (collection: 1.893s, learning 0.137s)
             Mean action noise std: 2.71
          Mean value_function loss: 103.0430
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.1629
                       Mean reward: 896.66
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 178.3275
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.03s
                      Time elapsed: 01:01:45
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 48426 steps/s (collection: 1.925s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 98.0233
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.1667
                       Mean reward: 887.95
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 177.7658
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.03s
                      Time elapsed: 01:01:47
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 49122 steps/s (collection: 1.886s, learning 0.115s)
             Mean action noise std: 2.71
          Mean value_function loss: 82.9121
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.1690
                       Mean reward: 880.74
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 0.8484
     Episode_Reward/lifting_object: 177.6204
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.00s
                      Time elapsed: 01:01:49
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 48961 steps/s (collection: 1.885s, learning 0.123s)
             Mean action noise std: 2.71
          Mean value_function loss: 84.4803
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.1738
                       Mean reward: 909.98
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.8554
     Episode_Reward/lifting_object: 179.7529
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.01s
                      Time elapsed: 01:01:51
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 48467 steps/s (collection: 1.906s, learning 0.122s)
             Mean action noise std: 2.71
          Mean value_function loss: 92.4251
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.1805
                       Mean reward: 909.65
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 176.8825
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.03s
                      Time elapsed: 01:01:53
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 49297 steps/s (collection: 1.904s, learning 0.091s)
             Mean action noise std: 2.71
          Mean value_function loss: 97.1602
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.1847
                       Mean reward: 904.78
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.8517
     Episode_Reward/lifting_object: 178.7919
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 1.99s
                      Time elapsed: 01:01:55
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 49715 steps/s (collection: 1.865s, learning 0.112s)
             Mean action noise std: 2.71
          Mean value_function loss: 83.0374
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.1894
                       Mean reward: 909.11
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.8587
     Episode_Reward/lifting_object: 180.4704
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.98s
                      Time elapsed: 01:01:57
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 49495 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 2.71
          Mean value_function loss: 86.6793
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.1936
                       Mean reward: 912.82
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.8508
     Episode_Reward/lifting_object: 178.2454
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 1.99s
                      Time elapsed: 01:01:59
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 49517 steps/s (collection: 1.879s, learning 0.106s)
             Mean action noise std: 2.71
          Mean value_function loss: 77.0813
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.1984
                       Mean reward: 896.34
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 179.5320
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 1.99s
                      Time elapsed: 01:02:01
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 49625 steps/s (collection: 1.877s, learning 0.104s)
             Mean action noise std: 2.71
          Mean value_function loss: 80.0825
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.2096
                       Mean reward: 906.13
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.8540
     Episode_Reward/lifting_object: 179.3739
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 1.98s
                      Time elapsed: 01:02:03
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 49113 steps/s (collection: 1.889s, learning 0.113s)
             Mean action noise std: 2.72
          Mean value_function loss: 97.5450
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.2215
                       Mean reward: 875.51
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 0.8484
     Episode_Reward/lifting_object: 178.2343
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.00s
                      Time elapsed: 01:02:05
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 47953 steps/s (collection: 1.870s, learning 0.180s)
             Mean action noise std: 2.72
          Mean value_function loss: 101.3061
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.2317
                       Mean reward: 877.28
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 0.8626
     Episode_Reward/lifting_object: 181.3446
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.05s
                      Time elapsed: 01:02:07
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 43194 steps/s (collection: 2.121s, learning 0.155s)
             Mean action noise std: 2.72
          Mean value_function loss: 126.9423
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.2378
                       Mean reward: 877.97
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 174.7407
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.28s
                      Time elapsed: 01:02:09
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 44894 steps/s (collection: 2.059s, learning 0.131s)
             Mean action noise std: 2.72
          Mean value_function loss: 111.3018
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 42.2406
                       Mean reward: 872.02
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 0.8344
     Episode_Reward/lifting_object: 174.8085
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.19s
                      Time elapsed: 01:02:12
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 47086 steps/s (collection: 1.993s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 104.6292
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.2435
                       Mean reward: 892.95
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 176.1771
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.09s
                      Time elapsed: 01:02:14
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 49368 steps/s (collection: 1.907s, learning 0.085s)
             Mean action noise std: 2.72
          Mean value_function loss: 83.8303
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 42.2488
                       Mean reward: 907.20
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.8525
     Episode_Reward/lifting_object: 179.1207
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 1.99s
                      Time elapsed: 01:02:16
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 48722 steps/s (collection: 1.918s, learning 0.100s)
             Mean action noise std: 2.72
          Mean value_function loss: 104.7860
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.2576
                       Mean reward: 866.34
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 0.8304
     Episode_Reward/lifting_object: 174.2760
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.02s
                      Time elapsed: 01:02:18
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 49650 steps/s (collection: 1.875s, learning 0.105s)
             Mean action noise std: 2.72
          Mean value_function loss: 109.6861
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.2622
                       Mean reward: 898.23
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 176.4211
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 1.98s
                      Time elapsed: 01:02:20
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 49172 steps/s (collection: 1.907s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 96.5766
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.2650
                       Mean reward: 931.49
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 176.8053
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.00s
                      Time elapsed: 01:02:22
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 49409 steps/s (collection: 1.881s, learning 0.108s)
             Mean action noise std: 2.72
          Mean value_function loss: 94.5478
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.2686
                       Mean reward: 898.70
               Mean episode length: 238.66
    Episode_Reward/reaching_object: 0.8535
     Episode_Reward/lifting_object: 179.8697
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 1.99s
                      Time elapsed: 01:02:24
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 49943 steps/s (collection: 1.854s, learning 0.115s)
             Mean action noise std: 2.72
          Mean value_function loss: 126.7946
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 42.2740
                       Mean reward: 839.20
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 176.4111
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.97s
                      Time elapsed: 01:02:26
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 47629 steps/s (collection: 1.957s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 113.7338
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 42.2762
                       Mean reward: 912.52
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 178.5656
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.06s
                      Time elapsed: 01:02:28
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 49552 steps/s (collection: 1.889s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 114.7285
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 42.2785
                       Mean reward: 860.69
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 0.8330
     Episode_Reward/lifting_object: 175.2070
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 1.98s
                      Time elapsed: 01:02:30
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 47448 steps/s (collection: 1.913s, learning 0.159s)
             Mean action noise std: 2.72
          Mean value_function loss: 99.7117
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 42.2802
                       Mean reward: 881.61
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.8249
     Episode_Reward/lifting_object: 172.9202
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.07s
                      Time elapsed: 01:02:32
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 47941 steps/s (collection: 1.957s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 115.1067
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 42.2812
                       Mean reward: 868.23
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.8352
     Episode_Reward/lifting_object: 175.5342
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.05s
                      Time elapsed: 01:02:34
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 48146 steps/s (collection: 1.949s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 107.5124
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 42.2816
                       Mean reward: 884.79
               Mean episode length: 236.12
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 176.1266
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.04s
                      Time elapsed: 01:02:36
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.889s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 103.7645
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 42.2819
                       Mean reward: 891.57
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: 177.7329
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.98s
                      Time elapsed: 01:02:38
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 49009 steps/s (collection: 1.900s, learning 0.105s)
             Mean action noise std: 2.72
          Mean value_function loss: 73.8175
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 42.2828
                       Mean reward: 908.43
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 0.8617
     Episode_Reward/lifting_object: 180.9143
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.01s
                      Time elapsed: 01:02:40
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 48841 steps/s (collection: 1.919s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 112.9690
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 42.2836
                       Mean reward: 868.05
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.8493
     Episode_Reward/lifting_object: 177.9483
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.01s
                      Time elapsed: 01:02:42
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 48151 steps/s (collection: 1.934s, learning 0.108s)
             Mean action noise std: 2.72
          Mean value_function loss: 87.4216
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 42.2842
                       Mean reward: 917.63
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.8407
     Episode_Reward/lifting_object: 175.6799
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.04s
                      Time elapsed: 01:02:44
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 50033 steps/s (collection: 1.873s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 96.7202
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 42.2853
                       Mean reward: 888.06
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 177.6770
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 1.96s
                      Time elapsed: 01:02:46
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 48594 steps/s (collection: 1.910s, learning 0.113s)
             Mean action noise std: 2.73
          Mean value_function loss: 87.5603
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 42.2871
                       Mean reward: 906.46
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 0.8486
     Episode_Reward/lifting_object: 177.5158
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.02s
                      Time elapsed: 01:02:48
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 49220 steps/s (collection: 1.888s, learning 0.109s)
             Mean action noise std: 2.73
          Mean value_function loss: 86.8923
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.2902
                       Mean reward: 896.51
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.8541
     Episode_Reward/lifting_object: 178.9678
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.00s
                      Time elapsed: 01:02:50
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 49811 steps/s (collection: 1.882s, learning 0.092s)
             Mean action noise std: 2.73
          Mean value_function loss: 103.3848
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 42.2935
                       Mean reward: 874.66
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.8516
     Episode_Reward/lifting_object: 178.0352
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 1.97s
                      Time elapsed: 01:02:52
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 49033 steps/s (collection: 1.913s, learning 0.092s)
             Mean action noise std: 2.73
          Mean value_function loss: 71.8523
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 42.2951
                       Mean reward: 907.11
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.8511
     Episode_Reward/lifting_object: 177.8910
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.00s
                      Time elapsed: 01:02:54
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 49397 steps/s (collection: 1.873s, learning 0.118s)
             Mean action noise std: 2.73
          Mean value_function loss: 89.6618
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.2994
                       Mean reward: 880.40
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 0.8483
     Episode_Reward/lifting_object: 177.4550
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 1.99s
                      Time elapsed: 01:02:56
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 49508 steps/s (collection: 1.895s, learning 0.091s)
             Mean action noise std: 2.73
          Mean value_function loss: 88.3558
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.3085
                       Mean reward: 889.72
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 0.8478
     Episode_Reward/lifting_object: 176.8053
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 1.99s
                      Time elapsed: 01:02:58
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 49504 steps/s (collection: 1.898s, learning 0.088s)
             Mean action noise std: 2.73
          Mean value_function loss: 93.4796
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.3189
                       Mean reward: 862.07
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 0.8433
     Episode_Reward/lifting_object: 176.5657
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 1.99s
                      Time elapsed: 01:03:00
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 49555 steps/s (collection: 1.893s, learning 0.091s)
             Mean action noise std: 2.73
          Mean value_function loss: 91.0673
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.3282
                       Mean reward: 910.84
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.8537
     Episode_Reward/lifting_object: 178.5262
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 1.98s
                      Time elapsed: 01:03:02
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 48027 steps/s (collection: 1.933s, learning 0.114s)
             Mean action noise std: 2.73
          Mean value_function loss: 80.7183
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.3343
                       Mean reward: 907.10
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.8554
     Episode_Reward/lifting_object: 179.0208
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.05s
                      Time elapsed: 01:03:04
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 49949 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 2.73
          Mean value_function loss: 84.7873
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.3430
                       Mean reward: 921.03
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 0.8643
     Episode_Reward/lifting_object: 181.3402
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 1.97s
                      Time elapsed: 01:03:06
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 49610 steps/s (collection: 1.873s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 80.6192
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.3499
                       Mean reward: 871.43
               Mean episode length: 232.41
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 174.8082
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 1.98s
                      Time elapsed: 01:03:08
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 49370 steps/s (collection: 1.888s, learning 0.104s)
             Mean action noise std: 2.74
          Mean value_function loss: 84.6215
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.3579
                       Mean reward: 930.49
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.8550
     Episode_Reward/lifting_object: 179.1921
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 1.99s
                      Time elapsed: 01:03:10
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 49485 steps/s (collection: 1.887s, learning 0.100s)
             Mean action noise std: 2.74
          Mean value_function loss: 88.2895
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.3650
                       Mean reward: 909.89
               Mean episode length: 240.39
    Episode_Reward/reaching_object: 0.8653
     Episode_Reward/lifting_object: 181.8468
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 1.99s
                      Time elapsed: 01:03:12
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 48784 steps/s (collection: 1.898s, learning 0.118s)
             Mean action noise std: 2.74
          Mean value_function loss: 119.7144
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.3715
                       Mean reward: 887.17
               Mean episode length: 236.18
    Episode_Reward/reaching_object: 0.8413
     Episode_Reward/lifting_object: 176.2222
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.02s
                      Time elapsed: 01:03:14
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 48732 steps/s (collection: 1.919s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 97.6608
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.3780
                       Mean reward: 904.60
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 175.9680
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.02s
                      Time elapsed: 01:03:16
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 50190 steps/s (collection: 1.873s, learning 0.085s)
             Mean action noise std: 2.74
          Mean value_function loss: 89.3521
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.3880
                       Mean reward: 901.06
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.8535
     Episode_Reward/lifting_object: 179.5386
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 1.96s
                      Time elapsed: 01:03:18
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 49742 steps/s (collection: 1.868s, learning 0.108s)
             Mean action noise std: 2.74
          Mean value_function loss: 81.5934
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.4002
                       Mean reward: 913.32
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 176.5033
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 1.98s
                      Time elapsed: 01:03:20
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 49300 steps/s (collection: 1.869s, learning 0.125s)
             Mean action noise std: 2.74
          Mean value_function loss: 99.7327
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 42.4084
                       Mean reward: 921.55
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.8415
     Episode_Reward/lifting_object: 176.5718
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 1.99s
                      Time elapsed: 01:03:22
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 49751 steps/s (collection: 1.876s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 136.8426
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.4113
                       Mean reward: 893.34
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.8526
     Episode_Reward/lifting_object: 179.4221
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 1.98s
                      Time elapsed: 01:03:24
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 49880 steps/s (collection: 1.882s, learning 0.089s)
             Mean action noise std: 2.75
          Mean value_function loss: 97.2522
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.4155
                       Mean reward: 866.76
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 0.8482
     Episode_Reward/lifting_object: 178.4651
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 1.97s
                      Time elapsed: 01:03:26
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 49831 steps/s (collection: 1.869s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 98.8990
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.4209
                       Mean reward: 876.92
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 176.7512
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 1.97s
                      Time elapsed: 01:03:28
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 49031 steps/s (collection: 1.921s, learning 0.084s)
             Mean action noise std: 2.75
          Mean value_function loss: 88.3943
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.4278
                       Mean reward: 901.78
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.8452
     Episode_Reward/lifting_object: 177.0630
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.00s
                      Time elapsed: 01:03:30
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 47733 steps/s (collection: 1.952s, learning 0.108s)
             Mean action noise std: 2.75
          Mean value_function loss: 83.2144
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.4329
                       Mean reward: 907.28
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.8620
     Episode_Reward/lifting_object: 181.3385
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.06s
                      Time elapsed: 01:03:32
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 49455 steps/s (collection: 1.902s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 129.1643
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.4396
                       Mean reward: 854.63
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 0.8252
     Episode_Reward/lifting_object: 172.9356
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 1.99s
                      Time elapsed: 01:03:34
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 49347 steps/s (collection: 1.896s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 109.1354
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.4489
                       Mean reward: 867.80
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 177.7453
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 1.99s
                      Time elapsed: 01:03:36
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 49503 steps/s (collection: 1.888s, learning 0.098s)
             Mean action noise std: 2.75
          Mean value_function loss: 101.7749
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.4583
                       Mean reward: 914.52
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 177.1244
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 1.99s
                      Time elapsed: 01:03:38
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 50115 steps/s (collection: 1.878s, learning 0.084s)
             Mean action noise std: 2.75
          Mean value_function loss: 70.9529
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.4648
                       Mean reward: 935.66
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.8689
     Episode_Reward/lifting_object: 182.8298
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 1.96s
                      Time elapsed: 01:03:40
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 49270 steps/s (collection: 1.897s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 102.7181
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.4682
                       Mean reward: 888.46
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 0.8428
     Episode_Reward/lifting_object: 176.5325
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.00s
                      Time elapsed: 01:03:42
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 50252 steps/s (collection: 1.859s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 90.3106
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.4729
                       Mean reward: 875.33
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 0.8528
     Episode_Reward/lifting_object: 178.4995
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 1.96s
                      Time elapsed: 01:03:44
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 48554 steps/s (collection: 1.909s, learning 0.116s)
             Mean action noise std: 2.76
          Mean value_function loss: 76.4876
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.4800
                       Mean reward: 900.58
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 0.8568
     Episode_Reward/lifting_object: 179.4989
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.02s
                      Time elapsed: 01:03:46
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 49666 steps/s (collection: 1.889s, learning 0.090s)
             Mean action noise std: 2.76
          Mean value_function loss: 83.9718
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.4889
                       Mean reward: 910.87
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.8496
     Episode_Reward/lifting_object: 177.8393
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 1.98s
                      Time elapsed: 01:03:48
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 49461 steps/s (collection: 1.886s, learning 0.101s)
             Mean action noise std: 2.76
          Mean value_function loss: 60.9946
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.4958
                       Mean reward: 908.47
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 0.8572
     Episode_Reward/lifting_object: 179.1157
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 1.99s
                      Time elapsed: 01:03:50
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 49405 steps/s (collection: 1.846s, learning 0.144s)
             Mean action noise std: 2.76
          Mean value_function loss: 84.1122
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.5039
                       Mean reward: 892.62
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.8412
     Episode_Reward/lifting_object: 175.2287
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 1.99s
                      Time elapsed: 01:03:52
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 49295 steps/s (collection: 1.901s, learning 0.093s)
             Mean action noise std: 2.76
          Mean value_function loss: 72.2705
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.5155
                       Mean reward: 884.13
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 0.8599
     Episode_Reward/lifting_object: 179.7681
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 1.99s
                      Time elapsed: 01:03:54
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 49484 steps/s (collection: 1.893s, learning 0.094s)
             Mean action noise std: 2.76
          Mean value_function loss: 56.5401
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.5290
                       Mean reward: 916.20
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.8668
     Episode_Reward/lifting_object: 181.2475
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 1.99s
                      Time elapsed: 01:03:56
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 49820 steps/s (collection: 1.857s, learning 0.116s)
             Mean action noise std: 2.77
          Mean value_function loss: 75.6535
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.5375
                       Mean reward: 901.81
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 180.8976
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 1.97s
                      Time elapsed: 01:03:58
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 49291 steps/s (collection: 1.854s, learning 0.141s)
             Mean action noise std: 2.77
          Mean value_function loss: 82.8253
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.5436
                       Mean reward: 917.54
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 177.7595
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 1.99s
                      Time elapsed: 01:04:00
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 48760 steps/s (collection: 1.876s, learning 0.140s)
             Mean action noise std: 2.77
          Mean value_function loss: 84.7483
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.5493
                       Mean reward: 897.77
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 0.8559
     Episode_Reward/lifting_object: 178.7958
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.02s
                      Time elapsed: 01:04:02
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 50170 steps/s (collection: 1.851s, learning 0.108s)
             Mean action noise std: 2.77
          Mean value_function loss: 81.5175
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.5518
                       Mean reward: 878.64
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 0.8467
     Episode_Reward/lifting_object: 176.5147
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 1.96s
                      Time elapsed: 01:04:04
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 48626 steps/s (collection: 1.905s, learning 0.117s)
             Mean action noise std: 2.77
          Mean value_function loss: 77.9528
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.5552
                       Mean reward: 878.27
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 0.8529
     Episode_Reward/lifting_object: 178.1214
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.02s
                      Time elapsed: 01:04:06
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 47982 steps/s (collection: 1.902s, learning 0.147s)
             Mean action noise std: 2.77
          Mean value_function loss: 96.9635
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 42.5600
                       Mean reward: 878.75
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: 177.9693
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.05s
                      Time elapsed: 01:04:08
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 49626 steps/s (collection: 1.875s, learning 0.106s)
             Mean action noise std: 2.77
          Mean value_function loss: 66.7911
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.5632
                       Mean reward: 892.78
               Mean episode length: 236.95
    Episode_Reward/reaching_object: 0.8628
     Episode_Reward/lifting_object: 180.2111
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 1.98s
                      Time elapsed: 01:04:10
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 50469 steps/s (collection: 1.850s, learning 0.098s)
             Mean action noise std: 2.77
          Mean value_function loss: 81.1580
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.5671
                       Mean reward: 885.80
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 178.1570
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 1.95s
                      Time elapsed: 01:04:12
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 49723 steps/s (collection: 1.875s, learning 0.102s)
             Mean action noise std: 2.77
          Mean value_function loss: 77.8987
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.5729
                       Mean reward: 912.50
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.8656
     Episode_Reward/lifting_object: 181.1361
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 1.98s
                      Time elapsed: 01:04:14
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 50087 steps/s (collection: 1.874s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 67.2911
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.5825
                       Mean reward: 869.38
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 0.8467
     Episode_Reward/lifting_object: 176.6055
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 1.96s
                      Time elapsed: 01:04:15
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 49372 steps/s (collection: 1.903s, learning 0.088s)
             Mean action noise std: 2.77
          Mean value_function loss: 87.1387
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.5892
                       Mean reward: 914.40
               Mean episode length: 242.60
    Episode_Reward/reaching_object: 0.8686
     Episode_Reward/lifting_object: 181.4789
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 1.99s
                      Time elapsed: 01:04:17
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 49698 steps/s (collection: 1.881s, learning 0.097s)
             Mean action noise std: 2.77
          Mean value_function loss: 76.8672
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.5928
                       Mean reward: 912.91
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.8601
     Episode_Reward/lifting_object: 179.7862
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 1.98s
                      Time elapsed: 01:04:19
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 49181 steps/s (collection: 1.890s, learning 0.109s)
             Mean action noise std: 2.77
          Mean value_function loss: 105.8919
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.5966
                       Mean reward: 911.26
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.8538
     Episode_Reward/lifting_object: 177.9928
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.00s
                      Time elapsed: 01:04:21
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.881s, learning 0.094s)
             Mean action noise std: 2.78
          Mean value_function loss: 114.5022
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.6012
                       Mean reward: 850.86
               Mean episode length: 226.19
    Episode_Reward/reaching_object: 0.8423
     Episode_Reward/lifting_object: 176.1337
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 1.97s
                      Time elapsed: 01:04:23
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 49660 steps/s (collection: 1.869s, learning 0.111s)
             Mean action noise std: 2.78
          Mean value_function loss: 104.5419
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.6071
                       Mean reward: 914.02
               Mean episode length: 241.99
    Episode_Reward/reaching_object: 0.8563
     Episode_Reward/lifting_object: 178.7706
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 1.98s
                      Time elapsed: 01:04:25
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 49846 steps/s (collection: 1.880s, learning 0.092s)
             Mean action noise std: 2.78
          Mean value_function loss: 85.6505
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.6135
                       Mean reward: 917.27
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: 179.4722
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 1.97s
                      Time elapsed: 01:04:27
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 50088 steps/s (collection: 1.873s, learning 0.090s)
             Mean action noise std: 2.78
          Mean value_function loss: 90.4524
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.6217
                       Mean reward: 867.65
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 0.8461
     Episode_Reward/lifting_object: 176.6034
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 1.96s
                      Time elapsed: 01:04:29
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 46998 steps/s (collection: 1.940s, learning 0.152s)
             Mean action noise std: 2.78
          Mean value_function loss: 70.2036
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.6286
                       Mean reward: 905.59
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 0.8534
     Episode_Reward/lifting_object: 178.5390
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.09s
                      Time elapsed: 01:04:31
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 48993 steps/s (collection: 1.920s, learning 0.086s)
             Mean action noise std: 2.78
          Mean value_function loss: 78.6155
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.6314
                       Mean reward: 904.81
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.8653
     Episode_Reward/lifting_object: 181.1469
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.01s
                      Time elapsed: 01:04:33
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 49485 steps/s (collection: 1.880s, learning 0.107s)
             Mean action noise std: 2.78
          Mean value_function loss: 95.5953
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.6381
                       Mean reward: 889.99
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 0.8525
     Episode_Reward/lifting_object: 178.1858
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 1.99s
                      Time elapsed: 01:04:35
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 48768 steps/s (collection: 1.910s, learning 0.106s)
             Mean action noise std: 2.78
          Mean value_function loss: 84.7915
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.6494
                       Mean reward: 895.60
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 175.0418
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.02s
                      Time elapsed: 01:04:37
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.888s, learning 0.086s)
             Mean action noise std: 2.79
          Mean value_function loss: 81.8941
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.6574
                       Mean reward: 909.78
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 0.8585
     Episode_Reward/lifting_object: 179.6440
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 1.97s
                      Time elapsed: 01:04:39
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 49332 steps/s (collection: 1.897s, learning 0.096s)
             Mean action noise std: 2.79
          Mean value_function loss: 72.2477
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.6634
                       Mean reward: 926.53
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.8583
     Episode_Reward/lifting_object: 179.6956
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 1.99s
                      Time elapsed: 01:04:41
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 48410 steps/s (collection: 1.921s, learning 0.110s)
             Mean action noise std: 2.79
          Mean value_function loss: 92.4279
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.6687
                       Mean reward: 848.53
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 0.8401
     Episode_Reward/lifting_object: 174.9808
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.03s
                      Time elapsed: 01:04:43
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 49888 steps/s (collection: 1.875s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 108.3948
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.6780
                       Mean reward: 871.66
               Mean episode length: 232.20
    Episode_Reward/reaching_object: 0.8511
     Episode_Reward/lifting_object: 177.8662
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.97s
                      Time elapsed: 01:04:45
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 48995 steps/s (collection: 1.899s, learning 0.107s)
             Mean action noise std: 2.79
          Mean value_function loss: 72.2441
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.6856
                       Mean reward: 903.25
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.8630
     Episode_Reward/lifting_object: 180.0899
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.01s
                      Time elapsed: 01:04:47
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 49166 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 84.3747
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.6917
                       Mean reward: 903.28
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 180.9599
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.00s
                      Time elapsed: 01:04:49
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 50028 steps/s (collection: 1.876s, learning 0.089s)
             Mean action noise std: 2.79
          Mean value_function loss: 82.2271
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.6994
                       Mean reward: 894.17
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.8534
     Episode_Reward/lifting_object: 178.3365
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 1.96s
                      Time elapsed: 01:04:51
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 48921 steps/s (collection: 1.901s, learning 0.109s)
             Mean action noise std: 2.79
          Mean value_function loss: 94.8662
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.7083
                       Mean reward: 884.75
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.8387
     Episode_Reward/lifting_object: 175.4259
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.01s
                      Time elapsed: 01:04:53
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 49455 steps/s (collection: 1.901s, learning 0.087s)
             Mean action noise std: 2.79
          Mean value_function loss: 85.7456
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.7173
                       Mean reward: 913.36
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.8545
     Episode_Reward/lifting_object: 178.6380
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 1.99s
                      Time elapsed: 01:04:55
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 49620 steps/s (collection: 1.884s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 89.7791
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.7237
                       Mean reward: 895.50
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.8478
     Episode_Reward/lifting_object: 176.9980
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 1.98s
                      Time elapsed: 01:04:57
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 49246 steps/s (collection: 1.905s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 76.0084
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.7296
                       Mean reward: 908.98
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.8512
     Episode_Reward/lifting_object: 178.2529
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.00s
                      Time elapsed: 01:04:59
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 49109 steps/s (collection: 1.896s, learning 0.105s)
             Mean action noise std: 2.80
          Mean value_function loss: 86.5049
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.7356
                       Mean reward: 893.83
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 0.8627
     Episode_Reward/lifting_object: 180.8993
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.00s
                      Time elapsed: 01:05:01
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 48880 steps/s (collection: 1.918s, learning 0.093s)
             Mean action noise std: 2.80
          Mean value_function loss: 82.1448
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.7431
                       Mean reward: 915.00
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.8543
     Episode_Reward/lifting_object: 179.1762
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.01s
                      Time elapsed: 01:05:03
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 48625 steps/s (collection: 1.903s, learning 0.119s)
             Mean action noise std: 2.80
          Mean value_function loss: 73.5384
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.7485
                       Mean reward: 875.28
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.8494
     Episode_Reward/lifting_object: 177.8461
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.02s
                      Time elapsed: 01:05:05
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 49201 steps/s (collection: 1.900s, learning 0.098s)
             Mean action noise std: 2.80
          Mean value_function loss: 95.2931
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.7533
                       Mean reward: 901.35
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.8539
     Episode_Reward/lifting_object: 179.2633
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.00s
                      Time elapsed: 01:05:07
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 49706 steps/s (collection: 1.882s, learning 0.096s)
             Mean action noise std: 2.80
          Mean value_function loss: 73.5589
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.7577
                       Mean reward: 917.10
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.8503
     Episode_Reward/lifting_object: 178.0322
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 1.98s
                      Time elapsed: 01:05:09
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 48633 steps/s (collection: 1.924s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 86.3019
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.7634
                       Mean reward: 880.38
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 0.8465
     Episode_Reward/lifting_object: 177.5002
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.02s
                      Time elapsed: 01:05:11
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 49887 steps/s (collection: 1.880s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 87.9202
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.7728
                       Mean reward: 892.50
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 174.6990
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 1.97s
                      Time elapsed: 01:05:13
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 49186 steps/s (collection: 1.907s, learning 0.091s)
             Mean action noise std: 2.81
          Mean value_function loss: 79.1978
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.7856
                       Mean reward: 910.26
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.8586
     Episode_Reward/lifting_object: 180.3181
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.00s
                      Time elapsed: 01:05:15
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 48770 steps/s (collection: 1.921s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 93.2547
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.8002
                       Mean reward: 898.14
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.8470
     Episode_Reward/lifting_object: 177.2904
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.02s
                      Time elapsed: 01:05:17
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 47551 steps/s (collection: 1.966s, learning 0.101s)
             Mean action noise std: 2.81
          Mean value_function loss: 81.7662
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.8086
                       Mean reward: 898.05
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 179.5915
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.07s
                      Time elapsed: 01:05:19
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 48169 steps/s (collection: 1.931s, learning 0.110s)
             Mean action noise std: 2.81
          Mean value_function loss: 94.1985
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.8109
                       Mean reward: 889.14
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 177.5757
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.04s
                      Time elapsed: 01:05:21
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 47839 steps/s (collection: 1.962s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 80.8347
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 42.8133
                       Mean reward: 885.41
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.8624
     Episode_Reward/lifting_object: 180.5177
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.05s
                      Time elapsed: 01:05:24
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 48701 steps/s (collection: 1.904s, learning 0.114s)
             Mean action noise std: 2.81
          Mean value_function loss: 94.3048
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.8153
                       Mean reward: 881.54
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 0.8425
     Episode_Reward/lifting_object: 176.3036
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.02s
                      Time elapsed: 01:05:26
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 48293 steps/s (collection: 1.931s, learning 0.105s)
             Mean action noise std: 2.81
          Mean value_function loss: 87.2001
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.8182
                       Mean reward: 924.82
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.8548
     Episode_Reward/lifting_object: 178.7367
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.04s
                      Time elapsed: 01:05:28
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 48180 steps/s (collection: 1.928s, learning 0.113s)
             Mean action noise std: 2.81
          Mean value_function loss: 88.5120
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.8236
                       Mean reward: 876.44
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 177.4896
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.04s
                      Time elapsed: 01:05:30
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 47066 steps/s (collection: 1.977s, learning 0.111s)
             Mean action noise std: 2.81
          Mean value_function loss: 74.3595
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.8295
                       Mean reward: 890.62
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 0.8576
     Episode_Reward/lifting_object: 179.8172
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.09s
                      Time elapsed: 01:05:32
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 48045 steps/s (collection: 1.945s, learning 0.101s)
             Mean action noise std: 2.81
          Mean value_function loss: 73.0938
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.8370
                       Mean reward: 882.59
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 0.8508
     Episode_Reward/lifting_object: 178.3983
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.05s
                      Time elapsed: 01:05:34
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 48227 steps/s (collection: 1.909s, learning 0.129s)
             Mean action noise std: 2.81
          Mean value_function loss: 75.8029
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.8436
                       Mean reward: 892.95
               Mean episode length: 237.14
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 179.1442
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.04s
                      Time elapsed: 01:05:36
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 48900 steps/s (collection: 1.908s, learning 0.103s)
             Mean action noise std: 2.82
          Mean value_function loss: 78.9832
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.8461
                       Mean reward: 923.61
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 0.8643
     Episode_Reward/lifting_object: 181.8298
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.01s
                      Time elapsed: 01:05:38
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 47787 steps/s (collection: 1.936s, learning 0.121s)
             Mean action noise std: 2.82
          Mean value_function loss: 70.2836
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.8509
                       Mean reward: 902.23
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 179.9470
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.06s
                      Time elapsed: 01:05:40
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 48810 steps/s (collection: 1.903s, learning 0.111s)
             Mean action noise std: 2.82
          Mean value_function loss: 62.4718
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.8590
                       Mean reward: 900.12
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 0.8669
     Episode_Reward/lifting_object: 181.8145
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.01s
                      Time elapsed: 01:05:42
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 48347 steps/s (collection: 1.943s, learning 0.091s)
             Mean action noise std: 2.82
          Mean value_function loss: 84.5715
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.8659
                       Mean reward: 853.75
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 0.8436
     Episode_Reward/lifting_object: 176.6969
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.03s
                      Time elapsed: 01:05:44
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 48493 steps/s (collection: 1.908s, learning 0.119s)
             Mean action noise std: 2.82
          Mean value_function loss: 99.0028
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.8711
                       Mean reward: 896.93
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 0.8527
     Episode_Reward/lifting_object: 178.9902
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.03s
                      Time elapsed: 01:05:46
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 48223 steps/s (collection: 1.942s, learning 0.096s)
             Mean action noise std: 2.82
          Mean value_function loss: 113.4956
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.8756
                       Mean reward: 879.30
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 174.1519
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.04s
                      Time elapsed: 01:05:48
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 49048 steps/s (collection: 1.899s, learning 0.105s)
             Mean action noise std: 2.82
          Mean value_function loss: 89.4870
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.8797
                       Mean reward: 893.12
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 0.8555
     Episode_Reward/lifting_object: 178.9963
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.00s
                      Time elapsed: 01:05:50
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 49236 steps/s (collection: 1.890s, learning 0.107s)
             Mean action noise std: 2.82
          Mean value_function loss: 82.1282
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.8894
                       Mean reward: 876.92
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.8425
     Episode_Reward/lifting_object: 176.1542
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.00s
                      Time elapsed: 01:05:52
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 48116 steps/s (collection: 1.926s, learning 0.117s)
             Mean action noise std: 2.82
          Mean value_function loss: 76.8432
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.9015
                       Mean reward: 892.16
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 177.7782
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.04s
                      Time elapsed: 01:05:54
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 48013 steps/s (collection: 1.943s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 103.5310
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.9103
                       Mean reward: 903.64
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.8399
     Episode_Reward/lifting_object: 176.2321
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.05s
                      Time elapsed: 01:05:56
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 47212 steps/s (collection: 1.982s, learning 0.100s)
             Mean action noise std: 2.83
          Mean value_function loss: 96.0403
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.9168
                       Mean reward: 859.12
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 0.8300
     Episode_Reward/lifting_object: 173.8019
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.08s
                      Time elapsed: 01:05:58
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 48559 steps/s (collection: 1.928s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 110.6698
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.9198
                       Mean reward: 834.05
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 0.8367
     Episode_Reward/lifting_object: 175.4187
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.02s
                      Time elapsed: 01:06:00
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 48132 steps/s (collection: 1.934s, learning 0.109s)
             Mean action noise std: 2.83
          Mean value_function loss: 112.1030
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.9231
                       Mean reward: 870.36
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 0.8388
     Episode_Reward/lifting_object: 175.4346
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.04s
                      Time elapsed: 01:06:02
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 47290 steps/s (collection: 1.971s, learning 0.108s)
             Mean action noise std: 2.83
          Mean value_function loss: 76.6941
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.9282
                       Mean reward: 886.20
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 179.8740
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.08s
                      Time elapsed: 01:06:04
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 47917 steps/s (collection: 1.955s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 98.0106
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.9408
                       Mean reward: 869.47
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.8406
     Episode_Reward/lifting_object: 176.2116
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.05s
                      Time elapsed: 01:06:06
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 46065 steps/s (collection: 2.002s, learning 0.132s)
             Mean action noise std: 2.83
          Mean value_function loss: 83.6832
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.9566
                       Mean reward: 895.44
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 0.8500
     Episode_Reward/lifting_object: 178.3161
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.13s
                      Time elapsed: 01:06:08
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 47037 steps/s (collection: 1.972s, learning 0.118s)
             Mean action noise std: 2.83
          Mean value_function loss: 98.1508
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 42.9667
                       Mean reward: 921.78
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.8479
     Episode_Reward/lifting_object: 177.2541
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.09s
                      Time elapsed: 01:06:11
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 47399 steps/s (collection: 1.964s, learning 0.110s)
             Mean action noise std: 2.83
          Mean value_function loss: 72.9908
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.9686
                       Mean reward: 904.41
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.8533
     Episode_Reward/lifting_object: 178.8956
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.07s
                      Time elapsed: 01:06:13
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 47150 steps/s (collection: 1.972s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 114.3284
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.9724
                       Mean reward: 906.77
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 0.8428
     Episode_Reward/lifting_object: 176.8932
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.08s
                      Time elapsed: 01:06:15
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 47198 steps/s (collection: 1.991s, learning 0.092s)
             Mean action noise std: 2.84
          Mean value_function loss: 84.3981
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.9818
                       Mean reward: 890.18
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 0.8538
     Episode_Reward/lifting_object: 179.0466
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.08s
                      Time elapsed: 01:06:17
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 47189 steps/s (collection: 1.974s, learning 0.109s)
             Mean action noise std: 2.84
          Mean value_function loss: 80.2117
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.9873
                       Mean reward: 875.46
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.8461
     Episode_Reward/lifting_object: 177.4760
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.08s
                      Time elapsed: 01:06:19
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 47570 steps/s (collection: 1.956s, learning 0.110s)
             Mean action noise std: 2.84
          Mean value_function loss: 86.4264
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.9900
                       Mean reward: 885.22
               Mean episode length: 235.05
    Episode_Reward/reaching_object: 0.8515
     Episode_Reward/lifting_object: 178.5349
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.07s
                      Time elapsed: 01:06:21
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 46595 steps/s (collection: 2.002s, learning 0.108s)
             Mean action noise std: 2.84
          Mean value_function loss: 117.4984
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.9958
                       Mean reward: 888.29
               Mean episode length: 236.54
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 174.5426
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.11s
                      Time elapsed: 01:06:23
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 47502 steps/s (collection: 1.974s, learning 0.096s)
             Mean action noise std: 2.84
          Mean value_function loss: 100.9895
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.0038
                       Mean reward: 860.05
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 176.4272
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.07s
                      Time elapsed: 01:06:25
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 46143 steps/s (collection: 2.006s, learning 0.125s)
             Mean action noise std: 2.84
          Mean value_function loss: 91.7793
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.0108
                       Mean reward: 880.29
               Mean episode length: 235.05
    Episode_Reward/reaching_object: 0.8494
     Episode_Reward/lifting_object: 178.2475
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.13s
                      Time elapsed: 01:06:27
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 45798 steps/s (collection: 1.986s, learning 0.160s)
             Mean action noise std: 2.84
          Mean value_function loss: 80.5740
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.0168
                       Mean reward: 907.92
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 177.1963
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.15s
                      Time elapsed: 01:06:29
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 46198 steps/s (collection: 2.033s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 92.4220
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.0211
                       Mean reward: 912.87
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.8486
     Episode_Reward/lifting_object: 178.0142
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.13s
                      Time elapsed: 01:06:32
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 46273 steps/s (collection: 2.011s, learning 0.113s)
             Mean action noise std: 2.84
          Mean value_function loss: 91.8869
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.0258
                       Mean reward: 888.72
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.8514
     Episode_Reward/lifting_object: 178.5885
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.12s
                      Time elapsed: 01:06:34
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 46130 steps/s (collection: 2.024s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 79.8734
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.0338
                       Mean reward: 900.10
               Mean episode length: 240.16
    Episode_Reward/reaching_object: 0.8517
     Episode_Reward/lifting_object: 178.0765
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.13s
                      Time elapsed: 01:06:36
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 47203 steps/s (collection: 1.993s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 84.2336
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.0421
                       Mean reward: 894.04
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 177.4850
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.08s
                      Time elapsed: 01:06:38
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 47367 steps/s (collection: 1.972s, learning 0.104s)
             Mean action noise std: 2.85
          Mean value_function loss: 68.6208
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.0478
                       Mean reward: 906.51
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.8551
     Episode_Reward/lifting_object: 178.8887
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.08s
                      Time elapsed: 01:06:40
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 47539 steps/s (collection: 1.972s, learning 0.096s)
             Mean action noise std: 2.85
          Mean value_function loss: 70.4801
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.0537
                       Mean reward: 909.96
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 178.9736
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.07s
                      Time elapsed: 01:06:42
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 46376 steps/s (collection: 1.978s, learning 0.142s)
             Mean action noise std: 2.85
          Mean value_function loss: 79.6610
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.0575
                       Mean reward: 867.19
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 176.0283
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.12s
                      Time elapsed: 01:06:44
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 45102 steps/s (collection: 2.044s, learning 0.136s)
             Mean action noise std: 2.85
          Mean value_function loss: 89.1915
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.0619
                       Mean reward: 906.49
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.8486
     Episode_Reward/lifting_object: 177.4278
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.18s
                      Time elapsed: 01:06:46
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 47069 steps/s (collection: 1.964s, learning 0.125s)
             Mean action noise std: 2.85
          Mean value_function loss: 97.9685
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.0684
                       Mean reward: 889.42
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 0.8514
     Episode_Reward/lifting_object: 177.8273
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.09s
                      Time elapsed: 01:06:48
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 46614 steps/s (collection: 2.019s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 65.6509
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.0739
                       Mean reward: 920.68
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.8582
     Episode_Reward/lifting_object: 179.5582
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.11s
                      Time elapsed: 01:06:51
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 47491 steps/s (collection: 1.973s, learning 0.097s)
             Mean action noise std: 2.85
          Mean value_function loss: 75.2412
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 43.0801
                       Mean reward: 907.38
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 177.1253
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.07s
                      Time elapsed: 01:06:53
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 46460 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 2.85
          Mean value_function loss: 88.4510
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 43.0869
                       Mean reward: 900.63
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 0.8551
     Episode_Reward/lifting_object: 178.7599
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.12s
                      Time elapsed: 01:06:55
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 45882 steps/s (collection: 2.018s, learning 0.124s)
             Mean action noise std: 2.85
          Mean value_function loss: 70.8135
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.0933
                       Mean reward: 887.13
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 0.8538
     Episode_Reward/lifting_object: 178.5968
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.14s
                      Time elapsed: 01:06:57
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 44808 steps/s (collection: 2.066s, learning 0.127s)
             Mean action noise std: 2.86
          Mean value_function loss: 77.3659
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.0963
                       Mean reward: 896.67
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 179.6100
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.19s
                      Time elapsed: 01:06:59
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 46318 steps/s (collection: 1.999s, learning 0.124s)
             Mean action noise std: 2.86
          Mean value_function loss: 65.3440
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.0978
                       Mean reward: 907.51
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 0.8687
     Episode_Reward/lifting_object: 181.6941
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.12s
                      Time elapsed: 01:07:01
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 46734 steps/s (collection: 1.999s, learning 0.104s)
             Mean action noise std: 2.86
          Mean value_function loss: 88.7213
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.1038
                       Mean reward: 908.86
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 177.3298
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.10s
                      Time elapsed: 01:07:03
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 47508 steps/s (collection: 1.955s, learning 0.114s)
             Mean action noise std: 2.86
          Mean value_function loss: 85.2873
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.1116
                       Mean reward: 907.19
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.8545
     Episode_Reward/lifting_object: 179.1231
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.07s
                      Time elapsed: 01:07:05
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 46823 steps/s (collection: 1.983s, learning 0.116s)
             Mean action noise std: 2.86
          Mean value_function loss: 89.6958
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.1176
                       Mean reward: 925.78
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 177.4945
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.10s
                      Time elapsed: 01:07:07
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 45620 steps/s (collection: 2.015s, learning 0.140s)
             Mean action noise std: 2.86
          Mean value_function loss: 85.2211
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.1236
                       Mean reward: 876.60
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 0.8548
     Episode_Reward/lifting_object: 179.3091
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.15s
                      Time elapsed: 01:07:10
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 46264 steps/s (collection: 2.035s, learning 0.090s)
             Mean action noise std: 2.86
          Mean value_function loss: 116.0797
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 43.1308
                       Mean reward: 879.93
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 0.8450
     Episode_Reward/lifting_object: 177.1488
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.12s
                      Time elapsed: 01:07:12
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 45340 steps/s (collection: 2.038s, learning 0.130s)
             Mean action noise std: 2.86
          Mean value_function loss: 75.2384
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.1361
                       Mean reward: 917.89
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.8545
     Episode_Reward/lifting_object: 179.2780
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.17s
                      Time elapsed: 01:07:14
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 46588 steps/s (collection: 1.996s, learning 0.114s)
             Mean action noise std: 2.86
          Mean value_function loss: 69.0749
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.1394
                       Mean reward: 894.51
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 0.8599
     Episode_Reward/lifting_object: 180.4453
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.11s
                      Time elapsed: 01:07:16
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 46720 steps/s (collection: 1.996s, learning 0.108s)
             Mean action noise std: 2.86
          Mean value_function loss: 78.5391
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.1439
                       Mean reward: 890.17
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 180.0861
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.10s
                      Time elapsed: 01:07:18
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 46537 steps/s (collection: 2.007s, learning 0.106s)
             Mean action noise std: 2.87
          Mean value_function loss: 96.0392
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.1524
                       Mean reward: 904.38
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.8435
     Episode_Reward/lifting_object: 177.4930
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.11s
                      Time elapsed: 01:07:20
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 46505 steps/s (collection: 2.023s, learning 0.091s)
             Mean action noise std: 2.87
          Mean value_function loss: 103.6588
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.1615
                       Mean reward: 888.01
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.8509
     Episode_Reward/lifting_object: 178.9987
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.11s
                      Time elapsed: 01:07:22
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 45855 steps/s (collection: 1.996s, learning 0.148s)
             Mean action noise std: 2.87
          Mean value_function loss: 85.5927
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.1666
                       Mean reward: 887.23
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 0.8375
     Episode_Reward/lifting_object: 176.0762
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.14s
                      Time elapsed: 01:07:24
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 46458 steps/s (collection: 1.987s, learning 0.129s)
             Mean action noise std: 2.87
          Mean value_function loss: 93.0719
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.1707
                       Mean reward: 881.99
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 0.8377
     Episode_Reward/lifting_object: 176.3409
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.12s
                      Time elapsed: 01:07:27
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 46403 steps/s (collection: 1.974s, learning 0.145s)
             Mean action noise std: 2.87
          Mean value_function loss: 89.7151
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.1782
                       Mean reward: 884.48
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.8435
     Episode_Reward/lifting_object: 177.7524
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.12s
                      Time elapsed: 01:07:29
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 45541 steps/s (collection: 2.018s, learning 0.140s)
             Mean action noise std: 2.87
          Mean value_function loss: 106.1572
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.1844
                       Mean reward: 895.88
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 0.8456
     Episode_Reward/lifting_object: 178.3775
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.16s
                      Time elapsed: 01:07:31
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 46591 steps/s (collection: 1.998s, learning 0.112s)
             Mean action noise std: 2.87
          Mean value_function loss: 84.5209
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 43.1924
                       Mean reward: 910.87
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 0.8571
     Episode_Reward/lifting_object: 181.1637
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.11s
                      Time elapsed: 01:07:33
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 47410 steps/s (collection: 1.968s, learning 0.106s)
             Mean action noise std: 2.87
          Mean value_function loss: 105.3547
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.1983
                       Mean reward: 887.70
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 178.7532
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.07s
                      Time elapsed: 01:07:35
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 46664 steps/s (collection: 1.988s, learning 0.119s)
             Mean action noise std: 2.87
          Mean value_function loss: 81.8156
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.2089
                       Mean reward: 904.23
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.8416
     Episode_Reward/lifting_object: 177.9205
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.11s
                      Time elapsed: 01:07:37
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 46753 steps/s (collection: 2.000s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 100.7025
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 43.2189
                       Mean reward: 870.00
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 0.8385
     Episode_Reward/lifting_object: 177.5993
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.10s
                      Time elapsed: 01:07:39
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 47089 steps/s (collection: 1.985s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 92.5377
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.2260
                       Mean reward: 906.29
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 180.0221
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.09s
                      Time elapsed: 01:07:41
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 47268 steps/s (collection: 1.959s, learning 0.121s)
             Mean action noise std: 2.88
          Mean value_function loss: 91.1989
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.2386
                       Mean reward: 877.90
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.8279
     Episode_Reward/lifting_object: 175.0550
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.08s
                      Time elapsed: 01:07:43
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 45850 steps/s (collection: 2.015s, learning 0.129s)
             Mean action noise std: 2.88
          Mean value_function loss: 94.1387
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.2501
                       Mean reward: 893.73
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 0.8416
     Episode_Reward/lifting_object: 178.4323
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.14s
                      Time elapsed: 01:07:46
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 46285 steps/s (collection: 1.992s, learning 0.132s)
             Mean action noise std: 2.88
          Mean value_function loss: 90.3550
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.2579
                       Mean reward: 882.02
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.8336
     Episode_Reward/lifting_object: 176.6893
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.12s
                      Time elapsed: 01:07:48
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 45077 steps/s (collection: 2.029s, learning 0.152s)
             Mean action noise std: 2.88
          Mean value_function loss: 97.4801
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.2656
                       Mean reward: 870.26
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.8457
     Episode_Reward/lifting_object: 179.5439
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.18s
                      Time elapsed: 01:07:50
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 45823 steps/s (collection: 2.053s, learning 0.093s)
             Mean action noise std: 2.88
          Mean value_function loss: 84.8021
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 43.2705
                       Mean reward: 886.10
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.8426
     Episode_Reward/lifting_object: 178.4097
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.15s
                      Time elapsed: 01:07:52
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 47500 steps/s (collection: 1.966s, learning 0.104s)
             Mean action noise std: 2.88
          Mean value_function loss: 92.8925
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.2743
                       Mean reward: 879.26
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 0.8318
     Episode_Reward/lifting_object: 176.0390
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.07s
                      Time elapsed: 01:07:54
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 46810 steps/s (collection: 2.000s, learning 0.100s)
             Mean action noise std: 2.89
          Mean value_function loss: 110.7887
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.2857
                       Mean reward: 866.34
               Mean episode length: 231.70
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 178.7940
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.10s
                      Time elapsed: 01:07:56
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 47123 steps/s (collection: 1.987s, learning 0.099s)
             Mean action noise std: 2.89
          Mean value_function loss: 93.6610
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.2928
                       Mean reward: 882.99
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 0.8377
     Episode_Reward/lifting_object: 177.4114
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.09s
                      Time elapsed: 01:07:58
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 47025 steps/s (collection: 1.977s, learning 0.113s)
             Mean action noise std: 2.89
          Mean value_function loss: 115.7526
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.2977
                       Mean reward: 886.66
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.8381
     Episode_Reward/lifting_object: 177.6078
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.09s
                      Time elapsed: 01:08:00
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 46037 steps/s (collection: 2.010s, learning 0.126s)
             Mean action noise std: 2.89
          Mean value_function loss: 93.5499
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 43.3024
                       Mean reward: 898.34
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.8400
     Episode_Reward/lifting_object: 177.9657
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.14s
                      Time elapsed: 01:08:03
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 46211 steps/s (collection: 1.998s, learning 0.129s)
             Mean action noise std: 2.89
          Mean value_function loss: 100.4487
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.3062
                       Mean reward: 903.05
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: 179.3556
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.13s
                      Time elapsed: 01:08:05
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 47239 steps/s (collection: 1.980s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 76.5258
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.3117
                       Mean reward: 877.04
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 177.6720
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.08s
                      Time elapsed: 01:08:07
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 46160 steps/s (collection: 1.980s, learning 0.150s)
             Mean action noise std: 2.89
          Mean value_function loss: 114.4793
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 43.3139
                       Mean reward: 883.37
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 0.8462
     Episode_Reward/lifting_object: 179.1828
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.13s
                      Time elapsed: 01:08:09
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 47549 steps/s (collection: 1.973s, learning 0.095s)
             Mean action noise std: 2.89
          Mean value_function loss: 76.6659
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.3163
                       Mean reward: 892.58
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 181.1638
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.07s
                      Time elapsed: 01:08:11
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 46808 steps/s (collection: 2.006s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 101.0576
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 43.3181
                       Mean reward: 883.36
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 0.8497
     Episode_Reward/lifting_object: 179.9981
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.10s
                      Time elapsed: 01:08:13
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 47095 steps/s (collection: 1.989s, learning 0.098s)
             Mean action noise std: 2.89
          Mean value_function loss: 109.8371
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.3210
                       Mean reward: 871.00
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 177.9617
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.09s
                      Time elapsed: 01:08:15
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 47513 steps/s (collection: 1.970s, learning 0.099s)
             Mean action noise std: 2.89
          Mean value_function loss: 115.8967
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 43.3262
                       Mean reward: 897.56
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.8465
     Episode_Reward/lifting_object: 179.0779
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.07s
                      Time elapsed: 01:08:17
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 47526 steps/s (collection: 1.969s, learning 0.100s)
             Mean action noise std: 2.89
          Mean value_function loss: 124.6795
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.3299
                       Mean reward: 887.76
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 0.8451
     Episode_Reward/lifting_object: 178.4377
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.07s
                      Time elapsed: 01:08:19
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 46976 steps/s (collection: 1.982s, learning 0.110s)
             Mean action noise std: 2.90
          Mean value_function loss: 96.5430
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.3379
                       Mean reward: 885.58
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 177.2169
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.09s
                      Time elapsed: 01:08:21
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 44938 steps/s (collection: 2.088s, learning 0.100s)
             Mean action noise std: 2.90
          Mean value_function loss: 101.0519
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.3479
                       Mean reward: 862.08
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 0.8472
     Episode_Reward/lifting_object: 178.7579
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.19s
                      Time elapsed: 01:08:24
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 46499 steps/s (collection: 1.990s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 97.4146
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 43.3539
                       Mean reward: 907.24
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.8412
     Episode_Reward/lifting_object: 177.2910
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.11s
                      Time elapsed: 01:08:26
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 46096 steps/s (collection: 1.988s, learning 0.145s)
             Mean action noise std: 2.90
          Mean value_function loss: 75.1396
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.3583
                       Mean reward: 863.22
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 177.2794
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.13s
                      Time elapsed: 01:08:28
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 44755 steps/s (collection: 2.080s, learning 0.117s)
             Mean action noise std: 2.90
          Mean value_function loss: 108.1798
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.3629
                       Mean reward: 879.62
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 0.8386
     Episode_Reward/lifting_object: 176.3402
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.20s
                      Time elapsed: 01:08:30
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 44079 steps/s (collection: 2.103s, learning 0.127s)
             Mean action noise std: 2.90
          Mean value_function loss: 104.0054
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.3676
                       Mean reward: 878.29
               Mean episode length: 234.22
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 175.7391
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.23s
                      Time elapsed: 01:08:32
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 44677 steps/s (collection: 2.061s, learning 0.140s)
             Mean action noise std: 2.90
          Mean value_function loss: 93.8484
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.3702
                       Mean reward: 908.66
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.8462
     Episode_Reward/lifting_object: 177.9410
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.20s
                      Time elapsed: 01:08:34
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 45561 steps/s (collection: 2.033s, learning 0.125s)
             Mean action noise std: 2.90
          Mean value_function loss: 97.9841
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.3734
                       Mean reward: 901.47
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 0.8484
     Episode_Reward/lifting_object: 178.2503
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.16s
                      Time elapsed: 01:08:37
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 46411 steps/s (collection: 2.015s, learning 0.103s)
             Mean action noise std: 2.90
          Mean value_function loss: 61.8727
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.3774
                       Mean reward: 914.82
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.8590
     Episode_Reward/lifting_object: 180.4864
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.12s
                      Time elapsed: 01:08:39
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 46108 steps/s (collection: 2.011s, learning 0.121s)
             Mean action noise std: 2.90
          Mean value_function loss: 83.5871
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.3815
                       Mean reward: 898.11
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 0.8531
     Episode_Reward/lifting_object: 178.7173
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.13s
                      Time elapsed: 01:08:41
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 46899 steps/s (collection: 1.997s, learning 0.100s)
             Mean action noise std: 2.91
          Mean value_function loss: 84.0680
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.3871
                       Mean reward: 920.00
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.8618
     Episode_Reward/lifting_object: 180.7450
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.10s
                      Time elapsed: 01:08:43
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 45803 steps/s (collection: 2.052s, learning 0.094s)
             Mean action noise std: 2.91
          Mean value_function loss: 78.0216
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.3967
                       Mean reward: 890.81
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 0.8494
     Episode_Reward/lifting_object: 177.7715
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.15s
                      Time elapsed: 01:08:45
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 46685 steps/s (collection: 1.999s, learning 0.106s)
             Mean action noise std: 2.91
          Mean value_function loss: 95.2860
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.4074
                       Mean reward: 892.19
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: 177.8854
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.11s
                      Time elapsed: 01:08:47
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 47276 steps/s (collection: 1.978s, learning 0.101s)
             Mean action noise std: 2.91
          Mean value_function loss: 93.4718
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.4164
                       Mean reward: 886.80
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 176.5649
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.08s
                      Time elapsed: 01:08:49
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 46883 steps/s (collection: 2.000s, learning 0.097s)
             Mean action noise std: 2.91
          Mean value_function loss: 95.6406
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.4204
                       Mean reward: 900.92
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 176.7568
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.10s
                      Time elapsed: 01:08:51
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 46816 steps/s (collection: 1.993s, learning 0.107s)
             Mean action noise std: 2.91
          Mean value_function loss: 55.0359
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.4249
                       Mean reward: 924.01
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.8611
     Episode_Reward/lifting_object: 180.0154
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.10s
                      Time elapsed: 01:08:53
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 47218 steps/s (collection: 1.982s, learning 0.100s)
             Mean action noise std: 2.91
          Mean value_function loss: 57.5683
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.4320
                       Mean reward: 912.12
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 0.8603
     Episode_Reward/lifting_object: 179.9726
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.08s
                      Time elapsed: 01:08:56
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 45774 steps/s (collection: 2.019s, learning 0.129s)
             Mean action noise std: 2.91
          Mean value_function loss: 76.7842
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.4367
                       Mean reward: 898.78
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 0.8679
     Episode_Reward/lifting_object: 181.7373
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.15s
                      Time elapsed: 01:08:58
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 45805 steps/s (collection: 2.013s, learning 0.133s)
             Mean action noise std: 2.91
          Mean value_function loss: 84.5660
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.4396
                       Mean reward: 880.90
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: 177.3815
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.15s
                      Time elapsed: 01:09:00
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 46468 steps/s (collection: 1.972s, learning 0.143s)
             Mean action noise std: 2.91
          Mean value_function loss: 101.7096
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.4415
                       Mean reward: 897.13
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 177.6043
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.12s
                      Time elapsed: 01:09:02
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 47518 steps/s (collection: 1.962s, learning 0.107s)
             Mean action noise std: 2.91
          Mean value_function loss: 90.5432
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.4468
                       Mean reward: 878.60
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 0.8494
     Episode_Reward/lifting_object: 177.3987
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.07s
                      Time elapsed: 01:09:04
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 46752 steps/s (collection: 2.001s, learning 0.102s)
             Mean action noise std: 2.92
          Mean value_function loss: 59.0938
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.4538
                       Mean reward: 902.06
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 0.8566
     Episode_Reward/lifting_object: 179.3466
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.10s
                      Time elapsed: 01:09:06
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 47276 steps/s (collection: 1.974s, learning 0.105s)
             Mean action noise std: 2.92
          Mean value_function loss: 70.2915
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.4600
                       Mean reward: 910.10
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.8598
     Episode_Reward/lifting_object: 180.0934
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.08s
                      Time elapsed: 01:09:08
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 46270 steps/s (collection: 2.015s, learning 0.110s)
             Mean action noise std: 2.92
          Mean value_function loss: 69.6806
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.4658
                       Mean reward: 912.47
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.8714
     Episode_Reward/lifting_object: 182.7007
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.12s
                      Time elapsed: 01:09:10
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 47331 steps/s (collection: 1.970s, learning 0.107s)
             Mean action noise std: 2.92
          Mean value_function loss: 80.2275
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.4693
                       Mean reward: 887.00
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.8625
     Episode_Reward/lifting_object: 180.7965
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.08s
                      Time elapsed: 01:09:12
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 46008 steps/s (collection: 2.025s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 83.6737
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 43.4709
                       Mean reward: 888.65
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 175.5912
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.14s
                      Time elapsed: 01:09:15
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 44750 steps/s (collection: 2.052s, learning 0.145s)
             Mean action noise std: 2.92
          Mean value_function loss: 79.3723
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 43.4717
                       Mean reward: 893.54
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 177.1537
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.20s
                      Time elapsed: 01:09:17
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 47200 steps/s (collection: 1.971s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 62.3917
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.4730
                       Mean reward: 890.25
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.8528
     Episode_Reward/lifting_object: 178.9082
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.08s
                      Time elapsed: 01:09:19
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 46302 steps/s (collection: 1.984s, learning 0.139s)
             Mean action noise std: 2.92
          Mean value_function loss: 81.0507
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.4762
                       Mean reward: 919.33
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 179.9696
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.12s
                      Time elapsed: 01:09:21
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 46397 steps/s (collection: 2.011s, learning 0.108s)
             Mean action noise std: 2.92
          Mean value_function loss: 76.5005
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.4844
                       Mean reward: 913.79
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.8486
     Episode_Reward/lifting_object: 178.0434
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.12s
                      Time elapsed: 01:09:23
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 46991 steps/s (collection: 1.983s, learning 0.109s)
             Mean action noise std: 2.92
          Mean value_function loss: 71.2485
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.4950
                       Mean reward: 895.10
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.8575
     Episode_Reward/lifting_object: 180.2175
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.09s
                      Time elapsed: 01:09:25
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 47554 steps/s (collection: 1.969s, learning 0.099s)
             Mean action noise std: 2.92
          Mean value_function loss: 88.7590
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.5053
                       Mean reward: 902.78
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 0.8482
     Episode_Reward/lifting_object: 177.8658
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.07s
                      Time elapsed: 01:09:27
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 46910 steps/s (collection: 1.997s, learning 0.099s)
             Mean action noise std: 2.93
          Mean value_function loss: 56.9290
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.5136
                       Mean reward: 911.14
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.8683
     Episode_Reward/lifting_object: 182.4757
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.10s
                      Time elapsed: 01:09:29
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 45844 steps/s (collection: 2.033s, learning 0.111s)
             Mean action noise std: 2.93
          Mean value_function loss: 71.1706
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.5213
                       Mean reward: 897.34
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: 179.0157
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.14s
                      Time elapsed: 01:09:31
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 47810 steps/s (collection: 1.951s, learning 0.105s)
             Mean action noise std: 2.93
          Mean value_function loss: 71.7882
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 43.5268
                       Mean reward: 910.12
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 0.8483
     Episode_Reward/lifting_object: 178.0406
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.06s
                      Time elapsed: 01:09:33
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 47448 steps/s (collection: 1.967s, learning 0.105s)
             Mean action noise std: 2.93
          Mean value_function loss: 113.7833
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.5312
                       Mean reward: 880.87
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 177.1342
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.07s
                      Time elapsed: 01:09:36
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 45156 steps/s (collection: 2.068s, learning 0.109s)
             Mean action noise std: 2.93
          Mean value_function loss: 99.9846
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.5371
                       Mean reward: 858.75
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 0.8434
     Episode_Reward/lifting_object: 177.0216
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.18s
                      Time elapsed: 01:09:38
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 45914 steps/s (collection: 1.995s, learning 0.146s)
             Mean action noise std: 2.93
          Mean value_function loss: 72.2015
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.5435
                       Mean reward: 902.44
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.8575
     Episode_Reward/lifting_object: 180.5848
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.14s
                      Time elapsed: 01:09:40
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 46611 steps/s (collection: 1.981s, learning 0.128s)
             Mean action noise std: 2.93
          Mean value_function loss: 106.0305
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.5485
                       Mean reward: 889.22
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 0.8536
     Episode_Reward/lifting_object: 179.6723
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.11s
                      Time elapsed: 01:09:42
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 47298 steps/s (collection: 1.964s, learning 0.114s)
             Mean action noise std: 2.93
          Mean value_function loss: 69.4654
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.5550
                       Mean reward: 906.23
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.8594
     Episode_Reward/lifting_object: 180.8071
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.08s
                      Time elapsed: 01:09:44
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 47940 steps/s (collection: 1.952s, learning 0.099s)
             Mean action noise std: 2.93
          Mean value_function loss: 76.1137
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.5590
                       Mean reward: 922.89
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.8537
     Episode_Reward/lifting_object: 179.3780
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.05s
                      Time elapsed: 01:09:46
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 47086 steps/s (collection: 1.988s, learning 0.100s)
             Mean action noise std: 2.93
          Mean value_function loss: 90.1290
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.5613
                       Mean reward: 840.94
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 0.8436
     Episode_Reward/lifting_object: 177.3254
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.09s
                      Time elapsed: 01:09:48
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 46740 steps/s (collection: 1.998s, learning 0.106s)
             Mean action noise std: 2.93
          Mean value_function loss: 62.0483
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.5644
                       Mean reward: 922.13
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.8595
     Episode_Reward/lifting_object: 180.6663
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.10s
                      Time elapsed: 01:09:50
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 47302 steps/s (collection: 1.979s, learning 0.099s)
             Mean action noise std: 2.93
          Mean value_function loss: 80.9258
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.5708
                       Mean reward: 902.55
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 178.4654
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.08s
                      Time elapsed: 01:09:52
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 46949 steps/s (collection: 1.986s, learning 0.108s)
             Mean action noise std: 2.94
          Mean value_function loss: 93.4502
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 43.5784
                       Mean reward: 901.12
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.8501
     Episode_Reward/lifting_object: 178.4477
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.09s
                      Time elapsed: 01:09:54
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 44218 steps/s (collection: 2.128s, learning 0.095s)
             Mean action noise std: 2.94
          Mean value_function loss: 81.6047
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.5847
                       Mean reward: 885.37
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 178.3971
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.22s
                      Time elapsed: 01:09:57
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 47832 steps/s (collection: 1.963s, learning 0.092s)
             Mean action noise std: 2.94
          Mean value_function loss: 74.0765
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.5919
                       Mean reward: 901.83
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.8452
     Episode_Reward/lifting_object: 177.4761
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.06s
                      Time elapsed: 01:09:59
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 47060 steps/s (collection: 1.979s, learning 0.110s)
             Mean action noise std: 2.94
          Mean value_function loss: 77.0946
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.5969
                       Mean reward: 888.07
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 177.6476
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.09s
                      Time elapsed: 01:10:01
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 46879 steps/s (collection: 1.999s, learning 0.098s)
             Mean action noise std: 2.94
          Mean value_function loss: 80.7354
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.6087
                       Mean reward: 892.15
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.8569
     Episode_Reward/lifting_object: 179.9931
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.10s
                      Time elapsed: 01:10:03
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 47830 steps/s (collection: 1.958s, learning 0.097s)
             Mean action noise std: 2.94
          Mean value_function loss: 53.9187
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.6208
                       Mean reward: 937.77
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.8719
     Episode_Reward/lifting_object: 183.4235
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.06s
                      Time elapsed: 01:10:05
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 47795 steps/s (collection: 1.961s, learning 0.096s)
             Mean action noise std: 2.94
          Mean value_function loss: 76.7635
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 43.6268
                       Mean reward: 894.05
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 178.1357
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.06s
                      Time elapsed: 01:10:07
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 48045 steps/s (collection: 1.949s, learning 0.097s)
             Mean action noise std: 2.94
          Mean value_function loss: 81.7931
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.6285
                       Mean reward: 900.29
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 0.8651
     Episode_Reward/lifting_object: 181.8781
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.05s
                      Time elapsed: 01:10:09
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 47171 steps/s (collection: 1.982s, learning 0.102s)
             Mean action noise std: 2.94
          Mean value_function loss: 83.2937
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.6324
                       Mean reward: 864.88
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 178.8770
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.08s
                      Time elapsed: 01:10:11
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 46717 steps/s (collection: 2.011s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 90.8696
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.6419
                       Mean reward: 872.71
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.8428
     Episode_Reward/lifting_object: 176.6233
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.10s
                      Time elapsed: 01:10:13
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 47565 steps/s (collection: 1.957s, learning 0.110s)
             Mean action noise std: 2.95
          Mean value_function loss: 76.6327
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.6516
                       Mean reward: 924.50
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 0.8573
     Episode_Reward/lifting_object: 180.1734
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.07s
                      Time elapsed: 01:10:15
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 47330 steps/s (collection: 1.986s, learning 0.091s)
             Mean action noise std: 2.95
          Mean value_function loss: 99.2973
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.6571
                       Mean reward: 881.96
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 0.8619
     Episode_Reward/lifting_object: 181.1144
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.08s
                      Time elapsed: 01:10:17
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 47363 steps/s (collection: 1.978s, learning 0.097s)
             Mean action noise std: 2.95
          Mean value_function loss: 73.8123
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.6616
                       Mean reward: 919.93
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 180.0936
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.08s
                      Time elapsed: 01:10:19
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 45786 steps/s (collection: 2.041s, learning 0.107s)
             Mean action noise std: 2.95
          Mean value_function loss: 81.7226
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.6657
                       Mean reward: 901.75
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 0.8462
     Episode_Reward/lifting_object: 177.7475
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.15s
                      Time elapsed: 01:10:22
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 45481 steps/s (collection: 2.046s, learning 0.115s)
             Mean action noise std: 2.95
          Mean value_function loss: 71.0908
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.6684
                       Mean reward: 893.48
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.8590
     Episode_Reward/lifting_object: 180.4490
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.16s
                      Time elapsed: 01:10:24
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 46255 steps/s (collection: 1.999s, learning 0.126s)
             Mean action noise std: 2.95
          Mean value_function loss: 78.0421
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.6714
                       Mean reward: 916.62
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 179.8446
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.13s
                      Time elapsed: 01:10:26
                               ETA: 00:00:02

