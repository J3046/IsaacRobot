################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 17293 steps/s (collection: 5.420s, learning 0.264s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 11.3625
                       Mean reward: 0.00
               Mean episode length: 21.31
    Episode_Reward/reaching_object: 0.0003
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.68s
                      Time elapsed: 00:00:05
                               ETA: 03:09:29

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 30820 steps/s (collection: 2.958s, learning 0.231s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 11.3989
                       Mean reward: 0.01
               Mean episode length: 45.35
    Episode_Reward/reaching_object: 0.0012
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.19s
                      Time elapsed: 00:00:08
                               ETA: 02:27:49

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 30764 steps/s (collection: 3.053s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 11.4288
                       Mean reward: 0.01
               Mean episode length: 69.71
    Episode_Reward/reaching_object: 0.0024
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 3.20s
                      Time elapsed: 00:00:12
                               ETA: 02:13:58

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 32925 steps/s (collection: 2.840s, learning 0.146s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 11.4592
                       Mean reward: 0.02
               Mean episode length: 93.33
    Episode_Reward/reaching_object: 0.0033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.99s
                      Time elapsed: 00:00:15
                               ETA: 02:05:16

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 32939 steps/s (collection: 2.843s, learning 0.142s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.4781
                       Mean reward: 0.02
               Mean episode length: 117.98
    Episode_Reward/reaching_object: 0.0042
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.98s
                      Time elapsed: 00:00:18
                               ETA: 02:00:01

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 32703 steps/s (collection: 2.854s, learning 0.152s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 11.4809
                       Mean reward: 0.02
               Mean episode length: 141.18
    Episode_Reward/reaching_object: 0.0056
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.01s
                      Time elapsed: 00:00:21
                               ETA: 01:56:37

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 32568 steps/s (collection: 2.868s, learning 0.150s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 11.5290
                       Mean reward: 0.03
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.0068
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 3.02s
                      Time elapsed: 00:00:24
                               ETA: 01:54:14

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 32767 steps/s (collection: 2.863s, learning 0.137s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 11.5443
                       Mean reward: 0.03
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.0081
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.00s
                      Time elapsed: 00:00:27
                               ETA: 01:52:22

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 29414 steps/s (collection: 3.225s, learning 0.117s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 11.5622
                       Mean reward: 0.05
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 0.0105
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0014
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.34s
                      Time elapsed: 00:00:30
                               ETA: 01:52:09

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 124241 steps/s (collection: 0.687s, learning 0.104s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 11.5740
                       Mean reward: 0.06
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.0113
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.79s
                      Time elapsed: 00:00:31
                               ETA: 01:43:31

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 129031 steps/s (collection: 0.670s, learning 0.092s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 11.5677
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0149
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.76s
                      Time elapsed: 00:00:31
                               ETA: 01:36:21

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 127929 steps/s (collection: 0.680s, learning 0.088s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 11.5443
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0163
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.77s
                      Time elapsed: 00:00:32
                               ETA: 01:30:24

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 123376 steps/s (collection: 0.703s, learning 0.093s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 11.5819
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0194
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.80s
                      Time elapsed: 00:00:33
                               ETA: 01:25:26

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 124708 steps/s (collection: 0.690s, learning 0.098s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 11.6122
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0236
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.79s
                      Time elapsed: 00:00:34
                               ETA: 01:21:09

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 123411 steps/s (collection: 0.708s, learning 0.089s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0196
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 11.6928
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0322
     Episode_Reward/lifting_object: 0.0222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.80s
                      Time elapsed: 00:00:35
                               ETA: 01:17:28

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 119916 steps/s (collection: 0.730s, learning 0.090s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.2410
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 11.7180
                       Mean reward: 0.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0417
     Episode_Reward/lifting_object: 0.0127
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.82s
                      Time elapsed: 00:00:35
                               ETA: 01:14:17

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 113828 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.2851
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 11.7716
                       Mean reward: 0.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0560
     Episode_Reward/lifting_object: 0.0154
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.86s
                      Time elapsed: 00:00:36
                               ETA: 01:11:33

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 110223 steps/s (collection: 0.795s, learning 0.097s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.4056
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 11.8386
                       Mean reward: 0.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0736
     Episode_Reward/lifting_object: 0.0283
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.89s
                      Time elapsed: 00:00:37
                               ETA: 01:09:11

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 112634 steps/s (collection: 0.767s, learning 0.106s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.4180
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.9289
                       Mean reward: 0.89
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.0835
     Episode_Reward/lifting_object: 0.0633
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.87s
                      Time elapsed: 00:00:38
                               ETA: 01:07:02

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 115593 steps/s (collection: 0.739s, learning 0.111s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.4039
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 12.0059
                       Mean reward: 1.43
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.1027
     Episode_Reward/lifting_object: 0.1080
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.85s
                      Time elapsed: 00:00:39
                               ETA: 01:05:03

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 114081 steps/s (collection: 0.744s, learning 0.118s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.3576
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 12.0913
                       Mean reward: 1.11
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.1150
     Episode_Reward/lifting_object: 0.1135
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.86s
                      Time elapsed: 00:00:40
                               ETA: 01:03:16

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 112156 steps/s (collection: 0.772s, learning 0.105s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.5521
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.1753
                       Mean reward: 1.61
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.1283
     Episode_Reward/lifting_object: 0.1103
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.88s
                      Time elapsed: 00:00:41
                               ETA: 01:01:41

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 114759 steps/s (collection: 0.755s, learning 0.102s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.2685
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 12.2436
                       Mean reward: 1.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1355
     Episode_Reward/lifting_object: 0.0788
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.86s
                      Time elapsed: 00:00:42
                               ETA: 01:00:12

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 113316 steps/s (collection: 0.781s, learning 0.087s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.3981
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 12.3083
                       Mean reward: 1.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1468
     Episode_Reward/lifting_object: 0.2030
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.87s
                      Time elapsed: 00:00:42
                               ETA: 00:58:51

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 112457 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.4531
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 12.3525
                       Mean reward: 1.81
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.1580
     Episode_Reward/lifting_object: 0.1871
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.87s
                      Time elapsed: 00:00:43
                               ETA: 00:57:37

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 113197 steps/s (collection: 0.773s, learning 0.095s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.8693
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 12.3966
                       Mean reward: 2.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1571
     Episode_Reward/lifting_object: 0.2737
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.87s
                      Time elapsed: 00:00:44
                               ETA: 00:56:28

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 100350 steps/s (collection: 0.851s, learning 0.129s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.8287
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 12.4839
                       Mean reward: 2.74
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.1635
     Episode_Reward/lifting_object: 0.2780
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.98s
                      Time elapsed: 00:00:45
                               ETA: 00:55:33

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 108127 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.6418
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 12.5894
                       Mean reward: 1.25
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.1629
     Episode_Reward/lifting_object: 0.1482
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.91s
                      Time elapsed: 00:00:46
                               ETA: 00:54:36

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 111516 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.7704
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.6963
                       Mean reward: 1.84
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.1652
     Episode_Reward/lifting_object: 0.2881
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.88s
                      Time elapsed: 00:00:47
                               ETA: 00:53:42

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 105469 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.6934
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.7728
                       Mean reward: 2.68
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.1642
     Episode_Reward/lifting_object: 0.4076
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.93s
                      Time elapsed: 00:00:48
                               ETA: 00:52:54

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 104623 steps/s (collection: 0.838s, learning 0.101s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.6364
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 12.8621
                       Mean reward: 2.09
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.1697
     Episode_Reward/lifting_object: 0.2723
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.94s
                      Time elapsed: 00:00:49
                               ETA: 00:52:10

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 112106 steps/s (collection: 0.779s, learning 0.098s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.6706
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 12.9086
                       Mean reward: 1.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1649
     Episode_Reward/lifting_object: 0.2560
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.88s
                      Time elapsed: 00:00:50
                               ETA: 00:51:24

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 110968 steps/s (collection: 0.784s, learning 0.102s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.5834
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 12.9631
                       Mean reward: 1.78
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.1677
     Episode_Reward/lifting_object: 0.2945
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.89s
                      Time elapsed: 00:00:51
                               ETA: 00:50:42

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 102245 steps/s (collection: 0.856s, learning 0.105s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7812
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 13.0164
                       Mean reward: 2.47
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.1658
     Episode_Reward/lifting_object: 0.2961
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.96s
                      Time elapsed: 00:00:51
                               ETA: 00:50:07

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 102294 steps/s (collection: 0.845s, learning 0.116s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.5066
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 13.0829
                       Mean reward: 3.22
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.1587
     Episode_Reward/lifting_object: 0.5522
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.96s
                      Time elapsed: 00:00:52
                               ETA: 00:49:33

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 109742 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.7413
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 13.1381
                       Mean reward: 3.00
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.1554
     Episode_Reward/lifting_object: 0.3212
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.90s
                      Time elapsed: 00:00:53
                               ETA: 00:48:58

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 107345 steps/s (collection: 0.811s, learning 0.105s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.5629
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 13.1598
                       Mean reward: 2.36
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.1617
     Episode_Reward/lifting_object: 0.3502
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.92s
                      Time elapsed: 00:00:54
                               ETA: 00:48:26

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 110953 steps/s (collection: 0.793s, learning 0.093s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.8082
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 13.1972
                       Mean reward: 2.51
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.1551
     Episode_Reward/lifting_object: 0.3251
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.89s
                      Time elapsed: 00:00:55
                               ETA: 00:47:54

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 110131 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 1.27
          Mean value_function loss: 1.0732
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 13.2320
                       Mean reward: 3.57
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.1615
     Episode_Reward/lifting_object: 0.4530
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.89s
                      Time elapsed: 00:00:56
                               ETA: 00:47:23

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 93946 steps/s (collection: 0.906s, learning 0.141s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.3550
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 13.2825
                       Mean reward: 2.24
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 0.1607
     Episode_Reward/lifting_object: 0.4931
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.05s
                      Time elapsed: 00:00:57
                               ETA: 00:47:02

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 105480 steps/s (collection: 0.837s, learning 0.095s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.1231
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 13.3245
                       Mean reward: 3.07
               Mean episode length: 247.32
    Episode_Reward/reaching_object: 0.1552
     Episode_Reward/lifting_object: 0.4885
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.93s
                      Time elapsed: 00:00:58
                               ETA: 00:46:36

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 100305 steps/s (collection: 0.859s, learning 0.122s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.1952
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.4030
                       Mean reward: 4.23
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.1621
     Episode_Reward/lifting_object: 0.5931
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.98s
                      Time elapsed: 00:00:59
                               ETA: 00:46:14

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 105073 steps/s (collection: 0.823s, learning 0.113s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.2564
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.5046
                       Mean reward: 4.08
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.1669
     Episode_Reward/lifting_object: 0.6180
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.94s
                      Time elapsed: 00:01:00
                               ETA: 00:45:51

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 111023 steps/s (collection: 0.781s, learning 0.105s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.3143
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 13.5791
                       Mean reward: 3.90
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.1700
     Episode_Reward/lifting_object: 0.5028
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.89s
                      Time elapsed: 00:01:01
                               ETA: 00:45:26

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 103268 steps/s (collection: 0.820s, learning 0.132s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.6234
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 13.6611
                       Mean reward: 4.72
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.1702
     Episode_Reward/lifting_object: 0.6573
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.95s
                      Time elapsed: 00:01:02
                               ETA: 00:45:06

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 104806 steps/s (collection: 0.822s, learning 0.116s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.4008
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.7002
                       Mean reward: 6.83
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.1712
     Episode_Reward/lifting_object: 0.8626
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.94s
                      Time elapsed: 00:01:03
                               ETA: 00:44:45

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 107982 steps/s (collection: 0.821s, learning 0.089s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.0066
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.7566
                       Mean reward: 4.53
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.1664
     Episode_Reward/lifting_object: 0.6861
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.91s
                      Time elapsed: 00:01:04
                               ETA: 00:44:25

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 97961 steps/s (collection: 0.808s, learning 0.195s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.8496
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 13.7988
                       Mean reward: 4.49
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 0.1678
     Episode_Reward/lifting_object: 0.7604
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.00s
                      Time elapsed: 00:01:05
                               ETA: 00:44:09

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 98495 steps/s (collection: 0.877s, learning 0.122s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.6855
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.8479
                       Mean reward: 4.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1675
     Episode_Reward/lifting_object: 0.8597
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.00s
                      Time elapsed: 00:01:06
                               ETA: 00:43:53

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 100884 steps/s (collection: 0.856s, learning 0.118s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.0758
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 13.8978
                       Mean reward: 3.80
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.1639
     Episode_Reward/lifting_object: 0.8781
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.97s
                      Time elapsed: 00:01:07
                               ETA: 00:43:37

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 112243 steps/s (collection: 0.771s, learning 0.105s)
             Mean action noise std: 1.38
          Mean value_function loss: 1.4796
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 13.9190
                       Mean reward: 4.75
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.1612
     Episode_Reward/lifting_object: 0.7014
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.88s
                      Time elapsed: 00:01:07
                               ETA: 00:43:18

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 96930 steps/s (collection: 0.916s, learning 0.099s)
             Mean action noise std: 1.39
          Mean value_function loss: 1.2658
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.9437
                       Mean reward: 5.59
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.1641
     Episode_Reward/lifting_object: 0.8070
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.01s
                      Time elapsed: 00:01:08
                               ETA: 00:43:05

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 111450 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 1.40
          Mean value_function loss: 1.2563
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.0002
                       Mean reward: 4.92
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.1602
     Episode_Reward/lifting_object: 0.8685
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.88s
                      Time elapsed: 00:01:09
                               ETA: 00:42:47

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 108681 steps/s (collection: 0.813s, learning 0.092s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.2433
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.0381
                       Mean reward: 5.62
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.1601
     Episode_Reward/lifting_object: 0.9255
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.90s
                      Time elapsed: 00:01:10
                               ETA: 00:42:31

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 114370 steps/s (collection: 0.772s, learning 0.088s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.1436
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.0992
                       Mean reward: 5.27
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.1550
     Episode_Reward/lifting_object: 0.9198
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.86s
                      Time elapsed: 00:01:11
                               ETA: 00:42:14

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 111409 steps/s (collection: 0.778s, learning 0.104s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.9760
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.1489
                       Mean reward: 4.28
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.1519
     Episode_Reward/lifting_object: 0.8376
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.88s
                      Time elapsed: 00:01:12
                               ETA: 00:41:58

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 106310 steps/s (collection: 0.796s, learning 0.128s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.7073
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.2181
                       Mean reward: 3.64
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.1512
     Episode_Reward/lifting_object: 0.7704
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.92s
                      Time elapsed: 00:01:13
                               ETA: 00:41:44

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 106791 steps/s (collection: 0.812s, learning 0.109s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.7753
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.2734
                       Mean reward: 4.28
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.1519
     Episode_Reward/lifting_object: 0.8750
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.92s
                      Time elapsed: 00:01:14
                               ETA: 00:41:30

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 96072 steps/s (collection: 0.892s, learning 0.132s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.9767
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.3357
                       Mean reward: 4.24
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.1451
     Episode_Reward/lifting_object: 0.8488
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.02s
                      Time elapsed: 00:01:15
                               ETA: 00:41:20

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 101899 steps/s (collection: 0.819s, learning 0.146s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.0848
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.3713
                       Mean reward: 4.81
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.1505
     Episode_Reward/lifting_object: 0.8509
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.96s
                      Time elapsed: 00:01:16
                               ETA: 00:41:09

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 97897 steps/s (collection: 0.861s, learning 0.143s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.1065
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.3898
                       Mean reward: 3.14
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.1514
     Episode_Reward/lifting_object: 0.6387
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.00s
                      Time elapsed: 00:01:17
                               ETA: 00:40:59

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 108706 steps/s (collection: 0.800s, learning 0.104s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.1973
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.3994
                       Mean reward: 6.19
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 0.1506
     Episode_Reward/lifting_object: 0.8642
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.90s
                      Time elapsed: 00:01:18
                               ETA: 00:40:47

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 107696 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.8343
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.4130
                       Mean reward: 6.47
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 0.1453
     Episode_Reward/lifting_object: 0.9798
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.91s
                      Time elapsed: 00:01:19
                               ETA: 00:40:35

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 110182 steps/s (collection: 0.802s, learning 0.090s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.7641
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.4309
                       Mean reward: 5.70
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 0.1576
     Episode_Reward/lifting_object: 0.8771
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.89s
                      Time elapsed: 00:01:20
                               ETA: 00:40:22

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 110338 steps/s (collection: 0.802s, learning 0.089s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.4829
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.4588
                       Mean reward: 5.62
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.1546
     Episode_Reward/lifting_object: 0.8559
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.89s
                      Time elapsed: 00:01:20
                               ETA: 00:40:10

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 105714 steps/s (collection: 0.830s, learning 0.100s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.7812
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.4829
                       Mean reward: 6.97
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.1673
     Episode_Reward/lifting_object: 1.2474
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.93s
                      Time elapsed: 00:01:21
                               ETA: 00:40:00

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 104877 steps/s (collection: 0.849s, learning 0.089s)
             Mean action noise std: 1.49
          Mean value_function loss: 3.8819
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.5163
                       Mean reward: 7.85
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.1651
     Episode_Reward/lifting_object: 1.1727
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.94s
                      Time elapsed: 00:01:22
                               ETA: 00:39:50

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 102837 steps/s (collection: 0.843s, learning 0.113s)
             Mean action noise std: 1.49
          Mean value_function loss: 5.9628
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.5489
                       Mean reward: 1.44
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 0.1780
     Episode_Reward/lifting_object: 0.9701
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.96s
                      Time elapsed: 00:01:23
                               ETA: 00:39:41

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 106996 steps/s (collection: 0.804s, learning 0.115s)
             Mean action noise std: 1.50
          Mean value_function loss: 3.4189
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.5761
                       Mean reward: 5.43
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.1770
     Episode_Reward/lifting_object: 1.2152
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.92s
                      Time elapsed: 00:01:24
                               ETA: 00:39:31

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 82403 steps/s (collection: 1.037s, learning 0.156s)
             Mean action noise std: 1.51
          Mean value_function loss: 2.6223
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.6118
                       Mean reward: 5.24
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.1789
     Episode_Reward/lifting_object: 1.1968
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.19s
                      Time elapsed: 00:01:25
                               ETA: 00:39:28

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 95789 steps/s (collection: 0.899s, learning 0.128s)
             Mean action noise std: 1.51
          Mean value_function loss: 3.2728
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.6533
                       Mean reward: 9.67
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.1792
     Episode_Reward/lifting_object: 1.2687
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.03s
                      Time elapsed: 00:01:26
                               ETA: 00:39:22

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 101520 steps/s (collection: 0.876s, learning 0.092s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.0152
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.6919
                       Mean reward: 8.25
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 0.1805
     Episode_Reward/lifting_object: 1.3070
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.97s
                      Time elapsed: 00:01:27
                               ETA: 00:39:14

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 101669 steps/s (collection: 0.861s, learning 0.106s)
             Mean action noise std: 1.53
          Mean value_function loss: 5.4707
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7337
                       Mean reward: 6.42
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.1709
     Episode_Reward/lifting_object: 1.0571
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.97s
                      Time elapsed: 00:01:28
                               ETA: 00:39:06

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 101313 steps/s (collection: 0.862s, learning 0.108s)
             Mean action noise std: 1.53
          Mean value_function loss: 3.7724
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7567
                       Mean reward: 7.83
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 0.1871
     Episode_Reward/lifting_object: 1.3937
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.97s
                      Time elapsed: 00:01:29
                               ETA: 00:38:58

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 100955 steps/s (collection: 0.870s, learning 0.104s)
             Mean action noise std: 1.54
          Mean value_function loss: 4.9041
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7686
                       Mean reward: 8.06
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.1897
     Episode_Reward/lifting_object: 1.4939
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.97s
                      Time elapsed: 00:01:30
                               ETA: 00:38:51

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 107786 steps/s (collection: 0.797s, learning 0.115s)
             Mean action noise std: 1.54
          Mean value_function loss: 7.3636
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.7894
                       Mean reward: 9.16
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 0.1891
     Episode_Reward/lifting_object: 1.5858
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.91s
                      Time elapsed: 00:01:31
                               ETA: 00:38:42

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 99948 steps/s (collection: 0.879s, learning 0.105s)
             Mean action noise std: 1.55
          Mean value_function loss: 4.5134
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.8132
                       Mean reward: 8.30
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 0.1897
     Episode_Reward/lifting_object: 1.5716
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.98s
                      Time elapsed: 00:01:32
                               ETA: 00:38:35

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 104300 steps/s (collection: 0.826s, learning 0.116s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.5455
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8339
                       Mean reward: 10.48
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 0.1977
     Episode_Reward/lifting_object: 1.7123
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.94s
                      Time elapsed: 00:01:33
                               ETA: 00:38:28

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 104417 steps/s (collection: 0.829s, learning 0.113s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.3382
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.8431
                       Mean reward: 9.06
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 0.2024
     Episode_Reward/lifting_object: 1.7449
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.94s
                      Time elapsed: 00:01:34
                               ETA: 00:38:20

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 104641 steps/s (collection: 0.830s, learning 0.109s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.2559
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.8533
                       Mean reward: 12.53
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 0.2087
     Episode_Reward/lifting_object: 1.8864
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.94s
                      Time elapsed: 00:01:35
                               ETA: 00:38:13

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 100063 steps/s (collection: 0.864s, learning 0.119s)
             Mean action noise std: 1.55
          Mean value_function loss: 6.1827
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8668
                       Mean reward: 13.51
               Mean episode length: 237.80
    Episode_Reward/reaching_object: 0.2052
     Episode_Reward/lifting_object: 2.0482
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.98s
                      Time elapsed: 00:01:36
                               ETA: 00:38:07

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 100523 steps/s (collection: 0.841s, learning 0.137s)
             Mean action noise std: 1.56
          Mean value_function loss: 8.6669
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8765
                       Mean reward: 9.40
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 0.2072
     Episode_Reward/lifting_object: 2.0436
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.98s
                      Time elapsed: 00:01:37
                               ETA: 00:38:00

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 101717 steps/s (collection: 0.826s, learning 0.141s)
             Mean action noise std: 1.56
          Mean value_function loss: 7.3544
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.8966
                       Mean reward: 14.00
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 0.2146
     Episode_Reward/lifting_object: 2.6749
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.97s
                      Time elapsed: 00:01:38
                               ETA: 00:37:54

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 106505 steps/s (collection: 0.797s, learning 0.126s)
             Mean action noise std: 1.57
          Mean value_function loss: 8.0774
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.9194
                       Mean reward: 12.44
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 0.2108
     Episode_Reward/lifting_object: 2.1136
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.92s
                      Time elapsed: 00:01:39
                               ETA: 00:37:47

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 103698 steps/s (collection: 0.795s, learning 0.153s)
             Mean action noise std: 1.57
          Mean value_function loss: 7.4652
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9340
                       Mean reward: 14.25
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 0.2085
     Episode_Reward/lifting_object: 2.3013
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.95s
                      Time elapsed: 00:01:40
                               ETA: 00:37:40

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 99907 steps/s (collection: 0.862s, learning 0.122s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.3030
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.9385
                       Mean reward: 13.74
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.2078
     Episode_Reward/lifting_object: 2.6345
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.98s
                      Time elapsed: 00:01:41
                               ETA: 00:37:35

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 109178 steps/s (collection: 0.788s, learning 0.113s)
             Mean action noise std: 1.57
          Mean value_function loss: 10.1384
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9480
                       Mean reward: 13.89
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.2118
     Episode_Reward/lifting_object: 2.8477
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.90s
                      Time elapsed: 00:01:42
                               ETA: 00:37:28

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 98165 steps/s (collection: 0.892s, learning 0.109s)
             Mean action noise std: 1.57
          Mean value_function loss: 10.9676
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.9589
                       Mean reward: 11.35
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 0.2110
     Episode_Reward/lifting_object: 2.5409
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.00s
                      Time elapsed: 00:01:43
                               ETA: 00:37:23

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 108932 steps/s (collection: 0.809s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 8.5161
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9687
                       Mean reward: 21.35
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.2199
     Episode_Reward/lifting_object: 3.1391
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.90s
                      Time elapsed: 00:01:44
                               ETA: 00:37:16

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 100369 steps/s (collection: 0.876s, learning 0.103s)
             Mean action noise std: 1.58
          Mean value_function loss: 9.4015
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.9743
                       Mean reward: 17.68
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 0.2116
     Episode_Reward/lifting_object: 3.0317
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.98s
                      Time elapsed: 00:01:45
                               ETA: 00:37:10

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 105724 steps/s (collection: 0.827s, learning 0.103s)
             Mean action noise std: 1.58
          Mean value_function loss: 10.2090
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.9846
                       Mean reward: 18.17
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 0.2189
     Episode_Reward/lifting_object: 2.9380
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.93s
                      Time elapsed: 00:01:45
                               ETA: 00:37:04

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 105828 steps/s (collection: 0.819s, learning 0.110s)
             Mean action noise std: 1.58
          Mean value_function loss: 9.7998
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.0075
                       Mean reward: 19.82
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 0.2133
     Episode_Reward/lifting_object: 3.3115
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.93s
                      Time elapsed: 00:01:46
                               ETA: 00:36:58

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 99263 steps/s (collection: 0.892s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 9.5261
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.0193
                       Mean reward: 18.69
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 0.2166
     Episode_Reward/lifting_object: 2.7200
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.99s
                      Time elapsed: 00:01:47
                               ETA: 00:36:53

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 84234 steps/s (collection: 0.998s, learning 0.169s)
             Mean action noise std: 1.59
          Mean value_function loss: 10.4177
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.0351
                       Mean reward: 18.06
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.2243
     Episode_Reward/lifting_object: 3.4320
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.17s
                      Time elapsed: 00:01:49
                               ETA: 00:36:52

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 84184 steps/s (collection: 1.048s, learning 0.120s)
             Mean action noise std: 1.59
          Mean value_function loss: 10.2079
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.0534
                       Mean reward: 16.57
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 0.2215
     Episode_Reward/lifting_object: 3.4491
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.17s
                      Time elapsed: 00:01:50
                               ETA: 00:36:51

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 73226 steps/s (collection: 1.176s, learning 0.166s)
             Mean action noise std: 1.60
          Mean value_function loss: 7.5283
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.0696
                       Mean reward: 22.30
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 0.2208
     Episode_Reward/lifting_object: 3.6101
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.34s
                      Time elapsed: 00:01:51
                               ETA: 00:36:54

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 82239 steps/s (collection: 1.074s, learning 0.121s)
             Mean action noise std: 1.60
          Mean value_function loss: 10.2017
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.0734
                       Mean reward: 17.65
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 0.2215
     Episode_Reward/lifting_object: 3.2273
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.20s
                      Time elapsed: 00:01:52
                               ETA: 00:36:53

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 93582 steps/s (collection: 0.912s, learning 0.139s)
             Mean action noise std: 1.60
          Mean value_function loss: 7.2395
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.0896
                       Mean reward: 18.53
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.2197
     Episode_Reward/lifting_object: 3.2658
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.05s
                      Time elapsed: 00:01:53
                               ETA: 00:36:50

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 99760 steps/s (collection: 0.853s, learning 0.132s)
             Mean action noise std: 1.60
          Mean value_function loss: 9.0758
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.1070
                       Mean reward: 16.57
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.2203
     Episode_Reward/lifting_object: 3.4759
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.99s
                      Time elapsed: 00:01:54
                               ETA: 00:36:45

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 74449 steps/s (collection: 1.207s, learning 0.114s)
             Mean action noise std: 1.61
          Mean value_function loss: 9.1358
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.1137
                       Mean reward: 22.94
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.2226
     Episode_Reward/lifting_object: 3.9391
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.32s
                      Time elapsed: 00:01:56
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 87449 steps/s (collection: 0.982s, learning 0.142s)
             Mean action noise std: 1.61
          Mean value_function loss: 8.8266
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.1227
                       Mean reward: 16.69
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 0.2176
     Episode_Reward/lifting_object: 3.7557
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.12s
                      Time elapsed: 00:01:57
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 89519 steps/s (collection: 0.923s, learning 0.175s)
             Mean action noise std: 1.61
          Mean value_function loss: 9.6547
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.1343
                       Mean reward: 18.76
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 0.2163
     Episode_Reward/lifting_object: 3.4322
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.10s
                      Time elapsed: 00:01:58
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 95768 steps/s (collection: 0.923s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 8.7785
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.1396
                       Mean reward: 21.18
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 0.2211
     Episode_Reward/lifting_object: 3.7085
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.03s
                      Time elapsed: 00:01:59
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 98194 steps/s (collection: 0.849s, learning 0.152s)
             Mean action noise std: 1.61
          Mean value_function loss: 7.7563
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.1363
                       Mean reward: 19.36
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 0.2258
     Episode_Reward/lifting_object: 3.9114
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.00s
                      Time elapsed: 00:02:00
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 102927 steps/s (collection: 0.840s, learning 0.115s)
             Mean action noise std: 1.61
          Mean value_function loss: 9.0182
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.1426
                       Mean reward: 21.31
               Mean episode length: 230.33
    Episode_Reward/reaching_object: 0.2231
     Episode_Reward/lifting_object: 4.0041
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.96s
                      Time elapsed: 00:02:01
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 106011 steps/s (collection: 0.832s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 8.0295
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.1516
                       Mean reward: 24.16
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 0.2309
     Episode_Reward/lifting_object: 4.3768
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.93s
                      Time elapsed: 00:02:02
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 102895 steps/s (collection: 0.849s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 9.2780
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.1615
                       Mean reward: 23.13
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.2363
     Episode_Reward/lifting_object: 4.4924
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.96s
                      Time elapsed: 00:02:03
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 105654 steps/s (collection: 0.829s, learning 0.102s)
             Mean action noise std: 1.62
          Mean value_function loss: 9.5129
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.1707
                       Mean reward: 23.14
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 0.2364
     Episode_Reward/lifting_object: 4.4573
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.93s
                      Time elapsed: 00:02:04
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 97992 steps/s (collection: 0.873s, learning 0.130s)
             Mean action noise std: 1.62
          Mean value_function loss: 10.2455
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.1842
                       Mean reward: 23.69
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 0.2339
     Episode_Reward/lifting_object: 4.5777
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.00s
                      Time elapsed: 00:02:05
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 99580 steps/s (collection: 0.874s, learning 0.113s)
             Mean action noise std: 1.62
          Mean value_function loss: 10.1828
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.1977
                       Mean reward: 24.91
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 0.2351
     Episode_Reward/lifting_object: 4.3696
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.99s
                      Time elapsed: 00:02:06
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 93833 steps/s (collection: 0.873s, learning 0.175s)
             Mean action noise std: 1.63
          Mean value_function loss: 9.5854
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.2026
                       Mean reward: 23.31
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 0.2357
     Episode_Reward/lifting_object: 4.4298
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.05s
                      Time elapsed: 00:02:07
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 102019 steps/s (collection: 0.871s, learning 0.093s)
             Mean action noise std: 1.63
          Mean value_function loss: 12.2739
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.2087
                       Mean reward: 20.38
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 0.2338
     Episode_Reward/lifting_object: 4.3164
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.96s
                      Time elapsed: 00:02:08
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 105970 steps/s (collection: 0.835s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 9.2090
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.2189
                       Mean reward: 23.45
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.2353
     Episode_Reward/lifting_object: 4.6601
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.93s
                      Time elapsed: 00:02:09
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 104671 steps/s (collection: 0.848s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 9.4854
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.2320
                       Mean reward: 27.02
               Mean episode length: 227.90
    Episode_Reward/reaching_object: 0.2388
     Episode_Reward/lifting_object: 5.0230
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.94s
                      Time elapsed: 00:02:10
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 97835 steps/s (collection: 0.825s, learning 0.180s)
             Mean action noise std: 1.64
          Mean value_function loss: 10.7756
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.2438
                       Mean reward: 26.41
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 0.2373
     Episode_Reward/lifting_object: 4.5775
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.00s
                      Time elapsed: 00:02:11
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 96916 steps/s (collection: 0.904s, learning 0.111s)
             Mean action noise std: 1.64
          Mean value_function loss: 9.4812
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.2571
                       Mean reward: 25.85
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 0.2427
     Episode_Reward/lifting_object: 4.7921
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.01s
                      Time elapsed: 00:02:12
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 103931 steps/s (collection: 0.838s, learning 0.108s)
             Mean action noise std: 1.64
          Mean value_function loss: 9.4427
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.2692
                       Mean reward: 25.35
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 0.2405
     Episode_Reward/lifting_object: 5.2949
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.95s
                      Time elapsed: 00:02:12
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 102474 steps/s (collection: 0.844s, learning 0.116s)
             Mean action noise std: 1.64
          Mean value_function loss: 10.4970
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.2687
                       Mean reward: 26.49
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.2434
     Episode_Reward/lifting_object: 4.9942
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.96s
                      Time elapsed: 00:02:13
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 109774 steps/s (collection: 0.790s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.1938
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.2630
                       Mean reward: 29.83
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 0.2454
     Episode_Reward/lifting_object: 5.3002
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.90s
                      Time elapsed: 00:02:14
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 106165 steps/s (collection: 0.825s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.3763
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.2584
                       Mean reward: 28.33
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 0.2474
     Episode_Reward/lifting_object: 5.5783
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.93s
                      Time elapsed: 00:02:15
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 110948 steps/s (collection: 0.800s, learning 0.086s)
             Mean action noise std: 1.64
          Mean value_function loss: 14.9731
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.2650
                       Mean reward: 24.75
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.2412
     Episode_Reward/lifting_object: 5.4765
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.89s
                      Time elapsed: 00:02:16
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 105719 steps/s (collection: 0.810s, learning 0.120s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.5615
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.2621
                       Mean reward: 30.88
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 0.2420
     Episode_Reward/lifting_object: 5.2548
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.93s
                      Time elapsed: 00:02:17
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 104993 steps/s (collection: 0.798s, learning 0.139s)
             Mean action noise std: 1.64
          Mean value_function loss: 12.1664
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.2678
                       Mean reward: 27.13
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 0.2483
     Episode_Reward/lifting_object: 5.2371
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.94s
                      Time elapsed: 00:02:18
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 100217 steps/s (collection: 0.848s, learning 0.133s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.6355
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.2720
                       Mean reward: 24.64
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 0.2442
     Episode_Reward/lifting_object: 5.1815
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.98s
                      Time elapsed: 00:02:19
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 108874 steps/s (collection: 0.804s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 12.3402
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 15.2746
                       Mean reward: 27.87
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 0.2460
     Episode_Reward/lifting_object: 5.8170
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.90s
                      Time elapsed: 00:02:20
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 104165 steps/s (collection: 0.814s, learning 0.130s)
             Mean action noise std: 1.64
          Mean value_function loss: 12.5498
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.2761
                       Mean reward: 33.15
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 0.2530
     Episode_Reward/lifting_object: 5.9442
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.94s
                      Time elapsed: 00:02:21
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 105540 steps/s (collection: 0.824s, learning 0.107s)
             Mean action noise std: 1.65
          Mean value_function loss: 12.4226
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2786
                       Mean reward: 28.22
               Mean episode length: 226.65
    Episode_Reward/reaching_object: 0.2599
     Episode_Reward/lifting_object: 6.2806
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.93s
                      Time elapsed: 00:02:22
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 100468 steps/s (collection: 0.876s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 12.7045
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2816
                       Mean reward: 40.66
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 0.2562
     Episode_Reward/lifting_object: 6.5094
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.98s
                      Time elapsed: 00:02:23
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 106061 steps/s (collection: 0.814s, learning 0.113s)
             Mean action noise std: 1.65
          Mean value_function loss: 16.7650
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.2828
                       Mean reward: 32.33
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.2529
     Episode_Reward/lifting_object: 6.4654
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.93s
                      Time elapsed: 00:02:24
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 102673 steps/s (collection: 0.856s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 13.8769
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.2859
                       Mean reward: 28.83
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 0.2551
     Episode_Reward/lifting_object: 6.0058
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.96s
                      Time elapsed: 00:02:25
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 84166 steps/s (collection: 0.975s, learning 0.193s)
             Mean action noise std: 1.65
          Mean value_function loss: 19.2910
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.2842
                       Mean reward: 33.90
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.2646
     Episode_Reward/lifting_object: 6.8511
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.17s
                      Time elapsed: 00:02:26
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 79255 steps/s (collection: 1.070s, learning 0.170s)
             Mean action noise std: 1.65
          Mean value_function loss: 16.2093
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.2843
                       Mean reward: 35.89
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.2590
     Episode_Reward/lifting_object: 7.0843
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.24s
                      Time elapsed: 00:02:27
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 85799 steps/s (collection: 0.981s, learning 0.165s)
             Mean action noise std: 1.64
          Mean value_function loss: 19.1645
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.2746
                       Mean reward: 40.57
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 0.2659
     Episode_Reward/lifting_object: 7.5689
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.15s
                      Time elapsed: 00:02:28
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 83741 steps/s (collection: 1.027s, learning 0.147s)
             Mean action noise std: 1.65
          Mean value_function loss: 14.3053
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 15.2721
                       Mean reward: 40.12
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 0.2591
     Episode_Reward/lifting_object: 7.1806
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.17s
                      Time elapsed: 00:02:29
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 85655 steps/s (collection: 1.036s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 16.9458
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.2766
                       Mean reward: 44.30
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 0.2691
     Episode_Reward/lifting_object: 7.8862
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.15s
                      Time elapsed: 00:02:31
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 77824 steps/s (collection: 1.064s, learning 0.199s)
             Mean action noise std: 1.65
          Mean value_function loss: 17.2081
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.2785
                       Mean reward: 41.61
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 0.2681
     Episode_Reward/lifting_object: 7.7865
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.26s
                      Time elapsed: 00:02:32
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 83052 steps/s (collection: 1.030s, learning 0.154s)
             Mean action noise std: 1.65
          Mean value_function loss: 15.7005
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.2799
                       Mean reward: 40.80
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 0.2694
     Episode_Reward/lifting_object: 7.9418
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.18s
                      Time elapsed: 00:02:33
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 99432 steps/s (collection: 0.889s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 19.1227
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.2834
                       Mean reward: 43.40
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 0.2704
     Episode_Reward/lifting_object: 8.0712
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.99s
                      Time elapsed: 00:02:34
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 98359 steps/s (collection: 0.833s, learning 0.166s)
             Mean action noise std: 1.65
          Mean value_function loss: 15.7751
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.2891
                       Mean reward: 43.68
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 0.2757
     Episode_Reward/lifting_object: 8.2148
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.00s
                      Time elapsed: 00:02:35
                               ETA: 00:34:42

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 97026 steps/s (collection: 0.840s, learning 0.173s)
             Mean action noise std: 1.65
          Mean value_function loss: 15.5363
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.2943
                       Mean reward: 39.39
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 0.2769
     Episode_Reward/lifting_object: 7.9546
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.01s
                      Time elapsed: 00:02:36
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 83594 steps/s (collection: 0.973s, learning 0.203s)
             Mean action noise std: 1.65
          Mean value_function loss: 17.0674
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.2951
                       Mean reward: 46.22
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 0.2783
     Episode_Reward/lifting_object: 8.3539
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.18s
                      Time elapsed: 00:02:37
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 108536 steps/s (collection: 0.813s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 17.0297
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.2932
                       Mean reward: 45.52
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 0.2705
     Episode_Reward/lifting_object: 8.1473
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.91s
                      Time elapsed: 00:02:38
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 98467 steps/s (collection: 0.833s, learning 0.166s)
             Mean action noise std: 1.65
          Mean value_function loss: 15.9194
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.2941
                       Mean reward: 40.56
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 0.2698
     Episode_Reward/lifting_object: 8.3640
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.00s
                      Time elapsed: 00:02:39
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 102100 steps/s (collection: 0.865s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 16.3303
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.3011
                       Mean reward: 41.89
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 0.2721
     Episode_Reward/lifting_object: 8.4405
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.96s
                      Time elapsed: 00:02:40
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 96610 steps/s (collection: 0.829s, learning 0.189s)
             Mean action noise std: 1.65
          Mean value_function loss: 20.3993
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.3042
                       Mean reward: 45.62
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 9.0494
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.02s
                      Time elapsed: 00:02:41
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 95462 steps/s (collection: 0.909s, learning 0.121s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.0875
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.3057
                       Mean reward: 48.49
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 0.2762
     Episode_Reward/lifting_object: 8.9914
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.03s
                      Time elapsed: 00:02:42
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 101433 steps/s (collection: 0.865s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.1002
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.3052
                       Mean reward: 44.50
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 0.2727
     Episode_Reward/lifting_object: 8.9871
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.97s
                      Time elapsed: 00:02:43
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 104344 steps/s (collection: 0.839s, learning 0.104s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.6574
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.3064
                       Mean reward: 43.27
               Mean episode length: 218.69
    Episode_Reward/reaching_object: 0.2738
     Episode_Reward/lifting_object: 8.6882
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.94s
                      Time elapsed: 00:02:44
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 103425 steps/s (collection: 0.827s, learning 0.123s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.0555
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.3095
                       Mean reward: 48.62
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 0.2812
     Episode_Reward/lifting_object: 9.0689
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.95s
                      Time elapsed: 00:02:45
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 102713 steps/s (collection: 0.845s, learning 0.112s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.4068
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.3138
                       Mean reward: 48.98
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 0.2764
     Episode_Reward/lifting_object: 8.7531
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.96s
                      Time elapsed: 00:02:46
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 102756 steps/s (collection: 0.827s, learning 0.130s)
             Mean action noise std: 1.66
          Mean value_function loss: 16.4233
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.3173
                       Mean reward: 45.52
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 0.2660
     Episode_Reward/lifting_object: 8.6307
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.96s
                      Time elapsed: 00:02:47
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 107094 steps/s (collection: 0.810s, learning 0.108s)
             Mean action noise std: 1.66
          Mean value_function loss: 16.3378
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.3179
                       Mean reward: 45.22
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 0.2714
     Episode_Reward/lifting_object: 8.7277
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.92s
                      Time elapsed: 00:02:48
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 105046 steps/s (collection: 0.845s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.1229
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.3191
                       Mean reward: 41.95
               Mean episode length: 223.22
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: 9.0930
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.94s
                      Time elapsed: 00:02:49
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 109409 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 18.3452
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.3180
                       Mean reward: 47.77
               Mean episode length: 225.72
    Episode_Reward/reaching_object: 0.2705
     Episode_Reward/lifting_object: 9.0089
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.90s
                      Time elapsed: 00:02:50
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 103955 steps/s (collection: 0.849s, learning 0.097s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.6840
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3189
                       Mean reward: 50.60
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 0.2719
     Episode_Reward/lifting_object: 9.1159
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.95s
                      Time elapsed: 00:02:51
                               ETA: 00:33:56

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 93191 steps/s (collection: 0.944s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 20.5714
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.3220
                       Mean reward: 51.82
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: 8.9721
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.05s
                      Time elapsed: 00:02:52
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 96011 steps/s (collection: 0.936s, learning 0.087s)
             Mean action noise std: 1.66
          Mean value_function loss: 17.7977
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.3236
                       Mean reward: 48.43
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 0.2724
     Episode_Reward/lifting_object: 9.1194
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.02s
                      Time elapsed: 00:02:53
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 108004 steps/s (collection: 0.819s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 18.4630
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.3261
                       Mean reward: 41.62
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 0.2736
     Episode_Reward/lifting_object: 9.2230
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.91s
                      Time elapsed: 00:02:54
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 103025 steps/s (collection: 0.850s, learning 0.104s)
             Mean action noise std: 1.66
          Mean value_function loss: 22.7732
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.3323
                       Mean reward: 41.95
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 0.2728
     Episode_Reward/lifting_object: 8.8272
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.95s
                      Time elapsed: 00:02:54
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 109659 steps/s (collection: 0.803s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.6243
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.3373
                       Mean reward: 48.30
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 10.3445
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.90s
                      Time elapsed: 00:02:55
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 94974 steps/s (collection: 0.923s, learning 0.112s)
             Mean action noise std: 1.67
          Mean value_function loss: 20.4493
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.3390
                       Mean reward: 50.36
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 0.2743
     Episode_Reward/lifting_object: 9.4962
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.04s
                      Time elapsed: 00:02:56
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 92323 steps/s (collection: 0.946s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 19.8813
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.3387
                       Mean reward: 47.96
               Mean episode length: 219.62
    Episode_Reward/reaching_object: 0.2743
     Episode_Reward/lifting_object: 9.7157
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.06s
                      Time elapsed: 00:02:57
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 101513 steps/s (collection: 0.843s, learning 0.126s)
             Mean action noise std: 1.67
          Mean value_function loss: 21.4615
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.3383
                       Mean reward: 45.13
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 9.3419
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.97s
                      Time elapsed: 00:02:58
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 101522 steps/s (collection: 0.864s, learning 0.105s)
             Mean action noise std: 1.67
          Mean value_function loss: 24.1516
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 15.3397
                       Mean reward: 44.65
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 0.2830
     Episode_Reward/lifting_object: 9.5109
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.97s
                      Time elapsed: 00:02:59
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 105920 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 1.67
          Mean value_function loss: 21.6784
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.3401
                       Mean reward: 55.72
               Mean episode length: 222.20
    Episode_Reward/reaching_object: 0.2844
     Episode_Reward/lifting_object: 10.6743
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.93s
                      Time elapsed: 00:03:00
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 91593 steps/s (collection: 0.954s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 21.9956
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.3384
                       Mean reward: 48.51
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 0.2892
     Episode_Reward/lifting_object: 10.8337
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.07s
                      Time elapsed: 00:03:01
                               ETA: 00:33:30

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 97657 steps/s (collection: 0.872s, learning 0.135s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.7856
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.3350
                       Mean reward: 48.06
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 0.2871
     Episode_Reward/lifting_object: 10.7698
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.01s
                      Time elapsed: 00:03:02
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 103719 steps/s (collection: 0.842s, learning 0.106s)
             Mean action noise std: 1.67
          Mean value_function loss: 30.1846
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.3320
                       Mean reward: 57.75
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 0.2900
     Episode_Reward/lifting_object: 10.8813
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.95s
                      Time elapsed: 00:03:03
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 102724 steps/s (collection: 0.818s, learning 0.139s)
             Mean action noise std: 1.67
          Mean value_function loss: 24.2043
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.3346
                       Mean reward: 52.98
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 10.6663
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.96s
                      Time elapsed: 00:03:04
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 103269 steps/s (collection: 0.854s, learning 0.098s)
             Mean action noise std: 1.67
          Mean value_function loss: 24.6547
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.3401
                       Mean reward: 57.74
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.2844
     Episode_Reward/lifting_object: 10.4790
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.95s
                      Time elapsed: 00:03:05
                               ETA: 00:33:20

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 102522 steps/s (collection: 0.870s, learning 0.089s)
             Mean action noise std: 1.67
          Mean value_function loss: 28.7819
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.3429
                       Mean reward: 57.44
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 0.2919
     Episode_Reward/lifting_object: 11.5089
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.96s
                      Time elapsed: 00:03:06
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 100062 steps/s (collection: 0.875s, learning 0.107s)
             Mean action noise std: 1.67
          Mean value_function loss: 26.1315
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.3469
                       Mean reward: 64.97
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 0.2957
     Episode_Reward/lifting_object: 11.7669
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.98s
                      Time elapsed: 00:03:07
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 103958 steps/s (collection: 0.859s, learning 0.087s)
             Mean action noise std: 1.67
          Mean value_function loss: 29.4114
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.3513
                       Mean reward: 63.18
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 0.2924
     Episode_Reward/lifting_object: 10.9434
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.95s
                      Time elapsed: 00:03:08
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 105540 steps/s (collection: 0.841s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 27.6058
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.3561
                       Mean reward: 58.40
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 10.8956
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.93s
                      Time elapsed: 00:03:09
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 105494 steps/s (collection: 0.821s, learning 0.111s)
             Mean action noise std: 1.67
          Mean value_function loss: 28.0730
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 15.3604
                       Mean reward: 58.69
               Mean episode length: 221.88
    Episode_Reward/reaching_object: 0.2828
     Episode_Reward/lifting_object: 10.6541
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.93s
                      Time elapsed: 00:03:10
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 105734 steps/s (collection: 0.830s, learning 0.100s)
             Mean action noise std: 1.67
          Mean value_function loss: 32.2542
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 15.3608
                       Mean reward: 45.61
               Mean episode length: 223.18
    Episode_Reward/reaching_object: 0.2936
     Episode_Reward/lifting_object: 10.8717
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.93s
                      Time elapsed: 00:03:11
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 106013 steps/s (collection: 0.831s, learning 0.096s)
             Mean action noise std: 1.67
          Mean value_function loss: 31.0689
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.3610
                       Mean reward: 68.20
               Mean episode length: 216.15
    Episode_Reward/reaching_object: 0.2916
     Episode_Reward/lifting_object: 12.1588
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.93s
                      Time elapsed: 00:03:12
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 103360 steps/s (collection: 0.823s, learning 0.128s)
             Mean action noise std: 1.67
          Mean value_function loss: 29.2784
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.3604
                       Mean reward: 61.18
               Mean episode length: 218.07
    Episode_Reward/reaching_object: 0.2979
     Episode_Reward/lifting_object: 12.3672
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.95s
                      Time elapsed: 00:03:13
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 103344 steps/s (collection: 0.833s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 33.4917
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3600
                       Mean reward: 70.75
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 0.3017
     Episode_Reward/lifting_object: 12.3681
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.95s
                      Time elapsed: 00:03:14
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 99611 steps/s (collection: 0.858s, learning 0.129s)
             Mean action noise std: 1.67
          Mean value_function loss: 33.5382
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 15.3602
                       Mean reward: 68.22
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 0.2969
     Episode_Reward/lifting_object: 13.2143
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.99s
                      Time elapsed: 00:03:15
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 101343 steps/s (collection: 0.834s, learning 0.136s)
             Mean action noise std: 1.67
          Mean value_function loss: 30.6529
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.3637
                       Mean reward: 60.88
               Mean episode length: 223.53
    Episode_Reward/reaching_object: 0.2926
     Episode_Reward/lifting_object: 11.5440
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.97s
                      Time elapsed: 00:03:16
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 105428 steps/s (collection: 0.832s, learning 0.101s)
             Mean action noise std: 1.68
          Mean value_function loss: 29.0661
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.3688
                       Mean reward: 75.25
               Mean episode length: 215.65
    Episode_Reward/reaching_object: 0.2947
     Episode_Reward/lifting_object: 13.8739
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.93s
                      Time elapsed: 00:03:17
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 106750 steps/s (collection: 0.820s, learning 0.101s)
             Mean action noise std: 1.68
          Mean value_function loss: 28.1378
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.3712
                       Mean reward: 63.71
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 0.2976
     Episode_Reward/lifting_object: 12.5523
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.92s
                      Time elapsed: 00:03:18
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 105514 steps/s (collection: 0.839s, learning 0.093s)
             Mean action noise std: 1.68
          Mean value_function loss: 28.2918
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.3719
                       Mean reward: 62.49
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 0.2902
     Episode_Reward/lifting_object: 12.7957
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.93s
                      Time elapsed: 00:03:19
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 99593 steps/s (collection: 0.884s, learning 0.103s)
             Mean action noise std: 1.68
          Mean value_function loss: 30.9840
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.3769
                       Mean reward: 61.52
               Mean episode length: 218.41
    Episode_Reward/reaching_object: 0.2888
     Episode_Reward/lifting_object: 12.5763
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.99s
                      Time elapsed: 00:03:20
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 104612 steps/s (collection: 0.836s, learning 0.104s)
             Mean action noise std: 1.68
          Mean value_function loss: 36.5598
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.3858
                       Mean reward: 59.94
               Mean episode length: 213.37
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 11.8599
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.94s
                      Time elapsed: 00:03:20
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 92052 steps/s (collection: 0.883s, learning 0.185s)
             Mean action noise std: 1.68
          Mean value_function loss: 27.5473
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.3904
                       Mean reward: 64.49
               Mean episode length: 212.64
    Episode_Reward/reaching_object: 0.2910
     Episode_Reward/lifting_object: 13.1858
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.07s
                      Time elapsed: 00:03:22
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 93744 steps/s (collection: 0.931s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 35.5955
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.3991
                       Mean reward: 62.22
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 13.3319
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.05s
                      Time elapsed: 00:03:23
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 104301 steps/s (collection: 0.841s, learning 0.102s)
             Mean action noise std: 1.68
          Mean value_function loss: 32.0313
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.4062
                       Mean reward: 63.39
               Mean episode length: 224.47
    Episode_Reward/reaching_object: 0.2869
     Episode_Reward/lifting_object: 12.2039
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.94s
                      Time elapsed: 00:03:24
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 104383 steps/s (collection: 0.816s, learning 0.126s)
             Mean action noise std: 1.68
          Mean value_function loss: 26.6176
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.4073
                       Mean reward: 58.41
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 0.2896
     Episode_Reward/lifting_object: 12.5264
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.94s
                      Time elapsed: 00:03:24
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 103096 steps/s (collection: 0.853s, learning 0.100s)
             Mean action noise std: 1.68
          Mean value_function loss: 26.2961
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.4052
                       Mean reward: 58.15
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 0.2821
     Episode_Reward/lifting_object: 11.2070
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.95s
                      Time elapsed: 00:03:25
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 103158 steps/s (collection: 0.856s, learning 0.097s)
             Mean action noise std: 1.68
          Mean value_function loss: 33.5685
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.4035
                       Mean reward: 58.62
               Mean episode length: 223.54
    Episode_Reward/reaching_object: 0.2912
     Episode_Reward/lifting_object: 11.9638
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.95s
                      Time elapsed: 00:03:26
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 102210 steps/s (collection: 0.860s, learning 0.102s)
             Mean action noise std: 1.68
          Mean value_function loss: 34.4657
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.4017
                       Mean reward: 54.50
               Mean episode length: 224.98
    Episode_Reward/reaching_object: 0.2792
     Episode_Reward/lifting_object: 11.1555
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.96s
                      Time elapsed: 00:03:27
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 104969 steps/s (collection: 0.840s, learning 0.096s)
             Mean action noise std: 1.68
          Mean value_function loss: 35.4146
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.4033
                       Mean reward: 68.81
               Mean episode length: 223.53
    Episode_Reward/reaching_object: 0.2927
     Episode_Reward/lifting_object: 12.9323
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.94s
                      Time elapsed: 00:03:28
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 94448 steps/s (collection: 0.926s, learning 0.115s)
             Mean action noise std: 1.68
          Mean value_function loss: 38.6015
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4068
                       Mean reward: 67.41
               Mean episode length: 216.56
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 12.2062
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.04s
                      Time elapsed: 00:03:29
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 104177 steps/s (collection: 0.826s, learning 0.118s)
             Mean action noise std: 1.69
          Mean value_function loss: 42.5533
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.4092
                       Mean reward: 58.56
               Mean episode length: 219.14
    Episode_Reward/reaching_object: 0.2854
     Episode_Reward/lifting_object: 13.2313
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.94s
                      Time elapsed: 00:03:30
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 95888 steps/s (collection: 0.920s, learning 0.105s)
             Mean action noise std: 1.69
          Mean value_function loss: 38.5507
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.4127
                       Mean reward: 65.47
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 0.2953
     Episode_Reward/lifting_object: 13.3688
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.03s
                      Time elapsed: 00:03:31
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 95864 steps/s (collection: 0.911s, learning 0.115s)
             Mean action noise std: 1.69
          Mean value_function loss: 35.0943
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.4192
                       Mean reward: 67.44
               Mean episode length: 220.80
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 13.0251
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.03s
                      Time elapsed: 00:03:32
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 97383 steps/s (collection: 0.908s, learning 0.102s)
             Mean action noise std: 1.69
          Mean value_function loss: 41.5068
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 15.4222
                       Mean reward: 52.53
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 12.4512
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.01s
                      Time elapsed: 00:03:33
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 100834 steps/s (collection: 0.879s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 38.9241
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.4233
                       Mean reward: 75.47
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 0.2969
     Episode_Reward/lifting_object: 13.9234
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.97s
                      Time elapsed: 00:03:34
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 96860 steps/s (collection: 0.890s, learning 0.125s)
             Mean action noise std: 1.69
          Mean value_function loss: 41.9679
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.4234
                       Mean reward: 71.81
               Mean episode length: 225.70
    Episode_Reward/reaching_object: 0.3027
     Episode_Reward/lifting_object: 15.3235
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.01s
                      Time elapsed: 00:03:35
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 99256 steps/s (collection: 0.903s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 50.3553
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.4243
                       Mean reward: 61.85
               Mean episode length: 219.53
    Episode_Reward/reaching_object: 0.3019
     Episode_Reward/lifting_object: 14.3796
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.99s
                      Time elapsed: 00:03:36
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 107572 steps/s (collection: 0.823s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 53.9038
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.4275
                       Mean reward: 83.26
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 0.3199
     Episode_Reward/lifting_object: 17.0162
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.91s
                      Time elapsed: 00:03:37
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 101864 steps/s (collection: 0.857s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 58.6130
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.4281
                       Mean reward: 76.63
               Mean episode length: 213.57
    Episode_Reward/reaching_object: 0.3059
     Episode_Reward/lifting_object: 14.1254
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.97s
                      Time elapsed: 00:03:38
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 104445 steps/s (collection: 0.852s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 54.2024
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.4278
                       Mean reward: 69.52
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 0.3196
     Episode_Reward/lifting_object: 14.9955
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.94s
                      Time elapsed: 00:03:39
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 103423 steps/s (collection: 0.862s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 52.6616
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.4283
                       Mean reward: 89.43
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 0.3188
     Episode_Reward/lifting_object: 16.2280
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.95s
                      Time elapsed: 00:03:40
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 102262 steps/s (collection: 0.871s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 54.7775
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.4293
                       Mean reward: 94.49
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 0.3228
     Episode_Reward/lifting_object: 18.2323
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.96s
                      Time elapsed: 00:03:41
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 102385 steps/s (collection: 0.851s, learning 0.110s)
             Mean action noise std: 1.69
          Mean value_function loss: 47.4590
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.4305
                       Mean reward: 80.31
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 0.3143
     Episode_Reward/lifting_object: 15.5728
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.96s
                      Time elapsed: 00:03:42
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 97566 steps/s (collection: 0.905s, learning 0.103s)
             Mean action noise std: 1.69
          Mean value_function loss: 53.3095
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4307
                       Mean reward: 88.97
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 0.3286
     Episode_Reward/lifting_object: 17.9450
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.01s
                      Time elapsed: 00:03:43
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 101927 steps/s (collection: 0.851s, learning 0.114s)
             Mean action noise std: 1.69
          Mean value_function loss: 52.0264
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.4331
                       Mean reward: 69.65
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 16.3163
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.96s
                      Time elapsed: 00:03:44
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 103085 steps/s (collection: 0.843s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 45.8352
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4384
                       Mean reward: 88.47
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 0.3210
     Episode_Reward/lifting_object: 16.9537
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.95s
                      Time elapsed: 00:03:45
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 93971 steps/s (collection: 0.927s, learning 0.119s)
             Mean action noise std: 1.69
          Mean value_function loss: 51.9031
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.4406
                       Mean reward: 81.47
               Mean episode length: 219.77
    Episode_Reward/reaching_object: 0.3177
     Episode_Reward/lifting_object: 16.7553
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.05s
                      Time elapsed: 00:03:46
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 92888 steps/s (collection: 0.884s, learning 0.175s)
             Mean action noise std: 1.69
          Mean value_function loss: 64.0878
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.4421
                       Mean reward: 82.21
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 0.3165
     Episode_Reward/lifting_object: 17.4693
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.06s
                      Time elapsed: 00:03:47
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 105414 steps/s (collection: 0.823s, learning 0.110s)
             Mean action noise std: 1.69
          Mean value_function loss: 58.1371
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4425
                       Mean reward: 99.56
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: 16.3801
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.93s
                      Time elapsed: 00:03:48
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 87745 steps/s (collection: 1.021s, learning 0.100s)
             Mean action noise std: 1.69
          Mean value_function loss: 60.5205
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.4410
                       Mean reward: 80.82
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 0.3129
     Episode_Reward/lifting_object: 17.2699
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.12s
                      Time elapsed: 00:03:49
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 106117 steps/s (collection: 0.809s, learning 0.118s)
             Mean action noise std: 1.70
          Mean value_function loss: 61.0117
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.4441
                       Mean reward: 89.66
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 0.3146
     Episode_Reward/lifting_object: 16.1505
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.93s
                      Time elapsed: 00:03:50
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 103995 steps/s (collection: 0.826s, learning 0.119s)
             Mean action noise std: 1.70
          Mean value_function loss: 61.0948
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.4502
                       Mean reward: 87.16
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 0.3055
     Episode_Reward/lifting_object: 16.5795
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.95s
                      Time elapsed: 00:03:51
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 94616 steps/s (collection: 0.940s, learning 0.099s)
             Mean action noise std: 1.70
          Mean value_function loss: 67.9297
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.4521
                       Mean reward: 84.07
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 0.3192
     Episode_Reward/lifting_object: 18.4522
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.04s
                      Time elapsed: 00:03:52
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 105848 steps/s (collection: 0.826s, learning 0.103s)
             Mean action noise std: 1.70
          Mean value_function loss: 64.8585
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.4527
                       Mean reward: 84.21
               Mean episode length: 214.27
    Episode_Reward/reaching_object: 0.3041
     Episode_Reward/lifting_object: 15.8731
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.93s
                      Time elapsed: 00:03:53
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 106422 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 1.70
          Mean value_function loss: 64.6078
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.4540
                       Mean reward: 82.71
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 0.3073
     Episode_Reward/lifting_object: 17.0609
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.92s
                      Time elapsed: 00:03:54
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 108036 steps/s (collection: 0.817s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 66.2726
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4562
                       Mean reward: 85.09
               Mean episode length: 231.47
    Episode_Reward/reaching_object: 0.3118
     Episode_Reward/lifting_object: 16.6093
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.91s
                      Time elapsed: 00:03:55
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 108398 steps/s (collection: 0.814s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 88.2076
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.4570
                       Mean reward: 89.89
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 0.3091
     Episode_Reward/lifting_object: 17.2717
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.91s
                      Time elapsed: 00:03:56
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 107618 steps/s (collection: 0.823s, learning 0.090s)
             Mean action noise std: 1.70
          Mean value_function loss: 69.7696
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.4612
                       Mean reward: 92.40
               Mean episode length: 229.75
    Episode_Reward/reaching_object: 0.3155
     Episode_Reward/lifting_object: 18.7014
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.91s
                      Time elapsed: 00:03:57
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 105934 steps/s (collection: 0.828s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 72.9493
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 15.4614
                       Mean reward: 92.92
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 0.3179
     Episode_Reward/lifting_object: 17.2028
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.93s
                      Time elapsed: 00:03:57
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 105646 steps/s (collection: 0.833s, learning 0.098s)
             Mean action noise std: 1.70
          Mean value_function loss: 71.5267
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.4622
                       Mean reward: 87.23
               Mean episode length: 223.83
    Episode_Reward/reaching_object: 0.3194
     Episode_Reward/lifting_object: 20.2820
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.93s
                      Time elapsed: 00:03:58
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 103750 steps/s (collection: 0.849s, learning 0.098s)
             Mean action noise std: 1.70
          Mean value_function loss: 89.8358
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.4652
                       Mean reward: 96.03
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 0.3238
     Episode_Reward/lifting_object: 18.6556
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.95s
                      Time elapsed: 00:03:59
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 100605 steps/s (collection: 0.883s, learning 0.095s)
             Mean action noise std: 1.70
          Mean value_function loss: 73.9055
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.4683
                       Mean reward: 116.98
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 0.3286
     Episode_Reward/lifting_object: 22.3145
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.98s
                      Time elapsed: 00:04:00
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 103620 steps/s (collection: 0.857s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 93.0599
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.4718
                       Mean reward: 80.67
               Mean episode length: 214.43
    Episode_Reward/reaching_object: 0.3091
     Episode_Reward/lifting_object: 19.4075
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.95s
                      Time elapsed: 00:04:01
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 103107 steps/s (collection: 0.840s, learning 0.113s)
             Mean action noise std: 1.70
          Mean value_function loss: 90.4380
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.4750
                       Mean reward: 100.27
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 0.3281
     Episode_Reward/lifting_object: 21.4684
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.95s
                      Time elapsed: 00:04:02
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 91696 steps/s (collection: 0.969s, learning 0.103s)
             Mean action noise std: 1.70
          Mean value_function loss: 102.0701
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.4774
                       Mean reward: 103.44
               Mean episode length: 221.32
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: 22.2300
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.07s
                      Time elapsed: 00:04:03
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 96680 steps/s (collection: 0.923s, learning 0.094s)
             Mean action noise std: 1.70
          Mean value_function loss: 107.7124
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.4795
                       Mean reward: 68.89
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 0.3138
     Episode_Reward/lifting_object: 18.7658
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.02s
                      Time elapsed: 00:04:04
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 102593 steps/s (collection: 0.863s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 106.9484
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.4801
                       Mean reward: 104.26
               Mean episode length: 219.63
    Episode_Reward/reaching_object: 0.3212
     Episode_Reward/lifting_object: 20.2770
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.96s
                      Time elapsed: 00:04:05
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 102555 steps/s (collection: 0.862s, learning 0.097s)
             Mean action noise std: 1.70
          Mean value_function loss: 121.3093
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.4815
                       Mean reward: 94.75
               Mean episode length: 216.34
    Episode_Reward/reaching_object: 0.3219
     Episode_Reward/lifting_object: 21.5889
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.96s
                      Time elapsed: 00:04:06
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 102516 steps/s (collection: 0.866s, learning 0.093s)
             Mean action noise std: 1.71
          Mean value_function loss: 123.4174
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.4892
                       Mean reward: 104.41
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 0.3240
     Episode_Reward/lifting_object: 21.6662
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.96s
                      Time elapsed: 00:04:07
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 105923 steps/s (collection: 0.837s, learning 0.091s)
             Mean action noise std: 1.71
          Mean value_function loss: 122.8406
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.4977
                       Mean reward: 113.14
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 0.3395
     Episode_Reward/lifting_object: 20.3347
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.93s
                      Time elapsed: 00:04:08
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 105424 steps/s (collection: 0.835s, learning 0.097s)
             Mean action noise std: 1.71
          Mean value_function loss: 124.1846
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5006
                       Mean reward: 122.30
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 0.3367
     Episode_Reward/lifting_object: 24.5936
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.93s
                      Time elapsed: 00:04:09
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 102873 steps/s (collection: 0.841s, learning 0.114s)
             Mean action noise std: 1.71
          Mean value_function loss: 120.5625
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5053
                       Mean reward: 122.25
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 0.3433
     Episode_Reward/lifting_object: 23.1726
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.96s
                      Time elapsed: 00:04:10
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 95699 steps/s (collection: 0.928s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 130.8653
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5099
                       Mean reward: 127.42
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 0.3433
     Episode_Reward/lifting_object: 23.9922
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.03s
                      Time elapsed: 00:04:11
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 96774 steps/s (collection: 0.914s, learning 0.102s)
             Mean action noise std: 1.71
          Mean value_function loss: 123.3233
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.5126
                       Mean reward: 153.12
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 0.3447
     Episode_Reward/lifting_object: 25.7721
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.02s
                      Time elapsed: 00:04:12
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 99138 steps/s (collection: 0.888s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 137.5933
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5181
                       Mean reward: 117.31
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 0.3443
     Episode_Reward/lifting_object: 26.5653
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.99s
                      Time elapsed: 00:04:13
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 100712 steps/s (collection: 0.855s, learning 0.121s)
             Mean action noise std: 1.71
          Mean value_function loss: 137.3110
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5176
                       Mean reward: 151.08
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 0.3652
     Episode_Reward/lifting_object: 30.9105
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.98s
                      Time elapsed: 00:04:14
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 98550 steps/s (collection: 0.878s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 152.3531
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.5167
                       Mean reward: 128.63
               Mean episode length: 218.77
    Episode_Reward/reaching_object: 0.3433
     Episode_Reward/lifting_object: 24.2015
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.00s
                      Time elapsed: 00:04:15
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 97573 steps/s (collection: 0.900s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 155.8478
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5171
                       Mean reward: 139.76
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: 27.1397
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.01s
                      Time elapsed: 00:04:16
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 99634 steps/s (collection: 0.880s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 140.1211
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5170
                       Mean reward: 144.80
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 0.3457
     Episode_Reward/lifting_object: 25.8557
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.99s
                      Time elapsed: 00:04:17
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 97505 steps/s (collection: 0.900s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 168.0975
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.5176
                       Mean reward: 192.39
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 0.3623
     Episode_Reward/lifting_object: 29.3932
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 1.01s
                      Time elapsed: 00:04:18
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 97587 steps/s (collection: 0.886s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 203.4109
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.5227
                       Mean reward: 213.34
               Mean episode length: 221.69
    Episode_Reward/reaching_object: 0.3596
     Episode_Reward/lifting_object: 32.1406
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.01s
                      Time elapsed: 00:04:19
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 100396 steps/s (collection: 0.883s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 205.5065
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 15.5259
                       Mean reward: 138.23
               Mean episode length: 223.33
    Episode_Reward/reaching_object: 0.3618
     Episode_Reward/lifting_object: 31.1071
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.98s
                      Time elapsed: 00:04:20
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 105788 steps/s (collection: 0.840s, learning 0.090s)
             Mean action noise std: 1.71
          Mean value_function loss: 196.8173
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5272
                       Mean reward: 121.57
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 0.3674
     Episode_Reward/lifting_object: 30.2411
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.93s
                      Time elapsed: 00:04:21
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 104232 steps/s (collection: 0.836s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 217.7194
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.5284
                       Mean reward: 173.52
               Mean episode length: 225.70
    Episode_Reward/reaching_object: 0.3654
     Episode_Reward/lifting_object: 32.2583
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.94s
                      Time elapsed: 00:04:22
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 100830 steps/s (collection: 0.855s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 211.4103
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5289
                       Mean reward: 176.32
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.3656
     Episode_Reward/lifting_object: 33.2003
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.97s
                      Time elapsed: 00:04:23
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 100261 steps/s (collection: 0.866s, learning 0.114s)
             Mean action noise std: 1.72
          Mean value_function loss: 218.4891
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5298
                       Mean reward: 176.42
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 0.3558
     Episode_Reward/lifting_object: 30.6914
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.98s
                      Time elapsed: 00:04:24
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 104612 steps/s (collection: 0.838s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 234.2266
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.5323
                       Mean reward: 203.42
               Mean episode length: 221.31
    Episode_Reward/reaching_object: 0.3623
     Episode_Reward/lifting_object: 33.5616
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.94s
                      Time elapsed: 00:04:25
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 107699 steps/s (collection: 0.818s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 210.1747
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.5346
                       Mean reward: 210.68
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 0.3651
     Episode_Reward/lifting_object: 34.2713
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.91s
                      Time elapsed: 00:04:26
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 107247 steps/s (collection: 0.825s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 241.4793
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5389
                       Mean reward: 182.46
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 0.3757
     Episode_Reward/lifting_object: 36.5932
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.92s
                      Time elapsed: 00:04:27
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 109293 steps/s (collection: 0.810s, learning 0.090s)
             Mean action noise std: 1.72
          Mean value_function loss: 255.7539
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5415
                       Mean reward: 186.71
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 0.3856
     Episode_Reward/lifting_object: 39.1802
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.90s
                      Time elapsed: 00:04:28
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 108158 steps/s (collection: 0.818s, learning 0.091s)
             Mean action noise std: 1.72
          Mean value_function loss: 268.1452
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5435
                       Mean reward: 158.16
               Mean episode length: 217.22
    Episode_Reward/reaching_object: 0.3719
     Episode_Reward/lifting_object: 39.4708
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.91s
                      Time elapsed: 00:04:28
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 110114 steps/s (collection: 0.807s, learning 0.086s)
             Mean action noise std: 1.72
          Mean value_function loss: 216.9429
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.5443
                       Mean reward: 182.45
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 0.3827
     Episode_Reward/lifting_object: 38.1509
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.89s
                      Time elapsed: 00:04:29
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 108766 steps/s (collection: 0.816s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 219.7713
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.5437
                       Mean reward: 213.32
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 0.3856
     Episode_Reward/lifting_object: 42.3014
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.90s
                      Time elapsed: 00:04:30
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 105508 steps/s (collection: 0.825s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 237.0852
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5445
                       Mean reward: 202.22
               Mean episode length: 228.51
    Episode_Reward/reaching_object: 0.3849
     Episode_Reward/lifting_object: 40.1970
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.93s
                      Time elapsed: 00:04:31
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 105056 steps/s (collection: 0.844s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 234.2654
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5440
                       Mean reward: 211.93
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 0.3852
     Episode_Reward/lifting_object: 42.8869
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.94s
                      Time elapsed: 00:04:32
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 104223 steps/s (collection: 0.821s, learning 0.122s)
             Mean action noise std: 1.72
          Mean value_function loss: 240.9299
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.5421
                       Mean reward: 324.90
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.4176
     Episode_Reward/lifting_object: 50.3415
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.94s
                      Time elapsed: 00:04:33
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 100964 steps/s (collection: 0.872s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 266.5686
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 15.5374
                       Mean reward: 206.50
               Mean episode length: 221.70
    Episode_Reward/reaching_object: 0.3928
     Episode_Reward/lifting_object: 44.0443
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.97s
                      Time elapsed: 00:04:34
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 101294 steps/s (collection: 0.854s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 219.3680
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 15.5373
                       Mean reward: 237.94
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 0.3988
     Episode_Reward/lifting_object: 46.6523
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.97s
                      Time elapsed: 00:04:35
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 99218 steps/s (collection: 0.888s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 227.8565
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.5373
                       Mean reward: 240.46
               Mean episode length: 214.04
    Episode_Reward/reaching_object: 0.3974
     Episode_Reward/lifting_object: 49.3997
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.99s
                      Time elapsed: 00:04:36
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 100311 steps/s (collection: 0.884s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 257.5577
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.5375
                       Mean reward: 257.79
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 0.4218
     Episode_Reward/lifting_object: 53.2829
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.98s
                      Time elapsed: 00:04:37
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 107039 steps/s (collection: 0.811s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 232.3128
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.5380
                       Mean reward: 269.22
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 0.3966
     Episode_Reward/lifting_object: 43.7667
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.92s
                      Time elapsed: 00:04:38
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 105039 steps/s (collection: 0.837s, learning 0.099s)
             Mean action noise std: 1.72
          Mean value_function loss: 246.8414
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5386
                       Mean reward: 290.86
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.4070
     Episode_Reward/lifting_object: 50.3587
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.94s
                      Time elapsed: 00:04:39
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 107056 steps/s (collection: 0.822s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 236.1404
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5394
                       Mean reward: 308.28
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 0.4226
     Episode_Reward/lifting_object: 54.1285
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.92s
                      Time elapsed: 00:04:40
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 100386 steps/s (collection: 0.842s, learning 0.137s)
             Mean action noise std: 1.72
          Mean value_function loss: 292.5530
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5370
                       Mean reward: 273.75
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 0.4254
     Episode_Reward/lifting_object: 54.6814
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.98s
                      Time elapsed: 00:04:41
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 91810 steps/s (collection: 0.956s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 317.5250
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.5310
                       Mean reward: 249.97
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 0.4290
     Episode_Reward/lifting_object: 55.0269
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 1.07s
                      Time elapsed: 00:04:42
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 101507 steps/s (collection: 0.857s, learning 0.111s)
             Mean action noise std: 1.72
          Mean value_function loss: 297.5226
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5280
                       Mean reward: 294.10
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 0.4139
     Episode_Reward/lifting_object: 54.0488
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.97s
                      Time elapsed: 00:04:43
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 98788 steps/s (collection: 0.875s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 294.6459
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5294
                       Mean reward: 273.82
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 0.4043
     Episode_Reward/lifting_object: 51.6702
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.00s
                      Time elapsed: 00:04:44
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 100946 steps/s (collection: 0.872s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 262.6251
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5316
                       Mean reward: 308.43
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 0.4264
     Episode_Reward/lifting_object: 58.5606
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.97s
                      Time elapsed: 00:04:45
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 94963 steps/s (collection: 0.881s, learning 0.154s)
             Mean action noise std: 1.72
          Mean value_function loss: 302.5009
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5321
                       Mean reward: 290.37
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.4208
     Episode_Reward/lifting_object: 56.4644
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.04s
                      Time elapsed: 00:04:46
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 100165 steps/s (collection: 0.878s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 276.2585
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.5343
                       Mean reward: 278.82
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 0.4345
     Episode_Reward/lifting_object: 58.9646
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.98s
                      Time elapsed: 00:04:47
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 103223 steps/s (collection: 0.841s, learning 0.111s)
             Mean action noise std: 1.72
          Mean value_function loss: 277.1939
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.5362
                       Mean reward: 312.64
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 0.4433
     Episode_Reward/lifting_object: 62.3971
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.95s
                      Time elapsed: 00:04:48
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 99546 steps/s (collection: 0.879s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 288.2972
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.5404
                       Mean reward: 297.40
               Mean episode length: 214.45
    Episode_Reward/reaching_object: 0.4209
     Episode_Reward/lifting_object: 57.7255
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.99s
                      Time elapsed: 00:04:49
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 101447 steps/s (collection: 0.851s, learning 0.118s)
             Mean action noise std: 1.72
          Mean value_function loss: 376.8181
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5450
                       Mean reward: 237.75
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 0.4225
     Episode_Reward/lifting_object: 56.5973
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.97s
                      Time elapsed: 00:04:50
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 101821 steps/s (collection: 0.855s, learning 0.111s)
             Mean action noise std: 1.72
          Mean value_function loss: 303.8745
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5462
                       Mean reward: 350.45
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 0.4556
     Episode_Reward/lifting_object: 67.0766
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.97s
                      Time elapsed: 00:04:51
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 105935 steps/s (collection: 0.821s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 309.2202
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5441
                       Mean reward: 374.22
               Mean episode length: 226.13
    Episode_Reward/reaching_object: 0.4517
     Episode_Reward/lifting_object: 65.5433
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.93s
                      Time elapsed: 00:04:52
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 98779 steps/s (collection: 0.882s, learning 0.113s)
             Mean action noise std: 1.72
          Mean value_function loss: 312.3643
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.5406
                       Mean reward: 349.51
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.4451
     Episode_Reward/lifting_object: 64.1068
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 1.00s
                      Time elapsed: 00:04:53
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 102314 steps/s (collection: 0.846s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 284.6782
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.5405
                       Mean reward: 306.11
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 0.4554
     Episode_Reward/lifting_object: 69.5298
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.96s
                      Time elapsed: 00:04:53
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 102850 steps/s (collection: 0.847s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 268.2066
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.5414
                       Mean reward: 331.74
               Mean episode length: 226.03
    Episode_Reward/reaching_object: 0.4495
     Episode_Reward/lifting_object: 66.3898
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.96s
                      Time elapsed: 00:04:54
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 112041 steps/s (collection: 0.789s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 309.8878
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.5417
                       Mean reward: 362.29
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 0.4408
     Episode_Reward/lifting_object: 64.7779
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.88s
                      Time elapsed: 00:04:55
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 109352 steps/s (collection: 0.811s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 287.0537
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5431
                       Mean reward: 339.73
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 0.4561
     Episode_Reward/lifting_object: 67.4946
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.90s
                      Time elapsed: 00:04:56
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 110198 steps/s (collection: 0.806s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 312.4438
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5463
                       Mean reward: 315.12
               Mean episode length: 224.90
    Episode_Reward/reaching_object: 0.4433
     Episode_Reward/lifting_object: 66.1946
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.89s
                      Time elapsed: 00:04:57
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 105058 steps/s (collection: 0.831s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 337.5806
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.5469
                       Mean reward: 349.65
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 0.4723
     Episode_Reward/lifting_object: 74.8153
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.94s
                      Time elapsed: 00:04:58
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 108072 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 1.72
          Mean value_function loss: 306.1571
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5466
                       Mean reward: 336.62
               Mean episode length: 223.34
    Episode_Reward/reaching_object: 0.4526
     Episode_Reward/lifting_object: 69.3812
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.91s
                      Time elapsed: 00:04:59
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 110993 steps/s (collection: 0.790s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 336.1259
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5490
                       Mean reward: 368.36
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 0.4547
     Episode_Reward/lifting_object: 69.2927
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.89s
                      Time elapsed: 00:05:00
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 107346 steps/s (collection: 0.804s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 357.0410
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.5499
                       Mean reward: 336.65
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 0.4335
     Episode_Reward/lifting_object: 66.0269
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.92s
                      Time elapsed: 00:05:01
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 101276 steps/s (collection: 0.839s, learning 0.132s)
             Mean action noise std: 1.72
          Mean value_function loss: 316.9613
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5477
                       Mean reward: 410.03
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 0.4515
     Episode_Reward/lifting_object: 71.1967
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.97s
                      Time elapsed: 00:05:02
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 95759 steps/s (collection: 0.906s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 334.0752
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 15.5467
                       Mean reward: 371.19
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 0.4500
     Episode_Reward/lifting_object: 67.5117
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.03s
                      Time elapsed: 00:05:03
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 93719 steps/s (collection: 0.924s, learning 0.125s)
             Mean action noise std: 1.72
          Mean value_function loss: 321.5706
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.5465
                       Mean reward: 350.58
               Mean episode length: 228.78
    Episode_Reward/reaching_object: 0.4612
     Episode_Reward/lifting_object: 71.0740
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.05s
                      Time elapsed: 00:05:04
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 101683 steps/s (collection: 0.870s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 303.9923
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.5456
                       Mean reward: 369.83
               Mean episode length: 221.70
    Episode_Reward/reaching_object: 0.4467
     Episode_Reward/lifting_object: 68.1378
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.97s
                      Time elapsed: 00:05:05
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 105167 steps/s (collection: 0.839s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 287.4515
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5438
                       Mean reward: 387.62
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 0.4476
     Episode_Reward/lifting_object: 68.9981
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.93s
                      Time elapsed: 00:05:06
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 107053 steps/s (collection: 0.811s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 300.6914
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.5437
                       Mean reward: 382.28
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.4709
     Episode_Reward/lifting_object: 74.4005
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.92s
                      Time elapsed: 00:05:07
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 99019 steps/s (collection: 0.889s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 282.9299
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.5439
                       Mean reward: 397.53
               Mean episode length: 225.23
    Episode_Reward/reaching_object: 0.4604
     Episode_Reward/lifting_object: 74.9658
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.99s
                      Time elapsed: 00:05:08
                               ETA: 00:29:27

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 108673 steps/s (collection: 0.814s, learning 0.091s)
             Mean action noise std: 1.72
          Mean value_function loss: 265.6090
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5442
                       Mean reward: 378.00
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 0.4857
     Episode_Reward/lifting_object: 84.0389
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.90s
                      Time elapsed: 00:05:09
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 104566 steps/s (collection: 0.842s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 260.1344
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5443
                       Mean reward: 379.61
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 0.4523
     Episode_Reward/lifting_object: 75.4002
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.94s
                      Time elapsed: 00:05:09
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 102263 steps/s (collection: 0.856s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 315.6245
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5424
                       Mean reward: 358.32
               Mean episode length: 218.34
    Episode_Reward/reaching_object: 0.4610
     Episode_Reward/lifting_object: 76.4945
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.96s
                      Time elapsed: 00:05:10
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 105226 steps/s (collection: 0.847s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 290.5800
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5403
                       Mean reward: 406.81
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 0.4858
     Episode_Reward/lifting_object: 85.3038
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.93s
                      Time elapsed: 00:05:11
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 106118 steps/s (collection: 0.835s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 279.1042
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5430
                       Mean reward: 373.41
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.4827
     Episode_Reward/lifting_object: 78.9836
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.93s
                      Time elapsed: 00:05:12
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 110146 steps/s (collection: 0.803s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 345.7013
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5476
                       Mean reward: 402.57
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 0.4657
     Episode_Reward/lifting_object: 76.5472
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.89s
                      Time elapsed: 00:05:13
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 106264 steps/s (collection: 0.838s, learning 0.087s)
             Mean action noise std: 1.73
          Mean value_function loss: 293.6832
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.5520
                       Mean reward: 405.09
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 0.4720
     Episode_Reward/lifting_object: 81.3451
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.93s
                      Time elapsed: 00:05:14
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 105392 steps/s (collection: 0.842s, learning 0.091s)
             Mean action noise std: 1.73
          Mean value_function loss: 309.2985
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5546
                       Mean reward: 468.43
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 0.4740
     Episode_Reward/lifting_object: 80.8047
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.93s
                      Time elapsed: 00:05:15
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 108456 steps/s (collection: 0.809s, learning 0.097s)
             Mean action noise std: 1.73
          Mean value_function loss: 308.6121
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5563
                       Mean reward: 374.16
               Mean episode length: 223.80
    Episode_Reward/reaching_object: 0.4416
     Episode_Reward/lifting_object: 69.7347
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.91s
                      Time elapsed: 00:05:16
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 106855 steps/s (collection: 0.814s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 325.4731
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.5581
                       Mean reward: 459.19
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.4704
     Episode_Reward/lifting_object: 78.2545
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.92s
                      Time elapsed: 00:05:17
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 109288 steps/s (collection: 0.810s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 336.5547
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.5580
                       Mean reward: 364.72
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 0.4699
     Episode_Reward/lifting_object: 79.0064
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.90s
                      Time elapsed: 00:05:18
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 108705 steps/s (collection: 0.813s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 334.6641
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5580
                       Mean reward: 413.35
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 0.5011
     Episode_Reward/lifting_object: 91.2439
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.90s
                      Time elapsed: 00:05:19
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 104656 steps/s (collection: 0.827s, learning 0.112s)
             Mean action noise std: 1.73
          Mean value_function loss: 383.3715
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5580
                       Mean reward: 415.97
               Mean episode length: 222.08
    Episode_Reward/reaching_object: 0.4918
     Episode_Reward/lifting_object: 86.4157
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.94s
                      Time elapsed: 00:05:20
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 105361 steps/s (collection: 0.823s, learning 0.110s)
             Mean action noise std: 1.73
          Mean value_function loss: 329.3283
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 15.5596
                       Mean reward: 392.15
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 0.4978
     Episode_Reward/lifting_object: 88.8221
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.93s
                      Time elapsed: 00:05:21
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 107980 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 313.0982
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.5633
                       Mean reward: 484.72
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 0.5021
     Episode_Reward/lifting_object: 91.1361
      Episode_Reward/object_height: 0.0319
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.91s
                      Time elapsed: 00:05:21
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 105514 steps/s (collection: 0.836s, learning 0.095s)
             Mean action noise std: 1.73
          Mean value_function loss: 348.2327
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5677
                       Mean reward: 388.18
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 0.4879
     Episode_Reward/lifting_object: 84.1491
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.93s
                      Time elapsed: 00:05:22
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 104231 steps/s (collection: 0.846s, learning 0.097s)
             Mean action noise std: 1.73
          Mean value_function loss: 362.3203
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5689
                       Mean reward: 473.37
               Mean episode length: 227.99
    Episode_Reward/reaching_object: 0.5109
     Episode_Reward/lifting_object: 91.1966
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.94s
                      Time elapsed: 00:05:23
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 104318 steps/s (collection: 0.837s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 356.5710
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5656
                       Mean reward: 432.89
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 0.5018
     Episode_Reward/lifting_object: 88.6008
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.94s
                      Time elapsed: 00:05:24
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 103682 steps/s (collection: 0.834s, learning 0.114s)
             Mean action noise std: 1.73
          Mean value_function loss: 379.1069
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5653
                       Mean reward: 427.43
               Mean episode length: 223.08
    Episode_Reward/reaching_object: 0.4862
     Episode_Reward/lifting_object: 85.3911
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.95s
                      Time elapsed: 00:05:25
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 106882 steps/s (collection: 0.816s, learning 0.103s)
             Mean action noise std: 1.73
          Mean value_function loss: 386.2530
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.5676
                       Mean reward: 421.42
               Mean episode length: 222.78
    Episode_Reward/reaching_object: 0.4964
     Episode_Reward/lifting_object: 87.2369
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.92s
                      Time elapsed: 00:05:26
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 102074 steps/s (collection: 0.842s, learning 0.121s)
             Mean action noise std: 1.73
          Mean value_function loss: 372.9504
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5720
                       Mean reward: 420.00
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 0.5016
     Episode_Reward/lifting_object: 89.3157
      Episode_Reward/object_height: 0.0327
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.96s
                      Time elapsed: 00:05:27
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 105123 steps/s (collection: 0.821s, learning 0.114s)
             Mean action noise std: 1.74
          Mean value_function loss: 395.3906
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5798
                       Mean reward: 439.93
               Mean episode length: 217.85
    Episode_Reward/reaching_object: 0.5004
     Episode_Reward/lifting_object: 89.0824
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.94s
                      Time elapsed: 00:05:28
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 104203 steps/s (collection: 0.834s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 361.9277
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5904
                       Mean reward: 425.91
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 0.4843
     Episode_Reward/lifting_object: 86.3031
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.94s
                      Time elapsed: 00:05:29
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 104052 steps/s (collection: 0.831s, learning 0.114s)
             Mean action noise std: 1.74
          Mean value_function loss: 357.6715
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.5949
                       Mean reward: 439.05
               Mean episode length: 222.34
    Episode_Reward/reaching_object: 0.5136
     Episode_Reward/lifting_object: 94.3854
      Episode_Reward/object_height: 0.0358
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.94s
                      Time elapsed: 00:05:30
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 104476 steps/s (collection: 0.831s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 366.9954
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5989
                       Mean reward: 426.87
               Mean episode length: 226.01
    Episode_Reward/reaching_object: 0.4792
     Episode_Reward/lifting_object: 83.7615
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.94s
                      Time elapsed: 00:05:31
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 106467 steps/s (collection: 0.830s, learning 0.094s)
             Mean action noise std: 1.74
          Mean value_function loss: 410.1791
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6066
                       Mean reward: 440.72
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 0.4831
     Episode_Reward/lifting_object: 86.9389
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.92s
                      Time elapsed: 00:05:32
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 105996 steps/s (collection: 0.822s, learning 0.105s)
             Mean action noise std: 1.74
          Mean value_function loss: 350.4982
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6126
                       Mean reward: 379.43
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 0.4953
     Episode_Reward/lifting_object: 88.5221
      Episode_Reward/object_height: 0.0336
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.93s
                      Time elapsed: 00:05:33
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 103447 steps/s (collection: 0.851s, learning 0.100s)
             Mean action noise std: 1.74
          Mean value_function loss: 355.8880
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.6141
                       Mean reward: 491.62
               Mean episode length: 229.50
    Episode_Reward/reaching_object: 0.5085
     Episode_Reward/lifting_object: 94.4479
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.95s
                      Time elapsed: 00:05:34
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 99329 steps/s (collection: 0.871s, learning 0.119s)
             Mean action noise std: 1.74
          Mean value_function loss: 413.6076
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.6175
                       Mean reward: 504.39
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 0.5310
     Episode_Reward/lifting_object: 99.3925
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.99s
                      Time elapsed: 00:05:35
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 98684 steps/s (collection: 0.891s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 351.9754
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.6219
                       Mean reward: 487.11
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.5116
     Episode_Reward/lifting_object: 94.4105
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 1.00s
                      Time elapsed: 00:05:36
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 104199 steps/s (collection: 0.825s, learning 0.118s)
             Mean action noise std: 1.75
          Mean value_function loss: 364.8287
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.6290
                       Mean reward: 445.55
               Mean episode length: 222.06
    Episode_Reward/reaching_object: 0.5150
     Episode_Reward/lifting_object: 96.8411
      Episode_Reward/object_height: 0.0366
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.94s
                      Time elapsed: 00:05:37
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 102040 steps/s (collection: 0.844s, learning 0.120s)
             Mean action noise std: 1.75
          Mean value_function loss: 388.5190
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.6356
                       Mean reward: 499.82
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 0.4911
     Episode_Reward/lifting_object: 91.2118
      Episode_Reward/object_height: 0.0346
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.96s
                      Time elapsed: 00:05:38
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 99081 steps/s (collection: 0.868s, learning 0.124s)
             Mean action noise std: 1.75
          Mean value_function loss: 387.7488
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6410
                       Mean reward: 521.21
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 0.5144
     Episode_Reward/lifting_object: 95.3001
      Episode_Reward/object_height: 0.0359
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.99s
                      Time elapsed: 00:05:39
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 94458 steps/s (collection: 0.912s, learning 0.129s)
             Mean action noise std: 1.75
          Mean value_function loss: 390.3828
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6443
                       Mean reward: 464.85
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 101.4856
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 1.04s
                      Time elapsed: 00:05:40
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 103489 steps/s (collection: 0.846s, learning 0.104s)
             Mean action noise std: 1.75
          Mean value_function loss: 371.0672
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.6445
                       Mean reward: 576.50
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 0.5539
     Episode_Reward/lifting_object: 109.7002
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.95s
                      Time elapsed: 00:05:41
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 110359 steps/s (collection: 0.791s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 361.6839
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.6476
                       Mean reward: 557.62
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 0.5357
     Episode_Reward/lifting_object: 100.8562
      Episode_Reward/object_height: 0.0381
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.89s
                      Time elapsed: 00:05:41
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 39935 steps/s (collection: 2.292s, learning 0.170s)
             Mean action noise std: 1.75
          Mean value_function loss: 428.9769
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6522
                       Mean reward: 529.83
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 0.5285
     Episode_Reward/lifting_object: 100.8493
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.46s
                      Time elapsed: 00:05:44
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 29508 steps/s (collection: 3.207s, learning 0.125s)
             Mean action noise std: 1.76
          Mean value_function loss: 434.1433
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.6600
                       Mean reward: 583.98
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.5467
     Episode_Reward/lifting_object: 107.6766
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 3.33s
                      Time elapsed: 00:05:47
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 36034 steps/s (collection: 2.607s, learning 0.121s)
             Mean action noise std: 1.76
          Mean value_function loss: 410.3531
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.6632
                       Mean reward: 528.82
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 0.5315
     Episode_Reward/lifting_object: 104.4655
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 2.73s
                      Time elapsed: 00:05:50
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 31864 steps/s (collection: 2.947s, learning 0.138s)
             Mean action noise std: 1.76
          Mean value_function loss: 409.8385
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6639
                       Mean reward: 513.46
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 0.5154
     Episode_Reward/lifting_object: 97.9402
      Episode_Reward/object_height: 0.0372
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 3.09s
                      Time elapsed: 00:05:53
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 31166 steps/s (collection: 3.021s, learning 0.133s)
             Mean action noise std: 1.76
          Mean value_function loss: 376.0775
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6686
                       Mean reward: 523.54
               Mean episode length: 218.56
    Episode_Reward/reaching_object: 0.5349
     Episode_Reward/lifting_object: 105.5012
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.15s
                      Time elapsed: 00:05:56
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 31764 steps/s (collection: 2.966s, learning 0.129s)
             Mean action noise std: 1.76
          Mean value_function loss: 400.0573
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.6746
                       Mean reward: 541.64
               Mean episode length: 229.10
    Episode_Reward/reaching_object: 0.5360
     Episode_Reward/lifting_object: 106.4982
      Episode_Reward/object_height: 0.0405
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.09s
                      Time elapsed: 00:05:59
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 29040 steps/s (collection: 3.249s, learning 0.136s)
             Mean action noise std: 1.76
          Mean value_function loss: 438.2703
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.6830
                       Mean reward: 515.81
               Mean episode length: 222.44
    Episode_Reward/reaching_object: 0.5416
     Episode_Reward/lifting_object: 106.5988
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.39s
                      Time elapsed: 00:06:03
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 29427 steps/s (collection: 3.186s, learning 0.155s)
             Mean action noise std: 1.76
          Mean value_function loss: 394.0393
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.6868
                       Mean reward: 532.06
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.5234
     Episode_Reward/lifting_object: 103.1807
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.34s
                      Time elapsed: 00:06:06
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 22299 steps/s (collection: 4.266s, learning 0.142s)
             Mean action noise std: 1.76
          Mean value_function loss: 415.5699
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6874
                       Mean reward: 544.45
               Mean episode length: 219.19
    Episode_Reward/reaching_object: 0.5221
     Episode_Reward/lifting_object: 103.2959
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.41s
                      Time elapsed: 00:06:10
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 101757 steps/s (collection: 0.872s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 371.0666
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.6907
                       Mean reward: 549.05
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 0.5283
     Episode_Reward/lifting_object: 104.9144
      Episode_Reward/object_height: 0.0408
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.97s
                      Time elapsed: 00:06:11
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 110830 steps/s (collection: 0.792s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 425.9339
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6976
                       Mean reward: 490.25
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 0.5428
     Episode_Reward/lifting_object: 107.2863
      Episode_Reward/object_height: 0.0417
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.89s
                      Time elapsed: 00:06:12
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 110219 steps/s (collection: 0.783s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 408.6903
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.7012
                       Mean reward: 552.68
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 0.5139
     Episode_Reward/lifting_object: 100.6850
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.89s
                      Time elapsed: 00:06:13
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 110547 steps/s (collection: 0.772s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 386.9744
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.7031
                       Mean reward: 512.49
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 0.5273
     Episode_Reward/lifting_object: 106.3881
      Episode_Reward/object_height: 0.0420
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.89s
                      Time elapsed: 00:06:14
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 111713 steps/s (collection: 0.777s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 383.8500
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7078
                       Mean reward: 583.56
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.5536
     Episode_Reward/lifting_object: 112.3757
      Episode_Reward/object_height: 0.0446
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.88s
                      Time elapsed: 00:06:15
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 107414 steps/s (collection: 0.791s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 393.6139
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.7143
                       Mean reward: 526.64
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.5330
     Episode_Reward/lifting_object: 103.2248
      Episode_Reward/object_height: 0.0410
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.92s
                      Time elapsed: 00:06:16
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 98601 steps/s (collection: 0.878s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 379.1050
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7205
                       Mean reward: 517.01
               Mean episode length: 227.57
    Episode_Reward/reaching_object: 0.5344
     Episode_Reward/lifting_object: 105.5428
      Episode_Reward/object_height: 0.0418
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.00s
                      Time elapsed: 00:06:17
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 98754 steps/s (collection: 0.875s, learning 0.121s)
             Mean action noise std: 1.77
          Mean value_function loss: 377.4525
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7235
                       Mean reward: 571.42
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.5450
     Episode_Reward/lifting_object: 108.5174
      Episode_Reward/object_height: 0.0436
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.00s
                      Time elapsed: 00:06:18
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 100579 steps/s (collection: 0.857s, learning 0.121s)
             Mean action noise std: 1.77
          Mean value_function loss: 370.2798
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7230
                       Mean reward: 519.22
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 112.6126
      Episode_Reward/object_height: 0.0449
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.98s
                      Time elapsed: 00:06:19
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 105645 steps/s (collection: 0.811s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 383.0986
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.7252
                       Mean reward: 512.61
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.5288
     Episode_Reward/lifting_object: 103.5268
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.93s
                      Time elapsed: 00:06:20
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 102547 steps/s (collection: 0.843s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 449.8754
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7274
                       Mean reward: 554.91
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 0.5662
     Episode_Reward/lifting_object: 113.9293
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.96s
                      Time elapsed: 00:06:21
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 107917 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 368.6218
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7268
                       Mean reward: 558.58
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 0.5692
     Episode_Reward/lifting_object: 117.3047
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.91s
                      Time elapsed: 00:06:22
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 103752 steps/s (collection: 0.850s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 355.6773
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.7266
                       Mean reward: 595.63
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 0.5755
     Episode_Reward/lifting_object: 119.1676
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.95s
                      Time elapsed: 00:06:23
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 109319 steps/s (collection: 0.802s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 381.3151
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7292
                       Mean reward: 657.38
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.6214
     Episode_Reward/lifting_object: 129.1863
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.90s
                      Time elapsed: 00:06:23
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 106689 steps/s (collection: 0.792s, learning 0.130s)
             Mean action noise std: 1.77
          Mean value_function loss: 336.0731
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7297
                       Mean reward: 635.02
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 0.5926
     Episode_Reward/lifting_object: 121.1877
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.92s
                      Time elapsed: 00:06:24
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 107427 steps/s (collection: 0.809s, learning 0.106s)
             Mean action noise std: 1.77
          Mean value_function loss: 335.1307
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.7261
                       Mean reward: 633.15
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 0.5686
     Episode_Reward/lifting_object: 115.5537
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.92s
                      Time elapsed: 00:06:25
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 108118 steps/s (collection: 0.790s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 318.4381
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7262
                       Mean reward: 651.89
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 0.5933
     Episode_Reward/lifting_object: 120.2634
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.91s
                      Time elapsed: 00:06:26
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 111197 steps/s (collection: 0.776s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 330.8153
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.7293
                       Mean reward: 634.77
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.5946
     Episode_Reward/lifting_object: 121.1548
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.88s
                      Time elapsed: 00:06:27
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 109124 steps/s (collection: 0.784s, learning 0.117s)
             Mean action noise std: 1.77
          Mean value_function loss: 324.1232
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7312
                       Mean reward: 629.33
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.5924
     Episode_Reward/lifting_object: 121.1754
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.90s
                      Time elapsed: 00:06:28
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 108959 steps/s (collection: 0.785s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 325.8727
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7342
                       Mean reward: 558.52
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 0.5756
     Episode_Reward/lifting_object: 115.8195
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.90s
                      Time elapsed: 00:06:29
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 110424 steps/s (collection: 0.785s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 324.4787
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.7360
                       Mean reward: 618.27
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.6167
     Episode_Reward/lifting_object: 127.0646
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.89s
                      Time elapsed: 00:06:30
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 106931 steps/s (collection: 0.811s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 324.5249
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7391
                       Mean reward: 636.08
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.6165
     Episode_Reward/lifting_object: 128.6614
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.92s
                      Time elapsed: 00:06:31
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 107824 steps/s (collection: 0.807s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 312.4811
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7427
                       Mean reward: 622.79
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 0.6127
     Episode_Reward/lifting_object: 127.3649
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.91s
                      Time elapsed: 00:06:32
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 109744 steps/s (collection: 0.788s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 314.1038
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 15.7411
                       Mean reward: 644.32
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.6196
     Episode_Reward/lifting_object: 130.3970
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.90s
                      Time elapsed: 00:06:33
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 109557 steps/s (collection: 0.785s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 314.3849
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.7392
                       Mean reward: 612.03
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.6149
     Episode_Reward/lifting_object: 129.0941
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.90s
                      Time elapsed: 00:06:33
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 98895 steps/s (collection: 0.879s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 305.5169
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7399
                       Mean reward: 672.41
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.5968
     Episode_Reward/lifting_object: 122.7395
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.99s
                      Time elapsed: 00:06:34
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 104490 steps/s (collection: 0.826s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 314.8813
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7410
                       Mean reward: 698.45
               Mean episode length: 242.59
    Episode_Reward/reaching_object: 0.6285
     Episode_Reward/lifting_object: 130.2827
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.94s
                      Time elapsed: 00:06:35
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 105466 steps/s (collection: 0.817s, learning 0.116s)
             Mean action noise std: 1.78
          Mean value_function loss: 308.6775
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7421
                       Mean reward: 691.84
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.6195
     Episode_Reward/lifting_object: 128.2028
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.93s
                      Time elapsed: 00:06:36
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 106809 steps/s (collection: 0.803s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 294.1506
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7447
                       Mean reward: 624.28
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.6160
     Episode_Reward/lifting_object: 128.9782
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.92s
                      Time elapsed: 00:06:37
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 109620 steps/s (collection: 0.783s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 295.1521
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7484
                       Mean reward: 582.49
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 0.6020
     Episode_Reward/lifting_object: 124.9902
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.90s
                      Time elapsed: 00:06:38
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 107343 steps/s (collection: 0.796s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 310.4743
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.7505
                       Mean reward: 672.43
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 130.4933
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.92s
                      Time elapsed: 00:06:39
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 101413 steps/s (collection: 0.855s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 279.7268
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7542
                       Mean reward: 695.79
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.6321
     Episode_Reward/lifting_object: 135.0249
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.97s
                      Time elapsed: 00:06:40
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 104260 steps/s (collection: 0.829s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 264.4877
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7543
                       Mean reward: 682.45
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.6477
     Episode_Reward/lifting_object: 138.6003
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.94s
                      Time elapsed: 00:06:41
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 101791 steps/s (collection: 0.861s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 299.3693
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7589
                       Mean reward: 689.80
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.6531
     Episode_Reward/lifting_object: 139.8607
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.97s
                      Time elapsed: 00:06:42
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 102161 steps/s (collection: 0.853s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 258.1645
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7630
                       Mean reward: 673.20
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.6319
     Episode_Reward/lifting_object: 131.9685
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.96s
                      Time elapsed: 00:06:43
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 102832 steps/s (collection: 0.831s, learning 0.125s)
             Mean action noise std: 1.79
          Mean value_function loss: 280.5433
               Mean surrogate loss: 0.0152
                 Mean entropy loss: 15.7654
                       Mean reward: 707.25
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.6506
     Episode_Reward/lifting_object: 141.1172
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.96s
                      Time elapsed: 00:06:44
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 106337 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 301.8570
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7660
                       Mean reward: 654.88
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 131.8299
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.92s
                      Time elapsed: 00:06:45
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 98629 steps/s (collection: 0.883s, learning 0.114s)
             Mean action noise std: 1.79
          Mean value_function loss: 321.1799
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7658
                       Mean reward: 653.86
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 0.6250
     Episode_Reward/lifting_object: 134.4009
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.00s
                      Time elapsed: 00:06:46
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 106906 steps/s (collection: 0.799s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 304.6480
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7650
                       Mean reward: 623.31
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 0.6095
     Episode_Reward/lifting_object: 127.8819
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.92s
                      Time elapsed: 00:06:47
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 105527 steps/s (collection: 0.822s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 288.5581
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7631
                       Mean reward: 679.01
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 0.6221
     Episode_Reward/lifting_object: 131.5730
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.93s
                      Time elapsed: 00:06:48
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 101288 steps/s (collection: 0.850s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 279.9766
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7622
                       Mean reward: 697.77
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 127.7694
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.97s
                      Time elapsed: 00:06:49
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 107929 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 259.5358
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7651
                       Mean reward: 693.54
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 0.6298
     Episode_Reward/lifting_object: 132.5251
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.91s
                      Time elapsed: 00:06:49
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 108620 steps/s (collection: 0.793s, learning 0.112s)
             Mean action noise std: 1.79
          Mean value_function loss: 270.2186
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7671
                       Mean reward: 695.19
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.6431
     Episode_Reward/lifting_object: 136.9710
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.91s
                      Time elapsed: 00:06:50
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 109381 steps/s (collection: 0.785s, learning 0.114s)
             Mean action noise std: 1.79
          Mean value_function loss: 245.9949
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7699
                       Mean reward: 750.13
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.6407
     Episode_Reward/lifting_object: 136.6499
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.90s
                      Time elapsed: 00:06:51
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 109380 steps/s (collection: 0.795s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 256.1010
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7766
                       Mean reward: 658.85
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 0.6429
     Episode_Reward/lifting_object: 136.3614
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.90s
                      Time elapsed: 00:06:52
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 111102 steps/s (collection: 0.776s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 234.7165
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.7805
                       Mean reward: 601.83
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 0.6040
     Episode_Reward/lifting_object: 127.4323
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.88s
                      Time elapsed: 00:06:53
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 111416 steps/s (collection: 0.768s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 247.0422
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.7859
                       Mean reward: 587.54
               Mean episode length: 233.05
    Episode_Reward/reaching_object: 0.5890
     Episode_Reward/lifting_object: 122.1751
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.88s
                      Time elapsed: 00:06:54
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 107797 steps/s (collection: 0.814s, learning 0.098s)
             Mean action noise std: 1.79
          Mean value_function loss: 256.5101
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7856
                       Mean reward: 660.18
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 0.6065
     Episode_Reward/lifting_object: 125.6019
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.91s
                      Time elapsed: 00:06:55
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 107558 steps/s (collection: 0.796s, learning 0.118s)
             Mean action noise std: 1.79
          Mean value_function loss: 237.7820
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7845
                       Mean reward: 667.30
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.6272
     Episode_Reward/lifting_object: 134.4979
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.91s
                      Time elapsed: 00:06:56
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 108810 steps/s (collection: 0.788s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 247.1212
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7857
                       Mean reward: 703.60
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.6430
     Episode_Reward/lifting_object: 136.9671
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.90s
                      Time elapsed: 00:06:57
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 110555 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 270.3751
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 15.7887
                       Mean reward: 728.01
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 135.6192
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.89s
                      Time elapsed: 00:06:58
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 107164 steps/s (collection: 0.801s, learning 0.117s)
             Mean action noise std: 1.80
          Mean value_function loss: 219.3085
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7917
                       Mean reward: 696.04
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.6577
     Episode_Reward/lifting_object: 144.3392
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.92s
                      Time elapsed: 00:06:58
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 106414 steps/s (collection: 0.804s, learning 0.120s)
             Mean action noise std: 1.80
          Mean value_function loss: 264.3666
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7935
                       Mean reward: 686.58
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 0.6396
     Episode_Reward/lifting_object: 138.2604
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.92s
                      Time elapsed: 00:06:59
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 106990 steps/s (collection: 0.804s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 236.8947
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.7937
                       Mean reward: 698.73
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 0.6418
     Episode_Reward/lifting_object: 139.5677
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.92s
                      Time elapsed: 00:07:00
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 108916 steps/s (collection: 0.803s, learning 0.100s)
             Mean action noise std: 1.80
          Mean value_function loss: 225.7225
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.7929
                       Mean reward: 662.07
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.6535
     Episode_Reward/lifting_object: 141.2053
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.90s
                      Time elapsed: 00:07:01
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 110728 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 1.80
          Mean value_function loss: 214.8559
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.7918
                       Mean reward: 710.23
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.6506
     Episode_Reward/lifting_object: 140.1984
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.89s
                      Time elapsed: 00:07:02
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 108664 steps/s (collection: 0.791s, learning 0.114s)
             Mean action noise std: 1.80
          Mean value_function loss: 210.4180
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7916
                       Mean reward: 644.22
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.6270
     Episode_Reward/lifting_object: 134.2861
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.90s
                      Time elapsed: 00:07:03
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 108960 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 215.7773
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7917
                       Mean reward: 713.86
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 0.6533
     Episode_Reward/lifting_object: 142.9565
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.90s
                      Time elapsed: 00:07:04
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 111151 steps/s (collection: 0.769s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 201.0765
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7917
                       Mean reward: 724.51
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.6412
     Episode_Reward/lifting_object: 139.9245
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.88s
                      Time elapsed: 00:07:05
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 110722 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 1.80
          Mean value_function loss: 209.8214
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7931
                       Mean reward: 721.41
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.6653
     Episode_Reward/lifting_object: 146.0548
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.89s
                      Time elapsed: 00:07:06
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 110795 steps/s (collection: 0.781s, learning 0.106s)
             Mean action noise std: 1.80
          Mean value_function loss: 200.6533
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7951
                       Mean reward: 650.77
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 0.6468
     Episode_Reward/lifting_object: 139.9804
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.89s
                      Time elapsed: 00:07:07
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 110735 steps/s (collection: 0.783s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 210.2589
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7971
                       Mean reward: 789.15
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.6845
     Episode_Reward/lifting_object: 148.7605
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.89s
                      Time elapsed: 00:07:07
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 111850 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 197.5568
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7992
                       Mean reward: 731.54
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.6668
     Episode_Reward/lifting_object: 146.6850
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.88s
                      Time elapsed: 00:07:08
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 108843 steps/s (collection: 0.786s, learning 0.117s)
             Mean action noise std: 1.80
          Mean value_function loss: 209.5018
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8022
                       Mean reward: 760.50
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.6863
     Episode_Reward/lifting_object: 150.5846
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.90s
                      Time elapsed: 00:07:09
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 108462 steps/s (collection: 0.789s, learning 0.118s)
             Mean action noise std: 1.80
          Mean value_function loss: 194.8273
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.8022
                       Mean reward: 711.88
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 0.6644
     Episode_Reward/lifting_object: 146.1727
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.91s
                      Time elapsed: 00:07:10
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 109804 steps/s (collection: 0.777s, learning 0.118s)
             Mean action noise std: 1.80
          Mean value_function loss: 176.6746
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.8007
                       Mean reward: 749.76
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 0.6568
     Episode_Reward/lifting_object: 144.1860
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.90s
                      Time elapsed: 00:07:11
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 111649 steps/s (collection: 0.788s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 191.6924
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7999
                       Mean reward: 732.44
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.6617
     Episode_Reward/lifting_object: 143.7550
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.88s
                      Time elapsed: 00:07:12
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 110042 steps/s (collection: 0.779s, learning 0.114s)
             Mean action noise std: 1.80
          Mean value_function loss: 181.3876
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.8009
                       Mean reward: 733.58
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 0.6668
     Episode_Reward/lifting_object: 145.9980
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.89s
                      Time elapsed: 00:07:13
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 110939 steps/s (collection: 0.776s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 194.7166
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 15.8047
                       Mean reward: 745.28
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.6785
     Episode_Reward/lifting_object: 148.6631
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.89s
                      Time elapsed: 00:07:14
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 104259 steps/s (collection: 0.823s, learning 0.120s)
             Mean action noise std: 1.80
          Mean value_function loss: 180.6225
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.8069
                       Mean reward: 718.63
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.6677
     Episode_Reward/lifting_object: 145.5162
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.94s
                      Time elapsed: 00:07:15
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 112642 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 176.4498
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.8068
                       Mean reward: 771.31
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.6657
     Episode_Reward/lifting_object: 144.8083
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.87s
                      Time elapsed: 00:07:16
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 109681 steps/s (collection: 0.794s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 181.9857
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.8066
                       Mean reward: 774.92
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.6894
     Episode_Reward/lifting_object: 153.0068
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.90s
                      Time elapsed: 00:07:16
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 108725 steps/s (collection: 0.790s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 209.3297
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8080
                       Mean reward: 796.55
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.7073
     Episode_Reward/lifting_object: 156.7999
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.90s
                      Time elapsed: 00:07:17
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 106954 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 183.2675
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.8099
                       Mean reward: 735.31
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: 148.1338
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.92s
                      Time elapsed: 00:07:18
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 106323 steps/s (collection: 0.810s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 181.9037
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.8108
                       Mean reward: 715.04
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 0.6906
     Episode_Reward/lifting_object: 151.8020
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.92s
                      Time elapsed: 00:07:19
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 100464 steps/s (collection: 0.861s, learning 0.118s)
             Mean action noise std: 1.80
          Mean value_function loss: 166.3177
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.8115
                       Mean reward: 771.14
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.6815
     Episode_Reward/lifting_object: 151.7222
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.98s
                      Time elapsed: 00:07:20
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 103082 steps/s (collection: 0.837s, learning 0.116s)
             Mean action noise std: 1.80
          Mean value_function loss: 181.8113
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8125
                       Mean reward: 756.79
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.6837
     Episode_Reward/lifting_object: 151.3053
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.95s
                      Time elapsed: 00:07:21
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 100071 steps/s (collection: 0.858s, learning 0.125s)
             Mean action noise std: 1.80
          Mean value_function loss: 191.9765
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.8144
                       Mean reward: 728.48
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 0.6687
     Episode_Reward/lifting_object: 146.9139
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.98s
                      Time elapsed: 00:07:22
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 104206 steps/s (collection: 0.829s, learning 0.114s)
             Mean action noise std: 1.80
          Mean value_function loss: 174.8132
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8169
                       Mean reward: 717.34
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 0.6616
     Episode_Reward/lifting_object: 143.1133
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.94s
                      Time elapsed: 00:07:23
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 99402 steps/s (collection: 0.863s, learning 0.126s)
             Mean action noise std: 1.81
          Mean value_function loss: 173.3171
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8185
                       Mean reward: 749.71
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.6667
     Episode_Reward/lifting_object: 146.4940
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.99s
                      Time elapsed: 00:07:24
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 104764 steps/s (collection: 0.818s, learning 0.121s)
             Mean action noise std: 1.81
          Mean value_function loss: 191.4635
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8197
                       Mean reward: 765.25
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.6939
     Episode_Reward/lifting_object: 151.1955
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.94s
                      Time elapsed: 00:07:25
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 92448 steps/s (collection: 0.900s, learning 0.164s)
             Mean action noise std: 1.81
          Mean value_function loss: 169.4831
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.8200
                       Mean reward: 776.27
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.6991
     Episode_Reward/lifting_object: 154.1067
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 1.06s
                      Time elapsed: 00:07:26
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 71207 steps/s (collection: 1.121s, learning 0.260s)
             Mean action noise std: 1.81
          Mean value_function loss: 154.6758
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.8197
                       Mean reward: 846.20
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.7040
     Episode_Reward/lifting_object: 156.9623
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 1.38s
                      Time elapsed: 00:07:27
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 87753 steps/s (collection: 0.950s, learning 0.170s)
             Mean action noise std: 1.81
          Mean value_function loss: 172.8529
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8212
                       Mean reward: 797.31
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.7007
     Episode_Reward/lifting_object: 155.7094
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 1.12s
                      Time elapsed: 00:07:28
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 78811 steps/s (collection: 1.053s, learning 0.195s)
             Mean action noise std: 1.81
          Mean value_function loss: 171.5900
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.8208
                       Mean reward: 757.40
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 153.8296
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 1.25s
                      Time elapsed: 00:07:30
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 91045 steps/s (collection: 0.921s, learning 0.159s)
             Mean action noise std: 1.81
          Mean value_function loss: 177.7600
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.8203
                       Mean reward: 772.33
               Mean episode length: 244.07
    Episode_Reward/reaching_object: 0.6903
     Episode_Reward/lifting_object: 153.1844
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 1.08s
                      Time elapsed: 00:07:31
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 83251 steps/s (collection: 0.935s, learning 0.246s)
             Mean action noise std: 1.81
          Mean value_function loss: 165.0702
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.8213
                       Mean reward: 769.42
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.6872
     Episode_Reward/lifting_object: 150.2266
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 1.18s
                      Time elapsed: 00:07:32
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 71272 steps/s (collection: 1.210s, learning 0.169s)
             Mean action noise std: 1.81
          Mean value_function loss: 194.2319
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8214
                       Mean reward: 786.91
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.7072
     Episode_Reward/lifting_object: 156.0816
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 1.38s
                      Time elapsed: 00:07:33
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 75287 steps/s (collection: 1.154s, learning 0.151s)
             Mean action noise std: 1.81
          Mean value_function loss: 171.0727
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8229
                       Mean reward: 800.96
               Mean episode length: 244.21
    Episode_Reward/reaching_object: 0.7082
     Episode_Reward/lifting_object: 156.6951
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 1.31s
                      Time elapsed: 00:07:35
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 77820 steps/s (collection: 1.055s, learning 0.208s)
             Mean action noise std: 1.81
          Mean value_function loss: 164.7507
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8259
                       Mean reward: 774.10
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 0.6863
     Episode_Reward/lifting_object: 152.4072
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 1.26s
                      Time elapsed: 00:07:36
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 75222 steps/s (collection: 1.092s, learning 0.215s)
             Mean action noise std: 1.81
          Mean value_function loss: 121.1621
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8283
                       Mean reward: 769.63
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 152.2851
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.31s
                      Time elapsed: 00:07:37
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 76982 steps/s (collection: 1.073s, learning 0.204s)
             Mean action noise std: 1.81
          Mean value_function loss: 149.4604
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8352
                       Mean reward: 795.66
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.7169
     Episode_Reward/lifting_object: 158.3609
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.28s
                      Time elapsed: 00:07:39
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 79268 steps/s (collection: 0.945s, learning 0.295s)
             Mean action noise std: 1.81
          Mean value_function loss: 124.2427
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8410
                       Mean reward: 810.60
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 158.9246
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 1.24s
                      Time elapsed: 00:07:40
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 71450 steps/s (collection: 1.246s, learning 0.130s)
             Mean action noise std: 1.81
          Mean value_function loss: 148.6205
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8444
                       Mean reward: 808.52
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.7109
     Episode_Reward/lifting_object: 158.5695
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 1.38s
                      Time elapsed: 00:07:41
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 81274 steps/s (collection: 1.029s, learning 0.181s)
             Mean action noise std: 1.81
          Mean value_function loss: 129.9484
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.8501
                       Mean reward: 788.42
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.6953
     Episode_Reward/lifting_object: 153.4103
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 1.21s
                      Time elapsed: 00:07:42
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 82644 steps/s (collection: 1.035s, learning 0.155s)
             Mean action noise std: 1.82
          Mean value_function loss: 119.2885
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8544
                       Mean reward: 751.42
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 0.6678
     Episode_Reward/lifting_object: 148.9153
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 1.19s
                      Time elapsed: 00:07:44
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 81111 steps/s (collection: 1.004s, learning 0.208s)
             Mean action noise std: 1.82
          Mean value_function loss: 132.8047
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8608
                       Mean reward: 765.65
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.6813
     Episode_Reward/lifting_object: 151.8076
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 1.21s
                      Time elapsed: 00:07:45
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 76000 steps/s (collection: 1.145s, learning 0.148s)
             Mean action noise std: 1.82
          Mean value_function loss: 114.9609
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8678
                       Mean reward: 813.36
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 0.6982
     Episode_Reward/lifting_object: 156.0291
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 1.29s
                      Time elapsed: 00:07:46
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 72762 steps/s (collection: 1.187s, learning 0.164s)
             Mean action noise std: 1.82
          Mean value_function loss: 119.0440
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8696
                       Mean reward: 808.60
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.7041
     Episode_Reward/lifting_object: 158.7439
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.35s
                      Time elapsed: 00:07:47
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 79910 steps/s (collection: 1.080s, learning 0.150s)
             Mean action noise std: 1.82
          Mean value_function loss: 110.2851
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.8703
                       Mean reward: 809.03
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.7051
     Episode_Reward/lifting_object: 158.0007
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 1.23s
                      Time elapsed: 00:07:49
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 62952 steps/s (collection: 1.297s, learning 0.265s)
             Mean action noise std: 1.82
          Mean value_function loss: 138.7374
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8707
                       Mean reward: 808.78
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 159.8541
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 1.56s
                      Time elapsed: 00:07:50
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 76281 steps/s (collection: 1.100s, learning 0.189s)
             Mean action noise std: 1.82
          Mean value_function loss: 126.7560
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8720
                       Mean reward: 788.87
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.7221
     Episode_Reward/lifting_object: 161.0655
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 1.29s
                      Time elapsed: 00:07:51
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 80970 steps/s (collection: 1.033s, learning 0.182s)
             Mean action noise std: 1.82
          Mean value_function loss: 127.7882
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8732
                       Mean reward: 806.62
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 161.9163
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 1.21s
                      Time elapsed: 00:07:53
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 77872 steps/s (collection: 1.002s, learning 0.261s)
             Mean action noise std: 1.82
          Mean value_function loss: 130.7148
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8769
                       Mean reward: 805.82
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7049
     Episode_Reward/lifting_object: 158.2213
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 1.26s
                      Time elapsed: 00:07:54
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 93497 steps/s (collection: 0.928s, learning 0.123s)
             Mean action noise std: 1.82
          Mean value_function loss: 104.7422
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.8813
                       Mean reward: 801.23
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 160.1023
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 1.05s
                      Time elapsed: 00:07:55
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 101992 steps/s (collection: 0.834s, learning 0.130s)
             Mean action noise std: 1.82
          Mean value_function loss: 108.9437
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8839
                       Mean reward: 790.99
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7263
     Episode_Reward/lifting_object: 162.9763
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.96s
                      Time elapsed: 00:07:56
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 103990 steps/s (collection: 0.816s, learning 0.130s)
             Mean action noise std: 1.82
          Mean value_function loss: 100.1165
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.8868
                       Mean reward: 806.04
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 164.0181
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.95s
                      Time elapsed: 00:07:57
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 77046 steps/s (collection: 1.017s, learning 0.259s)
             Mean action noise std: 1.82
          Mean value_function loss: 107.3901
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8897
                       Mean reward: 826.78
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7278
     Episode_Reward/lifting_object: 163.1139
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 1.28s
                      Time elapsed: 00:07:58
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 78573 steps/s (collection: 1.065s, learning 0.187s)
             Mean action noise std: 1.83
          Mean value_function loss: 119.5366
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8951
                       Mean reward: 811.15
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.7273
     Episode_Reward/lifting_object: 161.6115
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 1.25s
                      Time elapsed: 00:07:59
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 77191 steps/s (collection: 1.093s, learning 0.180s)
             Mean action noise std: 1.83
          Mean value_function loss: 137.2568
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.9039
                       Mean reward: 791.39
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.7293
     Episode_Reward/lifting_object: 162.7978
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 1.27s
                      Time elapsed: 00:08:01
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 66553 steps/s (collection: 1.228s, learning 0.249s)
             Mean action noise std: 1.83
          Mean value_function loss: 121.7284
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9052
                       Mean reward: 832.05
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 162.9693
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 1.48s
                      Time elapsed: 00:08:02
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 82781 steps/s (collection: 0.962s, learning 0.226s)
             Mean action noise std: 1.83
          Mean value_function loss: 98.2489
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.9083
                       Mean reward: 828.11
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7103
     Episode_Reward/lifting_object: 160.2495
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 1.19s
                      Time elapsed: 00:08:03
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 78277 steps/s (collection: 1.076s, learning 0.180s)
             Mean action noise std: 1.83
          Mean value_function loss: 115.4152
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 15.9112
                       Mean reward: 842.80
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.7280
     Episode_Reward/lifting_object: 162.3750
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 1.26s
                      Time elapsed: 00:08:05
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 86690 steps/s (collection: 0.981s, learning 0.153s)
             Mean action noise std: 1.83
          Mean value_function loss: 108.6027
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9116
                       Mean reward: 816.96
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.7174
     Episode_Reward/lifting_object: 160.5588
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 1.13s
                      Time elapsed: 00:08:06
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 92945 steps/s (collection: 0.941s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 119.0233
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9124
                       Mean reward: 815.41
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.7137
     Episode_Reward/lifting_object: 159.0448
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.06s
                      Time elapsed: 00:08:07
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 66880 steps/s (collection: 1.238s, learning 0.232s)
             Mean action noise std: 1.83
          Mean value_function loss: 115.3609
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 15.9145
                       Mean reward: 822.43
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7249
     Episode_Reward/lifting_object: 162.6945
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 1.47s
                      Time elapsed: 00:08:08
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 80975 steps/s (collection: 1.020s, learning 0.194s)
             Mean action noise std: 1.83
          Mean value_function loss: 134.1165
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9198
                       Mean reward: 795.58
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.7070
     Episode_Reward/lifting_object: 157.3444
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 1.21s
                      Time elapsed: 00:08:10
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 75260 steps/s (collection: 0.975s, learning 0.331s)
             Mean action noise std: 1.83
          Mean value_function loss: 131.6377
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.9252
                       Mean reward: 813.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 160.4932
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 1.31s
                      Time elapsed: 00:08:11
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 74743 steps/s (collection: 1.180s, learning 0.135s)
             Mean action noise std: 1.83
          Mean value_function loss: 104.1928
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9260
                       Mean reward: 808.04
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 0.7137
     Episode_Reward/lifting_object: 159.8649
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 1.32s
                      Time elapsed: 00:08:12
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 79120 steps/s (collection: 1.065s, learning 0.178s)
             Mean action noise std: 1.83
          Mean value_function loss: 97.3549
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.9299
                       Mean reward: 845.07
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7345
     Episode_Reward/lifting_object: 164.3273
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 1.24s
                      Time elapsed: 00:08:13
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 90708 steps/s (collection: 0.960s, learning 0.124s)
             Mean action noise std: 1.84
          Mean value_function loss: 97.0125
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.9338
                       Mean reward: 805.62
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 161.2381
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 1.08s
                      Time elapsed: 00:08:14
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 86702 steps/s (collection: 1.018s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 124.8717
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.9395
                       Mean reward: 828.94
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7346
     Episode_Reward/lifting_object: 164.9971
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 1.13s
                      Time elapsed: 00:08:16
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 70463 steps/s (collection: 1.141s, learning 0.255s)
             Mean action noise std: 1.84
          Mean value_function loss: 119.2111
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9432
                       Mean reward: 822.01
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 161.6657
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 1.40s
                      Time elapsed: 00:08:17
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 79671 steps/s (collection: 1.118s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 117.7437
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9435
                       Mean reward: 794.33
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: 161.1857
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 1.23s
                      Time elapsed: 00:08:18
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 77523 steps/s (collection: 1.102s, learning 0.166s)
             Mean action noise std: 1.84
          Mean value_function loss: 124.6585
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.9434
                       Mean reward: 821.26
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.7237
     Episode_Reward/lifting_object: 162.0923
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 1.27s
                      Time elapsed: 00:08:19
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 63950 steps/s (collection: 1.294s, learning 0.243s)
             Mean action noise std: 1.84
          Mean value_function loss: 121.9367
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.9425
                       Mean reward: 838.41
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 164.6970
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 1.54s
                      Time elapsed: 00:08:21
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 67018 steps/s (collection: 1.291s, learning 0.176s)
             Mean action noise std: 1.84
          Mean value_function loss: 134.2785
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.9419
                       Mean reward: 849.30
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 165.9852
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 1.47s
                      Time elapsed: 00:08:23
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 68732 steps/s (collection: 1.243s, learning 0.188s)
             Mean action noise std: 1.84
          Mean value_function loss: 111.9404
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.9460
                       Mean reward: 807.88
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: 164.1185
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 1.43s
                      Time elapsed: 00:08:24
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 82391 steps/s (collection: 1.040s, learning 0.153s)
             Mean action noise std: 1.84
          Mean value_function loss: 117.3008
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.9525
                       Mean reward: 798.15
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 0.7214
     Episode_Reward/lifting_object: 161.2915
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 1.19s
                      Time elapsed: 00:08:25
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 61250 steps/s (collection: 1.259s, learning 0.346s)
             Mean action noise std: 1.84
          Mean value_function loss: 122.1878
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9604
                       Mean reward: 788.00
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.7302
     Episode_Reward/lifting_object: 162.7316
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 1.60s
                      Time elapsed: 00:08:27
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 66934 steps/s (collection: 1.348s, learning 0.121s)
             Mean action noise std: 1.84
          Mean value_function loss: 105.4058
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9629
                       Mean reward: 795.78
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7224
     Episode_Reward/lifting_object: 160.4000
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 1.47s
                      Time elapsed: 00:08:28
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 93587 steps/s (collection: 0.927s, learning 0.124s)
             Mean action noise std: 1.84
          Mean value_function loss: 119.1568
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.9638
                       Mean reward: 794.38
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7186
     Episode_Reward/lifting_object: 158.6371
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 1.05s
                      Time elapsed: 00:08:29
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 86270 steps/s (collection: 0.962s, learning 0.178s)
             Mean action noise std: 1.84
          Mean value_function loss: 102.9682
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9660
                       Mean reward: 795.21
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 0.7123
     Episode_Reward/lifting_object: 157.5066
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 1.14s
                      Time elapsed: 00:08:30
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 85196 steps/s (collection: 0.969s, learning 0.185s)
             Mean action noise std: 1.84
          Mean value_function loss: 93.6360
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9665
                       Mean reward: 822.04
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.7269
     Episode_Reward/lifting_object: 162.2722
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 1.15s
                      Time elapsed: 00:08:32
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 98368 steps/s (collection: 0.872s, learning 0.127s)
             Mean action noise std: 1.84
          Mean value_function loss: 101.2919
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.9683
                       Mean reward: 791.91
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.7263
     Episode_Reward/lifting_object: 161.8660
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 1.00s
                      Time elapsed: 00:08:33
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 85343 steps/s (collection: 0.971s, learning 0.181s)
             Mean action noise std: 1.84
          Mean value_function loss: 108.5976
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.9686
                       Mean reward: 782.18
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 0.7233
     Episode_Reward/lifting_object: 160.4836
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 1.15s
                      Time elapsed: 00:08:34
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 96222 steps/s (collection: 0.899s, learning 0.123s)
             Mean action noise std: 1.84
          Mean value_function loss: 108.7964
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9680
                       Mean reward: 845.25
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 167.0472
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 1.02s
                      Time elapsed: 00:08:35
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 97933 steps/s (collection: 0.817s, learning 0.187s)
             Mean action noise std: 1.84
          Mean value_function loss: 76.8625
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.9690
                       Mean reward: 807.11
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.7283
     Episode_Reward/lifting_object: 162.9630
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 1.00s
                      Time elapsed: 00:08:36
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 95960 steps/s (collection: 0.865s, learning 0.159s)
             Mean action noise std: 1.85
          Mean value_function loss: 92.4913
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.9700
                       Mean reward: 831.56
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7340
     Episode_Reward/lifting_object: 164.2804
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 1.02s
                      Time elapsed: 00:08:37
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 98972 steps/s (collection: 0.839s, learning 0.154s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.2108
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.9749
                       Mean reward: 832.16
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 165.0337
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.99s
                      Time elapsed: 00:08:38
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 94991 steps/s (collection: 0.880s, learning 0.155s)
             Mean action noise std: 1.85
          Mean value_function loss: 100.8193
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9815
                       Mean reward: 818.50
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 165.9501
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 1.03s
                      Time elapsed: 00:08:39
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 104131 steps/s (collection: 0.840s, learning 0.104s)
             Mean action noise std: 1.85
          Mean value_function loss: 96.4686
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.9854
                       Mean reward: 842.33
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 165.1308
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.94s
                      Time elapsed: 00:08:40
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 92625 steps/s (collection: 0.928s, learning 0.133s)
             Mean action noise std: 1.85
          Mean value_function loss: 79.2948
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9883
                       Mean reward: 854.30
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 166.3624
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 1.06s
                      Time elapsed: 00:08:41
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 103075 steps/s (collection: 0.846s, learning 0.108s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.4684
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9919
                       Mean reward: 825.14
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 166.9874
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.95s
                      Time elapsed: 00:08:42
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 96191 steps/s (collection: 0.829s, learning 0.193s)
             Mean action noise std: 1.85
          Mean value_function loss: 93.8229
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9961
                       Mean reward: 822.99
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 164.6287
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 1.02s
                      Time elapsed: 00:08:43
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 98318 steps/s (collection: 0.880s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 89.1903
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9960
                       Mean reward: 813.73
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 161.5366
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 1.00s
                      Time elapsed: 00:08:44
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 100154 steps/s (collection: 0.826s, learning 0.156s)
             Mean action noise std: 1.85
          Mean value_function loss: 80.7123
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.9972
                       Mean reward: 825.89
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 165.5902
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.98s
                      Time elapsed: 00:08:45
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 103961 steps/s (collection: 0.814s, learning 0.132s)
             Mean action noise std: 1.85
          Mean value_function loss: 75.9775
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0036
                       Mean reward: 815.95
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 169.7169
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.95s
                      Time elapsed: 00:08:46
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 90777 steps/s (collection: 0.861s, learning 0.222s)
             Mean action noise std: 1.85
          Mean value_function loss: 71.1800
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0077
                       Mean reward: 829.72
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.7425
     Episode_Reward/lifting_object: 165.7884
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 1.08s
                      Time elapsed: 00:08:47
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 83420 steps/s (collection: 1.041s, learning 0.137s)
             Mean action noise std: 1.86
          Mean value_function loss: 64.5343
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0126
                       Mean reward: 827.01
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 167.1095
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 1.18s
                      Time elapsed: 00:08:48
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 98580 steps/s (collection: 0.902s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 83.9622
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.0166
                       Mean reward: 834.96
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 167.7563
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 1.00s
                      Time elapsed: 00:08:49
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 97714 steps/s (collection: 0.902s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 66.6168
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.0195
                       Mean reward: 836.94
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 167.2516
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 1.01s
                      Time elapsed: 00:08:50
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 91491 steps/s (collection: 0.906s, learning 0.168s)
             Mean action noise std: 1.86
          Mean value_function loss: 67.7663
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0220
                       Mean reward: 862.84
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 167.0754
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 1.07s
                      Time elapsed: 00:08:51
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 99576 steps/s (collection: 0.894s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 62.2675
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0233
                       Mean reward: 830.64
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 168.8043
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.99s
                      Time elapsed: 00:08:52
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 103505 steps/s (collection: 0.842s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 69.7943
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.0274
                       Mean reward: 835.00
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.1489
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.95s
                      Time elapsed: 00:08:53
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 102552 steps/s (collection: 0.849s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 66.2539
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.0293
                       Mean reward: 834.52
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 167.6239
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.96s
                      Time elapsed: 00:08:54
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 107933 steps/s (collection: 0.803s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 56.7266
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0339
                       Mean reward: 822.95
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 166.0879
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.91s
                      Time elapsed: 00:08:55
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 101153 steps/s (collection: 0.834s, learning 0.138s)
             Mean action noise std: 1.86
          Mean value_function loss: 66.4208
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0402
                       Mean reward: 847.66
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 167.5966
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.97s
                      Time elapsed: 00:08:56
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 95268 steps/s (collection: 0.817s, learning 0.215s)
             Mean action noise std: 1.86
          Mean value_function loss: 81.3100
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0446
                       Mean reward: 852.22
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 170.0906
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 1.03s
                      Time elapsed: 00:08:57
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 94125 steps/s (collection: 0.941s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 76.5198
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0487
                       Mean reward: 830.35
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 166.0416
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 1.04s
                      Time elapsed: 00:08:58
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 74893 steps/s (collection: 1.057s, learning 0.256s)
             Mean action noise std: 1.87
          Mean value_function loss: 72.2470
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0512
                       Mean reward: 840.63
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 168.4235
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 1.31s
                      Time elapsed: 00:08:59
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 83960 steps/s (collection: 1.051s, learning 0.120s)
             Mean action noise std: 1.87
          Mean value_function loss: 75.9274
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0513
                       Mean reward: 845.17
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 167.0793
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 1.17s
                      Time elapsed: 00:09:00
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 102675 steps/s (collection: 0.860s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 83.3616
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0555
                       Mean reward: 843.18
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 165.4904
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.96s
                      Time elapsed: 00:09:01
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 84059 steps/s (collection: 0.938s, learning 0.232s)
             Mean action noise std: 1.87
          Mean value_function loss: 74.8225
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0623
                       Mean reward: 842.23
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 166.8300
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 1.17s
                      Time elapsed: 00:09:02
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 94335 steps/s (collection: 0.931s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 74.2590
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0727
                       Mean reward: 854.21
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 168.9046
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 1.04s
                      Time elapsed: 00:09:04
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 107121 steps/s (collection: 0.794s, learning 0.124s)
             Mean action noise std: 1.87
          Mean value_function loss: 93.8616
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0849
                       Mean reward: 856.64
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 169.1429
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.92s
                      Time elapsed: 00:09:04
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 99596 steps/s (collection: 0.864s, learning 0.123s)
             Mean action noise std: 1.88
          Mean value_function loss: 89.0134
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0940
                       Mean reward: 830.07
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 164.6795
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.99s
                      Time elapsed: 00:09:05
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 97680 steps/s (collection: 0.871s, learning 0.136s)
             Mean action noise std: 1.88
          Mean value_function loss: 69.8814
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1037
                       Mean reward: 841.61
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.7380
     Episode_Reward/lifting_object: 165.6333
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 1.01s
                      Time elapsed: 00:09:06
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 96806 steps/s (collection: 0.892s, learning 0.124s)
             Mean action noise std: 1.88
          Mean value_function loss: 78.8276
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1148
                       Mean reward: 847.72
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 167.0078
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.02s
                      Time elapsed: 00:09:07
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 103381 steps/s (collection: 0.850s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 76.7505
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1242
                       Mean reward: 866.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 169.1882
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.95s
                      Time elapsed: 00:09:08
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 107210 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 88.2396
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1307
                       Mean reward: 829.39
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 166.6055
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.92s
                      Time elapsed: 00:09:09
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 106951 steps/s (collection: 0.806s, learning 0.113s)
             Mean action noise std: 1.88
          Mean value_function loss: 83.2374
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1346
                       Mean reward: 825.10
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.7310
     Episode_Reward/lifting_object: 164.5617
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.92s
                      Time elapsed: 00:09:10
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 109161 steps/s (collection: 0.797s, learning 0.103s)
             Mean action noise std: 1.88
          Mean value_function loss: 85.1967
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.1352
                       Mean reward: 798.78
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.7255
     Episode_Reward/lifting_object: 163.0532
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.90s
                      Time elapsed: 00:09:11
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 102266 steps/s (collection: 0.852s, learning 0.109s)
             Mean action noise std: 1.89
          Mean value_function loss: 74.1996
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.1368
                       Mean reward: 831.49
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 166.8889
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.96s
                      Time elapsed: 00:09:12
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 97203 steps/s (collection: 0.901s, learning 0.111s)
             Mean action noise std: 1.89
          Mean value_function loss: 69.8551
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1405
                       Mean reward: 808.63
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.7230
     Episode_Reward/lifting_object: 162.9778
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 1.01s
                      Time elapsed: 00:09:13
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 98395 steps/s (collection: 0.895s, learning 0.105s)
             Mean action noise std: 1.89
          Mean value_function loss: 57.9880
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.1461
                       Mean reward: 816.59
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 165.3904
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 1.00s
                      Time elapsed: 00:09:14
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 97647 steps/s (collection: 0.889s, learning 0.118s)
             Mean action noise std: 1.89
          Mean value_function loss: 54.2055
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1538
                       Mean reward: 824.95
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 0.7415
     Episode_Reward/lifting_object: 167.2045
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 1.01s
                      Time elapsed: 00:09:15
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 97340 steps/s (collection: 0.891s, learning 0.119s)
             Mean action noise std: 1.89
          Mean value_function loss: 71.0301
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.1648
                       Mean reward: 822.76
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 166.6870
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 1.01s
                      Time elapsed: 00:09:16
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 100987 steps/s (collection: 0.870s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 70.6973
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.1688
                       Mean reward: 844.08
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.7412
     Episode_Reward/lifting_object: 165.6965
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.97s
                      Time elapsed: 00:09:17
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 102084 steps/s (collection: 0.858s, learning 0.105s)
             Mean action noise std: 1.89
          Mean value_function loss: 60.5571
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.1704
                       Mean reward: 834.43
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 166.7979
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.96s
                      Time elapsed: 00:09:18
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 107215 steps/s (collection: 0.815s, learning 0.102s)
             Mean action noise std: 1.90
          Mean value_function loss: 56.0846
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.1777
                       Mean reward: 854.66
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 167.4743
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.92s
                      Time elapsed: 00:09:19
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 107627 steps/s (collection: 0.812s, learning 0.101s)
             Mean action noise std: 1.90
          Mean value_function loss: 76.0918
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.1915
                       Mean reward: 825.69
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 165.6687
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.91s
                      Time elapsed: 00:09:20
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 97013 steps/s (collection: 0.897s, learning 0.116s)
             Mean action noise std: 1.90
          Mean value_function loss: 66.5739
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.1997
                       Mean reward: 833.17
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 167.5597
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 1.01s
                      Time elapsed: 00:09:21
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 101353 steps/s (collection: 0.876s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 60.7684
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.2061
                       Mean reward: 831.67
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 168.0957
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.97s
                      Time elapsed: 00:09:22
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 104021 steps/s (collection: 0.848s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 67.3533
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.2158
                       Mean reward: 849.25
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 167.4869
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.95s
                      Time elapsed: 00:09:23
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 102740 steps/s (collection: 0.857s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 56.3412
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.2214
                       Mean reward: 852.54
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 168.1967
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.96s
                      Time elapsed: 00:09:24
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 103356 steps/s (collection: 0.855s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 72.7804
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.2241
                       Mean reward: 854.05
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 170.8379
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.95s
                      Time elapsed: 00:09:25
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 97094 steps/s (collection: 0.854s, learning 0.158s)
             Mean action noise std: 1.91
          Mean value_function loss: 56.5931
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2334
                       Mean reward: 820.72
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 167.9116
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 1.01s
                      Time elapsed: 00:09:26
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 79311 steps/s (collection: 1.037s, learning 0.202s)
             Mean action noise std: 1.91
          Mean value_function loss: 55.8859
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.2415
                       Mean reward: 828.42
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 167.2383
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 1.24s
                      Time elapsed: 00:09:27
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 83461 steps/s (collection: 1.009s, learning 0.169s)
             Mean action noise std: 1.91
          Mean value_function loss: 50.6492
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2455
                       Mean reward: 859.37
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.0134
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 1.18s
                      Time elapsed: 00:09:28
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 74408 steps/s (collection: 1.091s, learning 0.230s)
             Mean action noise std: 1.91
          Mean value_function loss: 50.3591
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.2508
                       Mean reward: 844.42
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 167.9301
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 1.32s
                      Time elapsed: 00:09:29
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 89053 steps/s (collection: 0.918s, learning 0.186s)
             Mean action noise std: 1.92
          Mean value_function loss: 50.1377
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.2603
                       Mean reward: 839.64
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 166.6908
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 1.10s
                      Time elapsed: 00:09:31
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 100847 steps/s (collection: 0.860s, learning 0.115s)
             Mean action noise std: 1.92
          Mean value_function loss: 83.4571
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2735
                       Mean reward: 845.67
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 169.2687
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.97s
                      Time elapsed: 00:09:32
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 97919 steps/s (collection: 0.843s, learning 0.161s)
             Mean action noise std: 1.92
          Mean value_function loss: 97.7407
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.2837
                       Mean reward: 835.00
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 168.4123
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 1.00s
                      Time elapsed: 00:09:33
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 101450 steps/s (collection: 0.861s, learning 0.108s)
             Mean action noise std: 1.92
          Mean value_function loss: 63.2375
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2919
                       Mean reward: 823.89
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 166.7906
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.97s
                      Time elapsed: 00:09:34
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 102674 steps/s (collection: 0.809s, learning 0.148s)
             Mean action noise std: 1.93
          Mean value_function loss: 62.4910
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.3021
                       Mean reward: 835.08
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 169.1264
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.96s
                      Time elapsed: 00:09:34
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 98815 steps/s (collection: 0.843s, learning 0.152s)
             Mean action noise std: 1.93
          Mean value_function loss: 58.8363
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.3101
                       Mean reward: 837.44
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.7462
     Episode_Reward/lifting_object: 166.7901
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.99s
                      Time elapsed: 00:09:35
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 99393 steps/s (collection: 0.823s, learning 0.166s)
             Mean action noise std: 1.93
          Mean value_function loss: 65.9255
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.3142
                       Mean reward: 846.69
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 167.5670
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.99s
                      Time elapsed: 00:09:36
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 95345 steps/s (collection: 0.927s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 72.5709
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3209
                       Mean reward: 813.21
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 168.2892
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 1.03s
                      Time elapsed: 00:09:38
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 95604 steps/s (collection: 0.907s, learning 0.121s)
             Mean action noise std: 1.93
          Mean value_function loss: 53.8784
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.3220
                       Mean reward: 840.50
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 167.5550
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 1.03s
                      Time elapsed: 00:09:39
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 99966 steps/s (collection: 0.836s, learning 0.148s)
             Mean action noise std: 1.93
          Mean value_function loss: 69.3662
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3284
                       Mean reward: 869.74
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.9784
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.98s
                      Time elapsed: 00:09:40
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 103578 steps/s (collection: 0.832s, learning 0.117s)
             Mean action noise std: 1.94
          Mean value_function loss: 61.8181
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.3449
                       Mean reward: 878.50
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 169.3534
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.95s
                      Time elapsed: 00:09:40
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 101615 steps/s (collection: 0.809s, learning 0.158s)
             Mean action noise std: 1.94
          Mean value_function loss: 66.7817
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3584
                       Mean reward: 853.81
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 168.7006
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.97s
                      Time elapsed: 00:09:41
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 100362 steps/s (collection: 0.822s, learning 0.157s)
             Mean action noise std: 1.94
          Mean value_function loss: 71.9129
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.3694
                       Mean reward: 838.89
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 168.6287
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.98s
                      Time elapsed: 00:09:42
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 94493 steps/s (collection: 0.863s, learning 0.177s)
             Mean action noise std: 1.94
          Mean value_function loss: 56.5969
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.3720
                       Mean reward: 845.74
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 169.3014
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 1.04s
                      Time elapsed: 00:09:43
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 98724 steps/s (collection: 0.872s, learning 0.124s)
             Mean action noise std: 1.94
          Mean value_function loss: 72.2683
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.3780
                       Mean reward: 838.65
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 167.5493
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 1.00s
                      Time elapsed: 00:09:44
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 102272 steps/s (collection: 0.815s, learning 0.146s)
             Mean action noise std: 1.94
          Mean value_function loss: 60.5683
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.3852
                       Mean reward: 837.86
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 169.9510
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.96s
                      Time elapsed: 00:09:45
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 101048 steps/s (collection: 0.827s, learning 0.146s)
             Mean action noise std: 1.95
          Mean value_function loss: 54.0189
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.3919
                       Mean reward: 821.00
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 167.4048
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.97s
                      Time elapsed: 00:09:46
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 107980 steps/s (collection: 0.812s, learning 0.098s)
             Mean action noise std: 1.95
          Mean value_function loss: 80.6560
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.4045
                       Mean reward: 861.06
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 171.5967
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.91s
                      Time elapsed: 00:09:47
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 108388 steps/s (collection: 0.812s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 55.7145
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.4162
                       Mean reward: 845.09
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 169.3367
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.91s
                      Time elapsed: 00:09:48
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 106575 steps/s (collection: 0.826s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 60.6000
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.4267
                       Mean reward: 842.24
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 169.8812
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.92s
                      Time elapsed: 00:09:49
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 104098 steps/s (collection: 0.834s, learning 0.110s)
             Mean action noise std: 1.96
          Mean value_function loss: 68.2842
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.4415
                       Mean reward: 842.54
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 168.6394
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.94s
                      Time elapsed: 00:09:50
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 103432 steps/s (collection: 0.840s, learning 0.111s)
             Mean action noise std: 1.96
          Mean value_function loss: 59.9675
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.4517
                       Mean reward: 834.98
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.4508
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.95s
                      Time elapsed: 00:09:51
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 100538 steps/s (collection: 0.872s, learning 0.106s)
             Mean action noise std: 1.96
          Mean value_function loss: 57.6436
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4589
                       Mean reward: 828.79
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 167.9397
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.98s
                      Time elapsed: 00:09:52
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 95898 steps/s (collection: 0.907s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 68.4652
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.4673
                       Mean reward: 854.74
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 168.0237
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 1.03s
                      Time elapsed: 00:09:53
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 99750 steps/s (collection: 0.882s, learning 0.103s)
             Mean action noise std: 1.97
          Mean value_function loss: 59.8258
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.4765
                       Mean reward: 849.84
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.6174
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.99s
                      Time elapsed: 00:09:54
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 84094 steps/s (collection: 0.991s, learning 0.178s)
             Mean action noise std: 1.97
          Mean value_function loss: 59.4307
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.4874
                       Mean reward: 863.66
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.7329
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 1.17s
                      Time elapsed: 00:09:55
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 92879 steps/s (collection: 0.957s, learning 0.101s)
             Mean action noise std: 1.97
          Mean value_function loss: 57.6897
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.4910
                       Mean reward: 862.08
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 170.0378
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 1.06s
                      Time elapsed: 00:09:56
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 82055 steps/s (collection: 1.039s, learning 0.159s)
             Mean action noise std: 1.97
          Mean value_function loss: 56.4472
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.5011
                       Mean reward: 860.51
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 170.6614
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 1.20s
                      Time elapsed: 00:09:57
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 86190 steps/s (collection: 0.931s, learning 0.209s)
             Mean action noise std: 1.97
          Mean value_function loss: 49.7957
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.5104
                       Mean reward: 863.53
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 169.6611
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 1.14s
                      Time elapsed: 00:09:59
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 79508 steps/s (collection: 1.051s, learning 0.186s)
             Mean action noise std: 1.98
          Mean value_function loss: 36.1301
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.5132
                       Mean reward: 847.63
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 170.6261
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 1.24s
                      Time elapsed: 00:10:00
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 95112 steps/s (collection: 0.883s, learning 0.150s)
             Mean action noise std: 1.98
          Mean value_function loss: 46.0283
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.5194
                       Mean reward: 874.68
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 171.9680
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 1.03s
                      Time elapsed: 00:10:01
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 80992 steps/s (collection: 1.031s, learning 0.183s)
             Mean action noise std: 1.98
          Mean value_function loss: 58.6868
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.5305
                       Mean reward: 837.22
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.0476
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.21s
                      Time elapsed: 00:10:02
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 89148 steps/s (collection: 1.003s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 48.3788
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.5434
                       Mean reward: 867.51
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 168.9207
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 1.10s
                      Time elapsed: 00:10:03
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 96180 steps/s (collection: 0.857s, learning 0.165s)
             Mean action noise std: 1.98
          Mean value_function loss: 58.3224
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.5488
                       Mean reward: 872.35
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 170.0258
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 1.02s
                      Time elapsed: 00:10:04
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 84100 steps/s (collection: 1.010s, learning 0.159s)
             Mean action noise std: 1.99
          Mean value_function loss: 48.4364
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.5576
                       Mean reward: 845.03
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 169.1175
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 1.17s
                      Time elapsed: 00:10:05
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 99675 steps/s (collection: 0.883s, learning 0.103s)
             Mean action noise std: 1.99
          Mean value_function loss: 47.2320
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5703
                       Mean reward: 867.90
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.7154
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.99s
                      Time elapsed: 00:10:06
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 101257 steps/s (collection: 0.828s, learning 0.143s)
             Mean action noise std: 1.99
          Mean value_function loss: 62.7888
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.5803
                       Mean reward: 861.08
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.8269
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.97s
                      Time elapsed: 00:10:07
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 96546 steps/s (collection: 0.878s, learning 0.141s)
             Mean action noise std: 2.00
          Mean value_function loss: 52.1882
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.5894
                       Mean reward: 869.52
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 170.6617
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 1.02s
                      Time elapsed: 00:10:08
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 99026 steps/s (collection: 0.835s, learning 0.158s)
             Mean action noise std: 2.00
          Mean value_function loss: 67.0537
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.6102
                       Mean reward: 839.32
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 168.5441
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.99s
                      Time elapsed: 00:10:09
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 104726 steps/s (collection: 0.818s, learning 0.121s)
             Mean action noise std: 2.00
          Mean value_function loss: 61.0354
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.6177
                       Mean reward: 835.78
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 168.2540
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.94s
                      Time elapsed: 00:10:10
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 105690 steps/s (collection: 0.816s, learning 0.114s)
             Mean action noise std: 2.00
          Mean value_function loss: 59.8308
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6233
                       Mean reward: 848.44
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 169.6226
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.93s
                      Time elapsed: 00:10:11
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 105651 steps/s (collection: 0.824s, learning 0.106s)
             Mean action noise std: 2.01
          Mean value_function loss: 45.5460
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.6336
                       Mean reward: 837.97
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 168.9539
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.93s
                      Time elapsed: 00:10:12
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 68137 steps/s (collection: 1.138s, learning 0.305s)
             Mean action noise std: 2.01
          Mean value_function loss: 58.1636
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6461
                       Mean reward: 871.72
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.6787
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 1.44s
                      Time elapsed: 00:10:14
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 72819 steps/s (collection: 1.214s, learning 0.136s)
             Mean action noise std: 2.01
          Mean value_function loss: 54.5180
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.6545
                       Mean reward: 856.97
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.5034
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 1.35s
                      Time elapsed: 00:10:15
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 74068 steps/s (collection: 1.108s, learning 0.219s)
             Mean action noise std: 2.02
          Mean value_function loss: 60.2964
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.6674
                       Mean reward: 823.99
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 168.8277
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 1.33s
                      Time elapsed: 00:10:16
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 69889 steps/s (collection: 1.151s, learning 0.256s)
             Mean action noise std: 2.02
          Mean value_function loss: 54.9536
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.6824
                       Mean reward: 868.80
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.3730
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 1.41s
                      Time elapsed: 00:10:18
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 75895 steps/s (collection: 1.075s, learning 0.221s)
             Mean action noise std: 2.02
          Mean value_function loss: 43.0156
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.6915
                       Mean reward: 832.63
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 167.1521
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 1.30s
                      Time elapsed: 00:10:19
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 86111 steps/s (collection: 1.021s, learning 0.121s)
             Mean action noise std: 2.02
          Mean value_function loss: 58.9256
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.7020
                       Mean reward: 834.96
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 169.9749
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.14s
                      Time elapsed: 00:10:20
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 78534 steps/s (collection: 1.030s, learning 0.222s)
             Mean action noise std: 2.03
          Mean value_function loss: 63.4865
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.7181
                       Mean reward: 869.42
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 171.4390
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 1.25s
                      Time elapsed: 00:10:21
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 81803 steps/s (collection: 0.979s, learning 0.223s)
             Mean action noise std: 2.03
          Mean value_function loss: 51.8223
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.7292
                       Mean reward: 852.59
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.7547
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 1.20s
                      Time elapsed: 00:10:23
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 82177 steps/s (collection: 1.080s, learning 0.116s)
             Mean action noise std: 2.04
          Mean value_function loss: 46.5534
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.7479
                       Mean reward: 862.38
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.5033
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 1.20s
                      Time elapsed: 00:10:24
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 89513 steps/s (collection: 0.937s, learning 0.162s)
             Mean action noise std: 2.04
          Mean value_function loss: 49.6171
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.7682
                       Mean reward: 846.34
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 166.4935
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 1.10s
                      Time elapsed: 00:10:25
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 87265 steps/s (collection: 0.973s, learning 0.153s)
             Mean action noise std: 2.04
          Mean value_function loss: 60.9639
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.7808
                       Mean reward: 876.48
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.3788
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 1.13s
                      Time elapsed: 00:10:26
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 85488 steps/s (collection: 0.931s, learning 0.219s)
             Mean action noise std: 2.04
          Mean value_function loss: 59.8225
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.7897
                       Mean reward: 822.41
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 168.7212
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 1.15s
                      Time elapsed: 00:10:27
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 63338 steps/s (collection: 1.257s, learning 0.295s)
             Mean action noise std: 2.05
          Mean value_function loss: 52.5976
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.7993
                       Mean reward: 873.47
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.2626
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 1.55s
                      Time elapsed: 00:10:29
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 56393 steps/s (collection: 1.452s, learning 0.291s)
             Mean action noise std: 2.05
          Mean value_function loss: 67.5972
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8156
                       Mean reward: 847.71
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 171.0856
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 1.74s
                      Time elapsed: 00:10:30
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 58182 steps/s (collection: 1.348s, learning 0.341s)
             Mean action noise std: 2.05
          Mean value_function loss: 63.1508
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.8310
                       Mean reward: 848.15
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 167.9622
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 1.69s
                      Time elapsed: 00:10:32
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 66414 steps/s (collection: 1.156s, learning 0.324s)
             Mean action noise std: 2.06
          Mean value_function loss: 54.7382
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.8395
                       Mean reward: 835.19
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 167.2530
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 1.48s
                      Time elapsed: 00:10:34
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 88266 steps/s (collection: 1.002s, learning 0.112s)
             Mean action noise std: 2.06
          Mean value_function loss: 62.1639
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.8586
                       Mean reward: 851.02
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.3584
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 1.11s
                      Time elapsed: 00:10:35
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 99835 steps/s (collection: 0.883s, learning 0.102s)
             Mean action noise std: 2.07
          Mean value_function loss: 61.2457
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8773
                       Mean reward: 858.15
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 169.7257
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.98s
                      Time elapsed: 00:10:36
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 88650 steps/s (collection: 0.949s, learning 0.160s)
             Mean action noise std: 2.07
          Mean value_function loss: 39.0902
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.8946
                       Mean reward: 842.59
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 168.8069
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 1.11s
                      Time elapsed: 00:10:37
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 98709 steps/s (collection: 0.884s, learning 0.112s)
             Mean action noise std: 2.07
          Mean value_function loss: 53.4405
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.9093
                       Mean reward: 834.36
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.7658
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 1.00s
                      Time elapsed: 00:10:38
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 100834 steps/s (collection: 0.859s, learning 0.116s)
             Mean action noise std: 2.08
          Mean value_function loss: 46.5604
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.9289
                       Mean reward: 837.86
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 169.0472
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.97s
                      Time elapsed: 00:10:39
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 102873 steps/s (collection: 0.846s, learning 0.110s)
             Mean action noise std: 2.08
          Mean value_function loss: 61.7219
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.9396
                       Mean reward: 873.39
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 171.1122
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.96s
                      Time elapsed: 00:10:40
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 98781 steps/s (collection: 0.878s, learning 0.117s)
             Mean action noise std: 2.08
          Mean value_function loss: 70.3923
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.9454
                       Mean reward: 833.57
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 168.4349
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 1.00s
                      Time elapsed: 00:10:41
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 100737 steps/s (collection: 0.863s, learning 0.113s)
             Mean action noise std: 2.08
          Mean value_function loss: 54.3599
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.9546
                       Mean reward: 841.49
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 167.8551
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.98s
                      Time elapsed: 00:10:42
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 96071 steps/s (collection: 0.902s, learning 0.122s)
             Mean action noise std: 2.09
          Mean value_function loss: 61.0264
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.9591
                       Mean reward: 832.74
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.4058
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 1.02s
                      Time elapsed: 00:10:43
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 97414 steps/s (collection: 0.889s, learning 0.120s)
             Mean action noise std: 2.09
          Mean value_function loss: 59.0870
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 16.9644
                       Mean reward: 847.47
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 168.8637
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 1.01s
                      Time elapsed: 00:10:44
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 94679 steps/s (collection: 0.922s, learning 0.117s)
             Mean action noise std: 2.09
          Mean value_function loss: 52.2944
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.9657
                       Mean reward: 848.18
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 167.5571
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 1.04s
                      Time elapsed: 00:10:45
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 86805 steps/s (collection: 0.983s, learning 0.150s)
             Mean action noise std: 2.09
          Mean value_function loss: 52.4996
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.9732
                       Mean reward: 842.11
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.1782
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 1.13s
                      Time elapsed: 00:10:46
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 93986 steps/s (collection: 0.922s, learning 0.124s)
             Mean action noise std: 2.09
          Mean value_function loss: 42.1863
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.9863
                       Mean reward: 866.02
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.9195
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 1.05s
                      Time elapsed: 00:10:47
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 94216 steps/s (collection: 0.943s, learning 0.100s)
             Mean action noise std: 2.10
          Mean value_function loss: 47.5785
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.9919
                       Mean reward: 857.50
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 171.6104
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 1.04s
                      Time elapsed: 00:10:48
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 99958 steps/s (collection: 0.868s, learning 0.115s)
             Mean action noise std: 2.10
          Mean value_function loss: 68.3021
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.0008
                       Mean reward: 859.00
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 169.1304
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.98s
                      Time elapsed: 00:10:49
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 97193 steps/s (collection: 0.896s, learning 0.116s)
             Mean action noise std: 2.10
          Mean value_function loss: 46.6645
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.0149
                       Mean reward: 851.55
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 168.0640
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 1.01s
                      Time elapsed: 00:10:50
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 93159 steps/s (collection: 0.944s, learning 0.111s)
             Mean action noise std: 2.11
          Mean value_function loss: 41.6371
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.0321
                       Mean reward: 874.97
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.1339
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 1.06s
                      Time elapsed: 00:10:51
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 94032 steps/s (collection: 0.937s, learning 0.109s)
             Mean action noise std: 2.11
          Mean value_function loss: 33.8388
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.0567
                       Mean reward: 848.99
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 169.2400
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 1.05s
                      Time elapsed: 00:10:52
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 108466 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 52.8175
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.0735
                       Mean reward: 836.66
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.9119
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.91s
                      Time elapsed: 00:10:53
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 95275 steps/s (collection: 0.914s, learning 0.118s)
             Mean action noise std: 2.12
          Mean value_function loss: 50.5802
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.0877
                       Mean reward: 846.17
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 169.3887
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 1.03s
                      Time elapsed: 00:10:54
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 93460 steps/s (collection: 0.938s, learning 0.114s)
             Mean action noise std: 2.12
          Mean value_function loss: 56.2918
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1076
                       Mean reward: 857.44
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 172.5373
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 1.05s
                      Time elapsed: 00:10:55
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 105755 steps/s (collection: 0.829s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 54.0883
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.1227
                       Mean reward: 884.14
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.1164
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.93s
                      Time elapsed: 00:10:56
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 108845 steps/s (collection: 0.801s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 61.5408
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.1333
                       Mean reward: 857.76
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.3614
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.90s
                      Time elapsed: 00:10:57
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 108653 steps/s (collection: 0.796s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 59.4114
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1450
                       Mean reward: 846.06
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7420
     Episode_Reward/lifting_object: 167.9796
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.90s
                      Time elapsed: 00:10:58
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 109315 steps/s (collection: 0.801s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 67.6562
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.1574
                       Mean reward: 862.32
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.8343
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.90s
                      Time elapsed: 00:10:59
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 110730 steps/s (collection: 0.785s, learning 0.103s)
             Mean action noise std: 2.14
          Mean value_function loss: 53.3886
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.1689
                       Mean reward: 869.89
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.1581
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.89s
                      Time elapsed: 00:11:00
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 109427 steps/s (collection: 0.790s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 66.6090
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1764
                       Mean reward: 860.91
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.3449
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.90s
                      Time elapsed: 00:11:00
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 110531 steps/s (collection: 0.782s, learning 0.108s)
             Mean action noise std: 2.14
          Mean value_function loss: 79.9709
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.1865
                       Mean reward: 837.23
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 166.9994
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.89s
                      Time elapsed: 00:11:01
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 108110 steps/s (collection: 0.808s, learning 0.102s)
             Mean action noise std: 2.15
          Mean value_function loss: 72.4711
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.1968
                       Mean reward: 853.37
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 168.2222
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.91s
                      Time elapsed: 00:11:02
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 107308 steps/s (collection: 0.818s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 87.6564
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2030
                       Mean reward: 850.06
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 167.9965
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.92s
                      Time elapsed: 00:11:03
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 105190 steps/s (collection: 0.821s, learning 0.114s)
             Mean action noise std: 2.15
          Mean value_function loss: 95.8264
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.2127
                       Mean reward: 844.26
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 166.7503
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.93s
                      Time elapsed: 00:11:04
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 106599 steps/s (collection: 0.810s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 92.1713
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2266
                       Mean reward: 844.44
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.7367
     Episode_Reward/lifting_object: 166.2220
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.92s
                      Time elapsed: 00:11:05
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 110377 steps/s (collection: 0.778s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 74.7649
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2446
                       Mean reward: 856.45
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 167.6386
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.89s
                      Time elapsed: 00:11:06
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 111991 steps/s (collection: 0.779s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 108.0352
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.2575
                       Mean reward: 847.15
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 167.5804
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.88s
                      Time elapsed: 00:11:07
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 113001 steps/s (collection: 0.778s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 96.3481
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2629
                       Mean reward: 843.92
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 167.3331
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.87s
                      Time elapsed: 00:11:08
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 111576 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 2.17
          Mean value_function loss: 87.1261
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.2714
                       Mean reward: 823.45
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.7374
     Episode_Reward/lifting_object: 165.6148
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.88s
                      Time elapsed: 00:11:09
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 109545 steps/s (collection: 0.797s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 98.9424
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.2912
                       Mean reward: 844.89
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 168.1652
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.90s
                      Time elapsed: 00:11:09
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 110726 steps/s (collection: 0.771s, learning 0.117s)
             Mean action noise std: 2.18
          Mean value_function loss: 86.5094
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.3080
                       Mean reward: 840.25
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 168.0323
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.89s
                      Time elapsed: 00:11:10
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 110300 steps/s (collection: 0.772s, learning 0.119s)
             Mean action noise std: 2.18
          Mean value_function loss: 59.2334
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3187
                       Mean reward: 831.47
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 166.1835
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.89s
                      Time elapsed: 00:11:11
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 103872 steps/s (collection: 0.846s, learning 0.101s)
             Mean action noise std: 2.18
          Mean value_function loss: 81.4903
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.3300
                       Mean reward: 846.42
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 169.3363
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.95s
                      Time elapsed: 00:11:12
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 110531 steps/s (collection: 0.784s, learning 0.105s)
             Mean action noise std: 2.18
          Mean value_function loss: 57.2948
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.3329
                       Mean reward: 852.43
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 167.9551
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.89s
                      Time elapsed: 00:11:13
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 111647 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 2.18
          Mean value_function loss: 63.1941
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.3349
                       Mean reward: 843.12
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 168.5497
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.88s
                      Time elapsed: 00:11:14
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 114961 steps/s (collection: 0.768s, learning 0.088s)
             Mean action noise std: 2.19
          Mean value_function loss: 70.2351
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.3422
                       Mean reward: 827.62
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 166.2971
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.86s
                      Time elapsed: 00:11:15
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 113556 steps/s (collection: 0.774s, learning 0.092s)
             Mean action noise std: 2.19
          Mean value_function loss: 63.0978
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.3539
                       Mean reward: 850.83
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 167.7095
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.87s
                      Time elapsed: 00:11:16
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 114685 steps/s (collection: 0.770s, learning 0.088s)
             Mean action noise std: 2.19
          Mean value_function loss: 72.7691
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 17.3618
                       Mean reward: 828.16
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 167.6791
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.86s
                      Time elapsed: 00:11:17
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 113797 steps/s (collection: 0.771s, learning 0.093s)
             Mean action noise std: 2.19
          Mean value_function loss: 61.8536
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3668
                       Mean reward: 858.30
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.5219
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.86s
                      Time elapsed: 00:11:17
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 113608 steps/s (collection: 0.772s, learning 0.093s)
             Mean action noise std: 2.20
          Mean value_function loss: 72.9410
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3792
                       Mean reward: 860.23
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.0672
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.87s
                      Time elapsed: 00:11:18
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 113800 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 2.20
          Mean value_function loss: 68.5216
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.3873
                       Mean reward: 868.86
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 169.5701
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.86s
                      Time elapsed: 00:11:19
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 107830 steps/s (collection: 0.817s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 57.8403
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.3928
                       Mean reward: 858.81
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 169.9694
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.91s
                      Time elapsed: 00:11:20
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 109670 steps/s (collection: 0.797s, learning 0.099s)
             Mean action noise std: 2.20
          Mean value_function loss: 60.1566
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3995
                       Mean reward: 859.39
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 169.8545
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.90s
                      Time elapsed: 00:11:21
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 112235 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 2.20
          Mean value_function loss: 54.3466
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4031
                       Mean reward: 861.87
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 171.3136
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.88s
                      Time elapsed: 00:11:22
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 111199 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 2.20
          Mean value_function loss: 51.4369
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.4052
                       Mean reward: 861.45
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 168.4382
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.88s
                      Time elapsed: 00:11:23
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 112124 steps/s (collection: 0.774s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 48.0612
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.4066
                       Mean reward: 837.11
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 168.7389
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.88s
                      Time elapsed: 00:11:24
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 111334 steps/s (collection: 0.785s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 40.1571
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.4146
                       Mean reward: 856.13
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.0932
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.88s
                      Time elapsed: 00:11:24
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 116693 steps/s (collection: 0.743s, learning 0.099s)
             Mean action noise std: 2.21
          Mean value_function loss: 64.9566
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.4246
                       Mean reward: 872.93
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.0359
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.84s
                      Time elapsed: 00:11:25
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 113140 steps/s (collection: 0.754s, learning 0.115s)
             Mean action noise std: 2.21
          Mean value_function loss: 38.1305
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.4338
                       Mean reward: 856.38
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.2101
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.87s
                      Time elapsed: 00:11:26
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 113982 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 2.21
          Mean value_function loss: 51.2871
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.4400
                       Mean reward: 857.92
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 171.8235
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.86s
                      Time elapsed: 00:11:27
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 117970 steps/s (collection: 0.745s, learning 0.089s)
             Mean action noise std: 2.21
          Mean value_function loss: 62.4768
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.4463
                       Mean reward: 870.96
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.2517
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.83s
                      Time elapsed: 00:11:28
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 114310 steps/s (collection: 0.766s, learning 0.094s)
             Mean action noise std: 2.22
          Mean value_function loss: 66.6321
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.4527
                       Mean reward: 849.23
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.3777
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.86s
                      Time elapsed: 00:11:29
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 113073 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 2.22
          Mean value_function loss: 70.4376
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.4588
                       Mean reward: 865.15
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.9295
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.87s
                      Time elapsed: 00:11:30
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 113060 steps/s (collection: 0.768s, learning 0.102s)
             Mean action noise std: 2.22
          Mean value_function loss: 55.2579
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.4665
                       Mean reward: 865.54
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 169.4541
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.87s
                      Time elapsed: 00:11:30
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 112081 steps/s (collection: 0.780s, learning 0.098s)
             Mean action noise std: 2.22
          Mean value_function loss: 55.5743
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 17.4724
                       Mean reward: 860.93
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.1555
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.88s
                      Time elapsed: 00:11:31
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 111730 steps/s (collection: 0.771s, learning 0.109s)
             Mean action noise std: 2.22
          Mean value_function loss: 48.3484
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.4727
                       Mean reward: 863.95
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.1838
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.88s
                      Time elapsed: 00:11:32
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 114160 steps/s (collection: 0.769s, learning 0.092s)
             Mean action noise std: 2.22
          Mean value_function loss: 65.2434
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.4732
                       Mean reward: 847.25
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.7886
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.86s
                      Time elapsed: 00:11:33
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 113408 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 2.22
          Mean value_function loss: 55.8328
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.4766
                       Mean reward: 862.04
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 170.3710
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.87s
                      Time elapsed: 00:11:34
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 115010 steps/s (collection: 0.765s, learning 0.089s)
             Mean action noise std: 2.23
          Mean value_function loss: 51.5485
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4890
                       Mean reward: 878.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 171.1309
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.85s
                      Time elapsed: 00:11:35
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 110802 steps/s (collection: 0.787s, learning 0.101s)
             Mean action noise std: 2.23
          Mean value_function loss: 44.2585
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.5088
                       Mean reward: 874.20
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.9582
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.89s
                      Time elapsed: 00:11:36
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 112608 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 2.24
          Mean value_function loss: 40.6470
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.5179
                       Mean reward: 873.34
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.5505
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.87s
                      Time elapsed: 00:11:37
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 117234 steps/s (collection: 0.752s, learning 0.087s)
             Mean action noise std: 2.24
          Mean value_function loss: 47.5547
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.5252
                       Mean reward: 840.66
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.1240
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.84s
                      Time elapsed: 00:11:37
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 113804 steps/s (collection: 0.770s, learning 0.094s)
             Mean action noise std: 2.24
          Mean value_function loss: 51.5858
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.5341
                       Mean reward: 864.51
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.5807
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.86s
                      Time elapsed: 00:11:38
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 110365 steps/s (collection: 0.776s, learning 0.115s)
             Mean action noise std: 2.24
          Mean value_function loss: 40.4435
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.5420
                       Mean reward: 866.23
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 170.0342
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.89s
                      Time elapsed: 00:11:39
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 112364 steps/s (collection: 0.769s, learning 0.106s)
             Mean action noise std: 2.25
          Mean value_function loss: 43.0339
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.5577
                       Mean reward: 874.15
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.0591
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.87s
                      Time elapsed: 00:11:40
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 114497 steps/s (collection: 0.749s, learning 0.110s)
             Mean action noise std: 2.25
          Mean value_function loss: 45.8278
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5742
                       Mean reward: 879.10
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.6382
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.86s
                      Time elapsed: 00:11:41
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 110113 steps/s (collection: 0.784s, learning 0.109s)
             Mean action noise std: 2.25
          Mean value_function loss: 44.8701
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.5766
                       Mean reward: 857.21
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.5723
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.89s
                      Time elapsed: 00:11:42
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 112135 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 2.25
          Mean value_function loss: 43.6680
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.5826
                       Mean reward: 861.83
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.4042
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.88s
                      Time elapsed: 00:11:43
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 65703 steps/s (collection: 1.395s, learning 0.101s)
             Mean action noise std: 2.26
          Mean value_function loss: 52.6123
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5888
                       Mean reward: 870.99
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 171.2658
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.50s
                      Time elapsed: 00:11:44
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 32496 steps/s (collection: 2.899s, learning 0.126s)
             Mean action noise std: 2.26
          Mean value_function loss: 61.4908
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.5985
                       Mean reward: 861.93
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.7800
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 3.03s
                      Time elapsed: 00:11:47
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 29847 steps/s (collection: 3.159s, learning 0.135s)
             Mean action noise std: 2.26
          Mean value_function loss: 45.2043
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.6098
                       Mean reward: 854.65
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 170.6303
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 3.29s
                      Time elapsed: 00:11:50
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 32909 steps/s (collection: 2.875s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 65.3169
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6262
                       Mean reward: 847.84
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.6242
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 2.99s
                      Time elapsed: 00:11:53
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 30454 steps/s (collection: 3.108s, learning 0.120s)
             Mean action noise std: 2.27
          Mean value_function loss: 51.1783
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.6372
                       Mean reward: 850.11
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.2590
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.23s
                      Time elapsed: 00:11:57
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 28463 steps/s (collection: 3.319s, learning 0.135s)
             Mean action noise std: 2.27
          Mean value_function loss: 53.1730
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6470
                       Mean reward: 862.46
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.1211
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.45s
                      Time elapsed: 00:12:00
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 26783 steps/s (collection: 3.527s, learning 0.143s)
             Mean action noise std: 2.28
          Mean value_function loss: 47.2052
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.6582
                       Mean reward: 849.70
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.2502
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.67s
                      Time elapsed: 00:12:04
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 30632 steps/s (collection: 3.075s, learning 0.134s)
             Mean action noise std: 2.28
          Mean value_function loss: 43.7716
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 17.6685
                       Mean reward: 866.28
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 170.6498
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.21s
                      Time elapsed: 00:12:07
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 31713 steps/s (collection: 2.985s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 57.4411
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6756
                       Mean reward: 853.11
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 171.2861
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.10s
                      Time elapsed: 00:12:10
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 36286 steps/s (collection: 2.602s, learning 0.107s)
             Mean action noise std: 2.28
          Mean value_function loss: 41.2076
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6826
                       Mean reward: 837.50
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 168.2456
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 2.71s
                      Time elapsed: 00:12:13
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 116950 steps/s (collection: 0.746s, learning 0.095s)
             Mean action noise std: 2.29
          Mean value_function loss: 47.0648
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6941
                       Mean reward: 856.11
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.5996
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.84s
                      Time elapsed: 00:12:14
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 112797 steps/s (collection: 0.781s, learning 0.091s)
             Mean action noise std: 2.29
          Mean value_function loss: 47.8611
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.7117
                       Mean reward: 853.32
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.9252
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.87s
                      Time elapsed: 00:12:15
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 109661 steps/s (collection: 0.789s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 46.0739
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.7202
                       Mean reward: 844.96
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 169.6858
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.90s
                      Time elapsed: 00:12:15
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 107318 steps/s (collection: 0.809s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 42.2942
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.7370
                       Mean reward: 870.92
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 171.1163
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.92s
                      Time elapsed: 00:12:16
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 108891 steps/s (collection: 0.795s, learning 0.108s)
             Mean action noise std: 2.30
          Mean value_function loss: 57.5403
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7542
                       Mean reward: 843.40
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.3658
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.90s
                      Time elapsed: 00:12:17
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 110192 steps/s (collection: 0.774s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 49.2432
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.7687
                       Mean reward: 836.42
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.2755
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.89s
                      Time elapsed: 00:12:18
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 109940 steps/s (collection: 0.792s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 41.7587
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.7765
                       Mean reward: 869.00
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 171.4974
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.89s
                      Time elapsed: 00:12:19
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 110580 steps/s (collection: 0.787s, learning 0.102s)
             Mean action noise std: 2.31
          Mean value_function loss: 40.2650
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.7849
                       Mean reward: 867.71
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.9705
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.89s
                      Time elapsed: 00:12:20
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 105546 steps/s (collection: 0.822s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 59.2175
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.7950
                       Mean reward: 883.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.5986
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.93s
                      Time elapsed: 00:12:21
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 112119 steps/s (collection: 0.777s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 57.1673
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.8025
                       Mean reward: 866.24
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.6610
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.88s
                      Time elapsed: 00:12:22
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 113139 steps/s (collection: 0.778s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 50.3438
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.8105
                       Mean reward: 849.66
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 170.5645
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.87s
                      Time elapsed: 00:12:23
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 113426 steps/s (collection: 0.778s, learning 0.089s)
             Mean action noise std: 2.32
          Mean value_function loss: 49.0043
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.8125
                       Mean reward: 873.84
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 169.4256
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.87s
                      Time elapsed: 00:12:23
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 112186 steps/s (collection: 0.775s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 47.3978
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.8161
                       Mean reward: 827.94
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 168.4171
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.88s
                      Time elapsed: 00:12:24
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 110475 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 47.5460
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.8221
                       Mean reward: 857.77
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.3119
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.89s
                      Time elapsed: 00:12:25
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 112610 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 2.33
          Mean value_function loss: 59.1471
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.8341
                       Mean reward: 854.42
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 172.1536
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.87s
                      Time elapsed: 00:12:26
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 116068 steps/s (collection: 0.754s, learning 0.093s)
             Mean action noise std: 2.33
          Mean value_function loss: 48.9864
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.8381
                       Mean reward: 838.26
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 168.6594
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.85s
                      Time elapsed: 00:12:27
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 115821 steps/s (collection: 0.758s, learning 0.091s)
             Mean action noise std: 2.33
          Mean value_function loss: 49.6487
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.8477
                       Mean reward: 866.66
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.4351
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.85s
                      Time elapsed: 00:12:28
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 116481 steps/s (collection: 0.758s, learning 0.086s)
             Mean action noise std: 2.34
          Mean value_function loss: 49.1173
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.8647
                       Mean reward: 861.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.1821
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.84s
                      Time elapsed: 00:12:29
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 116092 steps/s (collection: 0.754s, learning 0.093s)
             Mean action noise std: 2.34
          Mean value_function loss: 44.9314
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.8785
                       Mean reward: 857.93
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 170.6886
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.85s
                      Time elapsed: 00:12:30
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 117671 steps/s (collection: 0.747s, learning 0.088s)
             Mean action noise std: 2.34
          Mean value_function loss: 58.2186
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.8924
                       Mean reward: 859.59
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 171.0765
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.84s
                      Time elapsed: 00:12:30
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 115564 steps/s (collection: 0.763s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 44.6486
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.8957
                       Mean reward: 868.11
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 172.0002
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.85s
                      Time elapsed: 00:12:31
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 111164 steps/s (collection: 0.777s, learning 0.108s)
             Mean action noise std: 2.35
          Mean value_function loss: 47.6731
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.9003
                       Mean reward: 867.46
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 173.0115
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.88s
                      Time elapsed: 00:12:32
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 118460 steps/s (collection: 0.736s, learning 0.093s)
             Mean action noise std: 2.35
          Mean value_function loss: 43.1255
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.9113
                       Mean reward: 857.98
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 171.5382
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.83s
                      Time elapsed: 00:12:33
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 116768 steps/s (collection: 0.755s, learning 0.087s)
             Mean action noise std: 2.36
          Mean value_function loss: 35.6803
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.9239
                       Mean reward: 882.94
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.4163
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.84s
                      Time elapsed: 00:12:34
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 115916 steps/s (collection: 0.762s, learning 0.086s)
             Mean action noise std: 2.36
          Mean value_function loss: 40.7412
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.9342
                       Mean reward: 865.81
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.3791
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.85s
                      Time elapsed: 00:12:35
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 111788 steps/s (collection: 0.784s, learning 0.095s)
             Mean action noise std: 2.36
          Mean value_function loss: 39.8036
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.9449
                       Mean reward: 857.98
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 169.7757
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.88s
                      Time elapsed: 00:12:35
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 114608 steps/s (collection: 0.753s, learning 0.105s)
             Mean action noise std: 2.36
          Mean value_function loss: 52.4078
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.9578
                       Mean reward: 859.63
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 170.7644
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.86s
                      Time elapsed: 00:12:36
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 114869 steps/s (collection: 0.751s, learning 0.105s)
             Mean action noise std: 2.37
          Mean value_function loss: 38.6276
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.9677
                       Mean reward: 856.84
               Mean episode length: 247.55
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.2913
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.86s
                      Time elapsed: 00:12:37
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 117975 steps/s (collection: 0.730s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 51.5616
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.9813
                       Mean reward: 855.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.8916
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.83s
                      Time elapsed: 00:12:38
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 115417 steps/s (collection: 0.762s, learning 0.090s)
             Mean action noise std: 2.38
          Mean value_function loss: 30.5190
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.9943
                       Mean reward: 837.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.1390
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.85s
                      Time elapsed: 00:12:39
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 113369 steps/s (collection: 0.773s, learning 0.095s)
             Mean action noise std: 2.38
          Mean value_function loss: 37.3080
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0129
                       Mean reward: 860.78
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 170.6794
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.87s
                      Time elapsed: 00:12:40
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 104785 steps/s (collection: 0.817s, learning 0.121s)
             Mean action noise std: 2.39
          Mean value_function loss: 33.3678
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0272
                       Mean reward: 841.80
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 170.0208
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.94s
                      Time elapsed: 00:12:41
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 110168 steps/s (collection: 0.794s, learning 0.099s)
             Mean action noise std: 2.39
          Mean value_function loss: 36.0015
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.0355
                       Mean reward: 867.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.8536
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.89s
                      Time elapsed: 00:12:42
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 111425 steps/s (collection: 0.774s, learning 0.109s)
             Mean action noise std: 2.40
          Mean value_function loss: 50.3997
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.0520
                       Mean reward: 866.03
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.0054
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.88s
                      Time elapsed: 00:12:42
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 113544 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 2.40
          Mean value_function loss: 42.0765
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.0725
                       Mean reward: 863.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 171.7180
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.87s
                      Time elapsed: 00:12:43
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 116275 steps/s (collection: 0.732s, learning 0.114s)
             Mean action noise std: 2.40
          Mean value_function loss: 29.7526
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.0790
                       Mean reward: 875.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 173.7523
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.85s
                      Time elapsed: 00:12:44
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 115476 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 2.41
          Mean value_function loss: 38.4419
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.0879
                       Mean reward: 863.67
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.1322
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.85s
                      Time elapsed: 00:12:45
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 109201 steps/s (collection: 0.799s, learning 0.101s)
             Mean action noise std: 2.41
          Mean value_function loss: 35.1624
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.0955
                       Mean reward: 861.67
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.8571
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.90s
                      Time elapsed: 00:12:46
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 110455 steps/s (collection: 0.789s, learning 0.101s)
             Mean action noise std: 2.41
          Mean value_function loss: 31.8380
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.1036
                       Mean reward: 864.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.0164
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.89s
                      Time elapsed: 00:12:47
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 110452 steps/s (collection: 0.787s, learning 0.103s)
             Mean action noise std: 2.42
          Mean value_function loss: 39.3074
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.1212
                       Mean reward: 844.10
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.5997
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.89s
                      Time elapsed: 00:12:48
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 112252 steps/s (collection: 0.778s, learning 0.098s)
             Mean action noise std: 2.42
          Mean value_function loss: 45.0842
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.1371
                       Mean reward: 866.84
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 171.0707
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.88s
                      Time elapsed: 00:12:49
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 113329 steps/s (collection: 0.766s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 42.5921
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.1424
                       Mean reward: 852.61
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.8611
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.87s
                      Time elapsed: 00:12:49
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 113867 steps/s (collection: 0.766s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 35.0813
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1501
                       Mean reward: 862.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.6871
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.86s
                      Time elapsed: 00:12:50
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 117187 steps/s (collection: 0.738s, learning 0.101s)
             Mean action noise std: 2.43
          Mean value_function loss: 38.1922
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.1636
                       Mean reward: 869.90
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.7245
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.84s
                      Time elapsed: 00:12:51
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 112727 steps/s (collection: 0.777s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 45.6301
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.1790
                       Mean reward: 870.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 173.4212
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.87s
                      Time elapsed: 00:12:52
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 113406 steps/s (collection: 0.780s, learning 0.087s)
             Mean action noise std: 2.44
          Mean value_function loss: 37.4493
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.1902
                       Mean reward: 874.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.2703
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.87s
                      Time elapsed: 00:12:53
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 113655 steps/s (collection: 0.770s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 28.6699
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.2020
                       Mean reward: 832.09
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 169.6132
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.86s
                      Time elapsed: 00:12:54
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 115888 steps/s (collection: 0.763s, learning 0.085s)
             Mean action noise std: 2.45
          Mean value_function loss: 28.7000
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2168
                       Mean reward: 878.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.3579
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.85s
                      Time elapsed: 00:12:55
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 113514 steps/s (collection: 0.758s, learning 0.108s)
             Mean action noise std: 2.45
          Mean value_function loss: 32.6670
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2302
                       Mean reward: 827.40
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.6140
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.87s
                      Time elapsed: 00:12:55
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 116695 steps/s (collection: 0.752s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 32.2186
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.2475
                       Mean reward: 861.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.5610
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.84s
                      Time elapsed: 00:12:56
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 118028 steps/s (collection: 0.743s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 37.2510
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2544
                       Mean reward: 871.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.7858
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.83s
                      Time elapsed: 00:12:57
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 116757 steps/s (collection: 0.755s, learning 0.087s)
             Mean action noise std: 2.46
          Mean value_function loss: 36.9397
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.2608
                       Mean reward: 860.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.8372
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.84s
                      Time elapsed: 00:12:58
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 113410 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 2.46
          Mean value_function loss: 45.3877
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2695
                       Mean reward: 871.97
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 171.3792
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.87s
                      Time elapsed: 00:12:59
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 115246 steps/s (collection: 0.757s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 48.7578
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2858
                       Mean reward: 873.16
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 174.2475
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.85s
                      Time elapsed: 00:13:00
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 117166 steps/s (collection: 0.746s, learning 0.093s)
             Mean action noise std: 2.47
          Mean value_function loss: 38.6341
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 18.2975
                       Mean reward: 846.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.2544
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.84s
                      Time elapsed: 00:13:01
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 116437 steps/s (collection: 0.751s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 45.9575
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.3041
                       Mean reward: 862.84
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.5135
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.84s
                      Time elapsed: 00:13:01
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 114852 steps/s (collection: 0.765s, learning 0.091s)
             Mean action noise std: 2.48
          Mean value_function loss: 46.9045
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.3151
                       Mean reward: 855.23
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.3749
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.86s
                      Time elapsed: 00:13:02
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 112872 steps/s (collection: 0.770s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 44.4001
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3222
                       Mean reward: 859.00
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.2933
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.87s
                      Time elapsed: 00:13:03
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 114310 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 2.48
          Mean value_function loss: 53.7888
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3262
                       Mean reward: 863.22
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.7647
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.86s
                      Time elapsed: 00:13:04
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 113771 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 36.1154
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3338
                       Mean reward: 863.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.2248
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.86s
                      Time elapsed: 00:13:05
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 113213 steps/s (collection: 0.768s, learning 0.100s)
             Mean action noise std: 2.49
          Mean value_function loss: 44.7060
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.3468
                       Mean reward: 854.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 170.9143
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.87s
                      Time elapsed: 00:13:06
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 109719 steps/s (collection: 0.794s, learning 0.102s)
             Mean action noise std: 2.49
          Mean value_function loss: 41.3948
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.3529
                       Mean reward: 844.05
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.1455
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.90s
                      Time elapsed: 00:13:07
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 113318 steps/s (collection: 0.766s, learning 0.101s)
             Mean action noise std: 2.50
          Mean value_function loss: 48.0449
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3664
                       Mean reward: 863.48
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.9235
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.87s
                      Time elapsed: 00:13:07
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 115582 steps/s (collection: 0.758s, learning 0.092s)
             Mean action noise std: 2.51
          Mean value_function loss: 45.1628
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.3897
                       Mean reward: 863.32
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.3020
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.85s
                      Time elapsed: 00:13:08
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 113987 steps/s (collection: 0.758s, learning 0.105s)
             Mean action noise std: 2.51
          Mean value_function loss: 54.5535
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.4117
                       Mean reward: 834.88
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 168.3274
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.86s
                      Time elapsed: 00:13:09
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 112816 steps/s (collection: 0.778s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 44.0531
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.4256
                       Mean reward: 862.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.7223
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.87s
                      Time elapsed: 00:13:10
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 113602 steps/s (collection: 0.774s, learning 0.092s)
             Mean action noise std: 2.52
          Mean value_function loss: 42.6651
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.4341
                       Mean reward: 875.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 174.3476
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.87s
                      Time elapsed: 00:13:11
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 114403 steps/s (collection: 0.771s, learning 0.088s)
             Mean action noise std: 2.52
          Mean value_function loss: 42.4572
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.4394
                       Mean reward: 872.86
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.4706
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.86s
                      Time elapsed: 00:13:12
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 115796 steps/s (collection: 0.754s, learning 0.095s)
             Mean action noise std: 2.52
          Mean value_function loss: 40.2703
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4481
                       Mean reward: 855.47
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 167.5905
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.85s
                      Time elapsed: 00:13:13
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 105772 steps/s (collection: 0.830s, learning 0.100s)
             Mean action noise std: 2.52
          Mean value_function loss: 34.0005
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.4539
                       Mean reward: 852.72
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.9124
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.93s
                      Time elapsed: 00:13:14
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 105008 steps/s (collection: 0.834s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 37.7343
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.4632
                       Mean reward: 864.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.5044
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.94s
                      Time elapsed: 00:13:14
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 112695 steps/s (collection: 0.779s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 45.0827
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.4734
                       Mean reward: 843.44
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.7279
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.87s
                      Time elapsed: 00:13:15
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 106889 steps/s (collection: 0.822s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 32.0080
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.4780
                       Mean reward: 873.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 169.3971
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.92s
                      Time elapsed: 00:13:16
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 109554 steps/s (collection: 0.797s, learning 0.101s)
             Mean action noise std: 2.54
          Mean value_function loss: 30.8700
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.4863
                       Mean reward: 867.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 173.5343
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.90s
                      Time elapsed: 00:13:17
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 105615 steps/s (collection: 0.819s, learning 0.112s)
             Mean action noise std: 2.54
          Mean value_function loss: 39.8861
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.4976
                       Mean reward: 872.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.3452
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.93s
                      Time elapsed: 00:13:18
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 116422 steps/s (collection: 0.758s, learning 0.086s)
             Mean action noise std: 2.55
          Mean value_function loss: 49.7714
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.5196
                       Mean reward: 866.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.9164
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.84s
                      Time elapsed: 00:13:19
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 110193 steps/s (collection: 0.791s, learning 0.102s)
             Mean action noise std: 2.55
          Mean value_function loss: 32.3750
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.5358
                       Mean reward: 856.12
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.9500
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.89s
                      Time elapsed: 00:13:20
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 93058 steps/s (collection: 0.860s, learning 0.197s)
             Mean action noise std: 2.56
          Mean value_function loss: 49.5546
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 18.5493
                       Mean reward: 845.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.9559
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 1.06s
                      Time elapsed: 00:13:21
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 108263 steps/s (collection: 0.816s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 47.5913
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.5608
                       Mean reward: 852.51
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.1123
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.91s
                      Time elapsed: 00:13:22
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 115094 steps/s (collection: 0.768s, learning 0.087s)
             Mean action noise std: 2.56
          Mean value_function loss: 38.4716
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.5709
                       Mean reward: 860.84
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.2037
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.85s
                      Time elapsed: 00:13:23
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 111113 steps/s (collection: 0.790s, learning 0.095s)
             Mean action noise std: 2.57
          Mean value_function loss: 38.6417
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5822
                       Mean reward: 875.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.5352
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.88s
                      Time elapsed: 00:13:24
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 111485 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 2.58
          Mean value_function loss: 48.4079
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.6027
                       Mean reward: 855.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 169.6354
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.88s
                      Time elapsed: 00:13:24
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 113942 steps/s (collection: 0.772s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 39.2766
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.6208
                       Mean reward: 846.92
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 170.4657
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.86s
                      Time elapsed: 00:13:25
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 116364 steps/s (collection: 0.755s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 41.4280
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.6291
                       Mean reward: 871.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 172.7264
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.84s
                      Time elapsed: 00:13:26
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 110111 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 38.3562
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.6312
                       Mean reward: 840.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 169.7658
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.89s
                      Time elapsed: 00:13:27
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 111029 steps/s (collection: 0.788s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 50.3553
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 18.6383
                       Mean reward: 871.51
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.7827
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.89s
                      Time elapsed: 00:13:28
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 106393 steps/s (collection: 0.811s, learning 0.113s)
             Mean action noise std: 2.59
          Mean value_function loss: 39.3113
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.6507
                       Mean reward: 863.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.0447
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.92s
                      Time elapsed: 00:13:29
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 112099 steps/s (collection: 0.787s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 38.6931
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.6613
                       Mean reward: 854.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.5244
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.88s
                      Time elapsed: 00:13:30
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 101551 steps/s (collection: 0.869s, learning 0.099s)
             Mean action noise std: 2.59
          Mean value_function loss: 37.8096
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6684
                       Mean reward: 854.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 168.9406
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.97s
                      Time elapsed: 00:13:31
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 109295 steps/s (collection: 0.807s, learning 0.092s)
             Mean action noise std: 2.60
          Mean value_function loss: 37.6250
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.6732
                       Mean reward: 874.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 172.8769
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.90s
                      Time elapsed: 00:13:32
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 105867 steps/s (collection: 0.789s, learning 0.139s)
             Mean action noise std: 2.60
          Mean value_function loss: 37.4248
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.6787
                       Mean reward: 856.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.4871
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.93s
                      Time elapsed: 00:13:33
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 112605 steps/s (collection: 0.766s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 35.9942
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6939
                       Mean reward: 860.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 169.8278
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.87s
                      Time elapsed: 00:13:33
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 112543 steps/s (collection: 0.787s, learning 0.087s)
             Mean action noise std: 2.61
          Mean value_function loss: 48.7504
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.7056
                       Mean reward: 840.27
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 168.8000
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.87s
                      Time elapsed: 00:13:34
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 105087 steps/s (collection: 0.826s, learning 0.110s)
             Mean action noise std: 2.61
          Mean value_function loss: 39.0150
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.7125
                       Mean reward: 844.78
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 171.7084
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.94s
                      Time elapsed: 00:13:35
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 99776 steps/s (collection: 0.813s, learning 0.173s)
             Mean action noise std: 2.62
          Mean value_function loss: 36.2417
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.7260
                       Mean reward: 870.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 169.9809
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.99s
                      Time elapsed: 00:13:36
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 107747 steps/s (collection: 0.813s, learning 0.099s)
             Mean action noise std: 2.62
          Mean value_function loss: 37.7092
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.7446
                       Mean reward: 860.47
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 172.2959
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.91s
                      Time elapsed: 00:13:37
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 112830 steps/s (collection: 0.785s, learning 0.087s)
             Mean action noise std: 2.62
          Mean value_function loss: 49.1376
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.7531
                       Mean reward: 836.84
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 170.4204
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.87s
                      Time elapsed: 00:13:38
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 114460 steps/s (collection: 0.773s, learning 0.086s)
             Mean action noise std: 2.63
          Mean value_function loss: 35.7492
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.7636
                       Mean reward: 849.57
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 170.1480
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.86s
                      Time elapsed: 00:13:39
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 109273 steps/s (collection: 0.797s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 40.5534
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.7737
                       Mean reward: 843.82
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.5991
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.90s
                      Time elapsed: 00:13:40
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 104804 steps/s (collection: 0.823s, learning 0.115s)
             Mean action noise std: 2.64
          Mean value_function loss: 40.2143
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.7794
                       Mean reward: 841.37
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.1517
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.94s
                      Time elapsed: 00:13:41
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 89164 steps/s (collection: 0.912s, learning 0.191s)
             Mean action noise std: 2.64
          Mean value_function loss: 44.7118
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.7906
                       Mean reward: 876.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 171.7034
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 1.10s
                      Time elapsed: 00:13:42
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 99202 steps/s (collection: 0.874s, learning 0.117s)
             Mean action noise std: 2.65
          Mean value_function loss: 33.4070
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.8058
                       Mean reward: 878.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 170.8437
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.99s
                      Time elapsed: 00:13:43
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 87999 steps/s (collection: 0.938s, learning 0.179s)
             Mean action noise std: 2.65
          Mean value_function loss: 28.2923
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.8250
                       Mean reward: 858.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.2969
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 1.12s
                      Time elapsed: 00:13:44
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 107508 steps/s (collection: 0.823s, learning 0.091s)
             Mean action noise std: 2.65
          Mean value_function loss: 25.8282
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.8337
                       Mean reward: 845.14
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.1967
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.91s
                      Time elapsed: 00:13:45
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 105030 steps/s (collection: 0.797s, learning 0.139s)
             Mean action noise std: 2.66
          Mean value_function loss: 29.1588
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.8528
                       Mean reward: 845.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 169.3296
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.94s
                      Time elapsed: 00:13:46
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 109411 steps/s (collection: 0.811s, learning 0.087s)
             Mean action noise std: 2.67
          Mean value_function loss: 34.8658
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.8750
                       Mean reward: 872.17
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 171.9391
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.90s
                      Time elapsed: 00:13:47
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 103068 steps/s (collection: 0.791s, learning 0.163s)
             Mean action noise std: 2.67
          Mean value_function loss: 38.1778
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.8872
                       Mean reward: 868.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.6474
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.95s
                      Time elapsed: 00:13:48
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 108868 steps/s (collection: 0.773s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 38.6083
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.9004
                       Mean reward: 874.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 173.4996
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.90s
                      Time elapsed: 00:13:48
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 101680 steps/s (collection: 0.804s, learning 0.163s)
             Mean action noise std: 2.68
          Mean value_function loss: 26.5828
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.9134
                       Mean reward: 862.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 172.0452
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.97s
                      Time elapsed: 00:13:49
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 102943 steps/s (collection: 0.812s, learning 0.143s)
             Mean action noise std: 2.69
          Mean value_function loss: 39.7084
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.9318
                       Mean reward: 870.92
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.4065
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.95s
                      Time elapsed: 00:13:50
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 114329 steps/s (collection: 0.775s, learning 0.085s)
             Mean action noise std: 2.69
          Mean value_function loss: 38.0841
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.9480
                       Mean reward: 864.77
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.2775
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.86s
                      Time elapsed: 00:13:51
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 113676 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 2.69
          Mean value_function loss: 31.6591
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.9549
                       Mean reward: 866.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.5771
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.86s
                      Time elapsed: 00:13:52
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 110833 steps/s (collection: 0.785s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 37.5255
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.9621
                       Mean reward: 876.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 173.8075
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.89s
                      Time elapsed: 00:13:53
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 111290 steps/s (collection: 0.769s, learning 0.114s)
             Mean action noise std: 2.70
          Mean value_function loss: 27.4733
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.9708
                       Mean reward: 855.70
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 170.1927
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.88s
                      Time elapsed: 00:13:54
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 109610 steps/s (collection: 0.806s, learning 0.091s)
             Mean action noise std: 2.71
          Mean value_function loss: 32.8382
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.9863
                       Mean reward: 864.62
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 173.4662
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.90s
                      Time elapsed: 00:13:55
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 114126 steps/s (collection: 0.776s, learning 0.086s)
             Mean action noise std: 2.71
          Mean value_function loss: 33.2353
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.9949
                       Mean reward: 867.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 170.9576
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.86s
                      Time elapsed: 00:13:56
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 111870 steps/s (collection: 0.768s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 28.0997
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.0001
                       Mean reward: 847.98
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.3594
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.88s
                      Time elapsed: 00:13:57
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 106927 steps/s (collection: 0.794s, learning 0.125s)
             Mean action noise std: 2.71
          Mean value_function loss: 32.5055
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.0107
                       Mean reward: 859.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.6341
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.92s
                      Time elapsed: 00:13:57
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 115024 steps/s (collection: 0.764s, learning 0.091s)
             Mean action noise std: 2.72
          Mean value_function loss: 38.9100
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.0138
                       Mean reward: 850.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.5714
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.85s
                      Time elapsed: 00:13:58
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 111986 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 2.72
          Mean value_function loss: 38.2254
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 19.0162
                       Mean reward: 867.21
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.9062
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.88s
                      Time elapsed: 00:13:59
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 106882 steps/s (collection: 0.804s, learning 0.116s)
             Mean action noise std: 2.72
          Mean value_function loss: 32.3757
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.0183
                       Mean reward: 870.74
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 172.1458
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.92s
                      Time elapsed: 00:14:00
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 109112 steps/s (collection: 0.785s, learning 0.116s)
             Mean action noise std: 2.72
          Mean value_function loss: 32.5660
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.0252
                       Mean reward: 859.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.8061
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.90s
                      Time elapsed: 00:14:01
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 111734 steps/s (collection: 0.791s, learning 0.089s)
             Mean action noise std: 2.73
          Mean value_function loss: 27.0016
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.0369
                       Mean reward: 878.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.8135
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.88s
                      Time elapsed: 00:14:02
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 111461 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 38.4276
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.0522
                       Mean reward: 856.17
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.2266
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.88s
                      Time elapsed: 00:14:03
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 108188 steps/s (collection: 0.785s, learning 0.124s)
             Mean action noise std: 2.74
          Mean value_function loss: 29.6618
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.0668
                       Mean reward: 844.57
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.1260
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.91s
                      Time elapsed: 00:14:04
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 113009 steps/s (collection: 0.755s, learning 0.115s)
             Mean action noise std: 2.74
          Mean value_function loss: 22.2217
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.0880
                       Mean reward: 856.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.1050
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.87s
                      Time elapsed: 00:14:05
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 112223 steps/s (collection: 0.763s, learning 0.113s)
             Mean action noise std: 2.75
          Mean value_function loss: 28.6325
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.1046
                       Mean reward: 858.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 172.5021
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.88s
                      Time elapsed: 00:14:05
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 113850 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 2.75
          Mean value_function loss: 32.8007
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.1166
                       Mean reward: 885.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 174.3217
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.86s
                      Time elapsed: 00:14:06
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 109259 steps/s (collection: 0.793s, learning 0.107s)
             Mean action noise std: 2.76
          Mean value_function loss: 34.9419
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.1349
                       Mean reward: 857.54
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.4882
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.90s
                      Time elapsed: 00:14:07
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 101393 steps/s (collection: 0.883s, learning 0.087s)
             Mean action noise std: 2.77
          Mean value_function loss: 29.7176
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.1546
                       Mean reward: 865.27
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 170.8871
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.97s
                      Time elapsed: 00:14:08
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 112568 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 2.78
          Mean value_function loss: 28.0532
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.1745
                       Mean reward: 873.62
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.3999
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.87s
                      Time elapsed: 00:14:09
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 103770 steps/s (collection: 0.810s, learning 0.137s)
             Mean action noise std: 2.78
          Mean value_function loss: 41.2035
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.1927
                       Mean reward: 870.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.3234
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.95s
                      Time elapsed: 00:14:10
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 101837 steps/s (collection: 0.822s, learning 0.143s)
             Mean action noise std: 2.78
          Mean value_function loss: 41.9481
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.2032
                       Mean reward: 876.27
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.5671
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.97s
                      Time elapsed: 00:14:11
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 93154 steps/s (collection: 0.878s, learning 0.177s)
             Mean action noise std: 2.79
          Mean value_function loss: 28.5732
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.2182
                       Mean reward: 866.29
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.6246
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 1.06s
                      Time elapsed: 00:14:12
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 102626 steps/s (collection: 0.868s, learning 0.090s)
             Mean action noise std: 2.80
          Mean value_function loss: 25.6153
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.2282
                       Mean reward: 857.71
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.9754
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.96s
                      Time elapsed: 00:14:13
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 111964 steps/s (collection: 0.777s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 30.7799
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.2364
                       Mean reward: 871.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.0676
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.88s
                      Time elapsed: 00:14:14
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 108773 steps/s (collection: 0.805s, learning 0.099s)
             Mean action noise std: 2.80
          Mean value_function loss: 36.6935
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.2421
                       Mean reward: 879.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.7997
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.90s
                      Time elapsed: 00:14:15
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 107493 steps/s (collection: 0.784s, learning 0.130s)
             Mean action noise std: 2.81
          Mean value_function loss: 32.6784
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.2533
                       Mean reward: 866.16
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.9739
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.91s
                      Time elapsed: 00:14:16
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 115275 steps/s (collection: 0.753s, learning 0.100s)
             Mean action noise std: 2.81
          Mean value_function loss: 28.6337
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.2656
                       Mean reward: 862.08
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 170.2342
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.85s
                      Time elapsed: 00:14:16
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 111162 steps/s (collection: 0.761s, learning 0.123s)
             Mean action noise std: 2.81
          Mean value_function loss: 32.9600
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.2741
                       Mean reward: 865.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.0658
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.88s
                      Time elapsed: 00:14:17
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 113081 steps/s (collection: 0.784s, learning 0.086s)
             Mean action noise std: 2.82
          Mean value_function loss: 29.6614
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.2855
                       Mean reward: 866.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 173.2714
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.87s
                      Time elapsed: 00:14:18
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 111325 steps/s (collection: 0.772s, learning 0.111s)
             Mean action noise std: 2.83
          Mean value_function loss: 39.0683
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.3153
                       Mean reward: 875.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 172.3473
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.88s
                      Time elapsed: 00:14:19
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 110036 steps/s (collection: 0.804s, learning 0.090s)
             Mean action noise std: 2.84
          Mean value_function loss: 39.5117
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.3456
                       Mean reward: 865.71
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.8440
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.89s
                      Time elapsed: 00:14:20
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 108925 steps/s (collection: 0.807s, learning 0.095s)
             Mean action noise std: 2.85
          Mean value_function loss: 39.8903
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.3628
                       Mean reward: 861.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.6978
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.90s
                      Time elapsed: 00:14:21
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 109187 steps/s (collection: 0.792s, learning 0.108s)
             Mean action noise std: 2.85
          Mean value_function loss: 46.9217
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.3804
                       Mean reward: 881.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 172.5503
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.90s
                      Time elapsed: 00:14:22
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 108363 steps/s (collection: 0.815s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 44.6083
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.3964
                       Mean reward: 867.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 170.9612
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.91s
                      Time elapsed: 00:14:23
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 105658 steps/s (collection: 0.823s, learning 0.107s)
             Mean action noise std: 2.86
          Mean value_function loss: 42.4923
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.4169
                       Mean reward: 848.84
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.2539
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.93s
                      Time elapsed: 00:14:24
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 108824 steps/s (collection: 0.800s, learning 0.104s)
             Mean action noise std: 2.87
          Mean value_function loss: 43.0267
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.4347
                       Mean reward: 849.11
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 171.0042
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.90s
                      Time elapsed: 00:14:25
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 91013 steps/s (collection: 0.876s, learning 0.204s)
             Mean action noise std: 2.87
          Mean value_function loss: 49.1725
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 19.4479
                       Mean reward: 864.86
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.7662
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 1.08s
                      Time elapsed: 00:14:26
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 100077 steps/s (collection: 0.873s, learning 0.110s)
             Mean action noise std: 2.87
          Mean value_function loss: 48.9190
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.4523
                       Mean reward: 852.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 168.8035
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.98s
                      Time elapsed: 00:14:27
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 108294 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 44.7585
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.4604
                       Mean reward: 863.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.7794
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.91s
                      Time elapsed: 00:14:28
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 112436 steps/s (collection: 0.764s, learning 0.111s)
             Mean action noise std: 2.88
          Mean value_function loss: 48.0220
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.4715
                       Mean reward: 853.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.5749
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.87s
                      Time elapsed: 00:14:28
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 108494 steps/s (collection: 0.788s, learning 0.119s)
             Mean action noise std: 2.88
          Mean value_function loss: 48.6986
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.4788
                       Mean reward: 860.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.0069
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.91s
                      Time elapsed: 00:14:29
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 96999 steps/s (collection: 0.844s, learning 0.170s)
             Mean action noise std: 2.89
          Mean value_function loss: 41.2384
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 19.4846
                       Mean reward: 855.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 168.7615
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 1.01s
                      Time elapsed: 00:14:30
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 113212 steps/s (collection: 0.764s, learning 0.104s)
             Mean action noise std: 2.89
          Mean value_function loss: 42.2640
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 19.4961
                       Mean reward: 857.12
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 171.1645
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.87s
                      Time elapsed: 00:14:31
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 108006 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 47.2117
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.5051
                       Mean reward: 862.38
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 169.7869
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.91s
                      Time elapsed: 00:14:32
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 111022 steps/s (collection: 0.777s, learning 0.108s)
             Mean action noise std: 2.90
          Mean value_function loss: 34.9504
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.5271
                       Mean reward: 856.10
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 168.7308
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.89s
                      Time elapsed: 00:14:33
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 109634 steps/s (collection: 0.788s, learning 0.109s)
             Mean action noise std: 2.90
          Mean value_function loss: 29.6152
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 19.5443
                       Mean reward: 849.53
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 169.2298
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.90s
                      Time elapsed: 00:14:34
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 106436 steps/s (collection: 0.815s, learning 0.109s)
             Mean action noise std: 2.91
          Mean value_function loss: 45.3900
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.5496
                       Mean reward: 861.09
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.3076
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.92s
                      Time elapsed: 00:14:35
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 108034 steps/s (collection: 0.793s, learning 0.117s)
             Mean action noise std: 2.91
          Mean value_function loss: 49.0110
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.5577
                       Mean reward: 856.86
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 169.7437
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.91s
                      Time elapsed: 00:14:36
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 108966 steps/s (collection: 0.792s, learning 0.111s)
             Mean action noise std: 2.91
          Mean value_function loss: 40.9709
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.5609
                       Mean reward: 862.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.9445
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.90s
                      Time elapsed: 00:14:37
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 110800 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 2.92
          Mean value_function loss: 39.0156
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.5692
                       Mean reward: 874.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 172.1733
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.89s
                      Time elapsed: 00:14:38
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 110473 steps/s (collection: 0.784s, learning 0.106s)
             Mean action noise std: 2.92
          Mean value_function loss: 33.0934
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 19.5858
                       Mean reward: 865.67
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 171.6851
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.89s
                      Time elapsed: 00:14:38
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 112054 steps/s (collection: 0.760s, learning 0.117s)
             Mean action noise std: 2.92
          Mean value_function loss: 36.7818
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.5989
                       Mean reward: 849.44
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 169.9147
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.88s
                      Time elapsed: 00:14:39
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 114988 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 2.93
          Mean value_function loss: 32.0312
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.6026
                       Mean reward: 864.11
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 169.3691
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.85s
                      Time elapsed: 00:14:40
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 108583 steps/s (collection: 0.812s, learning 0.094s)
             Mean action noise std: 2.93
          Mean value_function loss: 28.2080
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.6116
                       Mean reward: 861.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.0921
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.91s
                      Time elapsed: 00:14:41
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 116405 steps/s (collection: 0.751s, learning 0.093s)
             Mean action noise std: 2.94
          Mean value_function loss: 26.4464
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.6283
                       Mean reward: 860.26
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 173.0382
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.84s
                      Time elapsed: 00:14:42
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 111475 steps/s (collection: 0.785s, learning 0.097s)
             Mean action noise std: 2.95
          Mean value_function loss: 32.7261
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.6480
                       Mean reward: 859.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.7397
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.88s
                      Time elapsed: 00:14:43
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 116601 steps/s (collection: 0.751s, learning 0.092s)
             Mean action noise std: 2.96
          Mean value_function loss: 28.6768
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.6726
                       Mean reward: 864.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.4256
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.84s
                      Time elapsed: 00:14:44
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 109056 steps/s (collection: 0.783s, learning 0.118s)
             Mean action noise std: 2.96
          Mean value_function loss: 31.4417
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 19.6962
                       Mean reward: 855.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 170.6630
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.90s
                      Time elapsed: 00:14:45
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 106289 steps/s (collection: 0.820s, learning 0.105s)
             Mean action noise std: 2.96
          Mean value_function loss: 35.3533
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.7020
                       Mean reward: 846.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 169.7903
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.92s
                      Time elapsed: 00:14:45
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 111328 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 2.97
          Mean value_function loss: 37.6384
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.7146
                       Mean reward: 858.12
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 172.7509
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.88s
                      Time elapsed: 00:14:46
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 112945 steps/s (collection: 0.779s, learning 0.092s)
             Mean action noise std: 2.97
          Mean value_function loss: 31.3156
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.7284
                       Mean reward: 864.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.5134
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.87s
                      Time elapsed: 00:14:47
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 113718 steps/s (collection: 0.777s, learning 0.087s)
             Mean action noise std: 2.98
          Mean value_function loss: 32.3237
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.7407
                       Mean reward: 843.95
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.1371
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.86s
                      Time elapsed: 00:14:48
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 112323 steps/s (collection: 0.780s, learning 0.095s)
             Mean action noise std: 2.98
          Mean value_function loss: 41.5133
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.7531
                       Mean reward: 869.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 172.0722
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.88s
                      Time elapsed: 00:14:49
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 107554 steps/s (collection: 0.818s, learning 0.096s)
             Mean action noise std: 2.99
          Mean value_function loss: 34.1805
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.7747
                       Mean reward: 862.60
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 171.6517
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.91s
                      Time elapsed: 00:14:50
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 114352 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 2.99
          Mean value_function loss: 47.1435
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.7976
                       Mean reward: 862.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.2045
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.86s
                      Time elapsed: 00:14:51
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 110074 steps/s (collection: 0.805s, learning 0.088s)
             Mean action noise std: 3.00
          Mean value_function loss: 43.0684
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.8080
                       Mean reward: 861.56
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 172.0310
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.89s
                      Time elapsed: 00:14:52
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 106904 steps/s (collection: 0.799s, learning 0.121s)
             Mean action noise std: 3.00
          Mean value_function loss: 47.1657
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.8230
                       Mean reward: 835.98
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 170.9863
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.92s
                      Time elapsed: 00:14:53
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 110317 steps/s (collection: 0.775s, learning 0.116s)
             Mean action noise std: 3.01
          Mean value_function loss: 40.9306
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.8361
                       Mean reward: 854.94
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.2125
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.89s
                      Time elapsed: 00:14:53
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 113992 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 3.01
          Mean value_function loss: 43.3242
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.8438
                       Mean reward: 840.22
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.0804
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.86s
                      Time elapsed: 00:14:54
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 108659 steps/s (collection: 0.791s, learning 0.114s)
             Mean action noise std: 3.01
          Mean value_function loss: 43.8926
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.8517
                       Mean reward: 868.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 172.8953
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.90s
                      Time elapsed: 00:14:55
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 110534 steps/s (collection: 0.791s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 46.4520
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.8704
                       Mean reward: 875.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 172.5030
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.89s
                      Time elapsed: 00:14:56
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 110052 steps/s (collection: 0.790s, learning 0.104s)
             Mean action noise std: 3.03
          Mean value_function loss: 50.1597
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.9000
                       Mean reward: 854.94
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 166.4849
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.89s
                      Time elapsed: 00:14:57
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 106477 steps/s (collection: 0.826s, learning 0.097s)
             Mean action noise std: 3.04
          Mean value_function loss: 38.8903
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.9149
                       Mean reward: 868.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.8963
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.92s
                      Time elapsed: 00:14:58
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 116547 steps/s (collection: 0.756s, learning 0.088s)
             Mean action noise std: 3.04
          Mean value_function loss: 42.7675
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9287
                       Mean reward: 847.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 168.2632
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.84s
                      Time elapsed: 00:14:59
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 113503 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 3.05
          Mean value_function loss: 32.2365
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9365
                       Mean reward: 838.57
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 169.4674
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.87s
                      Time elapsed: 00:15:00
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 116068 steps/s (collection: 0.750s, learning 0.097s)
             Mean action noise std: 3.05
          Mean value_function loss: 36.4257
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.9446
                       Mean reward: 849.91
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 169.3537
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.85s
                      Time elapsed: 00:15:00
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 102834 steps/s (collection: 0.849s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 34.8979
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.9693
                       Mean reward: 865.22
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 171.9620
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.96s
                      Time elapsed: 00:15:01
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 107159 steps/s (collection: 0.819s, learning 0.098s)
             Mean action noise std: 3.07
          Mean value_function loss: 42.3886
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.9867
                       Mean reward: 853.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 169.4087
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.92s
                      Time elapsed: 00:15:02
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 107544 steps/s (collection: 0.807s, learning 0.107s)
             Mean action noise std: 3.07
          Mean value_function loss: 43.2195
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9993
                       Mean reward: 863.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 170.4609
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.91s
                      Time elapsed: 00:15:03
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 105642 steps/s (collection: 0.836s, learning 0.094s)
             Mean action noise std: 3.07
          Mean value_function loss: 34.8008
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.0053
                       Mean reward: 866.40
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 168.6480
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.93s
                      Time elapsed: 00:15:04
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 104672 steps/s (collection: 0.827s, learning 0.112s)
             Mean action noise std: 3.08
          Mean value_function loss: 30.4826
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 20.0145
                       Mean reward: 871.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.5006
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.94s
                      Time elapsed: 00:15:05
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 110356 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 3.08
          Mean value_function loss: 50.7801
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 20.0181
                       Mean reward: 876.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 172.2365
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.89s
                      Time elapsed: 00:15:06
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 106405 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 3.08
          Mean value_function loss: 36.8891
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 20.0293
                       Mean reward: 860.52
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 170.0364
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.92s
                      Time elapsed: 00:15:07
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 107534 steps/s (collection: 0.801s, learning 0.114s)
             Mean action noise std: 3.09
          Mean value_function loss: 36.6543
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.0367
                       Mean reward: 851.08
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.6219
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.91s
                      Time elapsed: 00:15:08
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 106362 steps/s (collection: 0.821s, learning 0.104s)
             Mean action noise std: 3.09
          Mean value_function loss: 40.6177
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.0444
                       Mean reward: 863.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 173.0920
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.92s
                      Time elapsed: 00:15:09
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 109662 steps/s (collection: 0.786s, learning 0.110s)
             Mean action noise std: 3.10
          Mean value_function loss: 33.1550
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 20.0563
                       Mean reward: 842.74
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 169.3050
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.90s
                      Time elapsed: 00:15:10
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 99771 steps/s (collection: 0.872s, learning 0.113s)
             Mean action noise std: 3.10
          Mean value_function loss: 43.8947
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.0708
                       Mean reward: 871.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.4857
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.99s
                      Time elapsed: 00:15:11
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 102652 steps/s (collection: 0.854s, learning 0.104s)
             Mean action noise std: 3.11
          Mean value_function loss: 36.2230
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.0886
                       Mean reward: 866.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 171.6075
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.96s
                      Time elapsed: 00:15:12
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 104529 steps/s (collection: 0.811s, learning 0.129s)
             Mean action noise std: 3.12
          Mean value_function loss: 48.5314
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.1068
                       Mean reward: 855.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.1348
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.94s
                      Time elapsed: 00:15:13
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 101619 steps/s (collection: 0.859s, learning 0.108s)
             Mean action noise std: 3.12
          Mean value_function loss: 44.0213
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 20.1158
                       Mean reward: 861.96
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.8647
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.97s
                      Time elapsed: 00:15:13
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 106356 steps/s (collection: 0.806s, learning 0.118s)
             Mean action noise std: 3.13
          Mean value_function loss: 48.4431
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.1369
                       Mean reward: 833.42
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 168.3812
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.92s
                      Time elapsed: 00:15:14
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 104267 steps/s (collection: 0.843s, learning 0.100s)
             Mean action noise std: 3.14
          Mean value_function loss: 39.0938
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 20.1544
                       Mean reward: 854.66
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.7306
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.94s
                      Time elapsed: 00:15:15
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 99653 steps/s (collection: 0.868s, learning 0.119s)
             Mean action noise std: 3.14
          Mean value_function loss: 43.4623
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.1639
                       Mean reward: 862.38
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 169.9549
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.99s
                      Time elapsed: 00:15:16
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 106760 steps/s (collection: 0.802s, learning 0.119s)
             Mean action noise std: 3.14
          Mean value_function loss: 42.8330
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1704
                       Mean reward: 863.32
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.2140
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.92s
                      Time elapsed: 00:15:17
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 106763 steps/s (collection: 0.820s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 38.1864
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.1860
                       Mean reward: 868.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.4883
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.92s
                      Time elapsed: 00:15:18
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 107240 steps/s (collection: 0.820s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 39.4358
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.2023
                       Mean reward: 845.27
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 168.8168
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.92s
                      Time elapsed: 00:15:19
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 102589 steps/s (collection: 0.835s, learning 0.124s)
             Mean action noise std: 3.16
          Mean value_function loss: 46.0844
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.2167
                       Mean reward: 855.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.4775
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.96s
                      Time elapsed: 00:15:20
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 104428 steps/s (collection: 0.832s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 32.9786
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.2249
                       Mean reward: 835.49
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 167.5955
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.94s
                      Time elapsed: 00:15:21
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 95505 steps/s (collection: 0.903s, learning 0.127s)
             Mean action noise std: 3.17
          Mean value_function loss: 33.8541
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.2346
                       Mean reward: 842.11
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 169.9380
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 1.03s
                      Time elapsed: 00:15:22
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 100540 steps/s (collection: 0.833s, learning 0.144s)
             Mean action noise std: 3.17
          Mean value_function loss: 44.3793
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.2371
                       Mean reward: 841.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 170.7310
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.98s
                      Time elapsed: 00:15:23
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 98918 steps/s (collection: 0.848s, learning 0.146s)
             Mean action noise std: 3.17
          Mean value_function loss: 42.1357
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.2415
                       Mean reward: 862.28
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 169.8870
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.99s
                      Time elapsed: 00:15:24
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 108820 steps/s (collection: 0.802s, learning 0.102s)
             Mean action noise std: 3.17
          Mean value_function loss: 41.6932
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.2474
                       Mean reward: 837.27
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 169.8418
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.90s
                      Time elapsed: 00:15:25
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 106438 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 3.18
          Mean value_function loss: 40.4926
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.2626
                       Mean reward: 865.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.4615
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.92s
                      Time elapsed: 00:15:26
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 106182 steps/s (collection: 0.810s, learning 0.116s)
             Mean action noise std: 3.18
          Mean value_function loss: 36.8736
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 20.2683
                       Mean reward: 853.28
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.2237
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.93s
                      Time elapsed: 00:15:27
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 107452 steps/s (collection: 0.807s, learning 0.108s)
             Mean action noise std: 3.18
          Mean value_function loss: 40.7819
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.2713
                       Mean reward: 859.62
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 169.6637
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.91s
                      Time elapsed: 00:15:28
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 106428 steps/s (collection: 0.825s, learning 0.098s)
             Mean action noise std: 3.18
          Mean value_function loss: 38.1310
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.2762
                       Mean reward: 843.91
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 170.8591
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.92s
                      Time elapsed: 00:15:29
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 103179 steps/s (collection: 0.847s, learning 0.106s)
             Mean action noise std: 3.19
          Mean value_function loss: 43.6654
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.2829
                       Mean reward: 841.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 170.8632
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.95s
                      Time elapsed: 00:15:30
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 107226 steps/s (collection: 0.810s, learning 0.107s)
             Mean action noise std: 3.19
          Mean value_function loss: 35.8832
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 20.2954
                       Mean reward: 856.64
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 170.8745
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.92s
                      Time elapsed: 00:15:30
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 112542 steps/s (collection: 0.778s, learning 0.096s)
             Mean action noise std: 3.19
          Mean value_function loss: 41.8517
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.3028
                       Mean reward: 865.91
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.9081
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.87s
                      Time elapsed: 00:15:31
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 109218 steps/s (collection: 0.787s, learning 0.113s)
             Mean action noise std: 3.20
          Mean value_function loss: 47.7662
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.3099
                       Mean reward: 866.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.1718
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.90s
                      Time elapsed: 00:15:32
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 102920 steps/s (collection: 0.848s, learning 0.107s)
             Mean action noise std: 3.20
          Mean value_function loss: 58.1239
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.3233
                       Mean reward: 854.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 169.6985
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.96s
                      Time elapsed: 00:15:33
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 97382 steps/s (collection: 0.858s, learning 0.151s)
             Mean action noise std: 3.21
          Mean value_function loss: 43.2513
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.3434
                       Mean reward: 872.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.8980
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 1.01s
                      Time elapsed: 00:15:34
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 102911 steps/s (collection: 0.852s, learning 0.103s)
             Mean action noise std: 3.22
          Mean value_function loss: 52.4903
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.3660
                       Mean reward: 860.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.6412
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.96s
                      Time elapsed: 00:15:35
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 106561 steps/s (collection: 0.818s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 49.8281
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.3772
                       Mean reward: 836.41
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.1107
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.92s
                      Time elapsed: 00:15:36
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 105768 steps/s (collection: 0.831s, learning 0.099s)
             Mean action noise std: 3.22
          Mean value_function loss: 54.8417
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 20.3837
                       Mean reward: 844.54
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.9213
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.93s
                      Time elapsed: 00:15:37
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 109396 steps/s (collection: 0.793s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 41.9743
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.3862
                       Mean reward: 853.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 168.9034
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.90s
                      Time elapsed: 00:15:38
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 95591 steps/s (collection: 0.895s, learning 0.134s)
             Mean action noise std: 3.23
          Mean value_function loss: 40.3266
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.3908
                       Mean reward: 850.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 170.4629
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 1.03s
                      Time elapsed: 00:15:39
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 103347 steps/s (collection: 0.841s, learning 0.110s)
             Mean action noise std: 3.23
          Mean value_function loss: 47.9116
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.4001
                       Mean reward: 875.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 169.7054
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.95s
                      Time elapsed: 00:15:40
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 107756 steps/s (collection: 0.811s, learning 0.102s)
             Mean action noise std: 3.24
          Mean value_function loss: 48.4469
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.4124
                       Mean reward: 834.06
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 168.6967
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.91s
                      Time elapsed: 00:15:41
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 105219 steps/s (collection: 0.830s, learning 0.105s)
             Mean action noise std: 3.24
          Mean value_function loss: 45.5311
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 20.4212
                       Mean reward: 851.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 167.9184
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.93s
                      Time elapsed: 00:15:42
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 102908 steps/s (collection: 0.840s, learning 0.116s)
             Mean action noise std: 3.24
          Mean value_function loss: 42.1278
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.4296
                       Mean reward: 873.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.3902
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.96s
                      Time elapsed: 00:15:43
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 100166 steps/s (collection: 0.871s, learning 0.110s)
             Mean action noise std: 3.25
          Mean value_function loss: 45.2176
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.4428
                       Mean reward: 861.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 170.7462
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.98s
                      Time elapsed: 00:15:44
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 99953 steps/s (collection: 0.872s, learning 0.112s)
             Mean action noise std: 3.25
          Mean value_function loss: 38.8027
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.4489
                       Mean reward: 857.55
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.4780
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.98s
                      Time elapsed: 00:15:45
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 96886 steps/s (collection: 0.894s, learning 0.121s)
             Mean action noise std: 3.25
          Mean value_function loss: 32.4452
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 20.4508
                       Mean reward: 868.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.1860
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 1.01s
                      Time elapsed: 00:15:46
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 104054 steps/s (collection: 0.836s, learning 0.109s)
             Mean action noise std: 3.25
          Mean value_function loss: 45.9356
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.4548
                       Mean reward: 855.09
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.0928
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.94s
                      Time elapsed: 00:15:47
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 103499 steps/s (collection: 0.841s, learning 0.109s)
             Mean action noise std: 3.26
          Mean value_function loss: 45.8401
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.4645
                       Mean reward: 854.03
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.1384
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.95s
                      Time elapsed: 00:15:48
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 101604 steps/s (collection: 0.870s, learning 0.097s)
             Mean action noise std: 3.26
          Mean value_function loss: 49.4351
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.4793
                       Mean reward: 869.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.5419
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.97s
                      Time elapsed: 00:15:49
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 106976 steps/s (collection: 0.816s, learning 0.103s)
             Mean action noise std: 3.27
          Mean value_function loss: 48.6567
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.4880
                       Mean reward: 862.58
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.2324
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.92s
                      Time elapsed: 00:15:49
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 107210 steps/s (collection: 0.810s, learning 0.107s)
             Mean action noise std: 3.27
          Mean value_function loss: 45.1308
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.4905
                       Mean reward: 837.94
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 170.6554
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.92s
                      Time elapsed: 00:15:50
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 102421 steps/s (collection: 0.854s, learning 0.105s)
             Mean action noise std: 3.27
          Mean value_function loss: 51.4577
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.5003
                       Mean reward: 861.59
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.8799
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.96s
                      Time elapsed: 00:15:51
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 103670 steps/s (collection: 0.821s, learning 0.128s)
             Mean action noise std: 3.28
          Mean value_function loss: 45.7711
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 20.5087
                       Mean reward: 839.06
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 170.6891
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.95s
                      Time elapsed: 00:15:52
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 102311 steps/s (collection: 0.839s, learning 0.122s)
             Mean action noise std: 3.28
          Mean value_function loss: 44.0399
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.5135
                       Mean reward: 845.50
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 169.0848
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.96s
                      Time elapsed: 00:15:53
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 101700 steps/s (collection: 0.836s, learning 0.131s)
             Mean action noise std: 3.28
          Mean value_function loss: 50.0323
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.5214
                       Mean reward: 857.98
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 169.1396
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.97s
                      Time elapsed: 00:15:54
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 101543 steps/s (collection: 0.842s, learning 0.126s)
             Mean action noise std: 3.29
          Mean value_function loss: 41.6020
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.5298
                       Mean reward: 833.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 167.9790
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.97s
                      Time elapsed: 00:15:55
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 104408 steps/s (collection: 0.835s, learning 0.107s)
             Mean action noise std: 3.29
          Mean value_function loss: 42.7609
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.5365
                       Mean reward: 854.39
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 170.8360
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.94s
                      Time elapsed: 00:15:56
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 102183 steps/s (collection: 0.842s, learning 0.120s)
             Mean action noise std: 3.29
          Mean value_function loss: 41.3774
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.5436
                       Mean reward: 864.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.1896
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.96s
                      Time elapsed: 00:15:57
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 112310 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 3.30
          Mean value_function loss: 37.8649
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.5526
                       Mean reward: 870.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 170.9048
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.88s
                      Time elapsed: 00:15:58
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 103317 steps/s (collection: 0.843s, learning 0.108s)
             Mean action noise std: 3.30
          Mean value_function loss: 42.6119
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 20.5568
                       Mean reward: 871.77
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 172.7809
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.95s
                      Time elapsed: 00:15:59
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 109021 steps/s (collection: 0.801s, learning 0.101s)
             Mean action noise std: 3.31
          Mean value_function loss: 24.4933
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.5709
                       Mean reward: 874.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.4265
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.90s
                      Time elapsed: 00:16:00
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 107252 steps/s (collection: 0.799s, learning 0.117s)
             Mean action noise std: 3.31
          Mean value_function loss: 34.8644
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.5943
                       Mean reward: 872.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 172.3699
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.92s
                      Time elapsed: 00:16:01
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 111182 steps/s (collection: 0.780s, learning 0.104s)
             Mean action noise std: 3.33
          Mean value_function loss: 27.3599
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.6120
                       Mean reward: 878.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.5179
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.88s
                      Time elapsed: 00:16:02
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 107961 steps/s (collection: 0.806s, learning 0.104s)
             Mean action noise std: 3.33
          Mean value_function loss: 28.6366
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 20.6310
                       Mean reward: 848.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.4034
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.91s
                      Time elapsed: 00:16:03
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 108736 steps/s (collection: 0.801s, learning 0.103s)
             Mean action noise std: 3.34
          Mean value_function loss: 28.8966
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.6406
                       Mean reward: 862.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.1830
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.90s
                      Time elapsed: 00:16:03
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 108896 steps/s (collection: 0.802s, learning 0.101s)
             Mean action noise std: 3.34
          Mean value_function loss: 31.0093
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.6543
                       Mean reward: 878.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 174.0101
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.90s
                      Time elapsed: 00:16:04
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 104931 steps/s (collection: 0.829s, learning 0.108s)
             Mean action noise std: 3.35
          Mean value_function loss: 30.4408
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.6663
                       Mean reward: 863.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.0453
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.94s
                      Time elapsed: 00:16:05
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 106472 steps/s (collection: 0.812s, learning 0.111s)
             Mean action noise std: 3.35
          Mean value_function loss: 29.6677
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.6864
                       Mean reward: 880.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 173.9719
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.92s
                      Time elapsed: 00:16:06
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 109940 steps/s (collection: 0.786s, learning 0.108s)
             Mean action noise std: 3.36
          Mean value_function loss: 44.0596
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.6996
                       Mean reward: 873.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.8054
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.89s
                      Time elapsed: 00:16:07
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 106458 steps/s (collection: 0.822s, learning 0.102s)
             Mean action noise std: 3.36
          Mean value_function loss: 43.9995
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 20.7113
                       Mean reward: 875.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.5534
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.92s
                      Time elapsed: 00:16:08
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 105275 steps/s (collection: 0.819s, learning 0.115s)
             Mean action noise std: 3.37
          Mean value_function loss: 39.5312
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 20.7271
                       Mean reward: 870.20
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.0753
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.93s
                      Time elapsed: 00:16:09
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 105433 steps/s (collection: 0.817s, learning 0.115s)
             Mean action noise std: 3.37
          Mean value_function loss: 42.7432
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.7302
                       Mean reward: 854.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.0219
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.93s
                      Time elapsed: 00:16:10
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 108925 steps/s (collection: 0.804s, learning 0.098s)
             Mean action noise std: 3.37
          Mean value_function loss: 30.4009
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.7307
                       Mean reward: 866.06
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.3210
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.90s
                      Time elapsed: 00:16:11
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 106498 steps/s (collection: 0.812s, learning 0.112s)
             Mean action noise std: 3.37
          Mean value_function loss: 27.9396
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 20.7359
                       Mean reward: 863.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.6979
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.92s
                      Time elapsed: 00:16:12
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 106468 steps/s (collection: 0.808s, learning 0.116s)
             Mean action noise std: 3.38
          Mean value_function loss: 30.4329
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.7461
                       Mean reward: 857.63
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.1465
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.92s
                      Time elapsed: 00:16:13
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 105478 steps/s (collection: 0.822s, learning 0.110s)
             Mean action noise std: 3.38
          Mean value_function loss: 31.1119
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.7606
                       Mean reward: 869.09
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 173.7684
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.93s
                      Time elapsed: 00:16:14
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 105493 steps/s (collection: 0.826s, learning 0.106s)
             Mean action noise std: 3.39
          Mean value_function loss: 29.0595
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.7684
                       Mean reward: 845.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.1998
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.93s
                      Time elapsed: 00:16:14
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 107520 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 3.39
          Mean value_function loss: 36.8110
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.7727
                       Mean reward: 848.55
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 171.4296
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.91s
                      Time elapsed: 00:16:15
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 108107 steps/s (collection: 0.806s, learning 0.104s)
             Mean action noise std: 3.40
          Mean value_function loss: 34.7268
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.7815
                       Mean reward: 860.39
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 172.3708
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.91s
                      Time elapsed: 00:16:16
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 109299 steps/s (collection: 0.795s, learning 0.105s)
             Mean action noise std: 3.40
          Mean value_function loss: 37.9871
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.7943
                       Mean reward: 868.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.5934
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.90s
                      Time elapsed: 00:16:17
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 109676 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 3.41
          Mean value_function loss: 30.6051
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.8070
                       Mean reward: 878.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.6676
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.90s
                      Time elapsed: 00:16:18
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 103434 steps/s (collection: 0.840s, learning 0.111s)
             Mean action noise std: 3.41
          Mean value_function loss: 28.0842
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.8238
                       Mean reward: 853.22
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 171.8446
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.95s
                      Time elapsed: 00:16:19
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 106779 steps/s (collection: 0.819s, learning 0.102s)
             Mean action noise std: 3.42
          Mean value_function loss: 36.8976
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.8390
                       Mean reward: 856.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 173.2789
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.92s
                      Time elapsed: 00:16:20
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 108943 steps/s (collection: 0.802s, learning 0.101s)
             Mean action noise std: 3.43
          Mean value_function loss: 35.0122
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.8533
                       Mean reward: 863.43
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 172.7295
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.90s
                      Time elapsed: 00:16:21
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 107640 steps/s (collection: 0.802s, learning 0.111s)
             Mean action noise std: 3.43
          Mean value_function loss: 38.4736
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.8658
                       Mean reward: 859.07
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.4428
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.91s
                      Time elapsed: 00:16:22
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 107503 steps/s (collection: 0.816s, learning 0.098s)
             Mean action noise std: 3.44
          Mean value_function loss: 38.4187
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.8811
                       Mean reward: 872.92
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.6751
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.91s
                      Time elapsed: 00:16:23
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 104155 steps/s (collection: 0.816s, learning 0.128s)
             Mean action noise std: 3.45
          Mean value_function loss: 36.3336
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.9035
                       Mean reward: 867.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.6266
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.94s
                      Time elapsed: 00:16:24
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 105546 steps/s (collection: 0.809s, learning 0.123s)
             Mean action noise std: 3.46
          Mean value_function loss: 32.5007
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.9303
                       Mean reward: 876.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.9866
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.93s
                      Time elapsed: 00:16:25
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 105990 steps/s (collection: 0.813s, learning 0.114s)
             Mean action noise std: 3.46
          Mean value_function loss: 43.2494
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.9445
                       Mean reward: 880.64
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 174.2840
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.93s
                      Time elapsed: 00:16:26
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 104716 steps/s (collection: 0.819s, learning 0.120s)
             Mean action noise std: 3.47
          Mean value_function loss: 40.0683
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.9527
                       Mean reward: 855.51
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 170.8622
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.94s
                      Time elapsed: 00:16:26
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 108099 steps/s (collection: 0.812s, learning 0.097s)
             Mean action noise std: 3.47
          Mean value_function loss: 36.4541
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.9569
                       Mean reward: 856.79
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.3683
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.91s
                      Time elapsed: 00:16:27
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 107230 steps/s (collection: 0.788s, learning 0.129s)
             Mean action noise std: 3.47
          Mean value_function loss: 34.6799
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.9612
                       Mean reward: 860.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.4585
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.92s
                      Time elapsed: 00:16:28
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 97827 steps/s (collection: 0.872s, learning 0.133s)
             Mean action noise std: 3.48
          Mean value_function loss: 40.5553
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.9729
                       Mean reward: 876.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 173.0191
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 1.00s
                      Time elapsed: 00:16:29
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 108826 steps/s (collection: 0.799s, learning 0.104s)
             Mean action noise std: 3.48
          Mean value_function loss: 36.2265
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.9832
                       Mean reward: 870.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.6108
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.90s
                      Time elapsed: 00:16:30
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 108037 steps/s (collection: 0.814s, learning 0.096s)
             Mean action noise std: 3.48
          Mean value_function loss: 40.9408
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 20.9930
                       Mean reward: 855.01
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.6334
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.91s
                      Time elapsed: 00:16:31
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 104322 steps/s (collection: 0.843s, learning 0.099s)
             Mean action noise std: 3.49
          Mean value_function loss: 28.2074
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.9973
                       Mean reward: 863.90
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.2980
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.94s
                      Time elapsed: 00:16:32
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 106661 steps/s (collection: 0.824s, learning 0.098s)
             Mean action noise std: 3.49
          Mean value_function loss: 35.5477
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.0011
                       Mean reward: 873.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 173.3264
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.92s
                      Time elapsed: 00:16:33
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 105273 steps/s (collection: 0.829s, learning 0.105s)
             Mean action noise std: 3.49
          Mean value_function loss: 40.3651
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.0049
                       Mean reward: 838.78
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.9276
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.93s
                      Time elapsed: 00:16:34
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 107655 steps/s (collection: 0.806s, learning 0.107s)
             Mean action noise std: 3.49
          Mean value_function loss: 33.8809
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.0072
                       Mean reward: 851.89
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 173.0579
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.91s
                      Time elapsed: 00:16:35
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 105834 steps/s (collection: 0.822s, learning 0.107s)
             Mean action noise std: 3.50
          Mean value_function loss: 36.9543
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.0143
                       Mean reward: 862.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.3389
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.93s
                      Time elapsed: 00:16:36
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 108604 steps/s (collection: 0.808s, learning 0.097s)
             Mean action noise std: 3.50
          Mean value_function loss: 34.5234
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 21.0248
                       Mean reward: 872.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 172.8525
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.91s
                      Time elapsed: 00:16:37
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 107190 steps/s (collection: 0.812s, learning 0.105s)
             Mean action noise std: 3.50
          Mean value_function loss: 32.8259
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 21.0286
                       Mean reward: 856.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 171.7077
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.92s
                      Time elapsed: 00:16:38
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 107741 steps/s (collection: 0.807s, learning 0.106s)
             Mean action noise std: 3.51
          Mean value_function loss: 29.7096
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.0331
                       Mean reward: 855.02
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.5092
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.91s
                      Time elapsed: 00:16:38
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 107624 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 3.51
          Mean value_function loss: 28.8970
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 21.0403
                       Mean reward: 853.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.8661
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.91s
                      Time elapsed: 00:16:39
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 106812 steps/s (collection: 0.813s, learning 0.107s)
             Mean action noise std: 3.51
          Mean value_function loss: 38.7792
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.0447
                       Mean reward: 865.30
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 173.9819
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.92s
                      Time elapsed: 00:16:40
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 109997 steps/s (collection: 0.795s, learning 0.099s)
             Mean action noise std: 3.52
          Mean value_function loss: 44.9863
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.0507
                       Mean reward: 868.55
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.3409
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.89s
                      Time elapsed: 00:16:41
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 101429 steps/s (collection: 0.859s, learning 0.110s)
             Mean action noise std: 3.52
          Mean value_function loss: 39.7642
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.0622
                       Mean reward: 872.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 173.0129
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.97s
                      Time elapsed: 00:16:42
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 108037 steps/s (collection: 0.814s, learning 0.096s)
             Mean action noise std: 3.52
          Mean value_function loss: 46.2118
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.0716
                       Mean reward: 872.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.4329
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.91s
                      Time elapsed: 00:16:43
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 106625 steps/s (collection: 0.824s, learning 0.098s)
             Mean action noise std: 3.52
          Mean value_function loss: 42.8239
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 21.0770
                       Mean reward: 864.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.1981
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.92s
                      Time elapsed: 00:16:44
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 106466 steps/s (collection: 0.821s, learning 0.102s)
             Mean action noise std: 3.52
          Mean value_function loss: 38.7039
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 21.0776
                       Mean reward: 862.71
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.5378
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.92s
                      Time elapsed: 00:16:45
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 105395 steps/s (collection: 0.819s, learning 0.113s)
             Mean action noise std: 3.52
          Mean value_function loss: 42.4907
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 21.0781
                       Mean reward: 862.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 171.9624
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.93s
                      Time elapsed: 00:16:46
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 105669 steps/s (collection: 0.820s, learning 0.110s)
             Mean action noise std: 3.53
          Mean value_function loss: 42.5880
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.0803
                       Mean reward: 877.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 172.7568
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.93s
                      Time elapsed: 00:16:47
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 103663 steps/s (collection: 0.841s, learning 0.107s)
             Mean action noise std: 3.53
          Mean value_function loss: 48.8372
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.0858
                       Mean reward: 878.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 173.8158
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.95s
                      Time elapsed: 00:16:48
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 103252 steps/s (collection: 0.824s, learning 0.128s)
             Mean action noise std: 3.53
          Mean value_function loss: 46.0433
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.0936
                       Mean reward: 858.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.7190
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.95s
                      Time elapsed: 00:16:49
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 106415 steps/s (collection: 0.821s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 41.6033
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 21.1019
                       Mean reward: 843.36
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.6175
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.92s
                      Time elapsed: 00:16:50
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 105091 steps/s (collection: 0.817s, learning 0.119s)
             Mean action noise std: 3.54
          Mean value_function loss: 48.3835
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.1117
                       Mean reward: 840.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 167.8851
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.94s
                      Time elapsed: 00:16:51
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 104033 steps/s (collection: 0.835s, learning 0.110s)
             Mean action noise std: 3.55
          Mean value_function loss: 46.8567
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.1243
                       Mean reward: 851.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 167.2817
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.94s
                      Time elapsed: 00:16:51
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 107603 steps/s (collection: 0.807s, learning 0.107s)
             Mean action noise std: 3.55
          Mean value_function loss: 38.7207
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 21.1313
                       Mean reward: 863.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.9991
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.91s
                      Time elapsed: 00:16:52
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 104681 steps/s (collection: 0.820s, learning 0.119s)
             Mean action noise std: 3.55
          Mean value_function loss: 35.4994
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.1383
                       Mean reward: 850.06
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 169.7736
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.94s
                      Time elapsed: 00:16:53
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 97967 steps/s (collection: 0.873s, learning 0.131s)
             Mean action noise std: 3.56
          Mean value_function loss: 34.9584
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.1474
                       Mean reward: 868.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.2401
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 1.00s
                      Time elapsed: 00:16:54
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 103660 steps/s (collection: 0.816s, learning 0.132s)
             Mean action noise std: 3.56
          Mean value_function loss: 32.5447
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.1508
                       Mean reward: 854.53
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.8717
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.95s
                      Time elapsed: 00:16:55
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 106466 steps/s (collection: 0.792s, learning 0.132s)
             Mean action noise std: 3.56
          Mean value_function loss: 39.4195
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.1592
                       Mean reward: 856.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 170.9247
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.92s
                      Time elapsed: 00:16:56
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 106058 steps/s (collection: 0.810s, learning 0.117s)
             Mean action noise std: 3.57
          Mean value_function loss: 31.6076
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.1698
                       Mean reward: 851.20
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.0982
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.93s
                      Time elapsed: 00:16:57
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 108229 steps/s (collection: 0.807s, learning 0.101s)
             Mean action noise std: 3.57
          Mean value_function loss: 35.6235
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.1798
                       Mean reward: 859.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.8600
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.91s
                      Time elapsed: 00:16:58
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 103574 steps/s (collection: 0.827s, learning 0.123s)
             Mean action noise std: 3.58
          Mean value_function loss: 35.5495
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.1942
                       Mean reward: 848.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 171.0187
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.95s
                      Time elapsed: 00:16:59
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 104436 steps/s (collection: 0.825s, learning 0.117s)
             Mean action noise std: 3.59
          Mean value_function loss: 48.1984
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.2108
                       Mean reward: 877.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 173.7491
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.94s
                      Time elapsed: 00:17:00
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 107349 steps/s (collection: 0.802s, learning 0.114s)
             Mean action noise std: 3.59
          Mean value_function loss: 34.5410
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.2221
                       Mean reward: 871.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.1206
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.92s
                      Time elapsed: 00:17:01
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 105375 steps/s (collection: 0.818s, learning 0.115s)
             Mean action noise std: 3.60
          Mean value_function loss: 43.2530
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.2300
                       Mean reward: 870.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 171.5943
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.93s
                      Time elapsed: 00:17:02
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 107239 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 3.60
          Mean value_function loss: 32.4808
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.2370
                       Mean reward: 867.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 172.4502
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.92s
                      Time elapsed: 00:17:03
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 104030 steps/s (collection: 0.846s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 39.5267
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.2474
                       Mean reward: 860.94
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 173.1253
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.94s
                      Time elapsed: 00:17:04
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 105225 steps/s (collection: 0.831s, learning 0.104s)
             Mean action noise std: 3.61
          Mean value_function loss: 30.4354
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.2610
                       Mean reward: 852.03
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 170.1080
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.93s
                      Time elapsed: 00:17:05
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 107308 steps/s (collection: 0.817s, learning 0.099s)
             Mean action noise std: 3.62
          Mean value_function loss: 29.0917
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.2779
                       Mean reward: 846.39
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.3119
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.92s
                      Time elapsed: 00:17:05
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 106099 steps/s (collection: 0.817s, learning 0.110s)
             Mean action noise std: 3.63
          Mean value_function loss: 29.8231
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.2926
                       Mean reward: 857.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.1310
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.93s
                      Time elapsed: 00:17:06
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 109254 steps/s (collection: 0.797s, learning 0.103s)
             Mean action noise std: 3.63
          Mean value_function loss: 40.6401
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.3016
                       Mean reward: 868.18
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.9122
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.90s
                      Time elapsed: 00:17:07
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 33764 steps/s (collection: 2.783s, learning 0.128s)
             Mean action noise std: 3.64
          Mean value_function loss: 25.8098
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.3096
                       Mean reward: 878.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.5494
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.91s
                      Time elapsed: 00:17:10
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 32286 steps/s (collection: 2.914s, learning 0.130s)
             Mean action noise std: 3.65
          Mean value_function loss: 36.7144
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.3236
                       Mean reward: 875.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.9548
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.04s
                      Time elapsed: 00:17:13
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 31182 steps/s (collection: 3.029s, learning 0.123s)
             Mean action noise std: 3.66
          Mean value_function loss: 30.7619
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.3500
                       Mean reward: 855.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 172.0711
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.15s
                      Time elapsed: 00:17:16
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 31542 steps/s (collection: 2.996s, learning 0.121s)
             Mean action noise std: 3.66
          Mean value_function loss: 31.2710
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.3735
                       Mean reward: 872.29
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 173.8190
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.12s
                      Time elapsed: 00:17:20
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 31733 steps/s (collection: 2.986s, learning 0.112s)
             Mean action noise std: 3.67
          Mean value_function loss: 35.3101
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.3825
                       Mean reward: 862.82
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.1883
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.10s
                      Time elapsed: 00:17:23
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 31616 steps/s (collection: 2.991s, learning 0.118s)
             Mean action noise std: 3.67
          Mean value_function loss: 27.3952
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.3910
                       Mean reward: 877.99
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.0137
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 3.11s
                      Time elapsed: 00:17:26
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 32844 steps/s (collection: 2.870s, learning 0.123s)
             Mean action noise std: 3.68
          Mean value_function loss: 34.7240
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.3970
                       Mean reward: 859.97
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 171.8450
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 2.99s
                      Time elapsed: 00:17:29
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 31382 steps/s (collection: 2.992s, learning 0.140s)
             Mean action noise std: 3.68
          Mean value_function loss: 28.7695
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.4051
                       Mean reward: 867.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.8829
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.13s
                      Time elapsed: 00:17:32
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 27778 steps/s (collection: 3.415s, learning 0.124s)
             Mean action noise std: 3.69
          Mean value_function loss: 32.3686
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.4176
                       Mean reward: 861.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 173.7153
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.54s
                      Time elapsed: 00:17:35
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 105375 steps/s (collection: 0.816s, learning 0.117s)
             Mean action noise std: 3.70
          Mean value_function loss: 35.0391
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.4291
                       Mean reward: 858.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.9646
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.93s
                      Time elapsed: 00:17:36
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 109857 steps/s (collection: 0.794s, learning 0.101s)
             Mean action noise std: 3.70
          Mean value_function loss: 28.8874
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 21.4491
                       Mean reward: 867.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 172.4068
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.89s
                      Time elapsed: 00:17:37
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 104654 steps/s (collection: 0.839s, learning 0.101s)
             Mean action noise std: 3.70
          Mean value_function loss: 38.6615
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.4541
                       Mean reward: 862.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 173.1061
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.94s
                      Time elapsed: 00:17:38
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 106179 steps/s (collection: 0.825s, learning 0.101s)
             Mean action noise std: 3.71
          Mean value_function loss: 42.1099
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.4635
                       Mean reward: 877.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 174.5498
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.93s
                      Time elapsed: 00:17:39
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 108977 steps/s (collection: 0.797s, learning 0.105s)
             Mean action noise std: 3.71
          Mean value_function loss: 44.9914
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 21.4730
                       Mean reward: 872.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.7585
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.90s
                      Time elapsed: 00:17:40
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 108903 steps/s (collection: 0.799s, learning 0.104s)
             Mean action noise std: 3.71
          Mean value_function loss: 48.4508
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.4756
                       Mean reward: 856.97
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.1935
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.90s
                      Time elapsed: 00:17:41
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 107120 steps/s (collection: 0.811s, learning 0.107s)
             Mean action noise std: 3.71
          Mean value_function loss: 34.2346
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 21.4811
                       Mean reward: 841.62
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 173.2007
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.92s
                      Time elapsed: 00:17:42
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 107935 steps/s (collection: 0.802s, learning 0.109s)
             Mean action noise std: 3.72
          Mean value_function loss: 57.9992
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.4891
                       Mean reward: 854.08
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.2909
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.91s
                      Time elapsed: 00:17:43
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 105442 steps/s (collection: 0.823s, learning 0.109s)
             Mean action noise std: 3.73
          Mean value_function loss: 47.4117
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.4998
                       Mean reward: 830.20
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 169.5313
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.93s
                      Time elapsed: 00:17:44
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 105325 steps/s (collection: 0.823s, learning 0.111s)
             Mean action noise std: 3.73
          Mean value_function loss: 51.5813
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 21.5115
                       Mean reward: 851.24
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.8652
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.93s
                      Time elapsed: 00:17:45
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 103880 steps/s (collection: 0.839s, learning 0.108s)
             Mean action noise std: 3.73
          Mean value_function loss: 37.2058
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.5171
                       Mean reward: 875.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 172.5675
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.95s
                      Time elapsed: 00:17:46
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 103628 steps/s (collection: 0.840s, learning 0.109s)
             Mean action noise std: 3.74
          Mean value_function loss: 44.1516
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 21.5216
                       Mean reward: 853.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 171.9373
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.95s
                      Time elapsed: 00:17:47
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 104445 steps/s (collection: 0.827s, learning 0.114s)
             Mean action noise std: 3.74
          Mean value_function loss: 37.1320
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.5259
                       Mean reward: 846.64
               Mean episode length: 249.82
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 167.7474
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.94s
                      Time elapsed: 00:17:47
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 105323 steps/s (collection: 0.820s, learning 0.114s)
             Mean action noise std: 3.74
          Mean value_function loss: 39.1345
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.5295
                       Mean reward: 865.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 168.6997
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.93s
                      Time elapsed: 00:17:48
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 110163 steps/s (collection: 0.790s, learning 0.103s)
             Mean action noise std: 3.74
          Mean value_function loss: 37.6461
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.5310
                       Mean reward: 862.37
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 170.1323
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.89s
                      Time elapsed: 00:17:49
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 106355 steps/s (collection: 0.821s, learning 0.103s)
             Mean action noise std: 3.75
          Mean value_function loss: 37.6959
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.5358
                       Mean reward: 857.44
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.1472
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.92s
                      Time elapsed: 00:17:50
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 104598 steps/s (collection: 0.838s, learning 0.102s)
             Mean action noise std: 3.75
          Mean value_function loss: 34.4448
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.5411
                       Mean reward: 868.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 173.5935
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.94s
                      Time elapsed: 00:17:51
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 103281 steps/s (collection: 0.842s, learning 0.110s)
             Mean action noise std: 3.75
          Mean value_function loss: 28.2833
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.5477
                       Mean reward: 874.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 171.7831
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.95s
                      Time elapsed: 00:17:52
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 112261 steps/s (collection: 0.769s, learning 0.107s)
             Mean action noise std: 3.76
          Mean value_function loss: 29.6514
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.5631
                       Mean reward: 862.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.5425
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.88s
                      Time elapsed: 00:17:53
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 111033 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 3.76
          Mean value_function loss: 34.8176
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.5746
                       Mean reward: 860.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.9054
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.89s
                      Time elapsed: 00:17:54
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 108887 steps/s (collection: 0.802s, learning 0.101s)
             Mean action noise std: 3.77
          Mean value_function loss: 28.2578
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 21.5891
                       Mean reward: 857.17
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 171.8495
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.90s
                      Time elapsed: 00:17:55
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 108053 steps/s (collection: 0.813s, learning 0.097s)
             Mean action noise std: 3.77
          Mean value_function loss: 33.1114
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.5937
                       Mean reward: 856.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.7608
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.91s
                      Time elapsed: 00:17:56
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 109391 steps/s (collection: 0.797s, learning 0.101s)
             Mean action noise std: 3.77
          Mean value_function loss: 25.7181
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.5978
                       Mean reward: 866.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.5488
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.90s
                      Time elapsed: 00:17:57
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 102331 steps/s (collection: 0.834s, learning 0.127s)
             Mean action noise std: 3.78
          Mean value_function loss: 28.4052
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.6095
                       Mean reward: 868.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 170.6866
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.96s
                      Time elapsed: 00:17:58
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 108453 steps/s (collection: 0.785s, learning 0.122s)
             Mean action noise std: 3.80
          Mean value_function loss: 30.8355
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.6348
                       Mean reward: 860.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.4842
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.91s
                      Time elapsed: 00:17:58
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 106810 steps/s (collection: 0.811s, learning 0.109s)
             Mean action noise std: 3.80
          Mean value_function loss: 28.6654
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 21.6535
                       Mean reward: 868.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 172.0079
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.92s
                      Time elapsed: 00:17:59
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 110012 steps/s (collection: 0.775s, learning 0.119s)
             Mean action noise std: 3.81
          Mean value_function loss: 24.9632
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.6623
                       Mean reward: 867.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 171.7649
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.89s
                      Time elapsed: 00:18:00
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 103345 steps/s (collection: 0.832s, learning 0.119s)
             Mean action noise std: 3.81
          Mean value_function loss: 22.0942
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.6709
                       Mean reward: 867.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.1385
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.95s
                      Time elapsed: 00:18:01
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 110445 steps/s (collection: 0.795s, learning 0.095s)
             Mean action noise std: 3.82
          Mean value_function loss: 23.6974
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.6874
                       Mean reward: 869.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 173.9587
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.89s
                      Time elapsed: 00:18:02
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 107710 steps/s (collection: 0.796s, learning 0.117s)
             Mean action noise std: 3.83
          Mean value_function loss: 22.2995
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 21.7016
                       Mean reward: 868.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 172.4284
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.91s
                      Time elapsed: 00:18:03
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 107462 steps/s (collection: 0.795s, learning 0.120s)
             Mean action noise std: 3.83
          Mean value_function loss: 21.7461
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.7129
                       Mean reward: 850.50
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 172.4494
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.91s
                      Time elapsed: 00:18:04
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 109807 steps/s (collection: 0.802s, learning 0.094s)
             Mean action noise std: 3.84
          Mean value_function loss: 22.9771
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.7253
                       Mean reward: 871.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 173.4295
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.90s
                      Time elapsed: 00:18:05
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 107733 steps/s (collection: 0.811s, learning 0.101s)
             Mean action noise std: 3.84
          Mean value_function loss: 26.9875
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.7390
                       Mean reward: 878.34
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 173.6332
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.91s
                      Time elapsed: 00:18:06
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 109252 steps/s (collection: 0.778s, learning 0.122s)
             Mean action noise std: 3.85
          Mean value_function loss: 28.8933
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.7515
                       Mean reward: 877.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.6227
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.90s
                      Time elapsed: 00:18:07
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 110771 steps/s (collection: 0.767s, learning 0.121s)
             Mean action noise std: 3.86
          Mean value_function loss: 25.4147
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.7680
                       Mean reward: 871.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.9400
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.89s
                      Time elapsed: 00:18:08
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 109437 steps/s (collection: 0.780s, learning 0.118s)
             Mean action noise std: 3.87
          Mean value_function loss: 27.5318
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 21.7891
                       Mean reward: 865.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 174.1683
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.90s
                      Time elapsed: 00:18:08
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 109564 steps/s (collection: 0.781s, learning 0.116s)
             Mean action noise std: 3.87
          Mean value_function loss: 22.9070
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.7979
                       Mean reward: 863.04
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 173.2308
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.90s
                      Time elapsed: 00:18:09
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 106808 steps/s (collection: 0.797s, learning 0.124s)
             Mean action noise std: 3.88
          Mean value_function loss: 32.0524
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 21.8082
                       Mean reward: 877.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.6172
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.92s
                      Time elapsed: 00:18:10
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 110509 steps/s (collection: 0.774s, learning 0.115s)
             Mean action noise std: 3.89
          Mean value_function loss: 28.9457
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.8168
                       Mean reward: 866.50
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 173.0622
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.89s
                      Time elapsed: 00:18:11
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 111052 steps/s (collection: 0.775s, learning 0.110s)
             Mean action noise std: 3.89
          Mean value_function loss: 28.3795
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.8328
                       Mean reward: 877.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 174.7025
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.89s
                      Time elapsed: 00:18:12
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 111590 steps/s (collection: 0.775s, learning 0.106s)
             Mean action noise std: 3.90
          Mean value_function loss: 22.2302
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 21.8498
                       Mean reward: 867.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 173.5964
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.88s
                      Time elapsed: 00:18:13
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 108931 steps/s (collection: 0.773s, learning 0.130s)
             Mean action noise std: 3.91
          Mean value_function loss: 25.0714
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.8635
                       Mean reward: 867.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 172.5411
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.90s
                      Time elapsed: 00:18:14
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 112826 steps/s (collection: 0.759s, learning 0.113s)
             Mean action noise std: 3.91
          Mean value_function loss: 26.5164
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.8670
                       Mean reward: 871.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.3603
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.87s
                      Time elapsed: 00:18:15
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 112158 steps/s (collection: 0.756s, learning 0.120s)
             Mean action noise std: 3.91
          Mean value_function loss: 28.2175
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.8742
                       Mean reward: 868.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 172.5746
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.88s
                      Time elapsed: 00:18:16
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 111174 steps/s (collection: 0.767s, learning 0.117s)
             Mean action noise std: 3.92
          Mean value_function loss: 20.9148
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.8851
                       Mean reward: 861.76
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 170.4797
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.88s
                      Time elapsed: 00:18:16
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 114239 steps/s (collection: 0.753s, learning 0.108s)
             Mean action noise std: 3.93
          Mean value_function loss: 22.4559
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.9012
                       Mean reward: 858.18
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.7657
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.86s
                      Time elapsed: 00:18:17
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 112535 steps/s (collection: 0.768s, learning 0.105s)
             Mean action noise std: 3.93
          Mean value_function loss: 26.8130
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.9162
                       Mean reward: 858.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.7619
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.87s
                      Time elapsed: 00:18:18
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 115147 steps/s (collection: 0.749s, learning 0.104s)
             Mean action noise std: 3.93
          Mean value_function loss: 24.4673
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.9223
                       Mean reward: 867.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.0065
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.85s
                      Time elapsed: 00:18:19
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 114866 steps/s (collection: 0.741s, learning 0.115s)
             Mean action noise std: 3.94
          Mean value_function loss: 20.6897
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.9302
                       Mean reward: 865.09
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 173.3349
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.86s
                      Time elapsed: 00:18:20
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 113140 steps/s (collection: 0.766s, learning 0.103s)
             Mean action noise std: 3.95
          Mean value_function loss: 26.1216
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.9441
                       Mean reward: 882.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 174.5253
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.87s
                      Time elapsed: 00:18:21
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 113864 steps/s (collection: 0.751s, learning 0.113s)
             Mean action noise std: 3.96
          Mean value_function loss: 20.6279
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.9607
                       Mean reward: 878.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7860
     Episode_Reward/lifting_object: 174.6559
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.86s
                      Time elapsed: 00:18:22
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 113040 steps/s (collection: 0.759s, learning 0.111s)
             Mean action noise std: 3.96
          Mean value_function loss: 30.7140
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.9786
                       Mean reward: 874.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.3933
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.87s
                      Time elapsed: 00:18:22
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 111878 steps/s (collection: 0.771s, learning 0.107s)
             Mean action noise std: 3.97
          Mean value_function loss: 26.9507
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.9872
                       Mean reward: 881.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.7792
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.88s
                      Time elapsed: 00:18:23
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 109905 steps/s (collection: 0.794s, learning 0.100s)
             Mean action noise std: 3.97
          Mean value_function loss: 29.4972
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.9959
                       Mean reward: 867.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.9294
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.89s
                      Time elapsed: 00:18:24
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 101849 steps/s (collection: 0.845s, learning 0.121s)
             Mean action noise std: 3.98
          Mean value_function loss: 26.3260
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.0106
                       Mean reward: 866.19
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 173.2471
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.97s
                      Time elapsed: 00:18:25
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 112656 steps/s (collection: 0.775s, learning 0.098s)
             Mean action noise std: 3.99
          Mean value_function loss: 29.9222
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.0262
                       Mean reward: 876.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 174.4689
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.87s
                      Time elapsed: 00:18:26
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 113122 steps/s (collection: 0.754s, learning 0.115s)
             Mean action noise std: 3.99
          Mean value_function loss: 34.9359
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.0334
                       Mean reward: 874.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 172.6310
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.87s
                      Time elapsed: 00:18:27
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 113604 steps/s (collection: 0.750s, learning 0.115s)
             Mean action noise std: 4.00
          Mean value_function loss: 33.6030
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.0407
                       Mean reward: 874.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.9632
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.87s
                      Time elapsed: 00:18:28
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 111959 steps/s (collection: 0.763s, learning 0.115s)
             Mean action noise std: 4.00
          Mean value_function loss: 35.3703
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 22.0562
                       Mean reward: 874.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 171.8797
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.88s
                      Time elapsed: 00:18:29
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 110601 steps/s (collection: 0.784s, learning 0.105s)
             Mean action noise std: 4.01
          Mean value_function loss: 37.5000
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 22.0712
                       Mean reward: 880.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.1113
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.89s
                      Time elapsed: 00:18:30
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 117048 steps/s (collection: 0.745s, learning 0.095s)
             Mean action noise std: 4.01
          Mean value_function loss: 28.5325
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.0820
                       Mean reward: 879.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 173.5227
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.84s
                      Time elapsed: 00:18:30
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 115128 steps/s (collection: 0.763s, learning 0.091s)
             Mean action noise std: 4.02
          Mean value_function loss: 41.6793
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 22.0930
                       Mean reward: 874.69
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.9722
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.85s
                      Time elapsed: 00:18:31
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 114730 steps/s (collection: 0.751s, learning 0.106s)
             Mean action noise std: 4.03
          Mean value_function loss: 35.3034
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.1051
                       Mean reward: 852.58
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 171.2702
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.86s
                      Time elapsed: 00:18:32
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 113602 steps/s (collection: 0.749s, learning 0.116s)
             Mean action noise std: 4.03
          Mean value_function loss: 24.6734
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.1190
                       Mean reward: 875.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.5879
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.87s
                      Time elapsed: 00:18:33
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 113344 steps/s (collection: 0.756s, learning 0.112s)
             Mean action noise std: 4.03
          Mean value_function loss: 41.1616
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.1223
                       Mean reward: 886.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 174.0253
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.87s
                      Time elapsed: 00:18:34
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 109704 steps/s (collection: 0.779s, learning 0.118s)
             Mean action noise std: 4.04
          Mean value_function loss: 27.2368
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.1305
                       Mean reward: 850.26
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.4225
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.90s
                      Time elapsed: 00:18:35
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 109048 steps/s (collection: 0.777s, learning 0.125s)
             Mean action noise std: 4.04
          Mean value_function loss: 30.5550
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.1385
                       Mean reward: 851.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.6487
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.90s
                      Time elapsed: 00:18:36
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 115128 steps/s (collection: 0.749s, learning 0.105s)
             Mean action noise std: 4.05
          Mean value_function loss: 35.1110
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.1528
                       Mean reward: 854.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 171.2711
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.85s
                      Time elapsed: 00:18:37
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 116338 steps/s (collection: 0.741s, learning 0.104s)
             Mean action noise std: 4.06
          Mean value_function loss: 35.4333
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.1647
                       Mean reward: 858.15
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.1477
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.84s
                      Time elapsed: 00:18:37
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 109666 steps/s (collection: 0.773s, learning 0.124s)
             Mean action noise std: 4.06
          Mean value_function loss: 29.7107
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.1685
                       Mean reward: 852.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.2945
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.90s
                      Time elapsed: 00:18:38
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 110237 steps/s (collection: 0.785s, learning 0.107s)
             Mean action noise std: 4.07
          Mean value_function loss: 28.5608
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.1797
                       Mean reward: 856.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.6689
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.89s
                      Time elapsed: 00:18:39
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 112303 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 4.07
          Mean value_function loss: 33.8931
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 22.1955
                       Mean reward: 872.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 173.5647
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.88s
                      Time elapsed: 00:18:40
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 116328 steps/s (collection: 0.746s, learning 0.099s)
             Mean action noise std: 4.08
          Mean value_function loss: 27.6245
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.2034
                       Mean reward: 866.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 172.7556
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.85s
                      Time elapsed: 00:18:41
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 111352 steps/s (collection: 0.777s, learning 0.106s)
             Mean action noise std: 4.08
          Mean value_function loss: 21.7335
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.2103
                       Mean reward: 876.43
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 173.0107
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.88s
                      Time elapsed: 00:18:42
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 112156 steps/s (collection: 0.774s, learning 0.103s)
             Mean action noise std: 4.09
          Mean value_function loss: 47.6171
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.2286
                       Mean reward: 871.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.6377
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.88s
                      Time elapsed: 00:18:43
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 115074 steps/s (collection: 0.747s, learning 0.107s)
             Mean action noise std: 4.10
          Mean value_function loss: 27.8356
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.2442
                       Mean reward: 878.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 170.5891
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.85s
                      Time elapsed: 00:18:43
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 114584 steps/s (collection: 0.745s, learning 0.113s)
             Mean action noise std: 4.10
          Mean value_function loss: 34.3671
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.2520
                       Mean reward: 846.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 170.6806
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.86s
                      Time elapsed: 00:18:44
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 113877 steps/s (collection: 0.765s, learning 0.098s)
             Mean action noise std: 4.11
          Mean value_function loss: 30.6003
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.2592
                       Mean reward: 857.59
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 172.1185
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.86s
                      Time elapsed: 00:18:45
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 112179 steps/s (collection: 0.765s, learning 0.112s)
             Mean action noise std: 4.11
          Mean value_function loss: 30.1045
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.2699
                       Mean reward: 868.44
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 172.0585
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.88s
                      Time elapsed: 00:18:46
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 110744 steps/s (collection: 0.783s, learning 0.105s)
             Mean action noise std: 4.12
          Mean value_function loss: 30.1676
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.2812
                       Mean reward: 869.88
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.6787
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.89s
                      Time elapsed: 00:18:47
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 111664 steps/s (collection: 0.773s, learning 0.107s)
             Mean action noise std: 4.12
          Mean value_function loss: 25.7300
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.2941
                       Mean reward: 865.37
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.0578
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.88s
                      Time elapsed: 00:18:48
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 107846 steps/s (collection: 0.799s, learning 0.113s)
             Mean action noise std: 4.13
          Mean value_function loss: 38.5031
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.3049
                       Mean reward: 854.98
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 173.2250
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.91s
                      Time elapsed: 00:18:49
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 113805 steps/s (collection: 0.769s, learning 0.095s)
             Mean action noise std: 4.14
          Mean value_function loss: 40.5761
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.3154
                       Mean reward: 873.35
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 171.6814
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.86s
                      Time elapsed: 00:18:50
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 111180 steps/s (collection: 0.774s, learning 0.111s)
             Mean action noise std: 4.14
          Mean value_function loss: 35.3094
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 22.3238
                       Mean reward: 880.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.8369
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.88s
                      Time elapsed: 00:18:50
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 111376 steps/s (collection: 0.773s, learning 0.110s)
             Mean action noise std: 4.14
          Mean value_function loss: 39.6720
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.3251
                       Mean reward: 863.74
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 173.3508
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.88s
                      Time elapsed: 00:18:51
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 112962 steps/s (collection: 0.762s, learning 0.109s)
             Mean action noise std: 4.14
          Mean value_function loss: 39.4170
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.3287
                       Mean reward: 877.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.2133
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.87s
                      Time elapsed: 00:18:52
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 110987 steps/s (collection: 0.786s, learning 0.100s)
             Mean action noise std: 4.15
          Mean value_function loss: 36.0063
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.3361
                       Mean reward: 860.86
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 173.9262
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.89s
                      Time elapsed: 00:18:53
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 115193 steps/s (collection: 0.759s, learning 0.094s)
             Mean action noise std: 4.15
          Mean value_function loss: 30.3682
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.3419
                       Mean reward: 851.33
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.2707
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.85s
                      Time elapsed: 00:18:54
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 111977 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 4.15
          Mean value_function loss: 29.6314
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.3458
                       Mean reward: 874.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.9151
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.88s
                      Time elapsed: 00:18:55
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 112966 steps/s (collection: 0.754s, learning 0.117s)
             Mean action noise std: 4.16
          Mean value_function loss: 19.8423
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.3524
                       Mean reward: 858.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.0455
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.87s
                      Time elapsed: 00:18:56
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 115000 steps/s (collection: 0.743s, learning 0.112s)
             Mean action noise std: 4.17
          Mean value_function loss: 24.2869
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 22.3660
                       Mean reward: 867.75
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.9222
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.85s
                      Time elapsed: 00:18:57
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 110631 steps/s (collection: 0.767s, learning 0.122s)
             Mean action noise std: 4.17
          Mean value_function loss: 30.2526
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.3731
                       Mean reward: 861.12
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 169.3640
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.89s
                      Time elapsed: 00:18:57
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 113546 steps/s (collection: 0.751s, learning 0.115s)
             Mean action noise std: 4.18
          Mean value_function loss: 25.8595
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.3828
                       Mean reward: 859.72
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.4537
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.87s
                      Time elapsed: 00:18:58
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 114242 steps/s (collection: 0.750s, learning 0.111s)
             Mean action noise std: 4.18
          Mean value_function loss: 25.7181
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.3928
                       Mean reward: 862.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.3288
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.86s
                      Time elapsed: 00:18:59
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 111223 steps/s (collection: 0.780s, learning 0.104s)
             Mean action noise std: 4.19
          Mean value_function loss: 29.4834
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.4023
                       Mean reward: 873.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.0417
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.88s
                      Time elapsed: 00:19:00
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 115030 steps/s (collection: 0.753s, learning 0.102s)
             Mean action noise std: 4.20
          Mean value_function loss: 32.7320
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.4181
                       Mean reward: 873.81
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 174.2619
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.85s
                      Time elapsed: 00:19:01
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 106339 steps/s (collection: 0.797s, learning 0.128s)
             Mean action noise std: 4.20
          Mean value_function loss: 36.2884
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 22.4351
                       Mean reward: 854.83
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 171.7100
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.92s
                      Time elapsed: 00:19:02
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 104801 steps/s (collection: 0.818s, learning 0.120s)
             Mean action noise std: 4.21
          Mean value_function loss: 30.3444
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.4407
                       Mean reward: 856.55
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 173.5119
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.94s
                      Time elapsed: 00:19:03
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 107801 steps/s (collection: 0.800s, learning 0.112s)
             Mean action noise std: 4.21
          Mean value_function loss: 34.5169
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.4443
                       Mean reward: 862.89
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 172.0439
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.91s
                      Time elapsed: 00:19:04
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 107849 steps/s (collection: 0.793s, learning 0.118s)
             Mean action noise std: 4.21
          Mean value_function loss: 27.7852
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.4519
                       Mean reward: 876.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 173.1004
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.91s
                      Time elapsed: 00:19:05
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 112933 steps/s (collection: 0.758s, learning 0.112s)
             Mean action noise std: 4.22
          Mean value_function loss: 21.3531
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.4629
                       Mean reward: 860.79
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 172.6192
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.87s
                      Time elapsed: 00:19:05
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 110366 steps/s (collection: 0.767s, learning 0.124s)
             Mean action noise std: 4.22
          Mean value_function loss: 31.8227
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 22.4748
                       Mean reward: 867.92
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.1733
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.89s
                      Time elapsed: 00:19:06
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 115822 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 4.23
          Mean value_function loss: 23.9919
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.4785
                       Mean reward: 849.16
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 170.1383
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.85s
                      Time elapsed: 00:19:07
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 105232 steps/s (collection: 0.813s, learning 0.121s)
             Mean action noise std: 4.23
          Mean value_function loss: 30.4452
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.4840
                       Mean reward: 862.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.5247
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.93s
                      Time elapsed: 00:19:08
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 113727 steps/s (collection: 0.747s, learning 0.118s)
             Mean action noise std: 4.24
          Mean value_function loss: 25.4453
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4972
                       Mean reward: 869.56
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 172.2381
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.86s
                      Time elapsed: 00:19:09
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 113477 steps/s (collection: 0.759s, learning 0.107s)
             Mean action noise std: 4.24
          Mean value_function loss: 26.0591
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 22.5116
                       Mean reward: 863.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 173.2195
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.87s
                      Time elapsed: 00:19:10
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 111308 steps/s (collection: 0.769s, learning 0.114s)
             Mean action noise std: 4.25
          Mean value_function loss: 23.8776
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.5212
                       Mean reward: 844.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.0551
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.88s
                      Time elapsed: 00:19:11
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 109691 steps/s (collection: 0.788s, learning 0.109s)
             Mean action noise std: 4.26
          Mean value_function loss: 22.6477
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.5326
                       Mean reward: 873.00
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 171.8129
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.90s
                      Time elapsed: 00:19:12
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 112809 steps/s (collection: 0.761s, learning 0.111s)
             Mean action noise std: 4.26
          Mean value_function loss: 28.3902
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.5439
                       Mean reward: 860.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 172.0294
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.87s
                      Time elapsed: 00:19:13
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 113860 steps/s (collection: 0.749s, learning 0.115s)
             Mean action noise std: 4.27
          Mean value_function loss: 29.6136
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.5551
                       Mean reward: 870.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.9809
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.86s
                      Time elapsed: 00:19:13
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 115332 steps/s (collection: 0.750s, learning 0.103s)
             Mean action noise std: 4.27
          Mean value_function loss: 28.6998
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.5646
                       Mean reward: 876.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.3965
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.85s
                      Time elapsed: 00:19:14
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 112677 steps/s (collection: 0.766s, learning 0.106s)
             Mean action noise std: 4.28
          Mean value_function loss: 31.1199
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 22.5776
                       Mean reward: 877.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 173.9731
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.87s
                      Time elapsed: 00:19:15
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 114154 steps/s (collection: 0.755s, learning 0.106s)
             Mean action noise std: 4.29
          Mean value_function loss: 35.4235
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.5857
                       Mean reward: 874.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 171.9694
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.86s
                      Time elapsed: 00:19:16
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 107800 steps/s (collection: 0.800s, learning 0.112s)
             Mean action noise std: 4.29
          Mean value_function loss: 25.6418
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.5981
                       Mean reward: 869.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.8413
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.91s
                      Time elapsed: 00:19:17
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 110912 steps/s (collection: 0.777s, learning 0.109s)
             Mean action noise std: 4.30
          Mean value_function loss: 23.5905
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.6041
                       Mean reward: 871.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 173.3318
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.89s
                      Time elapsed: 00:19:18
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 113062 steps/s (collection: 0.763s, learning 0.106s)
             Mean action noise std: 4.30
          Mean value_function loss: 31.2164
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.6108
                       Mean reward: 880.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 174.3939
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.87s
                      Time elapsed: 00:19:19
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 112541 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 4.30
          Mean value_function loss: 32.3753
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.6157
                       Mean reward: 868.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 172.8296
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.87s
                      Time elapsed: 00:19:20
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 113341 steps/s (collection: 0.765s, learning 0.103s)
             Mean action noise std: 4.31
          Mean value_function loss: 34.4730
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.6289
                       Mean reward: 868.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 172.1985
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.87s
                      Time elapsed: 00:19:20
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 111970 steps/s (collection: 0.757s, learning 0.121s)
             Mean action noise std: 4.33
          Mean value_function loss: 29.2450
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.6495
                       Mean reward: 872.00
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 172.9416
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.88s
                      Time elapsed: 00:19:21
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 115071 steps/s (collection: 0.747s, learning 0.107s)
             Mean action noise std: 4.33
          Mean value_function loss: 34.5269
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 22.6665
                       Mean reward: 864.68
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 173.2332
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.85s
                      Time elapsed: 00:19:22
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 113412 steps/s (collection: 0.762s, learning 0.105s)
             Mean action noise std: 4.33
          Mean value_function loss: 34.4702
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.6720
                       Mean reward: 881.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 172.9717
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.87s
                      Time elapsed: 00:19:23
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 113135 steps/s (collection: 0.759s, learning 0.110s)
             Mean action noise std: 4.34
          Mean value_function loss: 32.8393
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 22.6796
                       Mean reward: 856.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.6895
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.87s
                      Time elapsed: 00:19:24
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 114663 steps/s (collection: 0.751s, learning 0.106s)
             Mean action noise std: 4.34
          Mean value_function loss: 29.2078
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.6876
                       Mean reward: 855.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.7485
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.86s
                      Time elapsed: 00:19:25
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 110952 steps/s (collection: 0.771s, learning 0.115s)
             Mean action noise std: 4.35
          Mean value_function loss: 34.5946
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.6992
                       Mean reward: 865.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.9498
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.89s
                      Time elapsed: 00:19:26
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 110155 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 4.35
          Mean value_function loss: 33.6625
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.7095
                       Mean reward: 867.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.8946
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.89s
                      Time elapsed: 00:19:27
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 106909 steps/s (collection: 0.792s, learning 0.127s)
             Mean action noise std: 4.36
          Mean value_function loss: 32.5667
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.7187
                       Mean reward: 869.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 172.1451
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.92s
                      Time elapsed: 00:19:27
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 106225 steps/s (collection: 0.803s, learning 0.122s)
             Mean action noise std: 4.36
          Mean value_function loss: 35.1756
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.7239
                       Mean reward: 856.04
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.7025
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.93s
                      Time elapsed: 00:19:28
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 116549 steps/s (collection: 0.735s, learning 0.109s)
             Mean action noise std: 4.36
          Mean value_function loss: 27.3520
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.7300
                       Mean reward: 871.67
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.2110
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.84s
                      Time elapsed: 00:19:29
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 114190 steps/s (collection: 0.763s, learning 0.098s)
             Mean action noise std: 4.37
          Mean value_function loss: 33.4854
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.7360
                       Mean reward: 845.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 171.5806
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.86s
                      Time elapsed: 00:19:30
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 114725 steps/s (collection: 0.751s, learning 0.106s)
             Mean action noise std: 4.37
          Mean value_function loss: 33.3328
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.7445
                       Mean reward: 840.80
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 171.9277
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.86s
                      Time elapsed: 00:19:31
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 113795 steps/s (collection: 0.758s, learning 0.106s)
             Mean action noise std: 4.38
          Mean value_function loss: 33.6756
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.7563
                       Mean reward: 859.84
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 172.9034
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.86s
                      Time elapsed: 00:19:32
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 114719 steps/s (collection: 0.753s, learning 0.104s)
             Mean action noise std: 4.38
          Mean value_function loss: 27.5811
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.7671
                       Mean reward: 869.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 170.9659
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.86s
                      Time elapsed: 00:19:33
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 115123 steps/s (collection: 0.738s, learning 0.116s)
             Mean action noise std: 4.39
          Mean value_function loss: 29.0998
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.7705
                       Mean reward: 857.38
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 170.5306
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.85s
                      Time elapsed: 00:19:33
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 114060 steps/s (collection: 0.746s, learning 0.116s)
             Mean action noise std: 4.39
          Mean value_function loss: 36.2022
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.7733
                       Mean reward: 877.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 171.9217
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.86s
                      Time elapsed: 00:19:34
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 111218 steps/s (collection: 0.776s, learning 0.108s)
             Mean action noise std: 4.39
          Mean value_function loss: 34.1595
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.7774
                       Mean reward: 859.30
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 171.2200
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.88s
                      Time elapsed: 00:19:35
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 112299 steps/s (collection: 0.775s, learning 0.100s)
             Mean action noise std: 4.40
          Mean value_function loss: 32.8785
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 22.7863
                       Mean reward: 870.82
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.9932
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.88s
                      Time elapsed: 00:19:36
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 111189 steps/s (collection: 0.772s, learning 0.112s)
             Mean action noise std: 4.40
          Mean value_function loss: 31.5000
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.7939
                       Mean reward: 867.21
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 173.6514
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.88s
                      Time elapsed: 00:19:37
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 119977 steps/s (collection: 0.731s, learning 0.088s)
             Mean action noise std: 4.40
          Mean value_function loss: 31.2051
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 22.8025
                       Mean reward: 864.22
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 170.9531
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.82s
                      Time elapsed: 00:19:38
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 112423 steps/s (collection: 0.774s, learning 0.101s)
             Mean action noise std: 4.41
          Mean value_function loss: 29.1247
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.8062
                       Mean reward: 859.32
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 171.3005
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.87s
                      Time elapsed: 00:19:39
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 113591 steps/s (collection: 0.751s, learning 0.114s)
             Mean action noise std: 4.41
          Mean value_function loss: 28.3221
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.8129
                       Mean reward: 853.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 170.5662
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.87s
                      Time elapsed: 00:19:40
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 112748 steps/s (collection: 0.751s, learning 0.121s)
             Mean action noise std: 4.42
          Mean value_function loss: 26.2277
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.8166
                       Mean reward: 860.62
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.3858
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.87s
                      Time elapsed: 00:19:40
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 112745 steps/s (collection: 0.759s, learning 0.113s)
             Mean action noise std: 4.42
          Mean value_function loss: 18.2526
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.8252
                       Mean reward: 880.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.9806
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.87s
                      Time elapsed: 00:19:41
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 115800 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 4.43
          Mean value_function loss: 26.3252
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.8340
                       Mean reward: 871.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.5609
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.85s
                      Time elapsed: 00:19:42
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 110648 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 4.43
          Mean value_function loss: 24.1184
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.8425
                       Mean reward: 864.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 173.2865
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.89s
                      Time elapsed: 00:19:43
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 116646 steps/s (collection: 0.742s, learning 0.101s)
             Mean action noise std: 4.44
          Mean value_function loss: 26.4480
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.8549
                       Mean reward: 861.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 173.0453
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.84s
                      Time elapsed: 00:19:44
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 114597 steps/s (collection: 0.750s, learning 0.108s)
             Mean action noise std: 4.45
          Mean value_function loss: 25.3520
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.8637
                       Mean reward: 874.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 173.5216
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.86s
                      Time elapsed: 00:19:45
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 115035 steps/s (collection: 0.751s, learning 0.103s)
             Mean action noise std: 4.45
          Mean value_function loss: 25.0625
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.8718
                       Mean reward: 876.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.1530
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.85s
                      Time elapsed: 00:19:46
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 113607 steps/s (collection: 0.749s, learning 0.117s)
             Mean action noise std: 4.45
          Mean value_function loss: 33.5885
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.8770
                       Mean reward: 884.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.5374
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.87s
                      Time elapsed: 00:19:46
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 111650 steps/s (collection: 0.762s, learning 0.119s)
             Mean action noise std: 4.46
          Mean value_function loss: 29.2048
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.8897
                       Mean reward: 867.89
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 171.8629
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.88s
                      Time elapsed: 00:19:47
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 110894 steps/s (collection: 0.773s, learning 0.114s)
             Mean action noise std: 4.46
          Mean value_function loss: 30.3503
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 22.9037
                       Mean reward: 870.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 173.9411
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.89s
                      Time elapsed: 00:19:48
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 113241 steps/s (collection: 0.763s, learning 0.106s)
             Mean action noise std: 4.47
          Mean value_function loss: 26.0036
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 22.9094
                       Mean reward: 873.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 172.3741
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.87s
                      Time elapsed: 00:19:49
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 111637 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 4.47
          Mean value_function loss: 33.2239
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.9195
                       Mean reward: 877.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 173.2672
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.88s
                      Time elapsed: 00:19:50
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 112624 steps/s (collection: 0.746s, learning 0.127s)
             Mean action noise std: 4.48
          Mean value_function loss: 33.7558
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.9285
                       Mean reward: 864.77
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 173.5838
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.87s
                      Time elapsed: 00:19:51
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 110285 steps/s (collection: 0.781s, learning 0.111s)
             Mean action noise std: 4.48
          Mean value_function loss: 27.9114
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.9342
                       Mean reward: 868.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 173.6691
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.89s
                      Time elapsed: 00:19:52
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 113348 steps/s (collection: 0.763s, learning 0.105s)
             Mean action noise std: 4.49
          Mean value_function loss: 36.3839
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.9458
                       Mean reward: 877.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.1140
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.87s
                      Time elapsed: 00:19:53
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 111537 steps/s (collection: 0.764s, learning 0.117s)
             Mean action noise std: 4.49
          Mean value_function loss: 31.4328
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.9574
                       Mean reward: 858.40
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 171.8178
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.88s
                      Time elapsed: 00:19:53
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 95208 steps/s (collection: 0.863s, learning 0.170s)
             Mean action noise std: 4.51
          Mean value_function loss: 24.3263
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.9734
                       Mean reward: 856.72
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 171.9866
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.03s
                      Time elapsed: 00:19:55
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 96913 steps/s (collection: 0.817s, learning 0.197s)
             Mean action noise std: 4.51
          Mean value_function loss: 31.9036
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 22.9895
                       Mean reward: 871.78
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 171.8536
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.01s
                      Time elapsed: 00:19:56
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 105466 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 4.51
          Mean value_function loss: 27.8662
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.9948
                       Mean reward: 852.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 171.5744
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.93s
                      Time elapsed: 00:19:56
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 99633 steps/s (collection: 0.838s, learning 0.149s)
             Mean action noise std: 4.52
          Mean value_function loss: 25.3765
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.0055
                       Mean reward: 867.33
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 173.7507
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.99s
                      Time elapsed: 00:19:57
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 112461 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 4.53
          Mean value_function loss: 27.6852
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.0190
                       Mean reward: 852.86
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 171.8003
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.87s
                      Time elapsed: 00:19:58
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 114680 steps/s (collection: 0.756s, learning 0.101s)
             Mean action noise std: 4.53
          Mean value_function loss: 29.3983
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.0262
                       Mean reward: 871.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 171.9207
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.86s
                      Time elapsed: 00:19:59
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 113573 steps/s (collection: 0.764s, learning 0.102s)
             Mean action noise std: 4.53
          Mean value_function loss: 29.5450
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.0299
                       Mean reward: 880.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 171.6679
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.87s
                      Time elapsed: 00:20:00
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 109801 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 4.54
          Mean value_function loss: 25.0406
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.0391
                       Mean reward: 871.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 172.1447
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.90s
                      Time elapsed: 00:20:01
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 115244 steps/s (collection: 0.748s, learning 0.105s)
             Mean action noise std: 4.55
          Mean value_function loss: 30.4844
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.0504
                       Mean reward: 850.41
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.0265
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.85s
                      Time elapsed: 00:20:02
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 112579 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 4.56
          Mean value_function loss: 34.7762
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.0684
                       Mean reward: 877.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 174.1602
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.87s
                      Time elapsed: 00:20:03
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 110484 steps/s (collection: 0.761s, learning 0.128s)
             Mean action noise std: 4.57
          Mean value_function loss: 37.3939
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.0837
                       Mean reward: 874.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 172.7484
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.89s
                      Time elapsed: 00:20:04
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 104349 steps/s (collection: 0.785s, learning 0.157s)
             Mean action noise std: 4.57
          Mean value_function loss: 38.6297
               Mean surrogate loss: 0.0151
                 Mean entropy loss: 23.1028
                       Mean reward: 871.73
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 172.7411
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.94s
                      Time elapsed: 00:20:05
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 107436 steps/s (collection: 0.772s, learning 0.143s)
             Mean action noise std: 4.58
          Mean value_function loss: 31.4279
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.1053
                       Mean reward: 873.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 172.6575
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.91s
                      Time elapsed: 00:20:05
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 111081 steps/s (collection: 0.769s, learning 0.116s)
             Mean action noise std: 4.58
          Mean value_function loss: 28.4559
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 23.1113
                       Mean reward: 843.82
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 171.3767
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.88s
                      Time elapsed: 00:20:06
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 108917 steps/s (collection: 0.794s, learning 0.109s)
             Mean action noise std: 4.59
          Mean value_function loss: 26.1967
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.1210
                       Mean reward: 875.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: 173.5981
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.90s
                      Time elapsed: 00:20:07
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 112608 steps/s (collection: 0.776s, learning 0.097s)
             Mean action noise std: 4.59
          Mean value_function loss: 29.3035
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.1300
                       Mean reward: 874.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7824
     Episode_Reward/lifting_object: 172.2606
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.87s
                      Time elapsed: 00:20:08
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 113787 steps/s (collection: 0.773s, learning 0.091s)
             Mean action noise std: 4.59
          Mean value_function loss: 33.7290
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.1399
                       Mean reward: 881.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 174.1528
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.86s
                      Time elapsed: 00:20:09
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 108175 steps/s (collection: 0.812s, learning 0.097s)
             Mean action noise std: 4.60
          Mean value_function loss: 35.5071
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 23.1427
                       Mean reward: 870.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 169.8855
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.91s
                      Time elapsed: 00:20:10
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 114500 steps/s (collection: 0.767s, learning 0.092s)
             Mean action noise std: 4.60
          Mean value_function loss: 33.3544
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.1508
                       Mean reward: 855.07
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.1006
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.86s
                      Time elapsed: 00:20:11
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 111708 steps/s (collection: 0.783s, learning 0.097s)
             Mean action noise std: 4.61
          Mean value_function loss: 31.1707
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 23.1623
                       Mean reward: 866.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.0952
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.88s
                      Time elapsed: 00:20:12
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 108752 steps/s (collection: 0.760s, learning 0.144s)
             Mean action noise std: 4.61
          Mean value_function loss: 32.0781
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.1651
                       Mean reward: 861.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.7117
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.90s
                      Time elapsed: 00:20:12
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 112243 steps/s (collection: 0.784s, learning 0.092s)
             Mean action noise std: 4.61
          Mean value_function loss: 32.3601
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.1667
                       Mean reward: 866.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 171.7586
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.88s
                      Time elapsed: 00:20:13
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 113189 steps/s (collection: 0.748s, learning 0.120s)
             Mean action noise std: 4.62
          Mean value_function loss: 32.2804
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.1729
                       Mean reward: 847.06
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.3012
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.87s
                      Time elapsed: 00:20:14
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 112444 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 4.62
          Mean value_function loss: 27.6563
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.1838
                       Mean reward: 862.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.6241
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.87s
                      Time elapsed: 00:20:15
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 108907 steps/s (collection: 0.788s, learning 0.114s)
             Mean action noise std: 4.63
          Mean value_function loss: 32.9706
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 23.1965
                       Mean reward: 854.79
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.3405
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.90s
                      Time elapsed: 00:20:16
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 106670 steps/s (collection: 0.813s, learning 0.109s)
             Mean action noise std: 4.63
          Mean value_function loss: 30.5260
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 23.2014
                       Mean reward: 876.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.2290
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.92s
                      Time elapsed: 00:20:17
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 107987 steps/s (collection: 0.796s, learning 0.114s)
             Mean action noise std: 4.64
          Mean value_function loss: 30.7372
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.2071
                       Mean reward: 874.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.6310
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.91s
                      Time elapsed: 00:20:18
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 110165 steps/s (collection: 0.787s, learning 0.105s)
             Mean action noise std: 4.64
          Mean value_function loss: 29.8471
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 23.2199
                       Mean reward: 861.51
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.5171
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.89s
                      Time elapsed: 00:20:19
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 111327 steps/s (collection: 0.776s, learning 0.107s)
             Mean action noise std: 4.65
          Mean value_function loss: 34.4415
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.2307
                       Mean reward: 881.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.8492
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.88s
                      Time elapsed: 00:20:20
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 108261 steps/s (collection: 0.770s, learning 0.138s)
             Mean action noise std: 4.66
          Mean value_function loss: 43.3054
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.2404
                       Mean reward: 868.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.7601
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.91s
                      Time elapsed: 00:20:21
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 109867 steps/s (collection: 0.759s, learning 0.136s)
             Mean action noise std: 4.66
          Mean value_function loss: 42.5170
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.2479
                       Mean reward: 877.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 174.5187
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.89s
                      Time elapsed: 00:20:21
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 100287 steps/s (collection: 0.833s, learning 0.148s)
             Mean action noise std: 4.67
          Mean value_function loss: 38.8660
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.2619
                       Mean reward: 866.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 170.3001
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.98s
                      Time elapsed: 00:20:22
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 113420 steps/s (collection: 0.753s, learning 0.114s)
             Mean action noise std: 4.68
          Mean value_function loss: 29.9570
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 23.2734
                       Mean reward: 869.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.4801
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.87s
                      Time elapsed: 00:20:23
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 107505 steps/s (collection: 0.819s, learning 0.096s)
             Mean action noise std: 4.68
          Mean value_function loss: 42.2214
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.2838
                       Mean reward: 854.87
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.7211
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.91s
                      Time elapsed: 00:20:24
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 112534 steps/s (collection: 0.787s, learning 0.087s)
             Mean action noise std: 4.69
          Mean value_function loss: 24.4795
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.2967
                       Mean reward: 875.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.1716
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.87s
                      Time elapsed: 00:20:25
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 106165 steps/s (collection: 0.810s, learning 0.116s)
             Mean action noise std: 4.70
          Mean value_function loss: 28.0430
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.3162
                       Mean reward: 873.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 170.5895
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.93s
                      Time elapsed: 00:20:26
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 101190 steps/s (collection: 0.826s, learning 0.145s)
             Mean action noise std: 4.71
          Mean value_function loss: 28.6624
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.3342
                       Mean reward: 867.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 173.1999
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.97s
                      Time elapsed: 00:20:27
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 103349 steps/s (collection: 0.844s, learning 0.108s)
             Mean action noise std: 4.71
          Mean value_function loss: 28.6583
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.3450
                       Mean reward: 880.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.3700
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.95s
                      Time elapsed: 00:20:28
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 107892 steps/s (collection: 0.810s, learning 0.101s)
             Mean action noise std: 4.72
          Mean value_function loss: 26.3847
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.3608
                       Mean reward: 872.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.6346
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.91s
                      Time elapsed: 00:20:29
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 115293 steps/s (collection: 0.759s, learning 0.094s)
             Mean action noise std: 4.73
          Mean value_function loss: 35.7996
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.3729
                       Mean reward: 872.77
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 172.6599
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.85s
                      Time elapsed: 00:20:30
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 107250 steps/s (collection: 0.800s, learning 0.117s)
             Mean action noise std: 4.73
          Mean value_function loss: 34.1136
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.3836
                       Mean reward: 880.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.7187
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.92s
                      Time elapsed: 00:20:31
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 111967 steps/s (collection: 0.770s, learning 0.108s)
             Mean action noise std: 4.74
          Mean value_function loss: 30.1743
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.3944
                       Mean reward: 853.78
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.0064
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.88s
                      Time elapsed: 00:20:31
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 111158 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 4.74
          Mean value_function loss: 32.9336
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 23.4025
                       Mean reward: 854.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.8022
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.88s
                      Time elapsed: 00:20:32
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 111354 steps/s (collection: 0.794s, learning 0.089s)
             Mean action noise std: 4.75
          Mean value_function loss: 36.3981
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.4102
                       Mean reward: 871.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 173.6479
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.88s
                      Time elapsed: 00:20:33
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 104586 steps/s (collection: 0.826s, learning 0.114s)
             Mean action noise std: 4.75
          Mean value_function loss: 31.1447
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 23.4177
                       Mean reward: 856.61
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.2570
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.94s
                      Time elapsed: 00:20:34
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 107346 steps/s (collection: 0.810s, learning 0.106s)
             Mean action noise std: 4.76
          Mean value_function loss: 23.4039
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.4265
                       Mean reward: 861.68
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.7693
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.92s
                      Time elapsed: 00:20:35
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 103837 steps/s (collection: 0.839s, learning 0.108s)
             Mean action noise std: 4.77
          Mean value_function loss: 28.2879
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.4372
                       Mean reward: 858.63
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 172.2484
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.95s
                      Time elapsed: 00:20:36
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 109904 steps/s (collection: 0.780s, learning 0.114s)
             Mean action noise std: 4.77
          Mean value_function loss: 32.3250
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.4538
                       Mean reward: 877.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.6314
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.89s
                      Time elapsed: 00:20:37
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 110801 steps/s (collection: 0.788s, learning 0.099s)
             Mean action noise std: 4.78
          Mean value_function loss: 29.9026
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 23.4682
                       Mean reward: 881.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.3269
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.89s
                      Time elapsed: 00:20:38
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 110091 steps/s (collection: 0.803s, learning 0.090s)
             Mean action noise std: 4.78
          Mean value_function loss: 34.5788
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.4738
                       Mean reward: 871.72
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 172.7289
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.89s
                      Time elapsed: 00:20:39
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 105878 steps/s (collection: 0.798s, learning 0.130s)
             Mean action noise std: 4.79
          Mean value_function loss: 23.1894
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.4852
                       Mean reward: 866.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 171.5367
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.93s
                      Time elapsed: 00:20:40
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 112990 steps/s (collection: 0.761s, learning 0.109s)
             Mean action noise std: 4.80
          Mean value_function loss: 26.7338
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 23.5000
                       Mean reward: 872.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 173.8082
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.87s
                      Time elapsed: 00:20:41
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 102672 steps/s (collection: 0.797s, learning 0.160s)
             Mean action noise std: 4.80
          Mean value_function loss: 23.4597
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.5022
                       Mean reward: 879.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 172.9981
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.96s
                      Time elapsed: 00:20:41
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 96290 steps/s (collection: 0.897s, learning 0.124s)
             Mean action noise std: 4.80
          Mean value_function loss: 23.8109
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.5097
                       Mean reward: 867.70
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.7149
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.02s
                      Time elapsed: 00:20:42
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 106216 steps/s (collection: 0.802s, learning 0.123s)
             Mean action noise std: 4.81
          Mean value_function loss: 20.6836
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.5186
                       Mean reward: 876.75
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 173.7989
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.93s
                      Time elapsed: 00:20:43
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 107510 steps/s (collection: 0.814s, learning 0.100s)
             Mean action noise std: 4.81
          Mean value_function loss: 25.4073
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.5260
                       Mean reward: 849.47
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 171.0540
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.91s
                      Time elapsed: 00:20:44
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 111277 steps/s (collection: 0.770s, learning 0.113s)
             Mean action noise std: 4.82
          Mean value_function loss: 18.3174
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.5350
                       Mean reward: 861.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 173.4092
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.88s
                      Time elapsed: 00:20:45
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 108213 steps/s (collection: 0.810s, learning 0.098s)
             Mean action noise std: 4.82
          Mean value_function loss: 28.0099
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 23.5445
                       Mean reward: 878.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 173.3412
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.91s
                      Time elapsed: 00:20:46
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 98544 steps/s (collection: 0.889s, learning 0.109s)
             Mean action noise std: 4.83
          Mean value_function loss: 32.3332
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.5505
                       Mean reward: 873.41
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 173.9789
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 1.00s
                      Time elapsed: 00:20:47
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 107283 steps/s (collection: 0.795s, learning 0.121s)
             Mean action noise std: 4.83
          Mean value_function loss: 24.1829
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.5617
                       Mean reward: 865.57
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 173.1307
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.92s
                      Time elapsed: 00:20:48
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 107963 steps/s (collection: 0.794s, learning 0.117s)
             Mean action noise std: 4.84
          Mean value_function loss: 21.2083
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.5677
                       Mean reward: 869.83
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 173.5239
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.91s
                      Time elapsed: 00:20:49
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 107205 steps/s (collection: 0.794s, learning 0.123s)
             Mean action noise std: 4.84
          Mean value_function loss: 26.9743
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 23.5726
                       Mean reward: 866.54
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 173.2894
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.92s
                      Time elapsed: 00:20:50
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 102349 steps/s (collection: 0.833s, learning 0.127s)
             Mean action noise std: 4.85
          Mean value_function loss: 28.9261
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.5808
                       Mean reward: 863.98
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 173.4711
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.96s
                      Time elapsed: 00:20:51
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 97675 steps/s (collection: 0.837s, learning 0.170s)
             Mean action noise std: 4.86
          Mean value_function loss: 30.2938
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.5954
                       Mean reward: 847.00
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 171.9685
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 1.01s
                      Time elapsed: 00:20:52
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 86749 steps/s (collection: 0.925s, learning 0.208s)
             Mean action noise std: 4.87
          Mean value_function loss: 26.8041
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 23.6098
                       Mean reward: 880.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 175.1291
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 1.13s
                      Time elapsed: 00:20:53
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 93435 steps/s (collection: 0.896s, learning 0.157s)
             Mean action noise std: 4.87
          Mean value_function loss: 31.2402
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 23.6265
                       Mean reward: 884.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7804
     Episode_Reward/lifting_object: 173.9613
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 1.05s
                      Time elapsed: 00:20:54
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 93896 steps/s (collection: 0.944s, learning 0.103s)
             Mean action noise std: 4.88
          Mean value_function loss: 45.2102
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 23.6365
                       Mean reward: 879.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 174.1781
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 1.05s
                      Time elapsed: 00:20:55
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 100162 steps/s (collection: 0.828s, learning 0.153s)
             Mean action noise std: 4.89
          Mean value_function loss: 42.3633
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.6441
                       Mean reward: 869.93
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 170.7407
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.98s
                      Time elapsed: 00:20:56
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 92014 steps/s (collection: 0.914s, learning 0.155s)
             Mean action noise std: 4.90
          Mean value_function loss: 42.2095
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.6612
                       Mean reward: 869.69
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 171.7740
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 1.07s
                      Time elapsed: 00:20:57
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 92054 steps/s (collection: 0.946s, learning 0.122s)
             Mean action noise std: 4.90
          Mean value_function loss: 45.7498
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.6696
                       Mean reward: 878.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.1442
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 1.07s
                      Time elapsed: 00:20:58
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 100845 steps/s (collection: 0.867s, learning 0.107s)
             Mean action noise std: 4.90
          Mean value_function loss: 35.2034
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.6728
                       Mean reward: 859.57
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.4070
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.97s
                      Time elapsed: 00:20:59
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 104032 steps/s (collection: 0.837s, learning 0.108s)
             Mean action noise std: 4.91
          Mean value_function loss: 29.4621
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.6763
                       Mean reward: 864.63
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 172.9934
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.94s
                      Time elapsed: 00:21:00
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 107642 steps/s (collection: 0.803s, learning 0.110s)
             Mean action noise std: 4.91
          Mean value_function loss: 25.0318
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.6876
                       Mean reward: 873.19
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 174.0242
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.91s
                      Time elapsed: 00:21:01
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 105987 steps/s (collection: 0.818s, learning 0.110s)
             Mean action noise std: 4.92
          Mean value_function loss: 32.9671
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.7004
                       Mean reward: 845.61
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.3038
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.93s
                      Time elapsed: 00:21:02
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 96210 steps/s (collection: 0.819s, learning 0.203s)
             Mean action noise std: 4.93
          Mean value_function loss: 31.2242
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.7136
                       Mean reward: 876.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.6578
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 1.02s
                      Time elapsed: 00:21:03
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 103754 steps/s (collection: 0.826s, learning 0.121s)
             Mean action noise std: 4.94
          Mean value_function loss: 49.3461
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.7279
                       Mean reward: 846.84
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 169.5974
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.95s
                      Time elapsed: 00:21:04
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 108083 steps/s (collection: 0.801s, learning 0.109s)
             Mean action noise std: 4.95
          Mean value_function loss: 34.9455
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.7414
                       Mean reward: 853.46
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 170.8523
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.91s
                      Time elapsed: 00:21:05
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 99688 steps/s (collection: 0.848s, learning 0.138s)
             Mean action noise std: 4.95
          Mean value_function loss: 40.3738
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.7487
                       Mean reward: 866.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.5165
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.99s
                      Time elapsed: 00:21:06
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 103698 steps/s (collection: 0.825s, learning 0.123s)
             Mean action noise std: 4.96
          Mean value_function loss: 43.7165
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 23.7537
                       Mean reward: 868.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.7959
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.95s
                      Time elapsed: 00:21:07
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 107624 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 4.96
          Mean value_function loss: 30.6491
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 23.7586
                       Mean reward: 859.29
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 170.6390
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.91s
                      Time elapsed: 00:21:08
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 99975 steps/s (collection: 0.816s, learning 0.167s)
             Mean action noise std: 4.97
          Mean value_function loss: 43.2654
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.7705
                       Mean reward: 883.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 173.4565
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.98s
                      Time elapsed: 00:21:09
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 100394 steps/s (collection: 0.838s, learning 0.141s)
             Mean action noise std: 4.98
          Mean value_function loss: 33.8650
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.7817
                       Mean reward: 856.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.5896
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.98s
                      Time elapsed: 00:21:10
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 103960 steps/s (collection: 0.826s, learning 0.120s)
             Mean action noise std: 4.98
          Mean value_function loss: 42.1143
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.7919
                       Mean reward: 863.50
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.0163
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.95s
                      Time elapsed: 00:21:11
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 110040 steps/s (collection: 0.784s, learning 0.110s)
             Mean action noise std: 4.99
          Mean value_function loss: 46.8496
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.8000
                       Mean reward: 852.93
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 171.1566
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.89s
                      Time elapsed: 00:21:11
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 105443 steps/s (collection: 0.792s, learning 0.140s)
             Mean action noise std: 4.99
          Mean value_function loss: 25.2644
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.8052
                       Mean reward: 859.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 167.7775
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.93s
                      Time elapsed: 00:21:12
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 98024 steps/s (collection: 0.848s, learning 0.155s)
             Mean action noise std: 4.99
          Mean value_function loss: 24.9454
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.8095
                       Mean reward: 858.51
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.4792
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 1.00s
                      Time elapsed: 00:21:13
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 107482 steps/s (collection: 0.804s, learning 0.111s)
             Mean action noise std: 5.00
          Mean value_function loss: 27.2242
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.8150
                       Mean reward: 860.41
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.3465
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.91s
                      Time elapsed: 00:21:14
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 101191 steps/s (collection: 0.854s, learning 0.117s)
             Mean action noise std: 5.00
          Mean value_function loss: 28.4904
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.8232
                       Mean reward: 879.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.4117
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.97s
                      Time elapsed: 00:21:15
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 109374 steps/s (collection: 0.789s, learning 0.109s)
             Mean action noise std: 5.01
          Mean value_function loss: 23.2815
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.8301
                       Mean reward: 869.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 173.6186
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.90s
                      Time elapsed: 00:21:16
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 101386 steps/s (collection: 0.842s, learning 0.128s)
             Mean action noise std: 5.01
          Mean value_function loss: 41.9828
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.8398
                       Mean reward: 842.08
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 170.8982
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.97s
                      Time elapsed: 00:21:17
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 105448 steps/s (collection: 0.804s, learning 0.128s)
             Mean action noise std: 5.02
          Mean value_function loss: 33.8717
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.8452
                       Mean reward: 863.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 173.3731
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.93s
                      Time elapsed: 00:21:18
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 105024 steps/s (collection: 0.843s, learning 0.093s)
             Mean action noise std: 5.02
          Mean value_function loss: 20.2568
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.8482
                       Mean reward: 864.08
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 173.5017
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.94s
                      Time elapsed: 00:21:19
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 102578 steps/s (collection: 0.842s, learning 0.116s)
             Mean action noise std: 5.03
          Mean value_function loss: 30.4044
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.8560
                       Mean reward: 846.64
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 170.5494
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.96s
                      Time elapsed: 00:21:20
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 101581 steps/s (collection: 0.822s, learning 0.146s)
             Mean action noise std: 5.03
          Mean value_function loss: 37.6709
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.8642
                       Mean reward: 865.56
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 173.1258
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.97s
                      Time elapsed: 00:21:21
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 102345 steps/s (collection: 0.859s, learning 0.101s)
             Mean action noise std: 5.04
          Mean value_function loss: 43.3591
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.8776
                       Mean reward: 861.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.3459
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.96s
                      Time elapsed: 00:21:22
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 108025 steps/s (collection: 0.794s, learning 0.116s)
             Mean action noise std: 5.05
          Mean value_function loss: 34.6980
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 23.8925
                       Mean reward: 861.84
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.2828
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.91s
                      Time elapsed: 00:21:23
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 106428 steps/s (collection: 0.800s, learning 0.124s)
             Mean action noise std: 5.05
          Mean value_function loss: 34.0775
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.8997
                       Mean reward: 857.50
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.2942
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.92s
                      Time elapsed: 00:21:24
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 107793 steps/s (collection: 0.800s, learning 0.112s)
             Mean action noise std: 5.05
          Mean value_function loss: 32.3198
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 23.9047
                       Mean reward: 860.52
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 173.3705
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.91s
                      Time elapsed: 00:21:25
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 105795 steps/s (collection: 0.793s, learning 0.137s)
             Mean action noise std: 5.06
          Mean value_function loss: 27.6828
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.9055
                       Mean reward: 857.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 173.3257
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.93s
                      Time elapsed: 00:21:26
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 105602 steps/s (collection: 0.804s, learning 0.127s)
             Mean action noise std: 5.06
          Mean value_function loss: 36.0479
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.9075
                       Mean reward: 866.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 172.3159
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.93s
                      Time elapsed: 00:21:27
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 105862 steps/s (collection: 0.787s, learning 0.141s)
             Mean action noise std: 5.06
          Mean value_function loss: 27.6541
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.9105
                       Mean reward: 871.77
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 173.3750
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.93s
                      Time elapsed: 00:21:27
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 110286 steps/s (collection: 0.798s, learning 0.094s)
             Mean action noise std: 5.07
          Mean value_function loss: 31.3145
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.9173
                       Mean reward: 878.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.0744
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.89s
                      Time elapsed: 00:21:28
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 109537 steps/s (collection: 0.795s, learning 0.103s)
             Mean action noise std: 5.07
          Mean value_function loss: 41.9855
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.9261
                       Mean reward: 866.91
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.7484
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.90s
                      Time elapsed: 00:21:29
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 107121 steps/s (collection: 0.826s, learning 0.092s)
             Mean action noise std: 5.08
          Mean value_function loss: 41.8997
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.9360
                       Mean reward: 853.22
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 170.0974
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.92s
                      Time elapsed: 00:21:30
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 110091 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 5.09
          Mean value_function loss: 32.9091
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.9512
                       Mean reward: 875.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.2828
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.89s
                      Time elapsed: 00:21:31
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 109501 steps/s (collection: 0.797s, learning 0.100s)
             Mean action noise std: 5.09
          Mean value_function loss: 38.7202
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.9601
                       Mean reward: 867.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7832
     Episode_Reward/lifting_object: 173.8053
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.90s
                      Time elapsed: 00:21:32
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 108775 steps/s (collection: 0.807s, learning 0.097s)
             Mean action noise std: 5.10
          Mean value_function loss: 20.5009
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.9745
                       Mean reward: 860.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 170.4903
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.90s
                      Time elapsed: 00:21:33
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 106520 steps/s (collection: 0.819s, learning 0.103s)
             Mean action noise std: 5.10
          Mean value_function loss: 31.8238
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 23.9853
                       Mean reward: 877.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 172.7274
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.92s
                      Time elapsed: 00:21:34
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 105270 steps/s (collection: 0.802s, learning 0.132s)
             Mean action noise std: 5.11
          Mean value_function loss: 27.6015
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.9884
                       Mean reward: 855.38
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 171.6686
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.93s
                      Time elapsed: 00:21:35
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 104965 steps/s (collection: 0.800s, learning 0.136s)
             Mean action noise std: 5.11
          Mean value_function loss: 27.5272
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.9970
                       Mean reward: 863.10
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7824
     Episode_Reward/lifting_object: 174.5312
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.94s
                      Time elapsed: 00:21:36
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 101861 steps/s (collection: 0.809s, learning 0.156s)
             Mean action noise std: 5.12
          Mean value_function loss: 23.5484
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 24.0086
                       Mean reward: 865.97
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 172.9513
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.97s
                      Time elapsed: 00:21:37
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 103582 steps/s (collection: 0.834s, learning 0.115s)
             Mean action noise std: 5.13
          Mean value_function loss: 30.7525
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 24.0212
                       Mean reward: 866.68
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 172.1808
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.95s
                      Time elapsed: 00:21:38
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 105710 steps/s (collection: 0.789s, learning 0.141s)
             Mean action noise std: 5.14
          Mean value_function loss: 28.9438
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 24.0366
                       Mean reward: 870.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 170.6353
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.93s
                      Time elapsed: 00:21:38
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 106684 steps/s (collection: 0.795s, learning 0.127s)
             Mean action noise std: 5.14
          Mean value_function loss: 26.6120
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 24.0434
                       Mean reward: 873.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.2787
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.92s
                      Time elapsed: 00:21:39
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 110639 steps/s (collection: 0.785s, learning 0.103s)
             Mean action noise std: 5.14
          Mean value_function loss: 27.0911
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 24.0442
                       Mean reward: 861.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 173.0895
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.89s
                      Time elapsed: 00:21:40
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 107884 steps/s (collection: 0.814s, learning 0.098s)
             Mean action noise std: 5.14
          Mean value_function loss: 20.9704
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.0461
                       Mean reward: 880.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 172.6360
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.91s
                      Time elapsed: 00:21:41
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 105825 steps/s (collection: 0.838s, learning 0.091s)
             Mean action noise std: 5.15
          Mean value_function loss: 32.6513
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.0498
                       Mean reward: 870.42
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 172.3833
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.93s
                      Time elapsed: 00:21:42
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 104201 steps/s (collection: 0.853s, learning 0.090s)
             Mean action noise std: 5.15
          Mean value_function loss: 31.6424
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 24.0558
                       Mean reward: 874.26
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 174.1130
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.94s
                      Time elapsed: 00:21:43
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 105489 steps/s (collection: 0.819s, learning 0.113s)
             Mean action noise std: 5.16
          Mean value_function loss: 29.3836
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.0657
                       Mean reward: 881.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7832
     Episode_Reward/lifting_object: 173.3636
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.93s
                      Time elapsed: 00:21:44
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 109614 steps/s (collection: 0.795s, learning 0.102s)
             Mean action noise std: 5.17
          Mean value_function loss: 28.2250
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.0781
                       Mean reward: 880.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 173.7851
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.90s
                      Time elapsed: 00:21:45
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 109956 steps/s (collection: 0.776s, learning 0.118s)
             Mean action noise std: 5.18
          Mean value_function loss: 39.4173
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.0933
                       Mean reward: 856.64
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.3160
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.89s
                      Time elapsed: 00:21:46
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 108674 steps/s (collection: 0.804s, learning 0.100s)
             Mean action noise std: 5.18
          Mean value_function loss: 22.6177
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.1082
                       Mean reward: 871.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 173.7934
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.90s
                      Time elapsed: 00:21:47
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 108320 steps/s (collection: 0.794s, learning 0.114s)
             Mean action noise std: 5.18
          Mean value_function loss: 28.3012
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.1128
                       Mean reward: 876.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 174.3900
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.91s
                      Time elapsed: 00:21:48
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 109697 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 5.18
          Mean value_function loss: 24.8780
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.1140
                       Mean reward: 867.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.5853
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.90s
                      Time elapsed: 00:21:49
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 98838 steps/s (collection: 0.823s, learning 0.171s)
             Mean action noise std: 5.19
          Mean value_function loss: 27.5243
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.1193
                       Mean reward: 872.91
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.0552
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.99s
                      Time elapsed: 00:21:50
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 109930 steps/s (collection: 0.801s, learning 0.094s)
             Mean action noise std: 5.20
          Mean value_function loss: 33.9840
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.1304
                       Mean reward: 874.48
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.3113
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.89s
                      Time elapsed: 00:21:50
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 111755 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 5.20
          Mean value_function loss: 31.0096
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.1374
                       Mean reward: 866.41
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 173.3335
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.88s
                      Time elapsed: 00:21:51
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 106979 steps/s (collection: 0.807s, learning 0.112s)
             Mean action noise std: 5.20
          Mean value_function loss: 34.9766
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.1440
                       Mean reward: 876.64
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.0243
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.92s
                      Time elapsed: 00:21:52
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 108098 steps/s (collection: 0.810s, learning 0.099s)
             Mean action noise std: 5.21
          Mean value_function loss: 31.9740
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.1542
                       Mean reward: 849.44
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 172.2014
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.91s
                      Time elapsed: 00:21:53
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 102126 steps/s (collection: 0.838s, learning 0.125s)
             Mean action noise std: 5.22
          Mean value_function loss: 37.5058
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.1654
                       Mean reward: 865.44
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 173.2781
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.96s
                      Time elapsed: 00:21:54
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 111544 steps/s (collection: 0.767s, learning 0.115s)
             Mean action noise std: 5.22
          Mean value_function loss: 29.0802
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 24.1703
                       Mean reward: 874.29
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.9769
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.88s
                      Time elapsed: 00:21:55
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 108420 steps/s (collection: 0.794s, learning 0.113s)
             Mean action noise std: 5.23
          Mean value_function loss: 19.7474
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 24.1778
                       Mean reward: 878.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 173.9529
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.91s
                      Time elapsed: 00:21:56
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 110568 steps/s (collection: 0.778s, learning 0.111s)
             Mean action noise std: 5.23
          Mean value_function loss: 27.8912
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.1830
                       Mean reward: 853.89
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 171.6056
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.89s
                      Time elapsed: 00:21:57
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 108534 steps/s (collection: 0.796s, learning 0.110s)
             Mean action noise std: 5.23
          Mean value_function loss: 24.3106
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.1852
                       Mean reward: 869.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.1004
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.91s
                      Time elapsed: 00:21:58
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 92730 steps/s (collection: 0.861s, learning 0.199s)
             Mean action noise std: 5.23
          Mean value_function loss: 32.8821
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.1874
                       Mean reward: 863.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 171.7406
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 1.06s
                      Time elapsed: 00:21:59
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 103697 steps/s (collection: 0.833s, learning 0.115s)
             Mean action noise std: 5.23
          Mean value_function loss: 31.0935
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.1900
                       Mean reward: 880.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.3212
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.95s
                      Time elapsed: 00:22:00
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 109778 steps/s (collection: 0.792s, learning 0.104s)
             Mean action noise std: 5.24
          Mean value_function loss: 28.8206
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.1947
                       Mean reward: 858.89
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 173.2773
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.90s
                      Time elapsed: 00:22:01
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 101535 steps/s (collection: 0.859s, learning 0.109s)
             Mean action noise std: 5.24
          Mean value_function loss: 22.2444
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.2046
                       Mean reward: 883.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 175.0805
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.97s
                      Time elapsed: 00:22:02
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 101666 steps/s (collection: 0.781s, learning 0.186s)
             Mean action noise std: 5.25
          Mean value_function loss: 30.7298
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.2097
                       Mean reward: 873.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 172.7287
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.97s
                      Time elapsed: 00:22:02
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 101813 steps/s (collection: 0.844s, learning 0.121s)
             Mean action noise std: 5.25
          Mean value_function loss: 29.0542
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.2150
                       Mean reward: 867.27
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 174.0268
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.97s
                      Time elapsed: 00:22:03
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 99202 steps/s (collection: 0.837s, learning 0.154s)
             Mean action noise std: 5.25
          Mean value_function loss: 24.0039
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.2207
                       Mean reward: 863.73
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 172.7209
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.99s
                      Time elapsed: 00:22:04
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 104698 steps/s (collection: 0.803s, learning 0.136s)
             Mean action noise std: 5.25
          Mean value_function loss: 34.4210
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.2238
                       Mean reward: 876.35
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.4488
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.94s
                      Time elapsed: 00:22:05
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 106387 steps/s (collection: 0.816s, learning 0.108s)
             Mean action noise std: 5.26
          Mean value_function loss: 24.6849
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.2256
                       Mean reward: 866.66
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: 173.1360
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.92s
                      Time elapsed: 00:22:06
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 104640 steps/s (collection: 0.824s, learning 0.115s)
             Mean action noise std: 5.26
          Mean value_function loss: 19.6484
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.2325
                       Mean reward: 861.16
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 171.6426
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.94s
                      Time elapsed: 00:22:07
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 106502 steps/s (collection: 0.814s, learning 0.109s)
             Mean action noise std: 5.27
          Mean value_function loss: 20.6671
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 24.2458
                       Mean reward: 880.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 174.8553
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.92s
                      Time elapsed: 00:22:08
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 101308 steps/s (collection: 0.855s, learning 0.115s)
             Mean action noise std: 5.27
          Mean value_function loss: 19.2061
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.2512
                       Mean reward: 871.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.1178
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.97s
                      Time elapsed: 00:22:09
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 106432 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 5.28
          Mean value_function loss: 18.4902
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.2571
                       Mean reward: 881.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 174.7435
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.92s
                      Time elapsed: 00:22:10
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 102055 steps/s (collection: 0.851s, learning 0.113s)
             Mean action noise std: 5.28
          Mean value_function loss: 23.4244
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 24.2659
                       Mean reward: 876.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 174.0255
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.96s
                      Time elapsed: 00:22:11
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 102460 steps/s (collection: 0.863s, learning 0.096s)
             Mean action noise std: 5.28
          Mean value_function loss: 25.5575
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 24.2686
                       Mean reward: 882.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 174.5402
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.96s
                      Time elapsed: 00:22:12
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 105404 steps/s (collection: 0.825s, learning 0.108s)
             Mean action noise std: 5.28
          Mean value_function loss: 30.3198
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.2693
                       Mean reward: 869.76
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7877
     Episode_Reward/lifting_object: 174.6801
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.93s
                      Time elapsed: 00:22:13
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 106165 steps/s (collection: 0.818s, learning 0.108s)
             Mean action noise std: 5.28
          Mean value_function loss: 22.2879
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 24.2712
                       Mean reward: 870.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7816
     Episode_Reward/lifting_object: 173.2785
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.93s
                      Time elapsed: 00:22:14
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 111093 steps/s (collection: 0.767s, learning 0.118s)
             Mean action noise std: 5.29
          Mean value_function loss: 25.6009
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.2767
                       Mean reward: 882.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 173.7061
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.88s
                      Time elapsed: 00:22:15
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 106712 steps/s (collection: 0.825s, learning 0.096s)
             Mean action noise std: 5.29
          Mean value_function loss: 33.5857
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.2827
                       Mean reward: 874.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 173.5093
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.92s
                      Time elapsed: 00:22:16
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 112685 steps/s (collection: 0.775s, learning 0.098s)
             Mean action noise std: 5.29
          Mean value_function loss: 20.4638
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.2841
                       Mean reward: 885.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7876
     Episode_Reward/lifting_object: 174.3582
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.87s
                      Time elapsed: 00:22:17
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 105912 steps/s (collection: 0.783s, learning 0.146s)
             Mean action noise std: 5.30
          Mean value_function loss: 24.4135
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 24.2891
                       Mean reward: 882.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 173.8825
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.93s
                      Time elapsed: 00:22:17
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 102256 steps/s (collection: 0.797s, learning 0.165s)
             Mean action noise std: 5.30
          Mean value_function loss: 27.6490
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.3002
                       Mean reward: 866.73
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 173.4438
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.96s
                      Time elapsed: 00:22:18
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 79591 steps/s (collection: 1.083s, learning 0.152s)
             Mean action noise std: 5.31
          Mean value_function loss: 18.4943
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.3081
                       Mean reward: 867.56
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 172.6210
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 1.24s
                      Time elapsed: 00:22:20
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 91541 steps/s (collection: 0.960s, learning 0.114s)
             Mean action noise std: 5.32
          Mean value_function loss: 22.6653
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.3197
                       Mean reward: 871.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 173.6290
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 1.07s
                      Time elapsed: 00:22:21
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 101076 steps/s (collection: 0.804s, learning 0.168s)
             Mean action noise std: 5.32
          Mean value_function loss: 24.8085
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.3258
                       Mean reward: 878.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 173.6336
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.97s
                      Time elapsed: 00:22:22
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 102671 steps/s (collection: 0.835s, learning 0.122s)
             Mean action noise std: 5.32
          Mean value_function loss: 20.3565
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.3306
                       Mean reward: 852.96
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 172.7650
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.96s
                      Time elapsed: 00:22:23
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 108061 steps/s (collection: 0.782s, learning 0.128s)
             Mean action noise std: 5.33
          Mean value_function loss: 20.0634
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 24.3424
                       Mean reward: 870.63
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7877
     Episode_Reward/lifting_object: 174.8473
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.91s
                      Time elapsed: 00:22:24
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 104962 steps/s (collection: 0.790s, learning 0.146s)
             Mean action noise std: 5.34
          Mean value_function loss: 26.0055
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.3547
                       Mean reward: 874.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 174.0548
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.94s
                      Time elapsed: 00:22:24
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 101112 steps/s (collection: 0.796s, learning 0.177s)
             Mean action noise std: 5.34
          Mean value_function loss: 22.5600
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.3630
                       Mean reward: 872.74
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 174.4735
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.97s
                      Time elapsed: 00:22:25
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 90770 steps/s (collection: 0.934s, learning 0.149s)
             Mean action noise std: 5.34
          Mean value_function loss: 20.4457
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.3682
                       Mean reward: 876.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7911
     Episode_Reward/lifting_object: 175.4408
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 1.08s
                      Time elapsed: 00:22:27
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 102043 steps/s (collection: 0.830s, learning 0.134s)
             Mean action noise std: 5.35
          Mean value_function loss: 20.6752
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.3754
                       Mean reward: 878.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: 174.1015
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.96s
                      Time elapsed: 00:22:28
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 100666 steps/s (collection: 0.784s, learning 0.192s)
             Mean action noise std: 5.35
          Mean value_function loss: 19.3886
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 24.3859
                       Mean reward: 884.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7911
     Episode_Reward/lifting_object: 175.0253
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.98s
                      Time elapsed: 00:22:28
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 98093 steps/s (collection: 0.862s, learning 0.141s)
             Mean action noise std: 5.36
          Mean value_function loss: 22.0112
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.3911
                       Mean reward: 873.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 174.2591
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 1.00s
                      Time elapsed: 00:22:29
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 99250 steps/s (collection: 0.784s, learning 0.206s)
             Mean action noise std: 5.37
          Mean value_function loss: 29.2818
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.4027
                       Mean reward: 872.75
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 172.8247
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.99s
                      Time elapsed: 00:22:30
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 39750 steps/s (collection: 2.351s, learning 0.122s)
             Mean action noise std: 5.37
          Mean value_function loss: 24.9268
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.4094
                       Mean reward: 874.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7857
     Episode_Reward/lifting_object: 174.4613
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.47s
                      Time elapsed: 00:22:33
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 30712 steps/s (collection: 3.071s, learning 0.130s)
             Mean action noise std: 5.38
          Mean value_function loss: 25.8270
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.4172
                       Mean reward: 872.99
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 174.2567
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 3.20s
                      Time elapsed: 00:22:36
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 32579 steps/s (collection: 2.899s, learning 0.119s)
             Mean action noise std: 5.38
          Mean value_function loss: 19.3170
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 24.4285
                       Mean reward: 873.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 172.6673
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.02s
                      Time elapsed: 00:22:39
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 30367 steps/s (collection: 3.096s, learning 0.142s)
             Mean action noise std: 5.39
          Mean value_function loss: 18.4116
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.4382
                       Mean reward: 879.52
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7880
     Episode_Reward/lifting_object: 175.2281
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.24s
                      Time elapsed: 00:22:42
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 29648 steps/s (collection: 3.149s, learning 0.167s)
             Mean action noise std: 5.39
          Mean value_function loss: 23.7448
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.4446
                       Mean reward: 881.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 173.9006
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.32s
                      Time elapsed: 00:22:46
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 29964 steps/s (collection: 3.161s, learning 0.120s)
             Mean action noise std: 5.40
          Mean value_function loss: 19.1536
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 24.4511
                       Mean reward: 873.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 172.7560
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.28s
                      Time elapsed: 00:22:49
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 29868 steps/s (collection: 3.149s, learning 0.143s)
             Mean action noise std: 5.40
          Mean value_function loss: 19.1615
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 24.4596
                       Mean reward: 877.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 172.8546
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.29s
                      Time elapsed: 00:22:52
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 30699 steps/s (collection: 2.991s, learning 0.211s)
             Mean action noise std: 5.41
          Mean value_function loss: 18.3671
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.4651
                       Mean reward: 855.47
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 172.6248
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.20s
                      Time elapsed: 00:22:56
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 20075 steps/s (collection: 4.764s, learning 0.133s)
             Mean action noise std: 5.41
          Mean value_function loss: 16.9231
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.4723
                       Mean reward: 884.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7917
     Episode_Reward/lifting_object: 175.2316
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.90s
                      Time elapsed: 00:23:00
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 99073 steps/s (collection: 0.842s, learning 0.150s)
             Mean action noise std: 5.42
          Mean value_function loss: 25.2109
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.4809
                       Mean reward: 876.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7866
     Episode_Reward/lifting_object: 173.8300
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.99s
                      Time elapsed: 00:23:01
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 106218 steps/s (collection: 0.839s, learning 0.086s)
             Mean action noise std: 5.42
          Mean value_function loss: 16.0747
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.4917
                       Mean reward: 866.77
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 173.6723
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.93s
                      Time elapsed: 00:23:02
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 113007 steps/s (collection: 0.763s, learning 0.107s)
             Mean action noise std: 5.43
          Mean value_function loss: 16.3542
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.4957
                       Mean reward: 880.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7864
     Episode_Reward/lifting_object: 174.2570
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.87s
                      Time elapsed: 00:23:03
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 114425 steps/s (collection: 0.769s, learning 0.090s)
             Mean action noise std: 5.44
          Mean value_function loss: 21.1549
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.5050
                       Mean reward: 874.11
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 174.7921
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.86s
                      Time elapsed: 00:23:04
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 116528 steps/s (collection: 0.746s, learning 0.098s)
             Mean action noise std: 5.44
          Mean value_function loss: 25.3935
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.5165
                       Mean reward: 868.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7870
     Episode_Reward/lifting_object: 173.5998
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.84s
                      Time elapsed: 00:23:05
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 113012 steps/s (collection: 0.762s, learning 0.108s)
             Mean action noise std: 5.45
          Mean value_function loss: 20.9483
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.5234
                       Mean reward: 880.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 174.6961
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.87s
                      Time elapsed: 00:23:06
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 117886 steps/s (collection: 0.739s, learning 0.095s)
             Mean action noise std: 5.45
          Mean value_function loss: 20.7687
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.5322
                       Mean reward: 879.42
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7866
     Episode_Reward/lifting_object: 174.8754
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.83s
                      Time elapsed: 00:23:07
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 113803 steps/s (collection: 0.737s, learning 0.127s)
             Mean action noise std: 5.45
          Mean value_function loss: 18.5903
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.5342
                       Mean reward: 873.57
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 173.5793
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.86s
                      Time elapsed: 00:23:07
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 113208 steps/s (collection: 0.765s, learning 0.103s)
             Mean action noise std: 5.46
          Mean value_function loss: 20.5384
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.5377
                       Mean reward: 874.79
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 174.2478
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.87s
                      Time elapsed: 00:23:08
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 107154 steps/s (collection: 0.798s, learning 0.120s)
             Mean action noise std: 5.46
          Mean value_function loss: 23.0356
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.5447
                       Mean reward: 859.94
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 173.6564
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.92s
                      Time elapsed: 00:23:09
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 112540 steps/s (collection: 0.785s, learning 0.088s)
             Mean action noise std: 5.47
          Mean value_function loss: 19.1292
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.5527
                       Mean reward: 878.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 174.9394
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.87s
                      Time elapsed: 00:23:10
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 113215 steps/s (collection: 0.767s, learning 0.102s)
             Mean action noise std: 5.48
          Mean value_function loss: 19.7472
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.5612
                       Mean reward: 878.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7947
     Episode_Reward/lifting_object: 175.6685
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.87s
                      Time elapsed: 00:23:11
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 114671 steps/s (collection: 0.752s, learning 0.106s)
             Mean action noise std: 5.48
          Mean value_function loss: 23.9055
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.5703
                       Mean reward: 863.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7918
     Episode_Reward/lifting_object: 173.3712
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.86s
                      Time elapsed: 00:23:12
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 110681 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 5.49
          Mean value_function loss: 23.1007
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.5789
                       Mean reward: 878.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 174.0472
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.89s
                      Time elapsed: 00:23:13
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 108998 steps/s (collection: 0.750s, learning 0.152s)
             Mean action noise std: 5.49
          Mean value_function loss: 23.9953
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 24.5865
                       Mean reward: 882.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 174.6067
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.90s
                      Time elapsed: 00:23:14
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 105375 steps/s (collection: 0.767s, learning 0.166s)
             Mean action noise std: 5.50
          Mean value_function loss: 18.3216
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.5977
                       Mean reward: 877.60
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7882
     Episode_Reward/lifting_object: 174.3030
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.93s
                      Time elapsed: 00:23:15
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 109814 steps/s (collection: 0.759s, learning 0.137s)
             Mean action noise std: 5.50
          Mean value_function loss: 24.4432
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.6062
                       Mean reward: 885.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7859
     Episode_Reward/lifting_object: 173.2598
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.90s
                      Time elapsed: 00:23:15
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 113724 steps/s (collection: 0.759s, learning 0.105s)
             Mean action noise std: 5.51
          Mean value_function loss: 21.9506
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.6114
                       Mean reward: 848.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 173.0798
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.86s
                      Time elapsed: 00:23:16
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 111147 steps/s (collection: 0.776s, learning 0.109s)
             Mean action noise std: 5.51
          Mean value_function loss: 30.7232
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.6158
                       Mean reward: 860.55
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 171.2851
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.88s
                      Time elapsed: 00:23:17
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 115597 steps/s (collection: 0.762s, learning 0.088s)
             Mean action noise std: 5.52
          Mean value_function loss: 25.4166
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.6224
                       Mean reward: 862.73
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 172.2188
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.85s
                      Time elapsed: 00:23:18
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 110884 steps/s (collection: 0.791s, learning 0.095s)
             Mean action noise std: 5.52
          Mean value_function loss: 18.6069
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 24.6288
                       Mean reward: 887.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 173.0838
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.89s
                      Time elapsed: 00:23:19
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 114888 steps/s (collection: 0.749s, learning 0.107s)
             Mean action noise std: 5.52
          Mean value_function loss: 23.8872
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.6353
                       Mean reward: 865.37
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 173.7798
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.86s
                      Time elapsed: 00:23:20
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 112899 steps/s (collection: 0.764s, learning 0.107s)
             Mean action noise std: 5.53
          Mean value_function loss: 25.6757
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.6405
                       Mean reward: 875.08
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 173.9534
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.87s
                      Time elapsed: 00:23:21
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 111163 steps/s (collection: 0.761s, learning 0.123s)
             Mean action noise std: 5.53
          Mean value_function loss: 30.1250
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.6435
                       Mean reward: 880.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 173.6238
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.88s
                      Time elapsed: 00:23:22
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 111418 steps/s (collection: 0.759s, learning 0.123s)
             Mean action noise std: 5.54
          Mean value_function loss: 22.0120
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.6497
                       Mean reward: 880.10
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7901
     Episode_Reward/lifting_object: 174.1808
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.88s
                      Time elapsed: 00:23:22
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 112542 steps/s (collection: 0.760s, learning 0.114s)
             Mean action noise std: 5.54
          Mean value_function loss: 28.3815
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.6560
                       Mean reward: 871.14
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 173.7289
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.87s
                      Time elapsed: 00:23:23
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 108100 steps/s (collection: 0.780s, learning 0.129s)
             Mean action noise std: 5.54
          Mean value_function loss: 22.8190
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 24.6653
                       Mean reward: 875.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7883
     Episode_Reward/lifting_object: 174.3756
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.91s
                      Time elapsed: 00:23:24
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 109600 steps/s (collection: 0.782s, learning 0.115s)
             Mean action noise std: 5.55
          Mean value_function loss: 23.0024
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.6666
                       Mean reward: 873.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 173.4474
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.90s
                      Time elapsed: 00:23:25
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 104630 steps/s (collection: 0.834s, learning 0.106s)
             Mean action noise std: 5.55
          Mean value_function loss: 23.8255
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.6710
                       Mean reward: 872.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7892
     Episode_Reward/lifting_object: 175.1908
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.94s
                      Time elapsed: 00:23:26
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 119319 steps/s (collection: 0.725s, learning 0.099s)
             Mean action noise std: 5.55
          Mean value_function loss: 24.1124
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.6760
                       Mean reward: 865.78
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 172.4505
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.82s
                      Time elapsed: 00:23:27
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 115065 steps/s (collection: 0.758s, learning 0.096s)
             Mean action noise std: 5.56
          Mean value_function loss: 25.2598
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.6807
                       Mean reward: 881.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 173.8775
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.85s
                      Time elapsed: 00:23:28
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 109259 steps/s (collection: 0.762s, learning 0.138s)
             Mean action noise std: 5.56
          Mean value_function loss: 30.6951
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 24.6887
                       Mean reward: 886.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7863
     Episode_Reward/lifting_object: 174.3934
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.90s
                      Time elapsed: 00:23:29
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 112100 steps/s (collection: 0.747s, learning 0.130s)
             Mean action noise std: 5.57
          Mean value_function loss: 28.0963
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 24.6991
                       Mean reward: 866.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 174.2539
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.88s
                      Time elapsed: 00:23:30
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 117305 steps/s (collection: 0.752s, learning 0.086s)
             Mean action noise std: 5.58
          Mean value_function loss: 25.5701
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.7072
                       Mean reward: 872.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 173.4831
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.84s
                      Time elapsed: 00:23:30
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 117327 steps/s (collection: 0.755s, learning 0.083s)
             Mean action noise std: 5.58
          Mean value_function loss: 22.0445
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.7213
                       Mean reward: 867.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 171.6822
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.84s
                      Time elapsed: 00:23:31
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 107374 steps/s (collection: 0.808s, learning 0.107s)
             Mean action noise std: 5.59
          Mean value_function loss: 21.0084
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.7312
                       Mean reward: 871.98
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7859
     Episode_Reward/lifting_object: 174.2251
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.92s
                      Time elapsed: 00:23:32
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 114193 steps/s (collection: 0.755s, learning 0.106s)
             Mean action noise std: 5.60
          Mean value_function loss: 23.1652
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.7418
                       Mean reward: 867.15
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 173.8272
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.86s
                      Time elapsed: 00:23:33
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 115684 steps/s (collection: 0.749s, learning 0.101s)
             Mean action noise std: 5.60
          Mean value_function loss: 25.5306
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.7487
                       Mean reward: 869.86
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 171.9543
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.85s
                      Time elapsed: 00:23:34
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 113428 steps/s (collection: 0.741s, learning 0.125s)
             Mean action noise std: 5.60
          Mean value_function loss: 22.2184
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 24.7525
                       Mean reward: 871.51
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 173.9727
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.87s
                      Time elapsed: 00:23:35
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 115277 steps/s (collection: 0.739s, learning 0.114s)
             Mean action noise std: 5.61
          Mean value_function loss: 28.5138
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 24.7593
                       Mean reward: 874.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 173.8140
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.85s
                      Time elapsed: 00:23:36
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 115294 steps/s (collection: 0.759s, learning 0.093s)
             Mean action noise std: 5.61
          Mean value_function loss: 30.0435
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.7638
                       Mean reward: 872.37
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 172.5074
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.85s
                      Time elapsed: 00:23:36
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 114993 steps/s (collection: 0.747s, learning 0.108s)
             Mean action noise std: 5.62
          Mean value_function loss: 21.0693
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.7707
                       Mean reward: 856.40
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: 172.0879
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.85s
                      Time elapsed: 00:23:37
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 115376 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 5.62
          Mean value_function loss: 22.0414
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.7764
                       Mean reward: 884.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7919
     Episode_Reward/lifting_object: 174.3383
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.85s
                      Time elapsed: 00:23:38
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 114044 steps/s (collection: 0.773s, learning 0.089s)
             Mean action noise std: 5.63
          Mean value_function loss: 28.0146
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.7869
                       Mean reward: 883.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7892
     Episode_Reward/lifting_object: 173.6089
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.86s
                      Time elapsed: 00:23:39
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 116557 steps/s (collection: 0.743s, learning 0.101s)
             Mean action noise std: 5.64
          Mean value_function loss: 36.1079
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.7974
                       Mean reward: 883.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7960
     Episode_Reward/lifting_object: 175.3525
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.84s
                      Time elapsed: 00:23:40
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 106253 steps/s (collection: 0.768s, learning 0.157s)
             Mean action noise std: 5.64
          Mean value_function loss: 28.8023
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.8021
                       Mean reward: 873.79
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 173.3221
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.93s
                      Time elapsed: 00:23:41
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 110603 steps/s (collection: 0.746s, learning 0.143s)
             Mean action noise std: 5.65
          Mean value_function loss: 27.4790
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.8082
                       Mean reward: 885.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7922
     Episode_Reward/lifting_object: 174.2383
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.89s
                      Time elapsed: 00:23:42
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 113831 steps/s (collection: 0.748s, learning 0.116s)
             Mean action noise std: 5.66
          Mean value_function loss: 31.4832
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.8171
                       Mean reward: 854.71
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 172.8006
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.86s
                      Time elapsed: 00:23:42
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 105892 steps/s (collection: 0.807s, learning 0.122s)
             Mean action noise std: 5.66
          Mean value_function loss: 28.6471
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.8288
                       Mean reward: 883.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 174.2867
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.93s
                      Time elapsed: 00:23:43
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 112242 steps/s (collection: 0.757s, learning 0.119s)
             Mean action noise std: 5.67
          Mean value_function loss: 27.2389
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.8421
                       Mean reward: 871.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 173.5940
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.88s
                      Time elapsed: 00:23:44
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 116379 steps/s (collection: 0.749s, learning 0.096s)
             Mean action noise std: 5.68
          Mean value_function loss: 21.0243
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.8549
                       Mean reward: 870.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7880
     Episode_Reward/lifting_object: 173.9269
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.84s
                      Time elapsed: 00:23:45
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 113618 steps/s (collection: 0.778s, learning 0.087s)
             Mean action noise std: 5.69
          Mean value_function loss: 22.8422
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.8665
                       Mean reward: 875.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 172.5140
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.87s
                      Time elapsed: 00:23:46
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 117064 steps/s (collection: 0.746s, learning 0.093s)
             Mean action noise std: 5.69
          Mean value_function loss: 27.3701
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.8744
                       Mean reward: 878.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 172.4966
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.84s
                      Time elapsed: 00:23:47
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 111394 steps/s (collection: 0.760s, learning 0.123s)
             Mean action noise std: 5.70
          Mean value_function loss: 27.6959
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.8825
                       Mean reward: 870.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 171.5429
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.88s
                      Time elapsed: 00:23:48
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 114265 steps/s (collection: 0.739s, learning 0.121s)
             Mean action noise std: 5.70
          Mean value_function loss: 23.1929
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.8936
                       Mean reward: 865.03
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 172.6859
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.86s
                      Time elapsed: 00:23:49
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 116856 steps/s (collection: 0.752s, learning 0.090s)
             Mean action noise std: 5.71
          Mean value_function loss: 28.1785
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.9042
                       Mean reward: 879.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 172.7927
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.84s
                      Time elapsed: 00:23:49
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 110720 steps/s (collection: 0.775s, learning 0.113s)
             Mean action noise std: 5.72
          Mean value_function loss: 24.5467
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.9153
                       Mean reward: 871.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 174.0700
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.89s
                      Time elapsed: 00:23:50
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 116464 steps/s (collection: 0.735s, learning 0.109s)
             Mean action noise std: 5.72
          Mean value_function loss: 22.4508
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.9229
                       Mean reward: 874.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 173.1235
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.84s
                      Time elapsed: 00:23:51
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 111760 steps/s (collection: 0.786s, learning 0.094s)
             Mean action noise std: 5.72
          Mean value_function loss: 28.8949
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.9257
                       Mean reward: 849.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 173.3861
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.88s
                      Time elapsed: 00:23:52
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 113054 steps/s (collection: 0.768s, learning 0.102s)
             Mean action noise std: 5.73
          Mean value_function loss: 23.7813
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.9360
                       Mean reward: 873.87
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 173.7742
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.87s
                      Time elapsed: 00:23:53
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 112472 steps/s (collection: 0.751s, learning 0.123s)
             Mean action noise std: 5.74
          Mean value_function loss: 21.1601
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 24.9487
                       Mean reward: 859.39
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 173.2041
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.87s
                      Time elapsed: 00:23:54
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 113942 steps/s (collection: 0.742s, learning 0.120s)
             Mean action noise std: 5.75
          Mean value_function loss: 24.5806
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 24.9588
                       Mean reward: 867.33
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7874
     Episode_Reward/lifting_object: 173.1092
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.86s
                      Time elapsed: 00:23:55
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 112428 steps/s (collection: 0.757s, learning 0.117s)
             Mean action noise std: 5.75
          Mean value_function loss: 25.6744
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.9675
                       Mean reward: 874.22
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7910
     Episode_Reward/lifting_object: 174.7159
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.87s
                      Time elapsed: 00:23:56
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 111467 steps/s (collection: 0.768s, learning 0.114s)
             Mean action noise std: 5.76
          Mean value_function loss: 16.8636
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.9753
                       Mean reward: 882.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 175.3405
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.88s
                      Time elapsed: 00:23:56
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 116217 steps/s (collection: 0.757s, learning 0.089s)
             Mean action noise std: 5.77
          Mean value_function loss: 25.9512
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 24.9893
                       Mean reward: 872.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 172.2734
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.85s
                      Time elapsed: 00:23:57
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 109143 steps/s (collection: 0.784s, learning 0.117s)
             Mean action noise std: 5.77
          Mean value_function loss: 33.0006
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.9977
                       Mean reward: 875.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7876
     Episode_Reward/lifting_object: 174.2423
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.90s
                      Time elapsed: 00:23:58
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 113729 steps/s (collection: 0.765s, learning 0.099s)
             Mean action noise std: 5.78
          Mean value_function loss: 34.9917
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.0033
                       Mean reward: 872.17
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7902
     Episode_Reward/lifting_object: 174.4109
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.86s
                      Time elapsed: 00:23:59
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 107314 steps/s (collection: 0.806s, learning 0.111s)
             Mean action noise std: 5.79
          Mean value_function loss: 32.2459
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.0103
                       Mean reward: 869.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 173.0248
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.92s
                      Time elapsed: 00:24:00
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 112739 steps/s (collection: 0.773s, learning 0.099s)
             Mean action noise std: 5.79
          Mean value_function loss: 33.5605
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.0184
                       Mean reward: 869.11
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 172.0558
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.87s
                      Time elapsed: 00:24:01
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 114870 steps/s (collection: 0.741s, learning 0.115s)
             Mean action noise std: 5.80
          Mean value_function loss: 27.0871
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 25.0291
                       Mean reward: 872.58
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 173.6129
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.86s
                      Time elapsed: 00:24:02
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 114715 steps/s (collection: 0.737s, learning 0.120s)
             Mean action noise std: 5.80
          Mean value_function loss: 36.1365
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 25.0383
                       Mean reward: 874.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.1123
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.86s
                      Time elapsed: 00:24:03
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 112619 steps/s (collection: 0.766s, learning 0.107s)
             Mean action noise std: 5.81
          Mean value_function loss: 44.0113
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.0455
                       Mean reward: 878.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 173.9230
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.87s
                      Time elapsed: 00:24:03
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 110768 steps/s (collection: 0.773s, learning 0.114s)
             Mean action noise std: 5.81
          Mean value_function loss: 32.0590
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 25.0529
                       Mean reward: 869.01
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 173.6650
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.89s
                      Time elapsed: 00:24:04
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 110600 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 5.82
          Mean value_function loss: 24.1433
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.0583
                       Mean reward: 834.90
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 172.2240
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.89s
                      Time elapsed: 00:24:05
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 112266 steps/s (collection: 0.787s, learning 0.089s)
             Mean action noise std: 5.82
          Mean value_function loss: 23.9420
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 25.0660
                       Mean reward: 857.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.4410
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.88s
                      Time elapsed: 00:24:06
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 114241 steps/s (collection: 0.755s, learning 0.105s)
             Mean action noise std: 5.83
          Mean value_function loss: 25.3406
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.0736
                       Mean reward: 866.37
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.8996
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.86s
                      Time elapsed: 00:24:07
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 114250 steps/s (collection: 0.758s, learning 0.103s)
             Mean action noise std: 5.83
          Mean value_function loss: 24.4488
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.0847
                       Mean reward: 872.97
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.8406
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.86s
                      Time elapsed: 00:24:08
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 107159 steps/s (collection: 0.742s, learning 0.175s)
             Mean action noise std: 5.84
          Mean value_function loss: 27.5712
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.0928
                       Mean reward: 855.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.3503
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.92s
                      Time elapsed: 00:24:09
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 114351 steps/s (collection: 0.757s, learning 0.102s)
             Mean action noise std: 5.84
          Mean value_function loss: 24.7300
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.1017
                       Mean reward: 874.67
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 172.0210
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.86s
                      Time elapsed: 00:24:10
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 102279 steps/s (collection: 0.821s, learning 0.141s)
             Mean action noise std: 5.85
          Mean value_function loss: 27.8634
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.1097
                       Mean reward: 875.31
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 172.3984
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.96s
                      Time elapsed: 00:24:10
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 108401 steps/s (collection: 0.764s, learning 0.143s)
             Mean action noise std: 5.85
          Mean value_function loss: 18.7951
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 25.1171
                       Mean reward: 873.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.7574
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.91s
                      Time elapsed: 00:24:11
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 114489 steps/s (collection: 0.758s, learning 0.101s)
             Mean action noise std: 5.86
          Mean value_function loss: 29.1562
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.1225
                       Mean reward: 867.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 173.4348
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.86s
                      Time elapsed: 00:24:12
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 118455 steps/s (collection: 0.727s, learning 0.103s)
             Mean action noise std: 5.87
          Mean value_function loss: 21.4997
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.1332
                       Mean reward: 881.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 173.5276
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.83s
                      Time elapsed: 00:24:13
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 109764 steps/s (collection: 0.783s, learning 0.113s)
             Mean action noise std: 5.87
          Mean value_function loss: 28.0049
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.1423
                       Mean reward: 881.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7889
     Episode_Reward/lifting_object: 174.9468
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.90s
                      Time elapsed: 00:24:14
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 118091 steps/s (collection: 0.743s, learning 0.090s)
             Mean action noise std: 5.88
          Mean value_function loss: 35.1744
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.1530
                       Mean reward: 865.94
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 173.7271
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.83s
                      Time elapsed: 00:24:15
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 110865 steps/s (collection: 0.764s, learning 0.123s)
             Mean action noise std: 5.89
          Mean value_function loss: 23.1541
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.1617
                       Mean reward: 872.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 173.2688
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.89s
                      Time elapsed: 00:24:16
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 114599 steps/s (collection: 0.749s, learning 0.109s)
             Mean action noise std: 5.89
          Mean value_function loss: 25.1096
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.1700
                       Mean reward: 877.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 173.9762
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.86s
                      Time elapsed: 00:24:17
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 113675 steps/s (collection: 0.749s, learning 0.116s)
             Mean action noise std: 5.90
          Mean value_function loss: 23.3179
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.1782
                       Mean reward: 876.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7857
     Episode_Reward/lifting_object: 174.2959
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.86s
                      Time elapsed: 00:24:17
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 109833 steps/s (collection: 0.781s, learning 0.114s)
             Mean action noise std: 5.91
          Mean value_function loss: 36.2549
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.1881
                       Mean reward: 872.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7782
     Episode_Reward/lifting_object: 173.2377
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.90s
                      Time elapsed: 00:24:18
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 115276 steps/s (collection: 0.755s, learning 0.097s)
             Mean action noise std: 5.91
          Mean value_function loss: 23.6750
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.1981
                       Mean reward: 855.91
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.8551
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.85s
                      Time elapsed: 00:24:19
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 115108 steps/s (collection: 0.761s, learning 0.093s)
             Mean action noise std: 5.92
          Mean value_function loss: 20.3052
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.2027
                       Mean reward: 883.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 174.3431
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.85s
                      Time elapsed: 00:24:20
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 111414 steps/s (collection: 0.775s, learning 0.108s)
             Mean action noise std: 5.92
          Mean value_function loss: 18.4119
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 25.2092
                       Mean reward: 880.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 173.1910
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.88s
                      Time elapsed: 00:24:21
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 112016 steps/s (collection: 0.755s, learning 0.122s)
             Mean action noise std: 5.93
          Mean value_function loss: 20.4496
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 25.2186
                       Mean reward: 877.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7866
     Episode_Reward/lifting_object: 174.1565
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.88s
                      Time elapsed: 00:24:22
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 109599 steps/s (collection: 0.779s, learning 0.118s)
             Mean action noise std: 5.94
          Mean value_function loss: 23.8287
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 25.2312
                       Mean reward: 859.55
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 173.3702
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.90s
                      Time elapsed: 00:24:23
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 109049 steps/s (collection: 0.751s, learning 0.150s)
             Mean action noise std: 5.94
          Mean value_function loss: 25.1300
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.2395
                       Mean reward: 865.33
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 173.5154
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.90s
                      Time elapsed: 00:24:24
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 105141 steps/s (collection: 0.783s, learning 0.152s)
             Mean action noise std: 5.95
          Mean value_function loss: 20.8363
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.2478
                       Mean reward: 875.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7901
     Episode_Reward/lifting_object: 174.2120
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.93s
                      Time elapsed: 00:24:25
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 100572 steps/s (collection: 0.830s, learning 0.147s)
             Mean action noise std: 5.95
          Mean value_function loss: 27.7684
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.2580
                       Mean reward: 870.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 173.4458
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.98s
                      Time elapsed: 00:24:25
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 112891 steps/s (collection: 0.754s, learning 0.117s)
             Mean action noise std: 5.96
          Mean value_function loss: 20.6629
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.2654
                       Mean reward: 878.94
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7854
     Episode_Reward/lifting_object: 174.3516
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.87s
                      Time elapsed: 00:24:26
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 109786 steps/s (collection: 0.803s, learning 0.093s)
             Mean action noise std: 5.96
          Mean value_function loss: 22.2132
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.2762
                       Mean reward: 874.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 173.1838
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.90s
                      Time elapsed: 00:24:27
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 113656 steps/s (collection: 0.768s, learning 0.097s)
             Mean action noise std: 5.97
          Mean value_function loss: 27.0998
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.2831
                       Mean reward: 878.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 173.9526
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.86s
                      Time elapsed: 00:24:28
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 112277 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 5.97
          Mean value_function loss: 29.0991
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.2866
                       Mean reward: 845.76
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.6149
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.88s
                      Time elapsed: 00:24:29
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 107605 steps/s (collection: 0.800s, learning 0.114s)
             Mean action noise std: 5.98
          Mean value_function loss: 25.8267
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.2928
                       Mean reward: 879.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 174.3290
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.91s
                      Time elapsed: 00:24:30
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 112804 steps/s (collection: 0.769s, learning 0.102s)
             Mean action noise std: 5.99
          Mean value_function loss: 25.3689
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 25.3037
                       Mean reward: 885.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 173.1606
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.87s
                      Time elapsed: 00:24:31
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 110776 steps/s (collection: 0.752s, learning 0.135s)
             Mean action noise std: 5.99
          Mean value_function loss: 20.4595
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.3156
                       Mean reward: 881.75
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 174.0938
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.89s
                      Time elapsed: 00:24:32
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 108801 steps/s (collection: 0.751s, learning 0.153s)
             Mean action noise std: 6.00
          Mean value_function loss: 25.2093
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.3260
                       Mean reward: 875.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7870
     Episode_Reward/lifting_object: 174.6190
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.90s
                      Time elapsed: 00:24:33
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 103809 steps/s (collection: 0.787s, learning 0.160s)
             Mean action noise std: 6.01
          Mean value_function loss: 24.0324
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.3352
                       Mean reward: 865.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 173.2310
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.95s
                      Time elapsed: 00:24:34
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 113311 steps/s (collection: 0.757s, learning 0.111s)
             Mean action noise std: 6.01
          Mean value_function loss: 26.7305
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.3401
                       Mean reward: 875.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 173.5326
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.87s
                      Time elapsed: 00:24:34
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 111515 steps/s (collection: 0.782s, learning 0.099s)
             Mean action noise std: 6.02
          Mean value_function loss: 29.1086
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.3488
                       Mean reward: 877.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 174.5364
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.88s
                      Time elapsed: 00:24:35
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 115935 steps/s (collection: 0.756s, learning 0.092s)
             Mean action noise std: 6.02
          Mean value_function loss: 18.5547
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.3577
                       Mean reward: 868.34
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7864
     Episode_Reward/lifting_object: 174.1305
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.85s
                      Time elapsed: 00:24:36
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 109955 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 6.02
          Mean value_function loss: 23.0877
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.3612
                       Mean reward: 880.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 172.5644
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.89s
                      Time elapsed: 00:24:37
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 111749 steps/s (collection: 0.781s, learning 0.099s)
             Mean action noise std: 6.03
          Mean value_function loss: 25.0048
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.3680
                       Mean reward: 877.55
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 174.1463
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.88s
                      Time elapsed: 00:24:38
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 116757 steps/s (collection: 0.737s, learning 0.105s)
             Mean action noise std: 6.04
          Mean value_function loss: 27.4999
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.3777
                       Mean reward: 857.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 173.8728
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.84s
                      Time elapsed: 00:24:39
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 112696 steps/s (collection: 0.766s, learning 0.106s)
             Mean action noise std: 6.05
          Mean value_function loss: 22.2552
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.3895
                       Mean reward: 858.34
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 173.0011
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.87s
                      Time elapsed: 00:24:40
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 112921 steps/s (collection: 0.767s, learning 0.103s)
             Mean action noise std: 6.05
          Mean value_function loss: 31.8452
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.3978
                       Mean reward: 879.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7871
     Episode_Reward/lifting_object: 174.6617
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.87s
                      Time elapsed: 00:24:40
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 113895 steps/s (collection: 0.762s, learning 0.102s)
             Mean action noise std: 6.06
          Mean value_function loss: 40.1777
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.4048
                       Mean reward: 856.54
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 171.2620
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.86s
                      Time elapsed: 00:24:41
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 112743 steps/s (collection: 0.768s, learning 0.104s)
             Mean action noise std: 6.07
          Mean value_function loss: 29.8897
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.4147
                       Mean reward: 868.22
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 173.1484
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.87s
                      Time elapsed: 00:24:42
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 108669 steps/s (collection: 0.789s, learning 0.116s)
             Mean action noise std: 6.07
          Mean value_function loss: 44.7697
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.4247
                       Mean reward: 874.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7899
     Episode_Reward/lifting_object: 174.3922
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.90s
                      Time elapsed: 00:24:43
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 109846 steps/s (collection: 0.804s, learning 0.091s)
             Mean action noise std: 6.08
          Mean value_function loss: 31.9875
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.4324
                       Mean reward: 876.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 172.3066
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.89s
                      Time elapsed: 00:24:44
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 107300 steps/s (collection: 0.794s, learning 0.123s)
             Mean action noise std: 6.09
          Mean value_function loss: 33.7167
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.4388
                       Mean reward: 871.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 174.0989
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.92s
                      Time elapsed: 00:24:45
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 110849 steps/s (collection: 0.793s, learning 0.094s)
             Mean action noise std: 6.09
          Mean value_function loss: 43.7157
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.4499
                       Mean reward: 884.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 174.1818
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.89s
                      Time elapsed: 00:24:46
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 109353 steps/s (collection: 0.779s, learning 0.120s)
             Mean action noise std: 6.10
          Mean value_function loss: 34.9002
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.4579
                       Mean reward: 862.00
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 172.8438
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.90s
                      Time elapsed: 00:24:47
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 105660 steps/s (collection: 0.766s, learning 0.164s)
             Mean action noise std: 6.11
          Mean value_function loss: 28.9196
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 25.4686
                       Mean reward: 857.04
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 173.4099
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.93s
                      Time elapsed: 00:24:48
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 114758 steps/s (collection: 0.759s, learning 0.097s)
             Mean action noise std: 6.11
          Mean value_function loss: 56.4620
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.4753
                       Mean reward: 859.90
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 172.4464
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.86s
                      Time elapsed: 00:24:49
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 110785 steps/s (collection: 0.766s, learning 0.122s)
             Mean action noise std: 6.11
          Mean value_function loss: 32.5939
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.4790
                       Mean reward: 864.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.9649
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.89s
                      Time elapsed: 00:24:49
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 116503 steps/s (collection: 0.751s, learning 0.093s)
             Mean action noise std: 6.12
          Mean value_function loss: 34.8434
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.4867
                       Mean reward: 864.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 171.3398
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.84s
                      Time elapsed: 00:24:50
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 114800 steps/s (collection: 0.768s, learning 0.089s)
             Mean action noise std: 6.13
          Mean value_function loss: 34.1769
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.4960
                       Mean reward: 864.35
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 171.1935
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.86s
                      Time elapsed: 00:24:51
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 116713 steps/s (collection: 0.754s, learning 0.089s)
             Mean action noise std: 6.13
          Mean value_function loss: 32.4092
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.5025
                       Mean reward: 876.14
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.0046
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.84s
                      Time elapsed: 00:24:52
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 113805 steps/s (collection: 0.773s, learning 0.091s)
             Mean action noise std: 6.14
          Mean value_function loss: 26.5644
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.5075
                       Mean reward: 864.16
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 173.2219
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.86s
                      Time elapsed: 00:24:53
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 114592 steps/s (collection: 0.755s, learning 0.103s)
             Mean action noise std: 6.14
          Mean value_function loss: 31.2298
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.5173
                       Mean reward: 865.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 173.3376
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.86s
                      Time elapsed: 00:24:54
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 107715 steps/s (collection: 0.780s, learning 0.133s)
             Mean action noise std: 6.15
          Mean value_function loss: 28.9491
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.5254
                       Mean reward: 857.51
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.2139
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.91s
                      Time elapsed: 00:24:55
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 115467 steps/s (collection: 0.756s, learning 0.095s)
             Mean action noise std: 6.15
          Mean value_function loss: 33.5194
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.5312
                       Mean reward: 853.38
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 172.1561
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.85s
                      Time elapsed: 00:24:55
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 116102 steps/s (collection: 0.754s, learning 0.093s)
             Mean action noise std: 6.16
          Mean value_function loss: 32.2091
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.5415
                       Mean reward: 866.36
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 171.9236
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.85s
                      Time elapsed: 00:24:56
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 108504 steps/s (collection: 0.797s, learning 0.109s)
             Mean action noise std: 6.17
          Mean value_function loss: 31.7754
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.5486
                       Mean reward: 877.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 172.6984
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.91s
                      Time elapsed: 00:24:57
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 110303 steps/s (collection: 0.782s, learning 0.109s)
             Mean action noise std: 6.17
          Mean value_function loss: 36.1096
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.5536
                       Mean reward: 881.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 172.8186
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.89s
                      Time elapsed: 00:24:58
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 110745 steps/s (collection: 0.776s, learning 0.112s)
             Mean action noise std: 6.18
          Mean value_function loss: 34.6870
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.5620
                       Mean reward: 866.25
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.7155
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.89s
                      Time elapsed: 00:24:59
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 112606 steps/s (collection: 0.781s, learning 0.092s)
             Mean action noise std: 6.19
          Mean value_function loss: 37.2264
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 25.5723
                       Mean reward: 869.34
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.0326
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.87s
                      Time elapsed: 00:25:00
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 110087 steps/s (collection: 0.787s, learning 0.106s)
             Mean action noise std: 6.19
          Mean value_function loss: 31.7848
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.5803
                       Mean reward: 878.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 174.0058
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.89s
                      Time elapsed: 00:25:01
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 108515 steps/s (collection: 0.798s, learning 0.108s)
             Mean action noise std: 6.19
          Mean value_function loss: 27.8606
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.5840
                       Mean reward: 850.78
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.7815
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.91s
                      Time elapsed: 00:25:02
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 109567 steps/s (collection: 0.764s, learning 0.134s)
             Mean action noise std: 6.20
          Mean value_function loss: 32.9517
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.5906
                       Mean reward: 875.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 173.1188
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.90s
                      Time elapsed: 00:25:03
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 115804 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 6.21
          Mean value_function loss: 30.6422
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.6013
                       Mean reward: 871.69
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 173.4941
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.85s
                      Time elapsed: 00:25:03
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 112821 steps/s (collection: 0.772s, learning 0.099s)
             Mean action noise std: 6.22
          Mean value_function loss: 29.7986
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.6129
                       Mean reward: 872.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 172.3118
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.87s
                      Time elapsed: 00:25:04
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 115682 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 6.22
          Mean value_function loss: 21.9510
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.6247
                       Mean reward: 858.49
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.3292
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.85s
                      Time elapsed: 00:25:05
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 112205 steps/s (collection: 0.781s, learning 0.095s)
             Mean action noise std: 6.23
          Mean value_function loss: 23.2032
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.6289
                       Mean reward: 876.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.8480
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.88s
                      Time elapsed: 00:25:06
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 114789 steps/s (collection: 0.768s, learning 0.088s)
             Mean action noise std: 6.23
          Mean value_function loss: 22.4376
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.6332
                       Mean reward: 850.91
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.5605
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.86s
                      Time elapsed: 00:25:07
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 112622 steps/s (collection: 0.760s, learning 0.113s)
             Mean action noise std: 6.24
          Mean value_function loss: 14.0854
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.6388
                       Mean reward: 868.21
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 171.6589
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.87s
                      Time elapsed: 00:25:08
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 109026 steps/s (collection: 0.766s, learning 0.136s)
             Mean action noise std: 6.24
          Mean value_function loss: 25.7029
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 25.6459
                       Mean reward: 873.30
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 173.2975
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.90s
                      Time elapsed: 00:25:09
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 112173 steps/s (collection: 0.757s, learning 0.120s)
             Mean action noise std: 6.25
          Mean value_function loss: 27.8375
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.6510
                       Mean reward: 877.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 173.7803
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.88s
                      Time elapsed: 00:25:09
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 108879 steps/s (collection: 0.776s, learning 0.127s)
             Mean action noise std: 6.26
          Mean value_function loss: 29.2511
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.6633
                       Mean reward: 863.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 172.7533
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.90s
                      Time elapsed: 00:25:10
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 114913 steps/s (collection: 0.754s, learning 0.101s)
             Mean action noise std: 6.27
          Mean value_function loss: 31.8007
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.6755
                       Mean reward: 881.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.0860
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.86s
                      Time elapsed: 00:25:11
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 109965 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 6.27
          Mean value_function loss: 25.8797
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.6877
                       Mean reward: 870.98
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 172.8330
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.89s
                      Time elapsed: 00:25:12
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 109256 steps/s (collection: 0.808s, learning 0.092s)
             Mean action noise std: 6.28
          Mean value_function loss: 28.5442
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.6995
                       Mean reward: 876.24
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 173.0719
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.90s
                      Time elapsed: 00:25:13
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 114364 steps/s (collection: 0.770s, learning 0.090s)
             Mean action noise std: 6.29
          Mean value_function loss: 29.6409
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.7069
                       Mean reward: 875.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 173.3291
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.86s
                      Time elapsed: 00:25:14
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 111361 steps/s (collection: 0.789s, learning 0.094s)
             Mean action noise std: 6.29
          Mean value_function loss: 35.7516
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.7149
                       Mean reward: 862.14
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7854
     Episode_Reward/lifting_object: 172.9183
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.88s
                      Time elapsed: 00:25:15
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 116009 steps/s (collection: 0.748s, learning 0.099s)
             Mean action noise std: 6.30
          Mean value_function loss: 26.2973
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.7231
                       Mean reward: 866.63
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 172.5773
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.85s
                      Time elapsed: 00:25:16
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 108947 steps/s (collection: 0.754s, learning 0.148s)
             Mean action noise std: 6.30
          Mean value_function loss: 28.7998
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.7273
                       Mean reward: 865.09
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.1893
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.90s
                      Time elapsed: 00:25:17
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 110624 steps/s (collection: 0.755s, learning 0.134s)
             Mean action noise std: 6.31
          Mean value_function loss: 24.6853
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.7357
                       Mean reward: 879.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 172.7063
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.89s
                      Time elapsed: 00:25:17
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 114526 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 6.31
          Mean value_function loss: 19.5967
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.7392
                       Mean reward: 872.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 173.3002
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.86s
                      Time elapsed: 00:25:18
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 110714 steps/s (collection: 0.789s, learning 0.099s)
             Mean action noise std: 6.31
          Mean value_function loss: 25.6804
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.7418
                       Mean reward: 880.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 173.8795
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.89s
                      Time elapsed: 00:25:19
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 110205 steps/s (collection: 0.790s, learning 0.102s)
             Mean action noise std: 6.32
          Mean value_function loss: 38.7708
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.7462
                       Mean reward: 869.41
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 172.5042
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 0.89s
                      Time elapsed: 00:25:20
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 112112 steps/s (collection: 0.767s, learning 0.110s)
             Mean action noise std: 6.32
          Mean value_function loss: 26.9137
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.7517
                       Mean reward: 857.66
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 172.6333
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.88s
                      Time elapsed: 00:25:21
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 109645 steps/s (collection: 0.783s, learning 0.114s)
             Mean action noise std: 6.33
          Mean value_function loss: 30.1898
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.7583
                       Mean reward: 872.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 172.7356
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.90s
                      Time elapsed: 00:25:22
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 110955 steps/s (collection: 0.777s, learning 0.109s)
             Mean action noise std: 6.33
          Mean value_function loss: 30.6100
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.7658
                       Mean reward: 864.64
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 172.6637
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.89s
                      Time elapsed: 00:25:23
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 108751 steps/s (collection: 0.778s, learning 0.126s)
             Mean action noise std: 6.34
          Mean value_function loss: 32.9984
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.7757
                       Mean reward: 875.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 173.9957
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.90s
                      Time elapsed: 00:25:24
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 100632 steps/s (collection: 0.814s, learning 0.163s)
             Mean action noise std: 6.35
          Mean value_function loss: 35.8156
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.7911
                       Mean reward: 874.96
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: 173.9702
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.98s
                      Time elapsed: 00:25:25
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 113596 steps/s (collection: 0.767s, learning 0.098s)
             Mean action noise std: 6.35
          Mean value_function loss: 25.5643
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.7980
                       Mean reward: 864.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 173.7743
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.87s
                      Time elapsed: 00:25:25
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 112814 steps/s (collection: 0.759s, learning 0.112s)
             Mean action noise std: 6.36
          Mean value_function loss: 27.6407
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.8064
                       Mean reward: 860.73
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.9302
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.87s
                      Time elapsed: 00:25:26
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 115059 steps/s (collection: 0.767s, learning 0.087s)
             Mean action noise std: 6.37
          Mean value_function loss: 30.1696
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.8165
                       Mean reward: 862.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 173.5812
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.85s
                      Time elapsed: 00:25:27
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 111450 steps/s (collection: 0.773s, learning 0.109s)
             Mean action noise std: 6.38
          Mean value_function loss: 28.3846
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.8272
                       Mean reward: 856.06
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 171.7781
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.88s
                      Time elapsed: 00:25:28
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 109230 steps/s (collection: 0.782s, learning 0.118s)
             Mean action noise std: 6.38
          Mean value_function loss: 38.9034
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.8389
                       Mean reward: 873.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7857
     Episode_Reward/lifting_object: 173.8690
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 0.90s
                      Time elapsed: 00:25:29
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 112355 steps/s (collection: 0.778s, learning 0.097s)
             Mean action noise std: 6.39
          Mean value_function loss: 42.4198
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.8453
                       Mean reward: 841.93
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 170.7135
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 0.87s
                      Time elapsed: 00:25:30
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 113135 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 6.39
          Mean value_function loss: 49.3755
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.8506
                       Mean reward: 873.86
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 173.3483
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.87s
                      Time elapsed: 00:25:31
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 113792 steps/s (collection: 0.761s, learning 0.103s)
             Mean action noise std: 6.39
          Mean value_function loss: 27.6150
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.8544
                       Mean reward: 869.77
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 172.0397
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.86s
                      Time elapsed: 00:25:32
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 113009 steps/s (collection: 0.762s, learning 0.108s)
             Mean action noise std: 6.40
          Mean value_function loss: 29.6024
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 25.8602
                       Mean reward: 862.05
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 171.6142
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.87s
                      Time elapsed: 00:25:32
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 118025 steps/s (collection: 0.746s, learning 0.087s)
             Mean action noise std: 6.40
          Mean value_function loss: 39.2985
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 25.8659
                       Mean reward: 865.86
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 172.7337
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.83s
                      Time elapsed: 00:25:33
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 111229 steps/s (collection: 0.778s, learning 0.106s)
             Mean action noise std: 6.40
          Mean value_function loss: 36.1880
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.8677
                       Mean reward: 861.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 171.1333
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.88s
                      Time elapsed: 00:25:34
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 113346 steps/s (collection: 0.746s, learning 0.121s)
             Mean action noise std: 6.41
          Mean value_function loss: 36.0374
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.8702
                       Mean reward: 871.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 172.5134
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.87s
                      Time elapsed: 00:25:35
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 109907 steps/s (collection: 0.790s, learning 0.104s)
             Mean action noise std: 6.42
          Mean value_function loss: 37.5787
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.8801
                       Mean reward: 870.20
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7871
     Episode_Reward/lifting_object: 173.3220
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.89s
                      Time elapsed: 00:25:36
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 114202 steps/s (collection: 0.764s, learning 0.097s)
             Mean action noise std: 6.42
          Mean value_function loss: 37.7234
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.8905
                       Mean reward: 832.16
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 171.2177
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.86s
                      Time elapsed: 00:25:37
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 111349 steps/s (collection: 0.781s, learning 0.102s)
             Mean action noise std: 6.43
          Mean value_function loss: 30.8909
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 25.8959
                       Mean reward: 875.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 172.8811
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 0.88s
                      Time elapsed: 00:25:38
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 115953 steps/s (collection: 0.762s, learning 0.086s)
             Mean action noise std: 6.44
          Mean value_function loss: 25.5213
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.9077
                       Mean reward: 866.93
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 171.0865
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.85s
                      Time elapsed: 00:25:39
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 113594 steps/s (collection: 0.764s, learning 0.102s)
             Mean action noise std: 6.45
          Mean value_function loss: 31.7056
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.9225
                       Mean reward: 851.85
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 172.8157
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.87s
                      Time elapsed: 00:25:39
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 104807 steps/s (collection: 0.779s, learning 0.159s)
             Mean action noise std: 6.46
          Mean value_function loss: 34.7707
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 25.9399
                       Mean reward: 867.01
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 171.6551
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.94s
                      Time elapsed: 00:25:40
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 95388 steps/s (collection: 0.875s, learning 0.156s)
             Mean action noise std: 6.47
          Mean value_function loss: 35.8762
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.9556
                       Mean reward: 862.27
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 172.3879
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 1.03s
                      Time elapsed: 00:25:41
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 94844 steps/s (collection: 0.859s, learning 0.178s)
             Mean action noise std: 6.48
          Mean value_function loss: 33.6037
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.9646
                       Mean reward: 862.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 170.9352
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 1.04s
                      Time elapsed: 00:25:42
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 103531 steps/s (collection: 0.838s, learning 0.111s)
             Mean action noise std: 6.49
          Mean value_function loss: 24.8847
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.9724
                       Mean reward: 869.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.1449
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 0.95s
                      Time elapsed: 00:25:43
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 115256 steps/s (collection: 0.756s, learning 0.097s)
             Mean action noise std: 6.49
          Mean value_function loss: 26.7116
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.9791
                       Mean reward: 863.13
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 172.3782
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 0.85s
                      Time elapsed: 00:25:44
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 111027 steps/s (collection: 0.793s, learning 0.092s)
             Mean action noise std: 6.50
          Mean value_function loss: 26.0142
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 25.9874
                       Mean reward: 883.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 173.0576
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.89s
                      Time elapsed: 00:25:45
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 116268 steps/s (collection: 0.758s, learning 0.087s)
             Mean action noise std: 6.51
          Mean value_function loss: 25.4039
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.9976
                       Mean reward: 889.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 175.1493
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.85s
                      Time elapsed: 00:25:46
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 109732 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 6.51
          Mean value_function loss: 33.8843
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.0029
                       Mean reward: 876.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 174.2506
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1155
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.90s
                      Time elapsed: 00:25:47
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 100363 steps/s (collection: 0.794s, learning 0.186s)
             Mean action noise std: 6.52
          Mean value_function loss: 27.0606
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.0109
                       Mean reward: 864.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 170.3690
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.98s
                      Time elapsed: 00:25:48
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 99973 steps/s (collection: 0.862s, learning 0.121s)
             Mean action noise std: 6.52
          Mean value_function loss: 24.9950
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 26.0175
                       Mean reward: 872.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 172.8065
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1165
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.98s
                      Time elapsed: 00:25:49
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 112042 steps/s (collection: 0.784s, learning 0.094s)
             Mean action noise std: 6.53
          Mean value_function loss: 29.4695
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.0220
                       Mean reward: 857.94
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 173.0111
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1167
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.88s
                      Time elapsed: 00:25:50
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 117615 steps/s (collection: 0.748s, learning 0.088s)
             Mean action noise std: 6.53
          Mean value_function loss: 27.6997
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.0308
                       Mean reward: 848.56
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.4714
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.84s
                      Time elapsed: 00:25:50
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 112226 steps/s (collection: 0.743s, learning 0.133s)
             Mean action noise std: 6.54
          Mean value_function loss: 23.7308
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.0389
                       Mean reward: 871.51
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 173.3111
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 0.88s
                      Time elapsed: 00:25:51
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 115011 steps/s (collection: 0.763s, learning 0.092s)
             Mean action noise std: 6.55
          Mean value_function loss: 20.3290
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.0543
                       Mean reward: 884.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 172.2146
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 0.85s
                      Time elapsed: 00:25:52
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 110768 steps/s (collection: 0.778s, learning 0.109s)
             Mean action noise std: 6.56
          Mean value_function loss: 28.5950
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 26.0641
                       Mean reward: 865.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 172.6648
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.89s
                      Time elapsed: 00:25:53
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 114191 steps/s (collection: 0.765s, learning 0.096s)
             Mean action noise std: 6.56
          Mean value_function loss: 29.0285
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.0729
                       Mean reward: 880.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 173.8187
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.86s
                      Time elapsed: 00:25:54
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 113546 steps/s (collection: 0.766s, learning 0.100s)
             Mean action noise std: 6.57
          Mean value_function loss: 24.0024
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.0806
                       Mean reward: 875.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.5774
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.87s
                      Time elapsed: 00:25:55
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 110301 steps/s (collection: 0.786s, learning 0.105s)
             Mean action noise std: 6.58
          Mean value_function loss: 25.6675
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.0915
                       Mean reward: 880.56
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 173.3233
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.89s
                      Time elapsed: 00:25:56
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 116209 steps/s (collection: 0.749s, learning 0.097s)
             Mean action noise std: 6.59
          Mean value_function loss: 27.5040
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.1032
                       Mean reward: 867.69
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.9242
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.85s
                      Time elapsed: 00:25:57
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 114324 steps/s (collection: 0.747s, learning 0.113s)
             Mean action noise std: 6.59
          Mean value_function loss: 22.6686
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.1095
                       Mean reward: 862.71
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 171.3318
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.86s
                      Time elapsed: 00:25:57
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 116313 steps/s (collection: 0.759s, learning 0.087s)
             Mean action noise std: 6.60
          Mean value_function loss: 25.4043
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.1214
                       Mean reward: 875.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 174.3538
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.85s
                      Time elapsed: 00:25:58
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 111830 steps/s (collection: 0.775s, learning 0.104s)
             Mean action noise std: 6.61
          Mean value_function loss: 25.0833
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.1285
                       Mean reward: 877.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 174.2187
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1174
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.88s
                      Time elapsed: 00:25:59
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 110385 steps/s (collection: 0.792s, learning 0.098s)
             Mean action noise std: 6.61
          Mean value_function loss: 20.5091
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.1354
                       Mean reward: 873.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: 174.2073
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.89s
                      Time elapsed: 00:26:00
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 112943 steps/s (collection: 0.774s, learning 0.096s)
             Mean action noise std: 6.62
          Mean value_function loss: 26.8373
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.1429
                       Mean reward: 869.34
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 173.2280
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 0.87s
                      Time elapsed: 00:26:01
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 113783 steps/s (collection: 0.771s, learning 0.093s)
             Mean action noise std: 6.63
          Mean value_function loss: 21.2568
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.1494
                       Mean reward: 870.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 173.3667
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.86s
                      Time elapsed: 00:26:02
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 108493 steps/s (collection: 0.801s, learning 0.106s)
             Mean action noise std: 6.63
          Mean value_function loss: 25.0148
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 26.1565
                       Mean reward: 873.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.3173
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.91s
                      Time elapsed: 00:26:03
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 107432 steps/s (collection: 0.756s, learning 0.159s)
             Mean action noise std: 6.64
          Mean value_function loss: 25.9470
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.1615
                       Mean reward: 866.03
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 172.9808
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.92s
                      Time elapsed: 00:26:04
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 101170 steps/s (collection: 0.852s, learning 0.120s)
             Mean action noise std: 6.64
          Mean value_function loss: 29.5350
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.1714
                       Mean reward: 867.30
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 173.4172
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.97s
                      Time elapsed: 00:26:05
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 111817 steps/s (collection: 0.757s, learning 0.122s)
             Mean action noise std: 6.65
          Mean value_function loss: 21.0597
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.1816
                       Mean reward: 859.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7816
     Episode_Reward/lifting_object: 173.0551
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.88s
                      Time elapsed: 00:26:05
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 113685 steps/s (collection: 0.756s, learning 0.109s)
             Mean action noise std: 6.66
          Mean value_function loss: 23.5332
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.1886
                       Mean reward: 870.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 174.8385
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 0.86s
                      Time elapsed: 00:26:06
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 113274 steps/s (collection: 0.779s, learning 0.089s)
             Mean action noise std: 6.67
          Mean value_function loss: 26.8043
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.1990
                       Mean reward: 881.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 173.8951
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 0.87s
                      Time elapsed: 00:26:07
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 111982 steps/s (collection: 0.786s, learning 0.092s)
             Mean action noise std: 6.68
          Mean value_function loss: 20.0028
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.2106
                       Mean reward: 875.54
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.5412
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.88s
                      Time elapsed: 00:26:08
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 110642 steps/s (collection: 0.787s, learning 0.101s)
             Mean action noise std: 6.69
          Mean value_function loss: 33.3315
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 26.2237
                       Mean reward: 867.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 174.0578
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 0.89s
                      Time elapsed: 00:26:09
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 111881 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 6.70
          Mean value_function loss: 28.5197
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.2324
                       Mean reward: 879.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 173.7549
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 0.88s
                      Time elapsed: 00:26:10
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 111985 steps/s (collection: 0.770s, learning 0.108s)
             Mean action noise std: 6.70
          Mean value_function loss: 23.9091
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.2450
                       Mean reward: 865.30
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 172.3021
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.88s
                      Time elapsed: 00:26:11
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 109396 steps/s (collection: 0.766s, learning 0.133s)
             Mean action noise std: 6.71
          Mean value_function loss: 32.7579
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.2522
                       Mean reward: 869.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 171.2284
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 0.90s
                      Time elapsed: 00:26:12
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 109479 steps/s (collection: 0.764s, learning 0.134s)
             Mean action noise std: 6.72
          Mean value_function loss: 21.1307
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.2642
                       Mean reward: 874.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 170.7726
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.90s
                      Time elapsed: 00:26:12
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 108500 steps/s (collection: 0.778s, learning 0.128s)
             Mean action noise std: 6.73
          Mean value_function loss: 37.6550
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.2785
                       Mean reward: 873.55
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 174.1921
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.91s
                      Time elapsed: 00:26:13
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 111090 steps/s (collection: 0.776s, learning 0.109s)
             Mean action noise std: 6.74
          Mean value_function loss: 28.3240
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.2894
                       Mean reward: 877.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 172.7642
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.88s
                      Time elapsed: 00:26:14
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 110838 steps/s (collection: 0.783s, learning 0.104s)
             Mean action noise std: 6.75
          Mean value_function loss: 22.5878
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.2979
                       Mean reward: 853.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 172.3566
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1192
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.89s
                      Time elapsed: 00:26:15
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 114200 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 6.75
          Mean value_function loss: 24.9307
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.3073
                       Mean reward: 874.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 173.9270
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.86s
                      Time elapsed: 00:26:16
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 112336 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 6.76
          Mean value_function loss: 29.9486
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.3150
                       Mean reward: 871.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7897
     Episode_Reward/lifting_object: 174.3606
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.88s
                      Time elapsed: 00:26:17
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 109379 steps/s (collection: 0.788s, learning 0.111s)
             Mean action noise std: 6.76
          Mean value_function loss: 30.1688
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.3240
                       Mean reward: 847.85
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 172.5114
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.90s
                      Time elapsed: 00:26:18
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 113145 steps/s (collection: 0.760s, learning 0.109s)
             Mean action noise std: 6.77
          Mean value_function loss: 29.5723
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 26.3291
                       Mean reward: 869.34
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 173.1946
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 0.87s
                      Time elapsed: 00:26:19
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 110638 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 6.77
          Mean value_function loss: 23.8753
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.3369
                       Mean reward: 881.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7917
     Episode_Reward/lifting_object: 174.5239
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1216
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 0.89s
                      Time elapsed: 00:26:20
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 110134 steps/s (collection: 0.781s, learning 0.112s)
             Mean action noise std: 6.78
          Mean value_function loss: 32.4787
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.3401
                       Mean reward: 848.15
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 172.4392
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 0.89s
                      Time elapsed: 00:26:20
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 107083 steps/s (collection: 0.779s, learning 0.139s)
             Mean action noise std: 6.78
          Mean value_function loss: 25.9070
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.3438
                       Mean reward: 871.36
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 172.2409
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1219
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 0.92s
                      Time elapsed: 00:26:21
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 110709 steps/s (collection: 0.782s, learning 0.106s)
             Mean action noise std: 6.79
          Mean value_function loss: 20.7841
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.3497
                       Mean reward: 868.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.0408
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 0.89s
                      Time elapsed: 00:26:22
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 110979 steps/s (collection: 0.776s, learning 0.110s)
             Mean action noise std: 6.79
          Mean value_function loss: 20.3356
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.3556
                       Mean reward: 866.34
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7899
     Episode_Reward/lifting_object: 173.8347
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 0.89s
                      Time elapsed: 00:26:23
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 99805 steps/s (collection: 0.844s, learning 0.141s)
             Mean action noise std: 6.80
          Mean value_function loss: 19.6868
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.3641
                       Mean reward: 862.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 171.4559
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.98s
                      Time elapsed: 00:26:24
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 109361 steps/s (collection: 0.801s, learning 0.098s)
             Mean action noise std: 6.80
          Mean value_function loss: 26.7725
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.3730
                       Mean reward: 857.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 173.1635
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 0.90s
                      Time elapsed: 00:26:25
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 109551 steps/s (collection: 0.787s, learning 0.111s)
             Mean action noise std: 6.81
          Mean value_function loss: 20.2333
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.3760
                       Mean reward: 876.57
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7860
     Episode_Reward/lifting_object: 172.9651
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1223
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 0.90s
                      Time elapsed: 00:26:26
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 108939 steps/s (collection: 0.801s, learning 0.102s)
             Mean action noise std: 6.81
          Mean value_function loss: 17.7631
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 26.3794
                       Mean reward: 876.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7875
     Episode_Reward/lifting_object: 172.8040
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1232
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 0.90s
                      Time elapsed: 00:26:27
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 101009 steps/s (collection: 0.828s, learning 0.146s)
             Mean action noise std: 6.81
          Mean value_function loss: 23.5242
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.3825
                       Mean reward: 874.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7881
     Episode_Reward/lifting_object: 173.4469
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1234
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.97s
                      Time elapsed: 00:26:28
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 105555 steps/s (collection: 0.827s, learning 0.105s)
             Mean action noise std: 6.82
          Mean value_function loss: 26.3849
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.3867
                       Mean reward: 875.67
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7883
     Episode_Reward/lifting_object: 173.5422
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 0.93s
                      Time elapsed: 00:26:29
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 106890 steps/s (collection: 0.824s, learning 0.096s)
             Mean action noise std: 6.82
          Mean value_function loss: 23.9547
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.3913
                       Mean reward: 884.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 175.1570
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.92s
                      Time elapsed: 00:26:30
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 111171 steps/s (collection: 0.773s, learning 0.111s)
             Mean action noise std: 6.83
          Mean value_function loss: 21.5155
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.3986
                       Mean reward: 861.28
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 173.4550
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1255
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.88s
                      Time elapsed: 00:26:31
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 103519 steps/s (collection: 0.802s, learning 0.147s)
             Mean action noise std: 6.84
          Mean value_function loss: 24.7913
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 26.4106
                       Mean reward: 868.89
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 173.3737
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.95s
                      Time elapsed: 00:26:31
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 105678 steps/s (collection: 0.779s, learning 0.151s)
             Mean action noise std: 6.85
          Mean value_function loss: 17.9254
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.4224
                       Mean reward: 873.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 174.0342
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1269
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 0.93s
                      Time elapsed: 00:26:32
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 108263 steps/s (collection: 0.779s, learning 0.129s)
             Mean action noise std: 6.86
          Mean value_function loss: 21.1594
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.4320
                       Mean reward: 877.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 173.4916
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1271
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 0.91s
                      Time elapsed: 00:26:33
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 112626 steps/s (collection: 0.768s, learning 0.105s)
             Mean action noise std: 6.86
          Mean value_function loss: 24.8438
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.4375
                       Mean reward: 877.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 173.1781
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 0.87s
                      Time elapsed: 00:26:34
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 101677 steps/s (collection: 0.853s, learning 0.114s)
             Mean action noise std: 6.87
          Mean value_function loss: 18.2583
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.4439
                       Mean reward: 869.93
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7950
     Episode_Reward/lifting_object: 175.2679
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 0.97s
                      Time elapsed: 00:26:35
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 108030 steps/s (collection: 0.806s, learning 0.104s)
             Mean action noise std: 6.87
          Mean value_function loss: 21.0274
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.4477
                       Mean reward: 874.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 175.2197
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 0.91s
                      Time elapsed: 00:26:36
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 109245 steps/s (collection: 0.811s, learning 0.089s)
             Mean action noise std: 6.87
          Mean value_function loss: 21.8516
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.4515
                       Mean reward: 875.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 173.4952
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1292
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 0.90s
                      Time elapsed: 00:26:37
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 107809 steps/s (collection: 0.821s, learning 0.091s)
             Mean action noise std: 6.88
          Mean value_function loss: 26.6143
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 26.4554
                       Mean reward: 863.20
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 172.3970
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1297
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 0.91s
                      Time elapsed: 00:26:38
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 107337 steps/s (collection: 0.816s, learning 0.100s)
             Mean action noise std: 6.88
          Mean value_function loss: 23.7553
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.4622
                       Mean reward: 867.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7927
     Episode_Reward/lifting_object: 175.4244
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1290
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 0.92s
                      Time elapsed: 00:26:39
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 104036 steps/s (collection: 0.836s, learning 0.109s)
             Mean action noise std: 6.89
          Mean value_function loss: 18.6653
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.4696
                       Mean reward: 875.71
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 174.9265
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1294
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 0.94s
                      Time elapsed: 00:26:40
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 107115 steps/s (collection: 0.810s, learning 0.107s)
             Mean action noise std: 6.90
          Mean value_function loss: 25.3208
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.4773
                       Mean reward: 864.67
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 174.6231
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 0.92s
                      Time elapsed: 00:26:41
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 107283 steps/s (collection: 0.804s, learning 0.112s)
             Mean action noise std: 6.90
          Mean value_function loss: 27.5794
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 26.4869
                       Mean reward: 876.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 172.5698
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 0.92s
                      Time elapsed: 00:26:42
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 114680 steps/s (collection: 0.756s, learning 0.102s)
             Mean action noise std: 6.91
          Mean value_function loss: 16.3585
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.4936
                       Mean reward: 877.63
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 173.6683
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 0.86s
                      Time elapsed: 00:26:42
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 105133 steps/s (collection: 0.778s, learning 0.157s)
             Mean action noise std: 6.91
          Mean value_function loss: 23.8376
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 26.4996
                       Mean reward: 876.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7894
     Episode_Reward/lifting_object: 173.6695
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 0.94s
                      Time elapsed: 00:26:43
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 106774 steps/s (collection: 0.789s, learning 0.132s)
             Mean action noise std: 6.92
          Mean value_function loss: 26.4815
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.5045
                       Mean reward: 870.14
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: 174.6980
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 0.92s
                      Time elapsed: 00:26:44
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 111169 steps/s (collection: 0.790s, learning 0.095s)
             Mean action noise std: 6.92
          Mean value_function loss: 25.8078
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.5128
                       Mean reward: 877.75
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 173.6278
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1305
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 0.88s
                      Time elapsed: 00:26:45
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 106764 steps/s (collection: 0.819s, learning 0.101s)
             Mean action noise std: 6.93
          Mean value_function loss: 18.4256
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.5220
                       Mean reward: 869.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 173.4290
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1308
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 0.92s
                      Time elapsed: 00:26:46
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 110349 steps/s (collection: 0.780s, learning 0.111s)
             Mean action noise std: 6.94
          Mean value_function loss: 21.1641
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.5311
                       Mean reward: 861.95
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 173.5393
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 0.89s
                      Time elapsed: 00:26:47
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 106694 steps/s (collection: 0.803s, learning 0.118s)
             Mean action noise std: 6.94
          Mean value_function loss: 19.7007
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.5392
                       Mean reward: 872.78
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7874
     Episode_Reward/lifting_object: 174.3655
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1312
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 0.92s
                      Time elapsed: 00:26:48
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 108396 steps/s (collection: 0.810s, learning 0.097s)
             Mean action noise std: 6.95
          Mean value_function loss: 23.1735
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.5440
                       Mean reward: 874.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 174.3165
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1313
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 0.91s
                      Time elapsed: 00:26:49
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 107897 steps/s (collection: 0.821s, learning 0.090s)
             Mean action noise std: 6.95
          Mean value_function loss: 24.9230
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.5467
                       Mean reward: 869.80
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7854
     Episode_Reward/lifting_object: 173.1147
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1308
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 0.91s
                      Time elapsed: 00:26:50
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 106498 steps/s (collection: 0.830s, learning 0.094s)
             Mean action noise std: 6.96
          Mean value_function loss: 23.0764
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.5525
                       Mean reward: 872.83
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 173.6054
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 0.92s
                      Time elapsed: 00:26:51
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 109497 steps/s (collection: 0.775s, learning 0.123s)
             Mean action noise std: 6.97
          Mean value_function loss: 29.9048
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.5615
                       Mean reward: 870.17
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 173.0034
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1311
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 0.90s
                      Time elapsed: 00:26:52
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 106299 steps/s (collection: 0.805s, learning 0.120s)
             Mean action noise std: 6.98
          Mean value_function loss: 25.9618
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.5746
                       Mean reward: 881.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 173.0482
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.1309
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 0.92s
                      Time elapsed: 00:26:52
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 109627 steps/s (collection: 0.773s, learning 0.124s)
             Mean action noise std: 6.98
          Mean value_function loss: 35.2171
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.5841
                       Mean reward: 876.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 173.6701
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 0.90s
                      Time elapsed: 00:26:53
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 107278 steps/s (collection: 0.774s, learning 0.143s)
             Mean action noise std: 6.99
          Mean value_function loss: 31.8472
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 26.5920
                       Mean reward: 878.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 174.6607
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1308
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 0.92s
                      Time elapsed: 00:26:54
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 102037 steps/s (collection: 0.819s, learning 0.144s)
             Mean action noise std: 7.00
          Mean value_function loss: 40.4833
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.6007
                       Mean reward: 872.02
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 174.0576
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1307
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 0.96s
                      Time elapsed: 00:26:55
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 112624 steps/s (collection: 0.774s, learning 0.099s)
             Mean action noise std: 7.01
          Mean value_function loss: 30.8561
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.6094
                       Mean reward: 863.96
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 172.7937
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 0.87s
                      Time elapsed: 00:26:56
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 107927 steps/s (collection: 0.798s, learning 0.113s)
             Mean action noise std: 7.01
          Mean value_function loss: 27.8973
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.6171
                       Mean reward: 867.79
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 172.7418
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 0.91s
                      Time elapsed: 00:26:57
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 112306 steps/s (collection: 0.784s, learning 0.091s)
             Mean action noise std: 7.02
          Mean value_function loss: 25.7860
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.6244
                       Mean reward: 879.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 174.1827
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1296
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 0.88s
                      Time elapsed: 00:26:58
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 106243 steps/s (collection: 0.810s, learning 0.115s)
             Mean action noise std: 7.02
          Mean value_function loss: 30.0837
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.6279
                       Mean reward: 876.35
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7911
     Episode_Reward/lifting_object: 174.8564
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 0.93s
                      Time elapsed: 00:26:59
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 107809 steps/s (collection: 0.817s, learning 0.095s)
             Mean action noise std: 7.03
          Mean value_function loss: 29.0903
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 26.6327
                       Mean reward: 860.96
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 172.4321
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1301
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 0.91s
                      Time elapsed: 00:27:00
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 108378 steps/s (collection: 0.818s, learning 0.089s)
             Mean action noise std: 7.04
          Mean value_function loss: 38.4305
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.6436
                       Mean reward: 853.18
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 173.9825
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1294
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 0.91s
                      Time elapsed: 00:27:01
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 112485 steps/s (collection: 0.776s, learning 0.098s)
             Mean action noise std: 7.05
          Mean value_function loss: 37.3320
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.6513
                       Mean reward: 869.70
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 173.9609
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 0.87s
                      Time elapsed: 00:27:02
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 100853 steps/s (collection: 0.803s, learning 0.171s)
             Mean action noise std: 7.06
          Mean value_function loss: 31.1516
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 26.6632
                       Mean reward: 865.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.7081
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 0.97s
                      Time elapsed: 00:27:03
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 103489 steps/s (collection: 0.819s, learning 0.131s)
             Mean action noise std: 7.06
          Mean value_function loss: 26.5474
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 26.6692
                       Mean reward: 853.65
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 173.4516
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 0.95s
                      Time elapsed: 00:27:03
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 105567 steps/s (collection: 0.788s, learning 0.143s)
             Mean action noise std: 7.06
          Mean value_function loss: 23.2621
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.6732
                       Mean reward: 879.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 174.5038
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 0.93s
                      Time elapsed: 00:27:04
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 104857 steps/s (collection: 0.807s, learning 0.130s)
             Mean action noise std: 7.07
          Mean value_function loss: 26.6996
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.6849
                       Mean reward: 860.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7880
     Episode_Reward/lifting_object: 173.4806
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 0.94s
                      Time elapsed: 00:27:05
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 105345 steps/s (collection: 0.801s, learning 0.132s)
             Mean action noise std: 7.08
          Mean value_function loss: 23.8523
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.6916
                       Mean reward: 875.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 173.6355
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 0.93s
                      Time elapsed: 00:27:06
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 110429 steps/s (collection: 0.785s, learning 0.105s)
             Mean action noise std: 7.09
          Mean value_function loss: 26.3419
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.6974
                       Mean reward: 855.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 171.6433
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 0.89s
                      Time elapsed: 00:27:07
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 104155 steps/s (collection: 0.831s, learning 0.113s)
             Mean action noise std: 7.09
          Mean value_function loss: 29.5127
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.7061
                       Mean reward: 875.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 171.9002
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 0.94s
                      Time elapsed: 00:27:08
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 110744 steps/s (collection: 0.791s, learning 0.097s)
             Mean action noise std: 7.10
          Mean value_function loss: 33.7475
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 26.7111
                       Mean reward: 863.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.4836
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 0.89s
                      Time elapsed: 00:27:09
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 105674 steps/s (collection: 0.810s, learning 0.121s)
             Mean action noise std: 7.10
          Mean value_function loss: 22.5908
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.7146
                       Mean reward: 874.62
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 172.6797
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 0.93s
                      Time elapsed: 00:27:10
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 109749 steps/s (collection: 0.805s, learning 0.091s)
             Mean action noise std: 7.11
          Mean value_function loss: 26.8627
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.7228
                       Mean reward: 862.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 172.0473
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 0.90s
                      Time elapsed: 00:27:11
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 108764 steps/s (collection: 0.796s, learning 0.108s)
             Mean action noise std: 7.11
          Mean value_function loss: 22.9588
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.7314
                       Mean reward: 865.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 171.4960
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 0.90s
                      Time elapsed: 00:27:12
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 106365 steps/s (collection: 0.807s, learning 0.117s)
             Mean action noise std: 7.12
          Mean value_function loss: 24.8644
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.7377
                       Mean reward: 870.98
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 173.8212
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1309
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 0.92s
                      Time elapsed: 00:27:13
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 105840 steps/s (collection: 0.803s, learning 0.126s)
             Mean action noise std: 7.13
          Mean value_function loss: 28.7110
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.7461
                       Mean reward: 872.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 173.4161
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1315
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 0.93s
                      Time elapsed: 00:27:14
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 101925 steps/s (collection: 0.845s, learning 0.119s)
             Mean action noise std: 7.14
          Mean value_function loss: 25.2804
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.7543
                       Mean reward: 864.50
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7859
     Episode_Reward/lifting_object: 173.4834
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1315
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 0.96s
                      Time elapsed: 00:27:15
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 101066 steps/s (collection: 0.821s, learning 0.152s)
             Mean action noise std: 7.14
          Mean value_function loss: 37.0934
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.7599
                       Mean reward: 861.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 173.1510
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1319
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 0.97s
                      Time elapsed: 00:27:15
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 101131 steps/s (collection: 0.822s, learning 0.150s)
             Mean action noise std: 7.15
          Mean value_function loss: 28.5918
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.7662
                       Mean reward: 880.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 172.8207
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1322
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 0.97s
                      Time elapsed: 00:27:16
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 109846 steps/s (collection: 0.795s, learning 0.100s)
             Mean action noise std: 7.15
          Mean value_function loss: 31.3653
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 26.7736
                       Mean reward: 854.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 173.3617
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 0.89s
                      Time elapsed: 00:27:17
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 107762 steps/s (collection: 0.786s, learning 0.127s)
             Mean action noise std: 7.16
          Mean value_function loss: 38.0341
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.7777
                       Mean reward: 867.69
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 173.1520
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1314
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 0.91s
                      Time elapsed: 00:27:18
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 104695 steps/s (collection: 0.821s, learning 0.118s)
             Mean action noise std: 7.16
          Mean value_function loss: 25.6815
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.7843
                       Mean reward: 853.52
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 172.6465
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1313
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 0.94s
                      Time elapsed: 00:27:19
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 111190 steps/s (collection: 0.788s, learning 0.096s)
             Mean action noise std: 7.16
          Mean value_function loss: 28.3815
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.7878
                       Mean reward: 863.43
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.2439
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1329
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 0.88s
                      Time elapsed: 00:27:20
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 106571 steps/s (collection: 0.823s, learning 0.099s)
             Mean action noise std: 7.17
          Mean value_function loss: 30.0060
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.7929
                       Mean reward: 854.29
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 172.2988
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1331
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 0.92s
                      Time elapsed: 00:27:21
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 112015 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 7.18
          Mean value_function loss: 37.1585
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.8033
                       Mean reward: 880.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 172.4754
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 0.88s
                      Time elapsed: 00:27:22
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 95020 steps/s (collection: 0.838s, learning 0.196s)
             Mean action noise std: 7.19
          Mean value_function loss: 36.4562
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.8160
                       Mean reward: 833.01
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 169.0525
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1322
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 1.03s
                      Time elapsed: 00:27:23
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 94283 steps/s (collection: 0.921s, learning 0.122s)
             Mean action noise std: 7.20
          Mean value_function loss: 34.8559
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.8227
                       Mean reward: 870.98
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 172.7274
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 1.04s
                      Time elapsed: 00:27:24
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 101703 steps/s (collection: 0.854s, learning 0.112s)
             Mean action noise std: 7.20
          Mean value_function loss: 31.0915
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.8292
                       Mean reward: 870.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 172.8971
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 0.97s
                      Time elapsed: 00:27:25
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 105141 steps/s (collection: 0.832s, learning 0.103s)
             Mean action noise std: 7.21
          Mean value_function loss: 32.2139
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.8354
                       Mean reward: 851.16
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 173.2812
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.1337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 0.93s
                      Time elapsed: 00:27:26
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 108707 steps/s (collection: 0.801s, learning 0.104s)
             Mean action noise std: 7.22
          Mean value_function loss: 29.8454
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.8430
                       Mean reward: 884.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7880
     Episode_Reward/lifting_object: 174.8733
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1348
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 0.90s
                      Time elapsed: 00:27:27
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 107677 steps/s (collection: 0.817s, learning 0.096s)
             Mean action noise std: 7.22
          Mean value_function loss: 37.6892
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 26.8502
                       Mean reward: 867.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 172.6442
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 0.91s
                      Time elapsed: 00:27:28
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 106325 steps/s (collection: 0.825s, learning 0.100s)
             Mean action noise std: 7.22
          Mean value_function loss: 33.7872
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.8522
                       Mean reward: 863.26
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.5467
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 0.92s
                      Time elapsed: 00:27:29
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 103213 steps/s (collection: 0.853s, learning 0.100s)
             Mean action noise std: 7.22
          Mean value_function loss: 34.4315
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.8568
                       Mean reward: 858.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 172.7682
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1351
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 0.95s
                      Time elapsed: 00:27:30
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 105097 steps/s (collection: 0.803s, learning 0.133s)
             Mean action noise std: 7.23
          Mean value_function loss: 32.7450
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.8624
                       Mean reward: 869.40
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.8764
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1357
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 0.94s
                      Time elapsed: 00:27:31
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 103377 steps/s (collection: 0.798s, learning 0.153s)
             Mean action noise std: 7.23
          Mean value_function loss: 39.5889
               Mean surrogate loss: 0.0154
                 Mean entropy loss: 26.8701
                       Mean reward: 877.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 172.9711
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.1372
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 0.95s
                      Time elapsed: 00:27:31
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 104757 steps/s (collection: 0.807s, learning 0.131s)
             Mean action noise std: 7.24
          Mean value_function loss: 32.7731
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.8730
                       Mean reward: 865.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 171.0376
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 0.94s
                      Time elapsed: 00:27:32
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 103835 steps/s (collection: 0.820s, learning 0.127s)
             Mean action noise std: 7.24
          Mean value_function loss: 39.9176
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.8757
                       Mean reward: 863.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 171.4683
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 0.95s
                      Time elapsed: 00:27:33
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 107571 steps/s (collection: 0.810s, learning 0.104s)
             Mean action noise std: 7.24
          Mean value_function loss: 43.5574
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.8785
                       Mean reward: 869.40
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 173.6275
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1372
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 0.91s
                      Time elapsed: 00:27:34
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 105054 steps/s (collection: 0.795s, learning 0.141s)
             Mean action noise std: 7.25
          Mean value_function loss: 42.0532
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.8826
                       Mean reward: 867.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.4654
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.1381
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 0.94s
                      Time elapsed: 00:27:35
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 103447 steps/s (collection: 0.834s, learning 0.116s)
             Mean action noise std: 7.26
          Mean value_function loss: 38.9393
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.8913
                       Mean reward: 866.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 172.4116
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1384
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 0.95s
                      Time elapsed: 00:27:36
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 109725 steps/s (collection: 0.803s, learning 0.093s)
             Mean action noise std: 7.26
          Mean value_function loss: 43.9215
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.8993
                       Mean reward: 850.51
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 171.3983
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1388
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 0.90s
                      Time elapsed: 00:27:37
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 108378 steps/s (collection: 0.815s, learning 0.092s)
             Mean action noise std: 7.27
          Mean value_function loss: 39.9863
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.9034
                       Mean reward: 874.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 174.2424
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1394
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 0.91s
                      Time elapsed: 00:27:38
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 107917 steps/s (collection: 0.808s, learning 0.103s)
             Mean action noise std: 7.27
          Mean value_function loss: 43.1594
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.9097
                       Mean reward: 884.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 173.5509
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1389
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 0.91s
                      Time elapsed: 00:27:39
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 108337 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 7.28
          Mean value_function loss: 39.9595
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.9208
                       Mean reward: 871.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 172.6902
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1393
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 0.91s
                      Time elapsed: 00:27:40
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 107501 steps/s (collection: 0.811s, learning 0.103s)
             Mean action noise std: 7.29
          Mean value_function loss: 51.2667
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.9291
                       Mean reward: 836.36
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 169.4670
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1394
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 0.91s
                      Time elapsed: 00:27:41
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 108241 steps/s (collection: 0.800s, learning 0.108s)
             Mean action noise std: 7.29
          Mean value_function loss: 47.2103
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.9328
                       Mean reward: 846.85
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 168.4530
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1387
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 0.91s
                      Time elapsed: 00:27:42
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 108342 steps/s (collection: 0.787s, learning 0.120s)
             Mean action noise std: 7.30
          Mean value_function loss: 51.9490
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.9405
                       Mean reward: 864.01
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.0855
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1382
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 0.91s
                      Time elapsed: 00:27:42
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 107991 steps/s (collection: 0.794s, learning 0.116s)
             Mean action noise std: 7.30
          Mean value_function loss: 51.7195
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 26.9468
                       Mean reward: 848.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 169.7930
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1396
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 0.91s
                      Time elapsed: 00:27:43
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 104468 steps/s (collection: 0.810s, learning 0.131s)
             Mean action noise std: 7.31
          Mean value_function loss: 33.1409
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.9518
                       Mean reward: 869.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.6669
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.1391
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 0.94s
                      Time elapsed: 00:27:44
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 107041 steps/s (collection: 0.800s, learning 0.119s)
             Mean action noise std: 7.31
          Mean value_function loss: 48.6486
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 26.9596
                       Mean reward: 842.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 168.7669
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 0.92s
                      Time elapsed: 00:27:45
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 106829 steps/s (collection: 0.786s, learning 0.134s)
             Mean action noise std: 7.31
          Mean value_function loss: 35.0990
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.9605
                       Mean reward: 852.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 169.8813
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 0.92s
                      Time elapsed: 00:27:46
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 106948 steps/s (collection: 0.824s, learning 0.095s)
             Mean action noise std: 7.32
          Mean value_function loss: 33.5217
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 26.9637
                       Mean reward: 851.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 170.1069
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1376
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 0.92s
                      Time elapsed: 00:27:47
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 110110 steps/s (collection: 0.787s, learning 0.105s)
             Mean action noise std: 7.33
          Mean value_function loss: 38.3399
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.9728
                       Mean reward: 858.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 170.5041
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1383
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 0.89s
                      Time elapsed: 00:27:48
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 105474 steps/s (collection: 0.817s, learning 0.115s)
             Mean action noise std: 7.33
          Mean value_function loss: 28.5016
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.9805
                       Mean reward: 859.16
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 171.0573
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 0.93s
                      Time elapsed: 00:27:49
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 106361 steps/s (collection: 0.817s, learning 0.108s)
             Mean action noise std: 7.34
          Mean value_function loss: 28.0727
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.9867
                       Mean reward: 858.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.0065
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1374
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 0.92s
                      Time elapsed: 00:27:50
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 60160 steps/s (collection: 1.536s, learning 0.098s)
             Mean action noise std: 7.35
          Mean value_function loss: 23.2852
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.9979
                       Mean reward: 842.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 172.6097
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.1390
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 1.63s
                      Time elapsed: 00:27:51
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 32824 steps/s (collection: 2.877s, learning 0.118s)
             Mean action noise std: 7.36
          Mean value_function loss: 23.9667
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.0066
                       Mean reward: 858.01
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 171.6149
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1387
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 2.99s
                      Time elapsed: 00:27:54
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 31189 steps/s (collection: 3.021s, learning 0.131s)
             Mean action noise std: 7.36
          Mean value_function loss: 37.8609
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.0138
                       Mean reward: 876.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 173.1709
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1396
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 3.15s
                      Time elapsed: 00:27:58
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 30299 steps/s (collection: 3.112s, learning 0.132s)
             Mean action noise std: 7.37
          Mean value_function loss: 35.2684
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.0216
                       Mean reward: 869.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 173.0215
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.1406
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 3.24s
                      Time elapsed: 00:28:01
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 30789 steps/s (collection: 3.067s, learning 0.126s)
             Mean action noise std: 7.38
          Mean value_function loss: 28.6093
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.0290
                       Mean reward: 848.18
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 171.4859
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1405
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 3.19s
                      Time elapsed: 00:28:04
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 31086 steps/s (collection: 3.033s, learning 0.129s)
             Mean action noise std: 7.38
          Mean value_function loss: 34.8111
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.0358
                       Mean reward: 880.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 173.1864
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1417
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 3.16s
                      Time elapsed: 00:28:07
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 29368 steps/s (collection: 3.202s, learning 0.145s)
             Mean action noise std: 7.39
          Mean value_function loss: 33.2200
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.0421
                       Mean reward: 861.26
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 172.9063
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1420
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 3.35s
                      Time elapsed: 00:28:11
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 29940 steps/s (collection: 3.159s, learning 0.125s)
             Mean action noise std: 7.40
          Mean value_function loss: 37.9170
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.0509
                       Mean reward: 863.21
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 173.3833
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.1425
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 3.28s
                      Time elapsed: 00:28:14
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 32893 steps/s (collection: 2.875s, learning 0.113s)
             Mean action noise std: 7.40
          Mean value_function loss: 24.4991
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 27.0583
                       Mean reward: 861.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 172.9970
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.1432
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 2.99s
                      Time elapsed: 00:28:17
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 29647 steps/s (collection: 3.147s, learning 0.169s)
             Mean action noise std: 7.41
          Mean value_function loss: 33.5707
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.0659
                       Mean reward: 877.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.6623
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.1436
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 3.32s
                      Time elapsed: 00:28:20
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 92419 steps/s (collection: 0.905s, learning 0.159s)
             Mean action noise std: 7.42
          Mean value_function loss: 20.2144
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.0733
                       Mean reward: 870.80
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 173.1798
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.1428
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.06s
                      Time elapsed: 00:28:21
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 101663 steps/s (collection: 0.856s, learning 0.111s)
             Mean action noise std: 7.42
          Mean value_function loss: 31.9106
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.0781
                       Mean reward: 865.55
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.8375
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.1430
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 0.97s
                      Time elapsed: 00:28:22
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 95254 steps/s (collection: 0.858s, learning 0.174s)
             Mean action noise std: 7.43
          Mean value_function loss: 22.6065
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.0832
                       Mean reward: 867.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.0275
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.1435
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 1.03s
                      Time elapsed: 00:28:23
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 102509 steps/s (collection: 0.839s, learning 0.120s)
             Mean action noise std: 7.43
          Mean value_function loss: 17.6680
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.0892
                       Mean reward: 867.58
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 173.6820
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.1444
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 0.96s
                      Time elapsed: 00:28:24
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 106223 steps/s (collection: 0.831s, learning 0.094s)
             Mean action noise std: 7.44
          Mean value_function loss: 25.2805
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.1007
                       Mean reward: 868.49
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 173.5275
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.1444
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 0.93s
                      Time elapsed: 00:28:25
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 108471 steps/s (collection: 0.803s, learning 0.103s)
             Mean action noise std: 7.45
          Mean value_function loss: 22.6868
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 27.1118
                       Mean reward: 868.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.1937
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.1452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 0.91s
                      Time elapsed: 00:28:26
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 107295 steps/s (collection: 0.820s, learning 0.096s)
             Mean action noise std: 7.46
          Mean value_function loss: 19.6958
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.1185
                       Mean reward: 856.37
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 171.8238
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1451
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 0.92s
                      Time elapsed: 00:28:27
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 110047 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 7.47
          Mean value_function loss: 33.8662
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.1256
                       Mean reward: 872.82
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 173.7156
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 0.89s
                      Time elapsed: 00:28:28
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 103701 steps/s (collection: 0.848s, learning 0.100s)
             Mean action noise std: 7.47
          Mean value_function loss: 36.4874
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.1351
                       Mean reward: 877.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: 173.8725
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1467
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 0.95s
                      Time elapsed: 00:28:29
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 113933 steps/s (collection: 0.765s, learning 0.098s)
             Mean action noise std: 7.48
          Mean value_function loss: 27.0654
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.1420
                       Mean reward: 873.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 173.4732
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1473
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 0.86s
                      Time elapsed: 00:28:30
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 106149 steps/s (collection: 0.794s, learning 0.132s)
             Mean action noise std: 7.48
          Mean value_function loss: 31.6037
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.1469
                       Mean reward: 870.10
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 173.4726
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1480
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 0.93s
                      Time elapsed: 00:28:31
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 106389 steps/s (collection: 0.802s, learning 0.122s)
             Mean action noise std: 7.49
          Mean value_function loss: 25.5293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.1531
                       Mean reward: 883.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 173.6704
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1483
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 0.92s
                      Time elapsed: 00:28:31
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 106028 steps/s (collection: 0.787s, learning 0.140s)
             Mean action noise std: 7.50
          Mean value_function loss: 28.5642
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.1611
                       Mean reward: 883.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 175.0465
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1507
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 0.93s
                      Time elapsed: 00:28:32
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 103922 steps/s (collection: 0.833s, learning 0.113s)
             Mean action noise std: 7.50
          Mean value_function loss: 16.7690
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.1675
                       Mean reward: 874.05
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7918
     Episode_Reward/lifting_object: 175.0496
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.1512
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 0.95s
                      Time elapsed: 00:28:33
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 106184 steps/s (collection: 0.800s, learning 0.126s)
             Mean action noise std: 7.51
          Mean value_function loss: 22.6003
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 27.1765
                       Mean reward: 865.55
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 174.0812
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1508
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 0.93s
                      Time elapsed: 00:28:34
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 104108 steps/s (collection: 0.838s, learning 0.106s)
             Mean action noise std: 7.52
          Mean value_function loss: 21.2430
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 27.1825
                       Mean reward: 874.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 174.7136
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.1518
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 0.94s
                      Time elapsed: 00:28:35
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 108475 steps/s (collection: 0.789s, learning 0.118s)
             Mean action noise std: 7.52
          Mean value_function loss: 28.4890
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.1844
                       Mean reward: 863.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.1138
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.1519
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 0.91s
                      Time elapsed: 00:28:36
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 106463 steps/s (collection: 0.812s, learning 0.111s)
             Mean action noise std: 7.52
          Mean value_function loss: 31.6595
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.1890
                       Mean reward: 866.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.4636
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1516
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 0.92s
                      Time elapsed: 00:28:37
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 111342 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 7.53
          Mean value_function loss: 28.1350
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.1942
                       Mean reward: 865.39
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 173.5630
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1512
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 0.88s
                      Time elapsed: 00:28:38
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 111617 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 7.54
          Mean value_function loss: 31.6523
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.2011
                       Mean reward: 875.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 172.6720
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1516
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 0.88s
                      Time elapsed: 00:28:39
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 110546 steps/s (collection: 0.794s, learning 0.095s)
             Mean action noise std: 7.54
          Mean value_function loss: 34.2318
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 27.2086
                       Mean reward: 883.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7870
     Episode_Reward/lifting_object: 174.7512
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1521
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 0.89s
                      Time elapsed: 00:28:40
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 108212 steps/s (collection: 0.762s, learning 0.147s)
             Mean action noise std: 7.54
          Mean value_function loss: 35.3063
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.2092
                       Mean reward: 849.50
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 171.8652
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1511
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 0.91s
                      Time elapsed: 00:28:41
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 109133 steps/s (collection: 0.774s, learning 0.127s)
             Mean action noise std: 7.54
          Mean value_function loss: 31.8003
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.2116
                       Mean reward: 879.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 174.4144
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1518
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 0.90s
                      Time elapsed: 00:28:42
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 104804 steps/s (collection: 0.795s, learning 0.143s)
             Mean action noise std: 7.55
          Mean value_function loss: 25.8663
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.2189
                       Mean reward: 872.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 173.3126
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1529
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 0.94s
                      Time elapsed: 00:28:42
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 106334 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 7.56
          Mean value_function loss: 23.3636
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.2314
                       Mean reward: 869.68
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.3787
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1508
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 0.92s
                      Time elapsed: 00:28:43
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 103885 steps/s (collection: 0.823s, learning 0.124s)
             Mean action noise std: 7.57
          Mean value_function loss: 31.8064
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.2406
                       Mean reward: 864.94
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 173.4556
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.1523
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 0.95s
                      Time elapsed: 00:28:44
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 105650 steps/s (collection: 0.832s, learning 0.098s)
             Mean action noise std: 7.58
          Mean value_function loss: 31.9924
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 27.2472
                       Mean reward: 844.75
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 171.8619
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1516
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 0.93s
                      Time elapsed: 00:28:45
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 104133 steps/s (collection: 0.837s, learning 0.107s)
             Mean action noise std: 7.59
          Mean value_function loss: 43.2134
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.2549
                       Mean reward: 865.54
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 173.3790
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1522
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 0.94s
                      Time elapsed: 00:28:46
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 108322 steps/s (collection: 0.811s, learning 0.096s)
             Mean action noise std: 7.60
          Mean value_function loss: 51.0238
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.2692
                       Mean reward: 871.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.6004
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1522
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 0.91s
                      Time elapsed: 00:28:47
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 110959 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 7.60
          Mean value_function loss: 47.9253
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.2759
                       Mean reward: 875.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 173.1620
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1530
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 0.89s
                      Time elapsed: 00:28:48
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 109231 steps/s (collection: 0.811s, learning 0.089s)
             Mean action noise std: 7.61
          Mean value_function loss: 39.7819
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.2789
                       Mean reward: 883.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 173.4004
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1533
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 0.90s
                      Time elapsed: 00:28:49
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 110103 steps/s (collection: 0.794s, learning 0.099s)
             Mean action noise std: 7.61
          Mean value_function loss: 41.9730
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.2856
                       Mean reward: 847.25
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 173.1318
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1550
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 0.89s
                      Time elapsed: 00:28:50
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 108078 steps/s (collection: 0.780s, learning 0.129s)
             Mean action noise std: 7.62
          Mean value_function loss: 41.8085
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.2945
                       Mean reward: 865.36
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 172.0989
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1555
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 0.91s
                      Time elapsed: 00:28:51
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 108626 steps/s (collection: 0.770s, learning 0.135s)
             Mean action noise std: 7.63
          Mean value_function loss: 39.8258
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.2998
                       Mean reward: 881.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 173.5780
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1549
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 0.90s
                      Time elapsed: 00:28:52
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 115817 steps/s (collection: 0.756s, learning 0.093s)
             Mean action noise std: 7.63
          Mean value_function loss: 40.6598
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.3065
                       Mean reward: 870.40
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 171.4548
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1572
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 0.85s
                      Time elapsed: 00:28:52
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 108584 steps/s (collection: 0.776s, learning 0.129s)
             Mean action noise std: 7.64
          Mean value_function loss: 32.9672
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.3130
                       Mean reward: 866.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.4910
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1578
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 0.91s
                      Time elapsed: 00:28:53
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 112157 steps/s (collection: 0.783s, learning 0.094s)
             Mean action noise std: 7.65
          Mean value_function loss: 37.9732
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.3186
                       Mean reward: 803.23
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 167.7564
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1582
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 0.88s
                      Time elapsed: 00:28:54
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 105742 steps/s (collection: 0.818s, learning 0.112s)
             Mean action noise std: 7.65
          Mean value_function loss: 41.6008
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.3233
                       Mean reward: 852.67
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.2433
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1587
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 0.93s
                      Time elapsed: 00:28:55
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 110175 steps/s (collection: 0.799s, learning 0.093s)
             Mean action noise std: 7.66
          Mean value_function loss: 37.1941
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3291
                       Mean reward: 860.34
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 171.0986
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1579
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 0.89s
                      Time elapsed: 00:28:56
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 108877 steps/s (collection: 0.817s, learning 0.086s)
             Mean action noise std: 7.66
          Mean value_function loss: 22.6119
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.3335
                       Mean reward: 859.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.4890
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1593
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 0.90s
                      Time elapsed: 00:28:57
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 112978 steps/s (collection: 0.775s, learning 0.095s)
             Mean action noise std: 7.67
          Mean value_function loss: 22.4182
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.3378
                       Mean reward: 861.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 173.6470
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.1597
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 0.87s
                      Time elapsed: 00:28:58
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 110530 steps/s (collection: 0.777s, learning 0.113s)
             Mean action noise std: 7.67
          Mean value_function loss: 28.1267
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.3445
                       Mean reward: 871.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 172.3281
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.1593
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 0.89s
                      Time elapsed: 00:28:59
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 109356 steps/s (collection: 0.783s, learning 0.116s)
             Mean action noise std: 7.68
          Mean value_function loss: 24.4339
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.3543
                       Mean reward: 860.39
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.1948
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1589
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 0.90s
                      Time elapsed: 00:29:00
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 106145 steps/s (collection: 0.782s, learning 0.144s)
             Mean action noise std: 7.69
          Mean value_function loss: 28.8444
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.3606
                       Mean reward: 875.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.7768
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1596
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 0.93s
                      Time elapsed: 00:29:01
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 104840 steps/s (collection: 0.819s, learning 0.118s)
             Mean action noise std: 7.69
          Mean value_function loss: 32.0591
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.3625
                       Mean reward: 862.58
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.5652
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1593
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 0.94s
                      Time elapsed: 00:29:01
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 112353 steps/s (collection: 0.753s, learning 0.122s)
             Mean action noise std: 7.70
          Mean value_function loss: 30.6545
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.3687
                       Mean reward: 868.63
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 171.7784
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1586
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 0.87s
                      Time elapsed: 00:29:02
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 113929 steps/s (collection: 0.756s, learning 0.106s)
             Mean action noise std: 7.70
          Mean value_function loss: 20.5597
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.3777
                       Mean reward: 880.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7813
     Episode_Reward/lifting_object: 173.8748
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.1582
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 0.86s
                      Time elapsed: 00:29:03
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 112525 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 7.71
          Mean value_function loss: 22.4182
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.3840
                       Mean reward: 881.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 172.7838
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.1582
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 0.87s
                      Time elapsed: 00:29:04
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 115946 steps/s (collection: 0.755s, learning 0.093s)
             Mean action noise std: 7.72
          Mean value_function loss: 22.3527
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.3907
                       Mean reward: 884.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 173.5660
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1579
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 0.85s
                      Time elapsed: 00:29:05
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 113667 steps/s (collection: 0.778s, learning 0.087s)
             Mean action noise std: 7.72
          Mean value_function loss: 24.2108
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 27.3961
                       Mean reward: 872.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7816
     Episode_Reward/lifting_object: 174.2814
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1589
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 0.86s
                      Time elapsed: 00:29:06
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 115339 steps/s (collection: 0.761s, learning 0.091s)
             Mean action noise std: 7.73
          Mean value_function loss: 23.9230
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.4022
                       Mean reward: 863.92
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.8747
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1585
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 0.85s
                      Time elapsed: 00:29:07
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 110479 steps/s (collection: 0.747s, learning 0.143s)
             Mean action noise std: 7.73
          Mean value_function loss: 23.5626
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 27.4124
                       Mean reward: 872.10
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 174.0365
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1585
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 0.89s
                      Time elapsed: 00:29:08
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 113827 steps/s (collection: 0.757s, learning 0.107s)
             Mean action noise std: 7.74
          Mean value_function loss: 29.2970
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.4186
                       Mean reward: 871.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.8124
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1593
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 0.86s
                      Time elapsed: 00:29:08
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 113205 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 7.75
          Mean value_function loss: 30.8161
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.4255
                       Mean reward: 867.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 171.9034
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1581
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 0.87s
                      Time elapsed: 00:29:09
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 114779 steps/s (collection: 0.769s, learning 0.087s)
             Mean action noise std: 7.75
          Mean value_function loss: 23.2621
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.4299
                       Mean reward: 866.10
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.1956
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1588
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 0.86s
                      Time elapsed: 00:29:10
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 111997 steps/s (collection: 0.778s, learning 0.100s)
             Mean action noise std: 7.76
          Mean value_function loss: 23.2724
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.4328
                       Mean reward: 876.55
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 173.4289
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1595
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 0.88s
                      Time elapsed: 00:29:11
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 112409 steps/s (collection: 0.780s, learning 0.095s)
             Mean action noise std: 7.76
          Mean value_function loss: 20.7845
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.4379
                       Mean reward: 871.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7921
     Episode_Reward/lifting_object: 174.3604
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1608
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 0.87s
                      Time elapsed: 00:29:12
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 110644 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 7.76
          Mean value_function loss: 25.4876
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 27.4420
                       Mean reward: 865.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 173.8749
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.1616
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 0.89s
                      Time elapsed: 00:29:13
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 110942 steps/s (collection: 0.774s, learning 0.112s)
             Mean action noise std: 7.77
          Mean value_function loss: 20.0867
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.4475
                       Mean reward: 875.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7881
     Episode_Reward/lifting_object: 173.5622
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1620
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 0.89s
                      Time elapsed: 00:29:14
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 108739 steps/s (collection: 0.778s, learning 0.126s)
             Mean action noise std: 7.77
          Mean value_function loss: 20.1261
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.4531
                       Mean reward: 869.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7920
     Episode_Reward/lifting_object: 173.1857
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1629
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 0.90s
                      Time elapsed: 00:29:15
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 111353 steps/s (collection: 0.761s, learning 0.122s)
             Mean action noise std: 7.78
          Mean value_function loss: 25.6072
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.4590
                       Mean reward: 870.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7892
     Episode_Reward/lifting_object: 174.2617
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1643
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 0.88s
                      Time elapsed: 00:29:15
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 112798 steps/s (collection: 0.755s, learning 0.117s)
             Mean action noise std: 7.78
          Mean value_function loss: 21.7565
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.4650
                       Mean reward: 868.68
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 173.2645
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1651
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 0.87s
                      Time elapsed: 00:29:16
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 111664 steps/s (collection: 0.773s, learning 0.107s)
             Mean action noise std: 7.79
          Mean value_function loss: 21.0439
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.4671
                       Mean reward: 863.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 172.5365
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1665
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 0.88s
                      Time elapsed: 00:29:17
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 119774 steps/s (collection: 0.733s, learning 0.088s)
             Mean action noise std: 7.79
          Mean value_function loss: 31.5610
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.4724
                       Mean reward: 879.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 174.1019
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1668
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 0.82s
                      Time elapsed: 00:29:18
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 117176 steps/s (collection: 0.743s, learning 0.096s)
             Mean action noise std: 7.80
          Mean value_function loss: 21.8668
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.4789
                       Mean reward: 872.02
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7882
     Episode_Reward/lifting_object: 173.4204
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1672
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 0.84s
                      Time elapsed: 00:29:19
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 113815 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 7.80
          Mean value_function loss: 25.2111
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.4856
                       Mean reward: 865.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 173.0304
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1671
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 0.86s
                      Time elapsed: 00:29:20
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 113400 steps/s (collection: 0.760s, learning 0.107s)
             Mean action noise std: 7.81
          Mean value_function loss: 28.1457
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.4895
                       Mean reward: 866.85
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.1938
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.1676
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 0.87s
                      Time elapsed: 00:29:21
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 110314 steps/s (collection: 0.755s, learning 0.136s)
             Mean action noise std: 7.81
          Mean value_function loss: 17.3642
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.4931
                       Mean reward: 861.71
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7882
     Episode_Reward/lifting_object: 173.6586
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1675
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 0.89s
                      Time elapsed: 00:29:21
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 105792 steps/s (collection: 0.754s, learning 0.176s)
             Mean action noise std: 7.82
          Mean value_function loss: 29.3483
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.5019
                       Mean reward: 868.73
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 173.8072
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1674
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 0.93s
                      Time elapsed: 00:29:22
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 110903 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 7.83
          Mean value_function loss: 24.0815
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.5103
                       Mean reward: 871.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 172.2983
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.1696
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 0.89s
                      Time elapsed: 00:29:23
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 118331 steps/s (collection: 0.740s, learning 0.091s)
             Mean action noise std: 7.84
          Mean value_function loss: 22.2580
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.5171
                       Mean reward: 868.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 173.8434
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.1686
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 0.83s
                      Time elapsed: 00:29:24
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 109435 steps/s (collection: 0.786s, learning 0.113s)
             Mean action noise std: 7.84
          Mean value_function loss: 25.7590
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 27.5263
                       Mean reward: 858.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 173.6314
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.1695
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 0.90s
                      Time elapsed: 00:29:25
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 116199 steps/s (collection: 0.759s, learning 0.087s)
             Mean action noise std: 7.85
          Mean value_function loss: 17.2960
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.5306
                       Mean reward: 859.00
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 172.7342
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1686
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 0.85s
                      Time elapsed: 00:29:26
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 113135 steps/s (collection: 0.782s, learning 0.087s)
             Mean action noise std: 7.85
          Mean value_function loss: 18.6050
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.5362
                       Mean reward: 858.34
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 173.4981
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 0.87s
                      Time elapsed: 00:29:27
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 114262 steps/s (collection: 0.735s, learning 0.125s)
             Mean action noise std: 7.86
          Mean value_function loss: 20.2577
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 27.5404
                       Mean reward: 876.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 173.4728
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1704
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 0.86s
                      Time elapsed: 00:29:28
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 114324 steps/s (collection: 0.747s, learning 0.113s)
             Mean action noise std: 7.86
          Mean value_function loss: 20.0982
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.5413
                       Mean reward: 870.86
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 173.1316
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1691
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 0.86s
                      Time elapsed: 00:29:28
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 112740 steps/s (collection: 0.753s, learning 0.119s)
             Mean action noise std: 7.86
          Mean value_function loss: 26.3859
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.5442
                       Mean reward: 875.98
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7897
     Episode_Reward/lifting_object: 174.2637
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1715
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 0.87s
                      Time elapsed: 00:29:29
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 112041 steps/s (collection: 0.786s, learning 0.091s)
             Mean action noise std: 7.87
          Mean value_function loss: 23.9366
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.5512
                       Mean reward: 867.68
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 172.3657
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1717
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 0.88s
                      Time elapsed: 00:29:30
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 118114 steps/s (collection: 0.742s, learning 0.090s)
             Mean action noise std: 7.87
          Mean value_function loss: 26.0942
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.5573
                       Mean reward: 867.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 171.9570
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1729
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 0.83s
                      Time elapsed: 00:29:31
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 114259 steps/s (collection: 0.760s, learning 0.101s)
             Mean action noise std: 7.88
          Mean value_function loss: 23.6683
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.5618
                       Mean reward: 854.03
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 172.8936
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1729
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 0.86s
                      Time elapsed: 00:29:32
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 115822 steps/s (collection: 0.756s, learning 0.093s)
             Mean action noise std: 7.88
          Mean value_function loss: 36.4194
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.5677
                       Mean reward: 869.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 173.6592
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1738
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 0.85s
                      Time elapsed: 00:29:33
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 112467 steps/s (collection: 0.766s, learning 0.108s)
             Mean action noise std: 7.89
          Mean value_function loss: 26.6231
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.5708
                       Mean reward: 880.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 174.1456
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1737
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 0.87s
                      Time elapsed: 00:29:34
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 108026 steps/s (collection: 0.781s, learning 0.129s)
             Mean action noise std: 7.89
          Mean value_function loss: 26.7928
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.5749
                       Mean reward: 867.67
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 174.6094
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1733
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 0.91s
                      Time elapsed: 00:29:35
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 107285 steps/s (collection: 0.744s, learning 0.173s)
             Mean action noise std: 7.89
          Mean value_function loss: 21.7574
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.5769
                       Mean reward: 869.10
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 174.1412
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1739
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 0.92s
                      Time elapsed: 00:29:35
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 117281 steps/s (collection: 0.753s, learning 0.086s)
             Mean action noise std: 7.90
          Mean value_function loss: 20.1976
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.5811
                       Mean reward: 859.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7889
     Episode_Reward/lifting_object: 173.5238
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1747
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 0.84s
                      Time elapsed: 00:29:36
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 115053 steps/s (collection: 0.754s, learning 0.101s)
             Mean action noise std: 7.90
          Mean value_function loss: 22.3568
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.5856
                       Mean reward: 870.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7884
     Episode_Reward/lifting_object: 173.5746
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1747
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 0.85s
                      Time elapsed: 00:29:37
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 110498 steps/s (collection: 0.802s, learning 0.088s)
             Mean action noise std: 7.91
          Mean value_function loss: 25.1974
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.5916
                       Mean reward: 872.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7910
     Episode_Reward/lifting_object: 173.5257
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.1752
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 0.89s
                      Time elapsed: 00:29:38
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 109894 steps/s (collection: 0.795s, learning 0.100s)
             Mean action noise std: 7.91
          Mean value_function loss: 25.1282
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.5963
                       Mean reward: 869.30
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7847
     Episode_Reward/lifting_object: 173.4980
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1758
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 0.89s
                      Time elapsed: 00:29:39
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 109069 steps/s (collection: 0.801s, learning 0.101s)
             Mean action noise std: 7.92
          Mean value_function loss: 17.9164
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.6021
                       Mean reward: 872.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7880
     Episode_Reward/lifting_object: 173.5289
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1751
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 0.90s
                      Time elapsed: 00:29:40
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 114931 steps/s (collection: 0.758s, learning 0.097s)
             Mean action noise std: 7.93
          Mean value_function loss: 23.7034
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.6130
                       Mean reward: 859.74
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 173.0515
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.1755
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 0.86s
                      Time elapsed: 00:29:41
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 118882 steps/s (collection: 0.732s, learning 0.095s)
             Mean action noise std: 7.93
          Mean value_function loss: 18.1956
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.6192
                       Mean reward: 864.49
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 173.1806
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1756
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 0.83s
                      Time elapsed: 00:29:42
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 108419 steps/s (collection: 0.775s, learning 0.132s)
             Mean action noise std: 7.94
          Mean value_function loss: 20.4079
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.6239
                       Mean reward: 872.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 173.8416
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 0.91s
                      Time elapsed: 00:29:42
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 117960 steps/s (collection: 0.747s, learning 0.087s)
             Mean action noise std: 7.94
          Mean value_function loss: 20.4375
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.6281
                       Mean reward: 881.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 174.9160
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 0.83s
                      Time elapsed: 00:29:43
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 116665 steps/s (collection: 0.757s, learning 0.086s)
             Mean action noise std: 7.95
          Mean value_function loss: 19.2445
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.6339
                       Mean reward: 873.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7897
     Episode_Reward/lifting_object: 174.1274
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1767
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 0.84s
                      Time elapsed: 00:29:44
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 114083 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 7.95
          Mean value_function loss: 19.9842
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.6394
                       Mean reward: 875.59
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 173.2724
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1767
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 0.86s
                      Time elapsed: 00:29:45
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 112963 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 7.96
          Mean value_function loss: 27.1033
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.6447
                       Mean reward: 876.37
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7982
     Episode_Reward/lifting_object: 174.6612
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1766
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 0.87s
                      Time elapsed: 00:29:46
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 111725 steps/s (collection: 0.783s, learning 0.097s)
             Mean action noise std: 7.97
          Mean value_function loss: 20.8735
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.6528
                       Mean reward: 856.45
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 171.5232
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1749
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 0.88s
                      Time elapsed: 00:29:47
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 116951 steps/s (collection: 0.743s, learning 0.098s)
             Mean action noise std: 7.97
          Mean value_function loss: 17.5062
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.6588
                       Mean reward: 882.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7929
     Episode_Reward/lifting_object: 174.0290
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1752
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 0.84s
                      Time elapsed: 00:29:48
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 112883 steps/s (collection: 0.765s, learning 0.106s)
             Mean action noise std: 7.98
          Mean value_function loss: 19.0644
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.6638
                       Mean reward: 872.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7881
     Episode_Reward/lifting_object: 173.8215
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.1746
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 0.87s
                      Time elapsed: 00:29:48
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 112707 steps/s (collection: 0.760s, learning 0.113s)
             Mean action noise std: 7.98
          Mean value_function loss: 22.5382
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.6715
                       Mean reward: 870.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7930
     Episode_Reward/lifting_object: 173.4877
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1744
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 0.87s
                      Time elapsed: 00:29:49
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 111162 steps/s (collection: 0.781s, learning 0.103s)
             Mean action noise std: 7.99
          Mean value_function loss: 19.1717
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.6748
                       Mean reward: 880.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 173.9891
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1742
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 0.88s
                      Time elapsed: 00:29:50
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 110147 steps/s (collection: 0.789s, learning 0.103s)
             Mean action noise std: 7.99
          Mean value_function loss: 24.2942
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.6792
                       Mean reward: 879.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7942
     Episode_Reward/lifting_object: 174.4323
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1736
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 0.89s
                      Time elapsed: 00:29:51
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 114394 steps/s (collection: 0.756s, learning 0.103s)
             Mean action noise std: 7.99
          Mean value_function loss: 19.1373
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 27.6822
                       Mean reward: 877.96
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 173.6228
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.1736
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 0.86s
                      Time elapsed: 00:29:52
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 112623 steps/s (collection: 0.765s, learning 0.108s)
             Mean action noise std: 8.00
          Mean value_function loss: 22.4479
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.6847
                       Mean reward: 861.38
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 173.9162
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1740
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 0.87s
                      Time elapsed: 00:29:53
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 107865 steps/s (collection: 0.802s, learning 0.109s)
             Mean action noise std: 8.00
          Mean value_function loss: 18.6076
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 27.6886
                       Mean reward: 882.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7966
     Episode_Reward/lifting_object: 174.9596
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1748
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 0.91s
                      Time elapsed: 00:29:54
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 119789 steps/s (collection: 0.726s, learning 0.095s)
             Mean action noise std: 8.01
          Mean value_function loss: 24.0265
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.6959
                       Mean reward: 876.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7993
     Episode_Reward/lifting_object: 175.2399
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 0.82s
                      Time elapsed: 00:29:55
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 116734 steps/s (collection: 0.744s, learning 0.098s)
             Mean action noise std: 8.01
          Mean value_function loss: 19.7768
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.6997
                       Mean reward: 876.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7876
     Episode_Reward/lifting_object: 173.8436
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1769
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 0.84s
                      Time elapsed: 00:29:55
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 111956 steps/s (collection: 0.768s, learning 0.110s)
             Mean action noise std: 8.01
          Mean value_function loss: 19.9078
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.7032
                       Mean reward: 880.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7952
     Episode_Reward/lifting_object: 174.3293
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.1764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 0.88s
                      Time elapsed: 00:29:56
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 116237 steps/s (collection: 0.745s, learning 0.101s)
             Mean action noise std: 8.02
          Mean value_function loss: 17.9838
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.7118
                       Mean reward: 880.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7922
     Episode_Reward/lifting_object: 174.2501
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1776
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 0.85s
                      Time elapsed: 00:29:57
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 111779 steps/s (collection: 0.775s, learning 0.104s)
             Mean action noise std: 8.03
          Mean value_function loss: 20.7580
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.7170
                       Mean reward: 880.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7946
     Episode_Reward/lifting_object: 174.6012
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1790
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 0.88s
                      Time elapsed: 00:29:58
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 112013 steps/s (collection: 0.767s, learning 0.111s)
             Mean action noise std: 8.03
          Mean value_function loss: 20.6476
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.7216
                       Mean reward: 873.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7919
     Episode_Reward/lifting_object: 174.3440
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1800
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 0.88s
                      Time elapsed: 00:29:59
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 112261 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 8.04
          Mean value_function loss: 18.5624
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.7280
                       Mean reward: 876.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7907
     Episode_Reward/lifting_object: 173.2579
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1798
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 0.88s
                      Time elapsed: 00:30:00
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 114702 steps/s (collection: 0.766s, learning 0.091s)
             Mean action noise std: 8.04
          Mean value_function loss: 17.2220
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.7324
                       Mean reward: 850.87
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: 172.8101
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 0.86s
                      Time elapsed: 00:30:01
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 109312 steps/s (collection: 0.755s, learning 0.145s)
             Mean action noise std: 8.05
          Mean value_function loss: 21.0176
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.7361
                       Mean reward: 868.56
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7976
     Episode_Reward/lifting_object: 174.4897
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1802
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 0.90s
                      Time elapsed: 00:30:01
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 113338 steps/s (collection: 0.748s, learning 0.120s)
             Mean action noise std: 8.05
          Mean value_function loss: 21.0364
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.7418
                       Mean reward: 863.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7935
     Episode_Reward/lifting_object: 174.5750
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1815
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 0.87s
                      Time elapsed: 00:30:02
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 118220 steps/s (collection: 0.737s, learning 0.095s)
             Mean action noise std: 8.05
          Mean value_function loss: 16.7626
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.7456
                       Mean reward: 883.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7991
     Episode_Reward/lifting_object: 175.6330
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1816
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 0.83s
                      Time elapsed: 00:30:03
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 112565 steps/s (collection: 0.762s, learning 0.112s)
             Mean action noise std: 8.06
          Mean value_function loss: 21.9292
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.7504
                       Mean reward: 861.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 173.1663
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1827
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 0.87s
                      Time elapsed: 00:30:04
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 114324 steps/s (collection: 0.764s, learning 0.096s)
             Mean action noise std: 8.07
          Mean value_function loss: 13.9138
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.7569
                       Mean reward: 883.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7963
     Episode_Reward/lifting_object: 174.6864
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1824
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 0.86s
                      Time elapsed: 00:30:05
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 115040 steps/s (collection: 0.755s, learning 0.099s)
             Mean action noise std: 8.07
          Mean value_function loss: 23.4531
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.7606
                       Mean reward: 882.26
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7942
     Episode_Reward/lifting_object: 174.6253
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.1834
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 0.85s
                      Time elapsed: 00:30:06
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 112817 steps/s (collection: 0.783s, learning 0.089s)
             Mean action noise std: 8.08
          Mean value_function loss: 18.4211
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.7665
                       Mean reward: 879.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7903
     Episode_Reward/lifting_object: 173.7838
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.1846
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 0.87s
                      Time elapsed: 00:30:07
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 118023 steps/s (collection: 0.735s, learning 0.098s)
             Mean action noise std: 8.08
          Mean value_function loss: 19.3003
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.7748
                       Mean reward: 876.02
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7982
     Episode_Reward/lifting_object: 174.9292
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.1849
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 0.83s
                      Time elapsed: 00:30:07
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 113463 steps/s (collection: 0.751s, learning 0.115s)
             Mean action noise std: 8.09
          Mean value_function loss: 20.2821
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.7809
                       Mean reward: 883.21
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7919
     Episode_Reward/lifting_object: 174.6394
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.1850
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 0.87s
                      Time elapsed: 00:30:08
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 114184 steps/s (collection: 0.756s, learning 0.105s)
             Mean action noise std: 8.09
          Mean value_function loss: 21.8036
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.7847
                       Mean reward: 873.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7903
     Episode_Reward/lifting_object: 173.8746
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1864
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 0.86s
                      Time elapsed: 00:30:09
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 117151 steps/s (collection: 0.742s, learning 0.098s)
             Mean action noise std: 8.10
          Mean value_function loss: 20.7909
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 27.7918
                       Mean reward: 875.28
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 172.5319
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1854
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 0.84s
                      Time elapsed: 00:30:10
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 115491 steps/s (collection: 0.758s, learning 0.093s)
             Mean action noise std: 8.11
          Mean value_function loss: 29.8068
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.7985
                       Mean reward: 868.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 174.3996
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1886
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 0.85s
                      Time elapsed: 00:30:11
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 114139 steps/s (collection: 0.758s, learning 0.103s)
             Mean action noise std: 8.11
          Mean value_function loss: 17.3406
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.8046
                       Mean reward: 874.21
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7961
     Episode_Reward/lifting_object: 174.3704
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 0.86s
                      Time elapsed: 00:30:12
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 109473 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 8.12
          Mean value_function loss: 22.9730
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.8110
                       Mean reward: 872.80
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7921
     Episode_Reward/lifting_object: 173.4147
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 0.90s
                      Time elapsed: 00:30:13
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 110132 steps/s (collection: 0.769s, learning 0.124s)
             Mean action noise std: 8.13
          Mean value_function loss: 19.8964
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.8176
                       Mean reward: 877.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 173.7359
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1899
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 0.89s
                      Time elapsed: 00:30:14
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 106507 steps/s (collection: 0.762s, learning 0.161s)
             Mean action noise std: 8.13
          Mean value_function loss: 28.8488
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.8240
                       Mean reward: 879.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7941
     Episode_Reward/lifting_object: 174.7026
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1906
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 0.92s
                      Time elapsed: 00:30:14
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 113542 steps/s (collection: 0.769s, learning 0.097s)
             Mean action noise std: 8.14
          Mean value_function loss: 24.2361
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.8334
                       Mean reward: 874.88
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7912
     Episode_Reward/lifting_object: 174.4628
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1900
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 0.87s
                      Time elapsed: 00:30:15
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 114497 steps/s (collection: 0.749s, learning 0.110s)
             Mean action noise std: 8.15
          Mean value_function loss: 24.4164
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.8421
                       Mean reward: 874.43
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 174.6202
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.1914
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 0.86s
                      Time elapsed: 00:30:16
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 112688 steps/s (collection: 0.763s, learning 0.109s)
             Mean action noise std: 8.15
          Mean value_function loss: 16.3178
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.8479
                       Mean reward: 863.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: 174.0277
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1899
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 0.87s
                      Time elapsed: 00:30:17
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 114247 steps/s (collection: 0.772s, learning 0.088s)
             Mean action noise std: 8.16
          Mean value_function loss: 26.2431
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.8528
                       Mean reward: 880.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 173.5674
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1895
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 0.86s
                      Time elapsed: 00:30:18
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 114760 steps/s (collection: 0.750s, learning 0.107s)
             Mean action noise std: 8.17
          Mean value_function loss: 18.3073
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.8587
                       Mean reward: 852.84
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 172.4444
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1892
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 0.86s
                      Time elapsed: 00:30:19
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 112078 steps/s (collection: 0.770s, learning 0.108s)
             Mean action noise std: 8.17
          Mean value_function loss: 16.4002
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.8652
                       Mean reward: 873.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.5556
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1897
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 0.88s
                      Time elapsed: 00:30:20
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 114181 steps/s (collection: 0.761s, learning 0.100s)
             Mean action noise std: 8.18
          Mean value_function loss: 17.9347
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 27.8721
                       Mean reward: 886.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7924
     Episode_Reward/lifting_object: 174.0655
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.1889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 0.86s
                      Time elapsed: 00:30:21
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 104666 steps/s (collection: 0.752s, learning 0.188s)
             Mean action noise std: 8.18
          Mean value_function loss: 17.7796
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.8753
                       Mean reward: 853.19
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7901
     Episode_Reward/lifting_object: 174.0112
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 0.94s
                      Time elapsed: 00:30:21
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 107464 steps/s (collection: 0.752s, learning 0.163s)
             Mean action noise std: 8.19
          Mean value_function loss: 19.9805
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.8785
                       Mean reward: 866.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 173.0577
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1891
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 0.91s
                      Time elapsed: 00:30:22
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 116858 steps/s (collection: 0.730s, learning 0.111s)
             Mean action noise std: 8.19
          Mean value_function loss: 18.7134
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.8843
                       Mean reward: 876.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7910
     Episode_Reward/lifting_object: 174.3196
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1887
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 0.84s
                      Time elapsed: 00:30:23
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 107467 steps/s (collection: 0.790s, learning 0.125s)
             Mean action noise std: 8.20
          Mean value_function loss: 15.2636
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.8899
                       Mean reward: 873.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 174.5917
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1890
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 0.91s
                      Time elapsed: 00:30:24
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 110276 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 8.20
          Mean value_function loss: 17.4828
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.8933
                       Mean reward: 867.06
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7937
     Episode_Reward/lifting_object: 174.3987
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 0.89s
                      Time elapsed: 00:30:25
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 108824 steps/s (collection: 0.784s, learning 0.119s)
             Mean action noise std: 8.21
          Mean value_function loss: 21.8186
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.9017
                       Mean reward: 872.25
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 173.2192
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1878
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 0.90s
                      Time elapsed: 00:30:26
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 113668 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 8.22
          Mean value_function loss: 18.7319
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.9082
                       Mean reward: 879.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7932
     Episode_Reward/lifting_object: 174.4560
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 0.86s
                      Time elapsed: 00:30:27
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 111727 steps/s (collection: 0.766s, learning 0.114s)
             Mean action noise std: 8.22
          Mean value_function loss: 15.9922
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.9174
                       Mean reward: 866.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7949
     Episode_Reward/lifting_object: 174.7247
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 0.88s
                      Time elapsed: 00:30:28
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 109386 steps/s (collection: 0.793s, learning 0.106s)
             Mean action noise std: 8.23
          Mean value_function loss: 16.5713
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.9245
                       Mean reward: 875.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7945
     Episode_Reward/lifting_object: 174.1133
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 0.90s
                      Time elapsed: 00:30:29
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 113063 steps/s (collection: 0.759s, learning 0.111s)
             Mean action noise std: 8.23
          Mean value_function loss: 20.2758
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.9294
                       Mean reward: 878.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7943
     Episode_Reward/lifting_object: 174.2969
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1880
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 0.87s
                      Time elapsed: 00:30:29
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 111319 steps/s (collection: 0.737s, learning 0.147s)
             Mean action noise std: 8.24
          Mean value_function loss: 19.4640
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 27.9355
                       Mean reward: 868.72
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7886
     Episode_Reward/lifting_object: 173.0614
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1873
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 0.88s
                      Time elapsed: 00:30:30
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 112612 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 8.25
          Mean value_function loss: 20.2056
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.9408
                       Mean reward: 880.52
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 173.6596
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1878
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 0.87s
                      Time elapsed: 00:30:31
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 115396 steps/s (collection: 0.744s, learning 0.108s)
             Mean action noise std: 8.25
          Mean value_function loss: 19.2306
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.9499
                       Mean reward: 880.25
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7889
     Episode_Reward/lifting_object: 174.2447
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1873
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 0.85s
                      Time elapsed: 00:30:32
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 109962 steps/s (collection: 0.793s, learning 0.100s)
             Mean action noise std: 8.26
          Mean value_function loss: 19.0386
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.9551
                       Mean reward: 870.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7943
     Episode_Reward/lifting_object: 174.7457
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1884
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 0.89s
                      Time elapsed: 00:30:33
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 113570 steps/s (collection: 0.753s, learning 0.113s)
             Mean action noise std: 8.27
          Mean value_function loss: 15.7833
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.9613
                       Mean reward: 869.78
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7971
     Episode_Reward/lifting_object: 173.8300
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 0.87s
                      Time elapsed: 00:30:34
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 114590 steps/s (collection: 0.756s, learning 0.102s)
             Mean action noise std: 8.27
          Mean value_function loss: 18.7783
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.9676
                       Mean reward: 867.34
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7936
     Episode_Reward/lifting_object: 173.6199
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 0.86s
                      Time elapsed: 00:30:35
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 115762 steps/s (collection: 0.748s, learning 0.101s)
             Mean action noise std: 8.28
          Mean value_function loss: 26.6380
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.9742
                       Mean reward: 879.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7952
     Episode_Reward/lifting_object: 173.8126
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.1899
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 0.85s
                      Time elapsed: 00:30:36
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 118333 steps/s (collection: 0.741s, learning 0.090s)
             Mean action noise std: 8.29
          Mean value_function loss: 16.3497
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 27.9862
                       Mean reward: 884.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 174.6521
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.1895
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 0.83s
                      Time elapsed: 00:30:36
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 109295 steps/s (collection: 0.768s, learning 0.132s)
             Mean action noise std: 8.29
          Mean value_function loss: 22.1730
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 27.9934
                       Mean reward: 876.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7996
     Episode_Reward/lifting_object: 175.6396
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.1899
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 0.90s
                      Time elapsed: 00:30:37
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 112001 steps/s (collection: 0.771s, learning 0.107s)
             Mean action noise std: 8.30
          Mean value_function loss: 21.2966
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.9956
                       Mean reward: 861.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7968
     Episode_Reward/lifting_object: 174.7798
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1906
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 0.88s
                      Time elapsed: 00:30:38
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 114618 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 8.30
          Mean value_function loss: 19.4279
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.0014
                       Mean reward: 875.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7949
     Episode_Reward/lifting_object: 174.0918
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1897
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 0.86s
                      Time elapsed: 00:30:39
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 112877 steps/s (collection: 0.767s, learning 0.104s)
             Mean action noise std: 8.31
          Mean value_function loss: 27.1750
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.0075
                       Mean reward: 874.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7926
     Episode_Reward/lifting_object: 174.7088
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1904
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 0.87s
                      Time elapsed: 00:30:40
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 115276 steps/s (collection: 0.754s, learning 0.099s)
             Mean action noise std: 8.32
          Mean value_function loss: 20.4528
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.0135
                       Mean reward: 878.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7934
     Episode_Reward/lifting_object: 173.9765
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1887
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 0.85s
                      Time elapsed: 00:30:41
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 116634 steps/s (collection: 0.745s, learning 0.098s)
             Mean action noise std: 8.32
          Mean value_function loss: 26.9627
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 28.0204
                       Mean reward: 876.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 174.3765
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.1884
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 0.84s
                      Time elapsed: 00:30:42
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 113079 steps/s (collection: 0.751s, learning 0.119s)
             Mean action noise std: 8.32
          Mean value_function loss: 27.9665
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.0218
                       Mean reward: 882.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7980
     Episode_Reward/lifting_object: 175.2610
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1875
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 0.87s
                      Time elapsed: 00:30:42
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 109789 steps/s (collection: 0.762s, learning 0.134s)
             Mean action noise std: 8.33
          Mean value_function loss: 33.7933
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.0269
                       Mean reward: 870.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 171.4796
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 0.90s
                      Time elapsed: 00:30:43
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 112984 steps/s (collection: 0.772s, learning 0.098s)
             Mean action noise std: 8.34
          Mean value_function loss: 34.1913
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.0367
                       Mean reward: 874.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7904
     Episode_Reward/lifting_object: 173.1517
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1880
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 0.87s
                      Time elapsed: 00:30:44
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 112877 steps/s (collection: 0.753s, learning 0.118s)
             Mean action noise std: 8.34
          Mean value_function loss: 31.1586
               Mean surrogate loss: 0.0216
                 Mean entropy loss: 28.0438
                       Mean reward: 865.47
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 171.1313
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.1872
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 0.87s
                      Time elapsed: 00:30:45
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 112474 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 8.35
          Mean value_function loss: 36.0977
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.0447
                       Mean reward: 857.63
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.4169
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 0.87s
                      Time elapsed: 00:30:46
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 116625 steps/s (collection: 0.743s, learning 0.100s)
             Mean action noise std: 8.35
          Mean value_function loss: 32.1629
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.0493
                       Mean reward: 871.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 172.0514
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1875
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 0.84s
                      Time elapsed: 00:30:47
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 111732 steps/s (collection: 0.759s, learning 0.121s)
             Mean action noise std: 8.36
          Mean value_function loss: 29.3124
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 28.0563
                       Mean reward: 862.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 172.4453
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1879
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 0.88s
                      Time elapsed: 00:30:48
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 117344 steps/s (collection: 0.749s, learning 0.089s)
             Mean action noise std: 8.36
          Mean value_function loss: 36.1514
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.0612
                       Mean reward: 861.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 172.2467
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1863
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 0.84s
                      Time elapsed: 00:30:48
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 106287 steps/s (collection: 0.771s, learning 0.154s)
             Mean action noise std: 8.37
          Mean value_function loss: 26.8406
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.0684
                       Mean reward: 848.57
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 171.9594
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.1881
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 0.92s
                      Time elapsed: 00:30:49
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 114871 steps/s (collection: 0.744s, learning 0.112s)
             Mean action noise std: 8.37
          Mean value_function loss: 28.7923
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.0734
                       Mean reward: 874.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7882
     Episode_Reward/lifting_object: 172.7256
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1861
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 0.86s
                      Time elapsed: 00:30:50
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 107032 steps/s (collection: 0.781s, learning 0.137s)
             Mean action noise std: 8.38
          Mean value_function loss: 26.4407
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.0783
                       Mean reward: 873.01
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.7889
     Episode_Reward/lifting_object: 173.2192
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.1865
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 0.92s
                      Time elapsed: 00:30:51
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 112431 steps/s (collection: 0.767s, learning 0.107s)
             Mean action noise std: 8.38
          Mean value_function loss: 30.4929
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.0836
                       Mean reward: 872.36
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7939
     Episode_Reward/lifting_object: 173.4853
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.1863
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 0.87s
                      Time elapsed: 00:30:52
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 113784 steps/s (collection: 0.755s, learning 0.109s)
             Mean action noise std: 8.39
          Mean value_function loss: 28.7126
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.0904
                       Mean reward: 860.76
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 170.9196
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.1861
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 0.86s
                      Time elapsed: 00:30:53
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 112593 steps/s (collection: 0.782s, learning 0.091s)
             Mean action noise std: 8.40
          Mean value_function loss: 20.0633
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.0969
                       Mean reward: 849.85
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 169.7673
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1867
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 0.87s
                      Time elapsed: 00:30:54
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 106703 steps/s (collection: 0.825s, learning 0.097s)
             Mean action noise std: 8.41
          Mean value_function loss: 27.5171
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 28.1055
                       Mean reward: 858.92
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.0453
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1873
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 0.92s
                      Time elapsed: 00:30:55
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 112614 steps/s (collection: 0.756s, learning 0.117s)
             Mean action noise std: 8.42
          Mean value_function loss: 29.0417
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.1140
                       Mean reward: 863.31
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.7996
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.1878
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 0.87s
                      Time elapsed: 00:30:56
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 113770 steps/s (collection: 0.753s, learning 0.111s)
             Mean action noise std: 8.42
          Mean value_function loss: 27.4727
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.1229
                       Mean reward: 865.73
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7866
     Episode_Reward/lifting_object: 172.5642
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.1870
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 0.86s
                      Time elapsed: 00:30:56
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 116824 steps/s (collection: 0.742s, learning 0.099s)
             Mean action noise std: 8.43
          Mean value_function loss: 26.2029
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.1296
                       Mean reward: 857.54
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 173.4265
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1869
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 0.84s
                      Time elapsed: 00:30:57
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 112180 steps/s (collection: 0.749s, learning 0.127s)
             Mean action noise std: 8.45
          Mean value_function loss: 26.9762
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.1414
                       Mean reward: 864.96
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 172.6305
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.1863
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 0.88s
                      Time elapsed: 00:30:58
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 111298 steps/s (collection: 0.781s, learning 0.103s)
             Mean action noise std: 8.45
          Mean value_function loss: 16.7011
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.1512
                       Mean reward: 862.84
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 173.2520
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 0.88s
                      Time elapsed: 00:30:59
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 114591 steps/s (collection: 0.740s, learning 0.118s)
             Mean action noise std: 8.46
          Mean value_function loss: 30.7743
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.1548
                       Mean reward: 873.65
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7901
     Episode_Reward/lifting_object: 173.0942
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 0.86s
                      Time elapsed: 00:31:00
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 114470 steps/s (collection: 0.767s, learning 0.092s)
             Mean action noise std: 8.46
          Mean value_function loss: 29.2901
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1574
                       Mean reward: 873.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7891
     Episode_Reward/lifting_object: 172.9526
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.1876
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 0.86s
                      Time elapsed: 00:31:01
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 110188 steps/s (collection: 0.780s, learning 0.112s)
             Mean action noise std: 8.47
          Mean value_function loss: 21.5904
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.1633
                       Mean reward: 879.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7877
     Episode_Reward/lifting_object: 173.5474
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.1876
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 0.89s
                      Time elapsed: 00:31:02
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 110859 steps/s (collection: 0.770s, learning 0.117s)
             Mean action noise std: 8.47
          Mean value_function loss: 28.5575
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.1673
                       Mean reward: 876.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7900
     Episode_Reward/lifting_object: 173.0736
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 0.89s
                      Time elapsed: 00:31:03
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 113128 steps/s (collection: 0.758s, learning 0.111s)
             Mean action noise std: 8.47
          Mean value_function loss: 28.7594
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1712
                       Mean reward: 871.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7910
     Episode_Reward/lifting_object: 173.7318
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1882
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 0.87s
                      Time elapsed: 00:31:03
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 116298 steps/s (collection: 0.758s, learning 0.088s)
             Mean action noise std: 8.48
          Mean value_function loss: 23.8333
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.1749
                       Mean reward: 868.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 172.6720
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.1877
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 0.85s
                      Time elapsed: 00:31:04
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 111519 steps/s (collection: 0.774s, learning 0.108s)
             Mean action noise std: 8.48
          Mean value_function loss: 20.6329
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.1785
                       Mean reward: 853.42
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 172.6919
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.1858
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 0.88s
                      Time elapsed: 00:31:05
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 106499 steps/s (collection: 0.795s, learning 0.129s)
             Mean action noise std: 8.49
          Mean value_function loss: 26.3950
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1865
                       Mean reward: 868.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.2549
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.1879
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 0.92s
                      Time elapsed: 00:31:06
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 117468 steps/s (collection: 0.734s, learning 0.103s)
             Mean action noise std: 8.49
          Mean value_function loss: 23.1337
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.1922
                       Mean reward: 855.14
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 172.1913
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1872
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 0.84s
                      Time elapsed: 00:31:07
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 108377 steps/s (collection: 0.801s, learning 0.106s)
             Mean action noise std: 8.50
          Mean value_function loss: 21.5897
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1963
                       Mean reward: 868.50
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 172.3856
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1867
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 0.91s
                      Time elapsed: 00:31:08
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 113950 steps/s (collection: 0.761s, learning 0.102s)
             Mean action noise std: 8.50
          Mean value_function loss: 22.0132
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.2006
                       Mean reward: 876.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7871
     Episode_Reward/lifting_object: 173.3257
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1868
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 0.86s
                      Time elapsed: 00:31:09
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 112683 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 8.51
          Mean value_function loss: 14.6063
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.2039
                       Mean reward: 882.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7859
     Episode_Reward/lifting_object: 174.2748
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.1882
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 0.87s
                      Time elapsed: 00:31:10
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 112284 steps/s (collection: 0.754s, learning 0.121s)
             Mean action noise std: 8.52
          Mean value_function loss: 15.6612
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.2094
                       Mean reward: 875.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7970
     Episode_Reward/lifting_object: 175.0670
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1877
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 0.88s
                      Time elapsed: 00:31:10
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 114741 steps/s (collection: 0.740s, learning 0.117s)
             Mean action noise std: 8.52
          Mean value_function loss: 19.4862
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.2171
                       Mean reward: 874.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7882
     Episode_Reward/lifting_object: 173.7913
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1869
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 0.86s
                      Time elapsed: 00:31:11
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 113733 steps/s (collection: 0.754s, learning 0.110s)
             Mean action noise std: 8.53
          Mean value_function loss: 20.7745
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.2248
                       Mean reward: 871.81
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 173.9568
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 0.86s
                      Time elapsed: 00:31:12
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 112824 steps/s (collection: 0.762s, learning 0.110s)
             Mean action noise std: 8.54
          Mean value_function loss: 19.0720
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.2318
                       Mean reward: 868.88
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 174.0602
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1881
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 0.87s
                      Time elapsed: 00:31:13
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 114568 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 8.54
          Mean value_function loss: 14.6109
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 28.2388
                       Mean reward: 881.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7942
     Episode_Reward/lifting_object: 174.6910
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 0.86s
                      Time elapsed: 00:31:14
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 109155 steps/s (collection: 0.795s, learning 0.106s)
             Mean action noise std: 8.55
          Mean value_function loss: 20.1161
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.2450
                       Mean reward: 872.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7900
     Episode_Reward/lifting_object: 173.5757
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1890
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 0.90s
                      Time elapsed: 00:31:15
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 112468 steps/s (collection: 0.769s, learning 0.105s)
             Mean action noise std: 8.56
          Mean value_function loss: 16.9611
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.2530
                       Mean reward: 874.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8007
     Episode_Reward/lifting_object: 175.4462
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1892
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 0.87s
                      Time elapsed: 00:31:16
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 106975 steps/s (collection: 0.793s, learning 0.126s)
             Mean action noise std: 8.57
          Mean value_function loss: 17.5550
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.2637
                       Mean reward: 877.97
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7870
     Episode_Reward/lifting_object: 174.0123
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1909
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 0.92s
                      Time elapsed: 00:31:17
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 113491 steps/s (collection: 0.760s, learning 0.106s)
             Mean action noise std: 8.58
          Mean value_function loss: 19.8052
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.2707
                       Mean reward: 884.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7908
     Episode_Reward/lifting_object: 174.5773
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1903
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 0.87s
                      Time elapsed: 00:31:17
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 112798 steps/s (collection: 0.767s, learning 0.105s)
             Mean action noise std: 8.58
          Mean value_function loss: 21.2877
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 28.2758
                       Mean reward: 869.32
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.7969
     Episode_Reward/lifting_object: 175.3506
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1916
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 0.87s
                      Time elapsed: 00:31:18
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 111426 steps/s (collection: 0.760s, learning 0.122s)
             Mean action noise std: 8.59
          Mean value_function loss: 13.3210
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 28.2817
                       Mean reward: 872.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7931
     Episode_Reward/lifting_object: 173.9539
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1932
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 0.88s
                      Time elapsed: 00:31:19
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 113077 steps/s (collection: 0.776s, learning 0.093s)
             Mean action noise std: 8.59
          Mean value_function loss: 18.5016
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 28.2887
                       Mean reward: 871.56
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 173.5732
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.1931
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 0.87s
                      Time elapsed: 00:31:20
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 103660 steps/s (collection: 0.805s, learning 0.144s)
             Mean action noise std: 8.60
          Mean value_function loss: 21.0677
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.2917
                       Mean reward: 876.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7918
     Episode_Reward/lifting_object: 174.2430
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.1923
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 0.95s
                      Time elapsed: 00:31:21
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 105490 steps/s (collection: 0.817s, learning 0.115s)
             Mean action noise std: 8.61
          Mean value_function loss: 23.7262
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.2983
                       Mean reward: 855.96
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7874
     Episode_Reward/lifting_object: 173.2941
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1918
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 0.93s
                      Time elapsed: 00:31:22
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 111218 steps/s (collection: 0.788s, learning 0.096s)
             Mean action noise std: 8.62
          Mean value_function loss: 23.0022
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 28.3103
                       Mean reward: 865.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: 174.1584
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1937
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 0.88s
                      Time elapsed: 00:31:23
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 115460 steps/s (collection: 0.757s, learning 0.094s)
             Mean action noise std: 8.63
          Mean value_function loss: 22.5908
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.3198
                       Mean reward: 873.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 172.4066
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1940
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 0.85s
                      Time elapsed: 00:31:24
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 111159 steps/s (collection: 0.770s, learning 0.115s)
             Mean action noise std: 8.63
          Mean value_function loss: 21.6763
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.3270
                       Mean reward: 880.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7911
     Episode_Reward/lifting_object: 175.1612
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.1925
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 0.88s
                      Time elapsed: 00:31:25
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 111902 steps/s (collection: 0.760s, learning 0.118s)
             Mean action noise std: 8.64
          Mean value_function loss: 22.0617
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.3343
                       Mean reward: 869.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: 173.9798
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1933
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 0.88s
                      Time elapsed: 00:31:25
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 111854 steps/s (collection: 0.746s, learning 0.133s)
             Mean action noise std: 8.64
          Mean value_function loss: 25.6481
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.3401
                       Mean reward: 851.67
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.7851
     Episode_Reward/lifting_object: 173.6566
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1924
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 0.88s
                      Time elapsed: 00:31:26
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 113532 steps/s (collection: 0.773s, learning 0.093s)
             Mean action noise std: 8.65
          Mean value_function loss: 22.9354
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 28.3448
                       Mean reward: 884.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7910
     Episode_Reward/lifting_object: 173.9712
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1917
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 0.87s
                      Time elapsed: 00:31:27
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 112231 steps/s (collection: 0.773s, learning 0.103s)
             Mean action noise std: 8.66
          Mean value_function loss: 21.8774
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.3503
                       Mean reward: 865.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 172.2557
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.1926
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 0.88s
                      Time elapsed: 00:31:28
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 112434 steps/s (collection: 0.766s, learning 0.109s)
             Mean action noise std: 8.67
          Mean value_function loss: 28.6416
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.3594
                       Mean reward: 859.10
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 172.8869
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.1936
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 0.87s
                      Time elapsed: 00:31:29
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 108948 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 8.67
          Mean value_function loss: 33.5069
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 28.3691
                       Mean reward: 883.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 173.2343
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.1938
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 0.90s
                      Time elapsed: 00:31:30
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 115813 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 8.68
          Mean value_function loss: 25.5052
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.3724
                       Mean reward: 866.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 173.6814
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1946
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 0.85s
                      Time elapsed: 00:31:31
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 109852 steps/s (collection: 0.793s, learning 0.102s)
             Mean action noise std: 8.69
          Mean value_function loss: 30.3167
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.3801
                       Mean reward: 867.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 172.6364
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1941
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 0.89s
                      Time elapsed: 00:31:32
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 116847 steps/s (collection: 0.743s, learning 0.098s)
             Mean action noise std: 8.70
          Mean value_function loss: 18.6143
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 28.3890
                       Mean reward: 877.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 172.7597
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1949
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 0.84s
                      Time elapsed: 00:31:32
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 111793 steps/s (collection: 0.752s, learning 0.128s)
             Mean action noise std: 8.70
          Mean value_function loss: 19.6110
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 28.3964
                       Mean reward: 860.20
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 172.9426
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.1956
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 0.88s
                      Time elapsed: 00:31:33
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 113939 steps/s (collection: 0.757s, learning 0.106s)
             Mean action noise std: 8.71
          Mean value_function loss: 21.4203
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.4016
                       Mean reward: 863.27
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 172.8511
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1945
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 0.86s
                      Time elapsed: 00:31:34
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 113346 steps/s (collection: 0.768s, learning 0.099s)
             Mean action noise std: 8.71
          Mean value_function loss: 33.8998
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.4030
                       Mean reward: 876.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.7434
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1959
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 0.87s
                      Time elapsed: 00:31:35
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 111670 steps/s (collection: 0.783s, learning 0.098s)
             Mean action noise std: 8.72
          Mean value_function loss: 21.7867
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.4068
                       Mean reward: 868.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.8841
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1956
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 0.88s
                      Time elapsed: 00:31:36
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 114004 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 8.73
          Mean value_function loss: 19.9495
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 28.4152
                       Mean reward: 868.97
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 173.5615
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1958
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 0.86s
                      Time elapsed: 00:31:37
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 113731 steps/s (collection: 0.754s, learning 0.110s)
             Mean action noise std: 8.73
          Mean value_function loss: 17.3698
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.4228
                       Mean reward: 857.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 171.2154
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1949
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 0.86s
                      Time elapsed: 00:31:38
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 107350 steps/s (collection: 0.801s, learning 0.115s)
             Mean action noise std: 8.74
          Mean value_function loss: 18.4687
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.4274
                       Mean reward: 868.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.4703
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1957
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 0.92s
                      Time elapsed: 00:31:39
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 110354 steps/s (collection: 0.776s, learning 0.115s)
             Mean action noise std: 8.74
          Mean value_function loss: 27.7422
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.4344
                       Mean reward: 871.92
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 173.0638
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1959
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 0.89s
                      Time elapsed: 00:31:39
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 109498 steps/s (collection: 0.772s, learning 0.126s)
             Mean action noise std: 8.75
          Mean value_function loss: 16.9251
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.4403
                       Mean reward: 877.83
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7874
     Episode_Reward/lifting_object: 174.1152
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1949
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 0.90s
                      Time elapsed: 00:31:40
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 105338 steps/s (collection: 0.779s, learning 0.154s)
             Mean action noise std: 8.76
          Mean value_function loss: 19.7903
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.4480
                       Mean reward: 881.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 174.0417
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.1947
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 0.93s
                      Time elapsed: 00:31:41
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 105143 steps/s (collection: 0.765s, learning 0.170s)
             Mean action noise std: 8.77
          Mean value_function loss: 14.3463
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.4565
                       Mean reward: 883.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 173.7608
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.1955
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 0.93s
                      Time elapsed: 00:31:42
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 112181 steps/s (collection: 0.775s, learning 0.101s)
             Mean action noise std: 8.78
          Mean value_function loss: 17.7477
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.4634
                       Mean reward: 872.28
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.3978
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1963
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 0.88s
                      Time elapsed: 00:31:43
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 111860 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 8.78
          Mean value_function loss: 26.7066
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.4710
                       Mean reward: 878.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.8738
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.1966
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 0.88s
                      Time elapsed: 00:31:44
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 108513 steps/s (collection: 0.800s, learning 0.105s)
             Mean action noise std: 8.79
          Mean value_function loss: 15.9603
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.4751
                       Mean reward: 874.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 175.2073
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 0.91s
                      Time elapsed: 00:31:45
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 112945 steps/s (collection: 0.755s, learning 0.115s)
             Mean action noise std: 8.80
          Mean value_function loss: 16.5718
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.4796
                       Mean reward: 878.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 174.0721
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.1973
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 0.87s
                      Time elapsed: 00:31:46
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 111114 steps/s (collection: 0.769s, learning 0.116s)
             Mean action noise std: 8.81
          Mean value_function loss: 16.8982
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.4891
                       Mean reward: 868.41
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7858
     Episode_Reward/lifting_object: 174.4985
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.1982
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 0.88s
                      Time elapsed: 00:31:47
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 111357 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 8.81
          Mean value_function loss: 17.7644
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.4955
                       Mean reward: 882.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7872
     Episode_Reward/lifting_object: 173.8618
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.1980
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 0.88s
                      Time elapsed: 00:31:48
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 112352 steps/s (collection: 0.760s, learning 0.115s)
             Mean action noise std: 8.82
          Mean value_function loss: 17.3006
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.5001
                       Mean reward: 871.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 171.9037
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1986
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 0.87s
                      Time elapsed: 00:31:48
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 112908 steps/s (collection: 0.771s, learning 0.100s)
             Mean action noise std: 8.82
          Mean value_function loss: 18.7665
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.5028
                       Mean reward: 886.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7940
     Episode_Reward/lifting_object: 175.4753
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2008
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 0.87s
                      Time elapsed: 00:31:49
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 105589 steps/s (collection: 0.780s, learning 0.151s)
             Mean action noise std: 8.82
          Mean value_function loss: 14.4695
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.5063
                       Mean reward: 848.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 172.5746
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.2000
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 0.93s
                      Time elapsed: 00:31:50
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 106343 steps/s (collection: 0.782s, learning 0.143s)
             Mean action noise std: 8.83
          Mean value_function loss: 14.6597
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.5090
                       Mean reward: 878.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7870
     Episode_Reward/lifting_object: 174.6363
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.1992
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 0.92s
                      Time elapsed: 00:31:51
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 111053 steps/s (collection: 0.778s, learning 0.108s)
             Mean action noise std: 8.83
          Mean value_function loss: 15.7481
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.5149
                       Mean reward: 885.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 174.2352
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.2010
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 0.89s
                      Time elapsed: 00:31:52
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 109813 steps/s (collection: 0.780s, learning 0.115s)
             Mean action noise std: 8.84
          Mean value_function loss: 22.6916
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.5222
                       Mean reward: 864.85
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 173.6296
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.1996
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 0.90s
                      Time elapsed: 00:31:53
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 103964 steps/s (collection: 0.814s, learning 0.132s)
             Mean action noise std: 8.84
          Mean value_function loss: 23.7241
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.5285
                       Mean reward: 880.70
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 175.2327
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.2006
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 0.95s
                      Time elapsed: 00:31:54
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 113723 steps/s (collection: 0.773s, learning 0.092s)
             Mean action noise std: 8.85
          Mean value_function loss: 20.9024
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.5317
                       Mean reward: 858.99
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 172.9391
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2004
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 0.86s
                      Time elapsed: 00:31:55
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 108644 steps/s (collection: 0.815s, learning 0.090s)
             Mean action noise std: 8.86
          Mean value_function loss: 21.5168
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.5395
                       Mean reward: 876.44
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7911
     Episode_Reward/lifting_object: 173.7111
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.2011
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 0.90s
                      Time elapsed: 00:31:56
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 102658 steps/s (collection: 0.830s, learning 0.128s)
             Mean action noise std: 8.87
          Mean value_function loss: 31.0674
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.5468
                       Mean reward: 881.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7902
     Episode_Reward/lifting_object: 174.6874
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.2016
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 0.96s
                      Time elapsed: 00:31:57
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 105257 steps/s (collection: 0.817s, learning 0.117s)
             Mean action noise std: 8.87
          Mean value_function loss: 28.4878
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.5553
                       Mean reward: 871.40
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 174.3697
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.2012
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 0.93s
                      Time elapsed: 00:31:58
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 108399 steps/s (collection: 0.789s, learning 0.118s)
             Mean action noise std: 8.88
          Mean value_function loss: 28.0357
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.5601
                       Mean reward: 871.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 173.4579
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.2012
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 0.91s
                      Time elapsed: 00:31:58
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 105135 steps/s (collection: 0.784s, learning 0.151s)
             Mean action noise std: 8.88
          Mean value_function loss: 25.9082
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.5649
                       Mean reward: 873.34
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 174.1151
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2009
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 0.94s
                      Time elapsed: 00:31:59
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 108160 steps/s (collection: 0.782s, learning 0.127s)
             Mean action noise std: 8.89
          Mean value_function loss: 29.5092
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.5730
                       Mean reward: 863.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 173.8830
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.2029
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 0.91s
                      Time elapsed: 00:32:00
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 109531 steps/s (collection: 0.772s, learning 0.125s)
             Mean action noise std: 8.90
          Mean value_function loss: 34.4869
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 28.5802
                       Mean reward: 877.87
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 173.8537
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.2026
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 0.90s
                      Time elapsed: 00:32:01
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 105352 steps/s (collection: 0.799s, learning 0.135s)
             Mean action noise std: 8.90
          Mean value_function loss: 23.3417
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.5851
                       Mean reward: 869.88
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 173.6985
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.2023
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 0.93s
                      Time elapsed: 00:32:02
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 109584 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 8.91
          Mean value_function loss: 27.4627
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.5911
                       Mean reward: 847.46
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 171.4135
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.2030
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 0.90s
                      Time elapsed: 00:32:03
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 106744 steps/s (collection: 0.814s, learning 0.107s)
             Mean action noise std: 8.92
          Mean value_function loss: 26.4685
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.5971
                       Mean reward: 871.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7824
     Episode_Reward/lifting_object: 173.5500
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.2016
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 0.92s
                      Time elapsed: 00:32:04
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 108282 steps/s (collection: 0.813s, learning 0.095s)
             Mean action noise std: 8.92
          Mean value_function loss: 30.2845
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.6010
                       Mean reward: 851.20
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.4421
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.2019
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 0.91s
                      Time elapsed: 00:32:05
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 110221 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 8.93
          Mean value_function loss: 31.6321
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.6056
                       Mean reward: 874.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7851
     Episode_Reward/lifting_object: 173.8086
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.2015
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 0.89s
                      Time elapsed: 00:32:06
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 113663 steps/s (collection: 0.775s, learning 0.090s)
             Mean action noise std: 8.93
          Mean value_function loss: 21.1306
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.6113
                       Mean reward: 872.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.2858
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.2018
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 0.86s
                      Time elapsed: 00:32:07
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 110045 steps/s (collection: 0.794s, learning 0.099s)
             Mean action noise std: 8.93
          Mean value_function loss: 21.2969
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.6142
                       Mean reward: 860.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 172.0367
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.2014
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 0.89s
                      Time elapsed: 00:32:07
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 108880 steps/s (collection: 0.778s, learning 0.125s)
             Mean action noise std: 8.94
          Mean value_function loss: 25.2900
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.6183
                       Mean reward: 866.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 171.5370
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.2005
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 0.90s
                      Time elapsed: 00:32:08
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 103189 steps/s (collection: 0.816s, learning 0.137s)
             Mean action noise std: 8.94
          Mean value_function loss: 21.6761
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 28.6216
                       Mean reward: 864.59
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 171.5751
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.2002
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 0.95s
                      Time elapsed: 00:32:09
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 105767 steps/s (collection: 0.771s, learning 0.159s)
             Mean action noise std: 8.95
          Mean value_function loss: 21.3562
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 28.6263
                       Mean reward: 871.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7897
     Episode_Reward/lifting_object: 172.9626
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1995
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 0.93s
                      Time elapsed: 00:32:10
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 109254 steps/s (collection: 0.774s, learning 0.126s)
             Mean action noise std: 8.95
          Mean value_function loss: 19.3835
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.6295
                       Mean reward: 878.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7891
     Episode_Reward/lifting_object: 174.2791
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1990
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 0.90s
                      Time elapsed: 00:32:11
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 113699 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 8.96
          Mean value_function loss: 16.8303
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.6354
                       Mean reward: 879.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 172.9843
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1979
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 0.86s
                      Time elapsed: 00:32:12
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 108794 steps/s (collection: 0.810s, learning 0.094s)
             Mean action noise std: 8.96
          Mean value_function loss: 22.8563
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.6396
                       Mean reward: 863.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7884
     Episode_Reward/lifting_object: 173.5726
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1986
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 0.90s
                      Time elapsed: 00:32:13
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 113232 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 8.97
          Mean value_function loss: 21.7902
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 28.6444
                       Mean reward: 871.34
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 174.4865
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.1974
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 0.87s
                      Time elapsed: 00:32:14
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 106275 steps/s (collection: 0.811s, learning 0.114s)
             Mean action noise std: 8.98
          Mean value_function loss: 19.8551
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.6520
                       Mean reward: 885.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7918
     Episode_Reward/lifting_object: 175.0198
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1985
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 0.92s
                      Time elapsed: 00:32:15
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 108561 steps/s (collection: 0.791s, learning 0.115s)
             Mean action noise std: 8.98
          Mean value_function loss: 19.8150
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.6594
                       Mean reward: 864.25
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7885
     Episode_Reward/lifting_object: 174.0511
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1989
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 0.91s
                      Time elapsed: 00:32:16
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 106660 steps/s (collection: 0.798s, learning 0.123s)
             Mean action noise std: 8.99
          Mean value_function loss: 18.8566
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 28.6653
                       Mean reward: 878.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 174.0619
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.1993
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 0.92s
                      Time elapsed: 00:32:17
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 105263 steps/s (collection: 0.802s, learning 0.132s)
             Mean action noise std: 8.99
          Mean value_function loss: 22.1429
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.6690
                       Mean reward: 872.11
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 172.8811
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1999
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 0.93s
                      Time elapsed: 00:32:17
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 109490 steps/s (collection: 0.770s, learning 0.128s)
             Mean action noise std: 9.00
          Mean value_function loss: 19.7716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.6715
                       Mean reward: 860.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 173.5473
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2014
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 0.90s
                      Time elapsed: 00:32:18
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 103485 steps/s (collection: 0.810s, learning 0.140s)
             Mean action noise std: 9.00
          Mean value_function loss: 18.2124
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.6762
                       Mean reward: 881.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7946
     Episode_Reward/lifting_object: 175.5007
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.2015
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 0.95s
                      Time elapsed: 00:32:19
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 107230 steps/s (collection: 0.792s, learning 0.125s)
             Mean action noise std: 9.01
          Mean value_function loss: 34.9743
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.6845
                       Mean reward: 861.89
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.5246
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.2018
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 0.92s
                      Time elapsed: 00:32:20
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 92146 steps/s (collection: 0.899s, learning 0.168s)
             Mean action noise std: 9.02
          Mean value_function loss: 27.1932
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.6916
                       Mean reward: 882.43
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 173.4185
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.2015
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 1.07s
                      Time elapsed: 00:32:21
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 101088 steps/s (collection: 0.811s, learning 0.161s)
             Mean action noise std: 9.03
          Mean value_function loss: 29.1192
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.6975
                       Mean reward: 868.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 172.2572
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.2030
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 0.97s
                      Time elapsed: 00:32:22
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 104553 steps/s (collection: 0.786s, learning 0.155s)
             Mean action noise std: 9.03
          Mean value_function loss: 19.8616
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.7041
                       Mean reward: 873.41
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: 173.7047
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.2022
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 0.94s
                      Time elapsed: 00:32:23
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 110891 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 9.04
          Mean value_function loss: 18.1398
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.7093
                       Mean reward: 870.24
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 173.2434
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.2016
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 0.89s
                      Time elapsed: 00:32:24
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 110238 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 9.05
          Mean value_function loss: 15.2140
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.7141
                       Mean reward: 862.84
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: 174.0588
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.2021
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 0.89s
                      Time elapsed: 00:32:25
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 106797 steps/s (collection: 0.805s, learning 0.116s)
             Mean action noise std: 9.05
          Mean value_function loss: 20.1294
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.7199
                       Mean reward: 880.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 173.5980
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.2022
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 0.92s
                      Time elapsed: 00:32:26
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 105717 steps/s (collection: 0.829s, learning 0.101s)
             Mean action noise std: 9.05
          Mean value_function loss: 13.1271
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 28.7245
                       Mean reward: 879.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7883
     Episode_Reward/lifting_object: 173.9502
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2042
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 0.93s
                      Time elapsed: 00:32:27
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 103074 steps/s (collection: 0.858s, learning 0.096s)
             Mean action noise std: 9.06
          Mean value_function loss: 17.2245
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 28.7286
                       Mean reward: 886.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 175.2378
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.2040
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 0.95s
                      Time elapsed: 00:32:28
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 106228 steps/s (collection: 0.821s, learning 0.105s)
             Mean action noise std: 9.06
          Mean value_function loss: 20.5345
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.7303
                       Mean reward: 866.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.7086
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.2037
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 0.93s
                      Time elapsed: 00:32:29
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 105715 steps/s (collection: 0.826s, learning 0.104s)
             Mean action noise std: 9.07
          Mean value_function loss: 21.2826
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.7344
                       Mean reward: 882.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 174.2197
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.2039
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 0.93s
                      Time elapsed: 00:32:30
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 108794 steps/s (collection: 0.797s, learning 0.107s)
             Mean action noise std: 9.07
          Mean value_function loss: 21.6902
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.7410
                       Mean reward: 868.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 173.3641
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.2038
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 0.90s
                      Time elapsed: 00:32:31
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 103829 steps/s (collection: 0.844s, learning 0.103s)
             Mean action noise std: 9.08
          Mean value_function loss: 26.0260
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.7487
                       Mean reward: 875.96
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.4191
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2036
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 0.95s
                      Time elapsed: 00:32:31
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 106977 steps/s (collection: 0.814s, learning 0.105s)
             Mean action noise std: 9.09
          Mean value_function loss: 29.4030
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.7538
                       Mean reward: 866.80
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7863
     Episode_Reward/lifting_object: 174.4987
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.2036
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 0.92s
                      Time elapsed: 00:32:32
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 108561 steps/s (collection: 0.792s, learning 0.113s)
             Mean action noise std: 9.09
          Mean value_function loss: 24.9977
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.7563
                       Mean reward: 881.13
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 174.3376
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2032
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 0.91s
                      Time elapsed: 00:32:33
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 100558 steps/s (collection: 0.824s, learning 0.154s)
             Mean action noise std: 9.09
          Mean value_function loss: 19.3120
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 28.7606
                       Mean reward: 868.72
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 172.4590
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2049
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 0.98s
                      Time elapsed: 00:32:34
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 109969 steps/s (collection: 0.803s, learning 0.091s)
             Mean action noise std: 9.10
          Mean value_function loss: 20.8502
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.7640
                       Mean reward: 880.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 173.5564
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2050
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 0.89s
                      Time elapsed: 00:32:35
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 107237 steps/s (collection: 0.823s, learning 0.094s)
             Mean action noise std: 9.10
          Mean value_function loss: 22.6274
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.7670
                       Mean reward: 876.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 173.4081
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2057
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 0.92s
                      Time elapsed: 00:32:36
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 106088 steps/s (collection: 0.829s, learning 0.098s)
             Mean action noise std: 9.11
          Mean value_function loss: 20.3159
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.7702
                       Mean reward: 867.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 172.9734
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.2051
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 0.93s
                      Time elapsed: 00:32:37
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 103067 steps/s (collection: 0.844s, learning 0.109s)
             Mean action noise std: 9.11
          Mean value_function loss: 22.7011
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 28.7741
                       Mean reward: 876.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7828
     Episode_Reward/lifting_object: 173.5394
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.2045
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 0.95s
                      Time elapsed: 00:32:38
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 108101 steps/s (collection: 0.795s, learning 0.114s)
             Mean action noise std: 9.12
          Mean value_function loss: 14.4850
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.7796
                       Mean reward: 880.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7873
     Episode_Reward/lifting_object: 174.8380
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.2058
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 0.91s
                      Time elapsed: 00:32:39
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 106768 steps/s (collection: 0.828s, learning 0.093s)
             Mean action noise std: 9.13
          Mean value_function loss: 21.7621
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.7889
                       Mean reward: 883.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 175.0603
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2049
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 0.92s
                      Time elapsed: 00:32:40
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 102317 steps/s (collection: 0.854s, learning 0.106s)
             Mean action noise std: 9.13
          Mean value_function loss: 24.7184
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.7959
                       Mean reward: 872.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.4856
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2051
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 0.96s
                      Time elapsed: 00:32:41
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 108516 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 9.14
          Mean value_function loss: 22.7753
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.8001
                       Mean reward: 871.38
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 173.7884
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.2068
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 0.91s
                      Time elapsed: 00:32:42
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 105294 steps/s (collection: 0.810s, learning 0.124s)
             Mean action noise std: 9.15
          Mean value_function loss: 22.3441
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.8066
                       Mean reward: 878.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 173.0907
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2076
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 0.93s
                      Time elapsed: 00:32:43
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 110205 steps/s (collection: 0.797s, learning 0.095s)
             Mean action noise std: 9.16
          Mean value_function loss: 19.0730
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.8124
                       Mean reward: 870.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7824
     Episode_Reward/lifting_object: 174.1409
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.2068
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 0.89s
                      Time elapsed: 00:32:44
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 110636 steps/s (collection: 0.779s, learning 0.110s)
             Mean action noise std: 9.16
          Mean value_function loss: 19.1293
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.8183
                       Mean reward: 873.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7818
     Episode_Reward/lifting_object: 173.4728
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2061
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 0.89s
                      Time elapsed: 00:32:44
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 104254 steps/s (collection: 0.790s, learning 0.153s)
             Mean action noise std: 9.17
          Mean value_function loss: 23.1073
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.8257
                       Mean reward: 880.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 174.8300
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.2070
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 0.94s
                      Time elapsed: 00:32:45
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 105079 steps/s (collection: 0.830s, learning 0.105s)
             Mean action noise std: 9.18
          Mean value_function loss: 23.4717
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.8333
                       Mean reward: 876.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 174.0421
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.2068
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 0.94s
                      Time elapsed: 00:32:46
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 105260 steps/s (collection: 0.780s, learning 0.154s)
             Mean action noise std: 9.19
          Mean value_function loss: 25.9985
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.8383
                       Mean reward: 874.83
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7893
     Episode_Reward/lifting_object: 174.8748
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.2062
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 0.93s
                      Time elapsed: 00:32:47
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 108813 steps/s (collection: 0.813s, learning 0.090s)
             Mean action noise std: 9.19
          Mean value_function loss: 22.7508
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.8430
                       Mean reward: 871.14
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 174.4612
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.2051
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 0.90s
                      Time elapsed: 00:32:48
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 110918 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 9.20
          Mean value_function loss: 24.0477
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.8484
                       Mean reward: 863.24
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 172.9821
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.2051
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 0.89s
                      Time elapsed: 00:32:49
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 101479 steps/s (collection: 0.850s, learning 0.119s)
             Mean action noise std: 9.21
          Mean value_function loss: 16.9859
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.8551
                       Mean reward: 882.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 173.8206
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2049
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 0.97s
                      Time elapsed: 00:32:50
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 105156 steps/s (collection: 0.837s, learning 0.098s)
             Mean action noise std: 9.21
          Mean value_function loss: 23.0733
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.8615
                       Mean reward: 878.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 174.0473
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.2048
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 0.93s
                      Time elapsed: 00:32:51
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 105064 steps/s (collection: 0.839s, learning 0.097s)
             Mean action noise std: 9.22
          Mean value_function loss: 23.9437
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 28.8689
                       Mean reward: 870.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7891
     Episode_Reward/lifting_object: 174.0423
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.2045
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 0.94s
                      Time elapsed: 00:32:52
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 109001 steps/s (collection: 0.788s, learning 0.114s)
             Mean action noise std: 9.23
          Mean value_function loss: 27.0481
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.8747
                       Mean reward: 847.76
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 173.5417
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.2040
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 0.90s
                      Time elapsed: 00:32:53
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 105184 steps/s (collection: 0.810s, learning 0.124s)
             Mean action noise std: 9.23
          Mean value_function loss: 24.4759
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.8792
                       Mean reward: 884.00
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 173.4403
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.2054
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 0.93s
                      Time elapsed: 00:32:54
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 105753 steps/s (collection: 0.838s, learning 0.092s)
             Mean action noise std: 9.24
          Mean value_function loss: 15.0409
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.8824
                       Mean reward: 884.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 172.7772
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.2064
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 0.93s
                      Time elapsed: 00:32:55
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 104626 steps/s (collection: 0.821s, learning 0.118s)
             Mean action noise std: 9.24
          Mean value_function loss: 21.3895
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.8871
                       Mean reward: 885.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 174.1754
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.2054
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 0.94s
                      Time elapsed: 00:32:56
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 102930 steps/s (collection: 0.835s, learning 0.120s)
             Mean action noise std: 9.24
          Mean value_function loss: 28.4079
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.8899
                       Mean reward: 874.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 174.1019
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.2058
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 0.96s
                      Time elapsed: 00:32:57
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 112007 steps/s (collection: 0.780s, learning 0.098s)
             Mean action noise std: 9.25
          Mean value_function loss: 23.0570
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 28.8931
                       Mean reward: 866.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 173.2984
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2071
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 0.88s
                      Time elapsed: 00:32:57
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 102521 steps/s (collection: 0.830s, learning 0.129s)
             Mean action noise std: 9.25
          Mean value_function loss: 20.8827
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 28.8961
                       Mean reward: 880.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 174.5483
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2080
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 0.96s
                      Time elapsed: 00:32:58
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 103432 steps/s (collection: 0.812s, learning 0.138s)
             Mean action noise std: 9.25
          Mean value_function loss: 17.4125
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.8987
                       Mean reward: 868.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 173.3355
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2072
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 0.95s
                      Time elapsed: 00:32:59
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 105657 steps/s (collection: 0.808s, learning 0.122s)
             Mean action noise std: 9.26
          Mean value_function loss: 23.4965
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.9006
                       Mean reward: 873.65
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 174.4122
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 0.93s
                      Time elapsed: 00:33:00
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 104928 steps/s (collection: 0.807s, learning 0.130s)
             Mean action noise std: 9.26
          Mean value_function loss: 24.3415
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.9050
                       Mean reward: 871.40
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7813
     Episode_Reward/lifting_object: 173.3479
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.2109
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 0.94s
                      Time elapsed: 00:33:01
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 109416 steps/s (collection: 0.793s, learning 0.106s)
             Mean action noise std: 9.27
          Mean value_function loss: 22.9066
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.9108
                       Mean reward: 860.76
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7874
     Episode_Reward/lifting_object: 173.6876
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.2094
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 0.90s
                      Time elapsed: 00:33:02
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 105817 steps/s (collection: 0.834s, learning 0.095s)
             Mean action noise std: 9.28
          Mean value_function loss: 23.4657
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.9179
                       Mean reward: 869.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 173.9299
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.2105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 0.93s
                      Time elapsed: 00:33:03
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 112241 steps/s (collection: 0.785s, learning 0.091s)
             Mean action noise std: 9.28
          Mean value_function loss: 15.2463
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.9243
                       Mean reward: 861.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 173.3277
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.2103
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 0.88s
                      Time elapsed: 00:33:04
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 105615 steps/s (collection: 0.837s, learning 0.094s)
             Mean action noise std: 9.29
          Mean value_function loss: 24.4250
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.9307
                       Mean reward: 866.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 174.6503
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 0.93s
                      Time elapsed: 00:33:05
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 107603 steps/s (collection: 0.822s, learning 0.092s)
             Mean action noise std: 9.30
          Mean value_function loss: 23.4508
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.9382
                       Mean reward: 870.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 172.7075
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.2102
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 0.91s
                      Time elapsed: 00:33:06
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 109221 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 9.30
          Mean value_function loss: 27.0565
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.9427
                       Mean reward: 885.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7934
     Episode_Reward/lifting_object: 175.4633
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.2107
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 0.90s
                      Time elapsed: 00:33:07
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 109464 steps/s (collection: 0.797s, learning 0.101s)
             Mean action noise std: 9.30
          Mean value_function loss: 19.4231
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.9436
                       Mean reward: 875.31
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 174.7629
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2098
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 0.90s
                      Time elapsed: 00:33:08
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 106356 steps/s (collection: 0.813s, learning 0.111s)
             Mean action noise std: 9.31
          Mean value_function loss: 19.1286
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.9477
                       Mean reward: 876.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 174.2659
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.2106
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 0.92s
                      Time elapsed: 00:33:08
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 102151 steps/s (collection: 0.803s, learning 0.160s)
             Mean action noise std: 9.31
          Mean value_function loss: 17.4448
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.9517
                       Mean reward: 883.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7832
     Episode_Reward/lifting_object: 173.7888
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.2105
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 0.96s
                      Time elapsed: 00:33:09
                               ETA: 00:00:00

