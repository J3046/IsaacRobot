################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 9354 steps/s (collection: 10.103s, learning 0.407s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 36.9339
                       Mean reward: 0.00
               Mean episode length: 21.94
    Episode_Reward/reaching_object: 0.0009
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 10.51s
                      Time elapsed: 00:00:10
                               ETA: 05:50:18

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 12693 steps/s (collection: 7.586s, learning 0.158s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 37.0386
                       Mean reward: 0.01
               Mean episode length: 45.00
    Episode_Reward/reaching_object: 0.0028
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0011
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 7.74s
                      Time elapsed: 00:00:18
                               ETA: 05:04:04

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 13737 steps/s (collection: 7.007s, learning 0.149s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.0754
                       Mean reward: 0.02
               Mean episode length: 69.37
    Episode_Reward/reaching_object: 0.0048
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 7.16s
                      Time elapsed: 00:00:25
                               ETA: 04:42:02

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 13513 steps/s (collection: 7.116s, learning 0.159s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.1383
                       Mean reward: 0.02
               Mean episode length: 93.77
    Episode_Reward/reaching_object: 0.0069
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 7.27s
                      Time elapsed: 00:00:32
                               ETA: 04:31:57

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 13811 steps/s (collection: 6.953s, learning 0.164s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.1531
                       Mean reward: 0.03
               Mean episode length: 117.34
    Episode_Reward/reaching_object: 0.0095
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0033
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 7.12s
                      Time elapsed: 00:00:39
                               ETA: 04:24:48

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14310 steps/s (collection: 6.700s, learning 0.169s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 37.1852
                       Mean reward: 0.04
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.0124
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.87s
                      Time elapsed: 00:00:46
                               ETA: 04:18:37

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14059 steps/s (collection: 6.845s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 37.2237
                       Mean reward: 0.06
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.0181
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.99s
                      Time elapsed: 00:00:53
                               ETA: 04:14:45

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 13384 steps/s (collection: 7.192s, learning 0.153s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 37.2613
                       Mean reward: 0.08
               Mean episode length: 189.52
    Episode_Reward/reaching_object: 0.0207
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 7.34s
                      Time elapsed: 00:01:01
                               ETA: 04:13:18

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 14076 steps/s (collection: 6.866s, learning 0.118s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 37.2977
                       Mean reward: 0.10
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 0.0283
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 6.98s
                      Time elapsed: 00:01:07
                               ETA: 04:10:48

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 49142 steps/s (collection: 1.854s, learning 0.147s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 37.3604
                       Mean reward: 0.15
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.0373
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.00s
                      Time elapsed: 00:01:09
                               ETA: 03:52:15

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 47204 steps/s (collection: 1.964s, learning 0.118s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 37.3627
                       Mean reward: 0.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0426
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.08s
                      Time elapsed: 00:01:12
                               ETA: 03:37:18

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 52998 steps/s (collection: 1.748s, learning 0.107s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 37.3617
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0498
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.85s
                      Time elapsed: 00:01:13
                               ETA: 03:24:13

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 52009 steps/s (collection: 1.765s, learning 0.125s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 37.3955
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0618
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.89s
                      Time elapsed: 00:01:15
                               ETA: 03:13:14

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 48226 steps/s (collection: 1.920s, learning 0.119s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 37.4007
                       Mean reward: 0.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0814
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 2.04s
                      Time elapsed: 00:01:17
                               ETA: 03:04:10

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 47780 steps/s (collection: 1.923s, learning 0.135s)
             Mean action noise std: 1.02
          Mean value_function loss: 3.0577
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.4579
                       Mean reward: -0.92
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.0960
     Episode_Reward/lifting_object: -0.0754
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.06s
                      Time elapsed: 00:01:19
                               ETA: 02:56:20

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 46443 steps/s (collection: 1.972s, learning 0.145s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.8820
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.5394
                       Mean reward: 0.58
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.1169
     Episode_Reward/lifting_object: -0.3330
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.12s
                      Time elapsed: 00:01:22
                               ETA: 02:49:36

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 48804 steps/s (collection: 1.898s, learning 0.117s)
             Mean action noise std: 1.04
          Mean value_function loss: 1.5857
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 37.7558
                       Mean reward: -0.90
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.1435
     Episode_Reward/lifting_object: -0.2418
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.01s
                      Time elapsed: 00:01:24
                               ETA: 02:43:28

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 46014 steps/s (collection: 2.016s, learning 0.120s)
             Mean action noise std: 1.04
          Mean value_function loss: 3.1595
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 37.9136
                       Mean reward: -0.59
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.1786
     Episode_Reward/lifting_object: -0.3635
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.14s
                      Time elapsed: 00:01:26
                               ETA: 02:38:14

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 45638 steps/s (collection: 2.015s, learning 0.139s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.9683
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.1600
                       Mean reward: 0.37
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.1871
     Episode_Reward/lifting_object: -0.2673
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.15s
                      Time elapsed: 00:01:28
                               ETA: 02:33:34

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 46924 steps/s (collection: 1.958s, learning 0.137s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.4852
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.3180
                       Mean reward: 0.46
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.1958
     Episode_Reward/lifting_object: -0.0474
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.09s
                      Time elapsed: 00:01:30
                               ETA: 02:29:17

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 46187 steps/s (collection: 1.985s, learning 0.143s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3008
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.5239
                       Mean reward: 0.76
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.2038
     Episode_Reward/lifting_object: -0.1709
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.13s
                      Time elapsed: 00:01:32
                               ETA: 02:25:26

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 44952 steps/s (collection: 2.060s, learning 0.127s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.9682
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.6864
                       Mean reward: -0.20
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.2083
     Episode_Reward/lifting_object: -0.0914
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.19s
                      Time elapsed: 00:01:34
                               ETA: 02:22:02

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 47880 steps/s (collection: 1.951s, learning 0.103s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0881
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.8465
                       Mean reward: 0.96
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.2243
     Episode_Reward/lifting_object: -0.0795
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.05s
                      Time elapsed: 00:01:36
                               ETA: 02:18:44

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 48990 steps/s (collection: 1.853s, learning 0.153s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1251
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.0765
                       Mean reward: 0.90
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.2149
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.01s
                      Time elapsed: 00:01:38
                               ETA: 02:15:39

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 49086 steps/s (collection: 1.857s, learning 0.146s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.1947
                       Mean reward: 0.98
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.2038
     Episode_Reward/lifting_object: -0.0284
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.00s
                      Time elapsed: 00:01:40
                               ETA: 02:12:47

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 48785 steps/s (collection: 1.855s, learning 0.161s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0253
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.4029
                       Mean reward: 0.73
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.1794
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.02s
                      Time elapsed: 00:01:42
                               ETA: 02:10:10

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 47702 steps/s (collection: 1.902s, learning 0.159s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.2318
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.4703
                       Mean reward: 0.17
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.1761
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.06s
                      Time elapsed: 00:01:44
                               ETA: 02:07:48

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 48536 steps/s (collection: 1.917s, learning 0.108s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.3963
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.6584
                       Mean reward: -0.24
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.1692
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.03s
                      Time elapsed: 00:01:46
                               ETA: 02:05:33

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 49479 steps/s (collection: 1.857s, learning 0.130s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 39.7913
                       Mean reward: 0.84
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.1710
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.99s
                      Time elapsed: 00:01:48
                               ETA: 02:03:24

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 45023 steps/s (collection: 2.028s, learning 0.155s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.1779
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.9256
                       Mean reward: 0.41
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.1744
     Episode_Reward/lifting_object: -0.0447
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.18s
                      Time elapsed: 00:01:51
                               ETA: 02:01:37

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 41848 steps/s (collection: 2.231s, learning 0.118s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.9878
                       Mean reward: 0.72
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.1634
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.35s
                      Time elapsed: 00:01:53
                               ETA: 02:00:08

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 41926 steps/s (collection: 2.160s, learning 0.185s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.2035
                       Mean reward: 0.75
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.1704
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.34s
                      Time elapsed: 00:01:55
                               ETA: 01:58:43

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 46723 steps/s (collection: 1.981s, learning 0.123s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.5176
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3489
                       Mean reward: 0.04
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.1793
     Episode_Reward/lifting_object: -0.0918
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.10s
                      Time elapsed: 00:01:57
                               ETA: 01:57:09

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 48751 steps/s (collection: 1.896s, learning 0.120s)
             Mean action noise std: 1.15
          Mean value_function loss: 1.6320
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.4086
                       Mean reward: 1.01
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.1977
     Episode_Reward/lifting_object: -0.1435
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.02s
                      Time elapsed: 00:01:59
                               ETA: 01:55:36

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 48783 steps/s (collection: 1.891s, learning 0.124s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.7696
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.5081
                       Mean reward: 0.01
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.2183
     Episode_Reward/lifting_object: -0.1359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.02s
                      Time elapsed: 00:02:01
                               ETA: 01:54:07

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 47900 steps/s (collection: 1.929s, learning 0.123s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.8573
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.5897
                       Mean reward: 0.55
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.2349
     Episode_Reward/lifting_object: -0.2173
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.05s
                      Time elapsed: 00:02:03
                               ETA: 01:52:46

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 46412 steps/s (collection: 1.996s, learning 0.123s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2430
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.6933
                       Mean reward: 1.14
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.2429
     Episode_Reward/lifting_object: -0.0999
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.12s
                      Time elapsed: 00:02:06
                               ETA: 01:51:32

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 48173 steps/s (collection: 1.925s, learning 0.116s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.4198
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.8032
                       Mean reward: 1.02
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.2537
     Episode_Reward/lifting_object: -0.0793
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.04s
                      Time elapsed: 00:02:08
                               ETA: 01:50:18

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 48430 steps/s (collection: 1.910s, learning 0.120s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.1365
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.8995
                       Mean reward: 1.23
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.2575
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.03s
                      Time elapsed: 00:02:10
                               ETA: 01:49:07

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 48786 steps/s (collection: 1.899s, learning 0.116s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0317
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.0192
                       Mean reward: 1.17
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.2556
     Episode_Reward/lifting_object: -0.0470
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 2.01s
                      Time elapsed: 00:02:12
                               ETA: 01:47:59

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 48292 steps/s (collection: 1.911s, learning 0.125s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0529
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.2308
                       Mean reward: 1.29
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.2591
     Episode_Reward/lifting_object: -0.0237
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.04s
                      Time elapsed: 00:02:14
                               ETA: 01:46:55

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 47259 steps/s (collection: 1.951s, learning 0.129s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0556
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.2877
                       Mean reward: 0.93
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.2591
     Episode_Reward/lifting_object: -0.0295
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.08s
                      Time elapsed: 00:02:16
                               ETA: 01:45:56

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 49108 steps/s (collection: 1.897s, learning 0.105s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.2349
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.4240
                       Mean reward: 1.24
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.2609
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.00s
                      Time elapsed: 00:02:18
                               ETA: 01:44:56

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 49424 steps/s (collection: 1.875s, learning 0.114s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2869
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4739
                       Mean reward: 1.12
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.2536
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.99s
                      Time elapsed: 00:02:20
                               ETA: 01:43:58

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 49062 steps/s (collection: 1.895s, learning 0.109s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.6077
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.5716
                       Mean reward: 1.16
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.2621
     Episode_Reward/lifting_object: -0.1387
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.00s
                      Time elapsed: 00:02:22
                               ETA: 01:43:04

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 47745 steps/s (collection: 1.946s, learning 0.113s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.5208
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.6392
                       Mean reward: 0.78
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: -0.1147
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.06s
                      Time elapsed: 00:02:24
                               ETA: 01:42:14

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 48858 steps/s (collection: 1.903s, learning 0.109s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.6027
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.7365
                       Mean reward: 0.97
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.2530
     Episode_Reward/lifting_object: -0.1325
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.01s
                      Time elapsed: 00:02:26
                               ETA: 01:41:24

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 49295 steps/s (collection: 1.886s, learning 0.109s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.5727
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.8039
                       Mean reward: 1.22
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.2607
     Episode_Reward/lifting_object: -0.1120
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.99s
                      Time elapsed: 00:02:28
                               ETA: 01:40:35

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 47741 steps/s (collection: 1.953s, learning 0.107s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0623
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.9018
                       Mean reward: 0.82
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.2635
     Episode_Reward/lifting_object: -0.0311
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.06s
                      Time elapsed: 00:02:30
                               ETA: 01:39:51

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 48327 steps/s (collection: 1.914s, learning 0.120s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3665
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.0466
                       Mean reward: 1.31
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.2703
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.03s
                      Time elapsed: 00:02:32
                               ETA: 01:39:07

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 47600 steps/s (collection: 1.958s, learning 0.107s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1405
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.0954
                       Mean reward: 0.91
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.2908
     Episode_Reward/lifting_object: -0.0560
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.07s
                      Time elapsed: 00:02:34
                               ETA: 01:38:27

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 47548 steps/s (collection: 1.968s, learning 0.099s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.2050
                       Mean reward: 1.47
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.2908
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.07s
                      Time elapsed: 00:02:36
                               ETA: 01:37:48

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 48044 steps/s (collection: 1.926s, learning 0.120s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1657
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 42.3583
                       Mean reward: 1.04
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.2822
     Episode_Reward/lifting_object: -0.0269
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.05s
                      Time elapsed: 00:02:38
                               ETA: 01:37:09

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 48124 steps/s (collection: 1.923s, learning 0.120s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1240
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.4015
                       Mean reward: 1.33
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.2810
     Episode_Reward/lifting_object: -0.0339
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.04s
                      Time elapsed: 00:02:40
                               ETA: 01:36:32

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 49078 steps/s (collection: 1.909s, learning 0.094s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.2731
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.5231
                       Mean reward: 1.37
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 0.2943
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.00s
                      Time elapsed: 00:02:42
                               ETA: 01:35:55

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 47281 steps/s (collection: 1.982s, learning 0.098s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0849
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.5883
                       Mean reward: 1.22
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.2923
     Episode_Reward/lifting_object: -0.0485
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.08s
                      Time elapsed: 00:02:44
                               ETA: 01:35:21

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 48673 steps/s (collection: 1.920s, learning 0.100s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.2397
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 42.7602
                       Mean reward: 0.81
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.2797
     Episode_Reward/lifting_object: -0.0337
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.02s
                      Time elapsed: 00:02:46
                               ETA: 01:34:47

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 48043 steps/s (collection: 1.941s, learning 0.105s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.8139
                       Mean reward: 1.33
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.2932
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.05s
                      Time elapsed: 00:02:48
                               ETA: 01:34:14

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 48027 steps/s (collection: 1.944s, learning 0.103s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0389
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.9231
                       Mean reward: 1.41
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 0.2934
     Episode_Reward/lifting_object: -0.0155
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.05s
                      Time elapsed: 00:02:50
                               ETA: 01:33:43

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 46945 steps/s (collection: 1.988s, learning 0.106s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1741
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.0800
                       Mean reward: 1.55
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 0.3081
     Episode_Reward/lifting_object: -0.0034
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.09s
                      Time elapsed: 00:02:52
                               ETA: 01:33:14

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 46371 steps/s (collection: 2.009s, learning 0.111s)
             Mean action noise std: 1.27
          Mean value_function loss: 1.1571
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.1236
                       Mean reward: 0.88
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 0.3074
     Episode_Reward/lifting_object: -0.1438
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.12s
                      Time elapsed: 00:02:55
                               ETA: 01:32:47

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 47458 steps/s (collection: 1.959s, learning 0.112s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.0829
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.1907
                       Mean reward: -0.66
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: -0.1679
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.07s
                      Time elapsed: 00:02:57
                               ETA: 01:32:19

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 47701 steps/s (collection: 1.964s, learning 0.097s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.2436
                       Mean reward: 1.61
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 0.3401
     Episode_Reward/lifting_object: -0.0170
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.06s
                      Time elapsed: 00:02:59
                               ETA: 01:31:52

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 44969 steps/s (collection: 2.017s, learning 0.169s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1876
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.3498
                       Mean reward: 1.21
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 0.3327
     Episode_Reward/lifting_object: -0.0477
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.19s
                      Time elapsed: 00:03:01
                               ETA: 01:31:29

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 47178 steps/s (collection: 1.963s, learning 0.120s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0574
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.3945
                       Mean reward: 1.74
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 0.3481
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.08s
                      Time elapsed: 00:03:03
                               ETA: 01:31:04

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 48812 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0610
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.5458
                       Mean reward: 1.42
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 0.3394
     Episode_Reward/lifting_object: -0.0248
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.01s
                      Time elapsed: 00:03:05
                               ETA: 01:30:37

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 48354 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 43.6929
                       Mean reward: 1.76
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 0.3623
     Episode_Reward/lifting_object: 0.0015
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.03s
                      Time elapsed: 00:03:07
                               ETA: 01:30:12

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 47527 steps/s (collection: 1.973s, learning 0.096s)
             Mean action noise std: 1.30
          Mean value_function loss: 2.9239
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.7433
                       Mean reward: 1.70
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 0.3558
     Episode_Reward/lifting_object: -0.2136
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.07s
                      Time elapsed: 00:03:09
                               ETA: 01:29:49

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 46691 steps/s (collection: 2.012s, learning 0.093s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0558
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.7709
                       Mean reward: 1.55
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 0.3670
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.11s
                      Time elapsed: 00:03:11
                               ETA: 01:29:27

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 44989 steps/s (collection: 2.054s, learning 0.131s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.3220
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.8584
                       Mean reward: 1.75
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 0.3771
     Episode_Reward/lifting_object: -0.0568
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.19s
                      Time elapsed: 00:03:13
                               ETA: 01:29:08

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 45219 steps/s (collection: 2.082s, learning 0.092s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.1990
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.8940
                       Mean reward: 1.79
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 0.3722
     Episode_Reward/lifting_object: -0.0356
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.17s
                      Time elapsed: 00:03:16
                               ETA: 01:28:49

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 47651 steps/s (collection: 1.965s, learning 0.098s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.2169
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.0313
                       Mean reward: 1.64
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 0.3614
     Episode_Reward/lifting_object: -0.0357
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.06s
                      Time elapsed: 00:03:18
                               ETA: 01:28:27

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 47053 steps/s (collection: 1.977s, learning 0.112s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2249
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.1830
                       Mean reward: 1.55
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 0.3810
     Episode_Reward/lifting_object: -0.0616
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.09s
                      Time elapsed: 00:03:20
                               ETA: 01:28:07

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 44783 steps/s (collection: 2.068s, learning 0.128s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2094
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.2258
                       Mean reward: 1.86
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 0.3746
     Episode_Reward/lifting_object: -0.0358
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.20s
                      Time elapsed: 00:03:22
                               ETA: 01:27:50

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 42614 steps/s (collection: 2.185s, learning 0.122s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.9636
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.3102
                       Mean reward: 1.77
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.3697
     Episode_Reward/lifting_object: -0.0014
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.31s
                      Time elapsed: 00:03:24
                               ETA: 01:27:36

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 46647 steps/s (collection: 1.980s, learning 0.127s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1657
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.3585
                       Mean reward: 1.89
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 0.3686
     Episode_Reward/lifting_object: -0.0800
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.11s
                      Time elapsed: 00:03:26
                               ETA: 01:27:18

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 46966 steps/s (collection: 1.995s, learning 0.098s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1307
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.5169
                       Mean reward: 1.70
               Mean episode length: 215.83
    Episode_Reward/reaching_object: 0.3785
     Episode_Reward/lifting_object: -0.0150
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.09s
                      Time elapsed: 00:03:28
                               ETA: 01:26:59

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 46623 steps/s (collection: 2.003s, learning 0.105s)
             Mean action noise std: 1.35
          Mean value_function loss: 2.2228
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.5630
                       Mean reward: -0.62
               Mean episode length: 215.91
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: -0.1649
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.11s
                      Time elapsed: 00:03:31
                               ETA: 01:26:42

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 46665 steps/s (collection: 2.008s, learning 0.098s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0636
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.6176
                       Mean reward: 1.46
               Mean episode length: 217.96
    Episode_Reward/reaching_object: 0.3514
     Episode_Reward/lifting_object: -0.0329
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.11s
                      Time elapsed: 00:03:33
                               ETA: 01:26:24

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 46058 steps/s (collection: 2.042s, learning 0.092s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2869
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.7094
                       Mean reward: 1.31
               Mean episode length: 217.01
    Episode_Reward/reaching_object: 0.3704
     Episode_Reward/lifting_object: -0.0430
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.13s
                      Time elapsed: 00:03:35
                               ETA: 01:26:08

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 47198 steps/s (collection: 1.985s, learning 0.098s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0845
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.7729
                       Mean reward: 1.76
               Mean episode length: 215.01
    Episode_Reward/reaching_object: 0.3677
     Episode_Reward/lifting_object: -0.0255
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.08s
                      Time elapsed: 00:03:37
                               ETA: 01:25:51

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 45705 steps/s (collection: 2.053s, learning 0.098s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0633
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.9148
                       Mean reward: 1.71
               Mean episode length: 206.24
    Episode_Reward/reaching_object: 0.3566
     Episode_Reward/lifting_object: -0.0315
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.15s
                      Time elapsed: 00:03:39
                               ETA: 01:25:36

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 44461 steps/s (collection: 2.090s, learning 0.121s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.7409
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.9936
                       Mean reward: 0.52
               Mean episode length: 206.38
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: -0.1516
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.21s
                      Time elapsed: 00:03:41
                               ETA: 01:25:22

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 46364 steps/s (collection: 1.991s, learning 0.130s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.0490
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.0275
                       Mean reward: 0.56
               Mean episode length: 205.22
    Episode_Reward/reaching_object: 0.3759
     Episode_Reward/lifting_object: -0.0994
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.12s
                      Time elapsed: 00:03:43
                               ETA: 01:25:07

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 42353 steps/s (collection: 2.222s, learning 0.099s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0546
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.0811
                       Mean reward: 1.57
               Mean episode length: 201.73
    Episode_Reward/reaching_object: 0.3868
     Episode_Reward/lifting_object: -0.0434
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.32s
                      Time elapsed: 00:03:46
                               ETA: 01:24:57

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 44246 steps/s (collection: 2.120s, learning 0.102s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.6707
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.1807
                       Mean reward: 0.26
               Mean episode length: 202.74
    Episode_Reward/reaching_object: 0.3692
     Episode_Reward/lifting_object: -0.0866
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.22s
                      Time elapsed: 00:03:48
                               ETA: 01:24:44

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 46602 steps/s (collection: 2.005s, learning 0.105s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1945
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.2130
                       Mean reward: 1.91
               Mean episode length: 211.92
    Episode_Reward/reaching_object: 0.3905
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.11s
                      Time elapsed: 00:03:50
                               ETA: 01:24:30

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 46327 steps/s (collection: 2.006s, learning 0.116s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.7607
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.2928
                       Mean reward: 0.52
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 0.3842
     Episode_Reward/lifting_object: -0.1505
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.12s
                      Time elapsed: 00:03:52
                               ETA: 01:24:16

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 45965 steps/s (collection: 2.032s, learning 0.107s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1927
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.3273
                       Mean reward: 1.87
               Mean episode length: 213.93
    Episode_Reward/reaching_object: 0.3769
     Episode_Reward/lifting_object: -0.0540
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.14s
                      Time elapsed: 00:03:54
                               ETA: 01:24:02

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 46879 steps/s (collection: 1.985s, learning 0.112s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 45.3780
                       Mean reward: 1.86
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 0.3901
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.10s
                      Time elapsed: 00:03:56
                               ETA: 01:23:48

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 45722 steps/s (collection: 2.050s, learning 0.100s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.5652
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.4446
                       Mean reward: 1.87
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 0.3861
     Episode_Reward/lifting_object: -0.0197
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.15s
                      Time elapsed: 00:03:58
                               ETA: 01:23:35

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 45053 steps/s (collection: 2.081s, learning 0.101s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0095
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4706
                       Mean reward: 1.90
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 0.3971
     Episode_Reward/lifting_object: -0.0506
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.18s
                      Time elapsed: 00:04:01
                               ETA: 01:23:23

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 45905 steps/s (collection: 2.034s, learning 0.108s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0364
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.5587
                       Mean reward: 1.54
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 0.3593
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.14s
                      Time elapsed: 00:04:03
                               ETA: 01:23:11

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 44456 steps/s (collection: 2.096s, learning 0.115s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1267
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.6959
                       Mean reward: 1.27
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 0.3748
     Episode_Reward/lifting_object: -0.0412
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.21s
                      Time elapsed: 00:04:05
                               ETA: 01:23:00

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 45683 steps/s (collection: 2.048s, learning 0.104s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0479
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.7583
                       Mean reward: 1.36
               Mean episode length: 221.15
    Episode_Reward/reaching_object: 0.3595
     Episode_Reward/lifting_object: -0.0192
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.15s
                      Time elapsed: 00:04:07
                               ETA: 01:22:48

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 46264 steps/s (collection: 2.028s, learning 0.097s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.6808
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.8530
                       Mean reward: 1.77
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 0.3563
     Episode_Reward/lifting_object: -0.0993
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.12s
                      Time elapsed: 00:04:09
                               ETA: 01:22:36

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 46806 steps/s (collection: 2.006s, learning 0.095s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0367
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.8777
                       Mean reward: 1.69
               Mean episode length: 214.74
    Episode_Reward/reaching_object: 0.3540
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.10s
                      Time elapsed: 00:04:11
                               ETA: 01:22:24

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 45399 steps/s (collection: 2.060s, learning 0.106s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1890
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 45.9333
                       Mean reward: 1.75
               Mean episode length: 208.49
    Episode_Reward/reaching_object: 0.3629
     Episode_Reward/lifting_object: -0.0789
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.17s
                      Time elapsed: 00:04:14
                               ETA: 01:22:13

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 45113 steps/s (collection: 2.057s, learning 0.123s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.5295
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.9584
                       Mean reward: 0.84
               Mean episode length: 203.72
    Episode_Reward/reaching_object: 0.3689
     Episode_Reward/lifting_object: -0.0349
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.18s
                      Time elapsed: 00:04:16
                               ETA: 01:22:02

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 46052 steps/s (collection: 2.025s, learning 0.109s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.2465
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.0667
                       Mean reward: 1.85
               Mean episode length: 205.01
    Episode_Reward/reaching_object: 0.3811
     Episode_Reward/lifting_object: -0.0714
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.13s
                      Time elapsed: 00:04:18
                               ETA: 01:21:51

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 45625 steps/s (collection: 2.044s, learning 0.111s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.6593
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.2100
                       Mean reward: 0.73
               Mean episode length: 198.66
    Episode_Reward/reaching_object: 0.3837
     Episode_Reward/lifting_object: -0.1038
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.15s
                      Time elapsed: 00:04:20
                               ETA: 01:21:40

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 45783 steps/s (collection: 2.030s, learning 0.117s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.2156
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.2349
                       Mean reward: 0.67
               Mean episode length: 197.62
    Episode_Reward/reaching_object: 0.4010
     Episode_Reward/lifting_object: -0.1924
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.15s
                      Time elapsed: 00:04:22
                               ETA: 01:21:30

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 45286 steps/s (collection: 2.042s, learning 0.129s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.3942
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.2743
                       Mean reward: 1.14
               Mean episode length: 192.71
    Episode_Reward/reaching_object: 0.4016
     Episode_Reward/lifting_object: -0.0934
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.17s
                      Time elapsed: 00:04:24
                               ETA: 01:21:20

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 45261 steps/s (collection: 2.044s, learning 0.128s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0708
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.3460
                       Mean reward: 2.30
               Mean episode length: 201.46
    Episode_Reward/reaching_object: 0.4286
     Episode_Reward/lifting_object: 0.0119
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.17s
                      Time elapsed: 00:04:27
                               ETA: 01:21:10

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 45795 steps/s (collection: 2.047s, learning 0.100s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1369
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 46.4630
                       Mean reward: 1.69
               Mean episode length: 206.99
    Episode_Reward/reaching_object: 0.4322
     Episode_Reward/lifting_object: -0.0063
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.15s
                      Time elapsed: 00:04:29
                               ETA: 01:21:00

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 47041 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.3472
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.4942
                       Mean reward: 2.00
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.4440
     Episode_Reward/lifting_object: -0.0583
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.09s
                      Time elapsed: 00:04:31
                               ETA: 01:20:49

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 46788 steps/s (collection: 1.995s, learning 0.106s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1078
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.5357
                       Mean reward: 1.86
               Mean episode length: 204.10
    Episode_Reward/reaching_object: 0.4423
     Episode_Reward/lifting_object: -0.0610
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.10s
                      Time elapsed: 00:04:33
                               ETA: 01:20:38

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 46406 steps/s (collection: 2.009s, learning 0.110s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.5264
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.6329
                       Mean reward: 1.82
               Mean episode length: 221.06
    Episode_Reward/reaching_object: 0.4511
     Episode_Reward/lifting_object: -0.0169
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.12s
                      Time elapsed: 00:04:35
                               ETA: 01:20:28

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 43882 steps/s (collection: 2.115s, learning 0.125s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1839
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.6641
                       Mean reward: 2.21
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 0.4632
     Episode_Reward/lifting_object: -0.0779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.24s
                      Time elapsed: 00:04:37
                               ETA: 01:20:20

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 47159 steps/s (collection: 1.986s, learning 0.098s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.5116
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.7341
                       Mean reward: 2.23
               Mean episode length: 221.36
    Episode_Reward/reaching_object: 0.4507
     Episode_Reward/lifting_object: -0.0491
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.08s
                      Time elapsed: 00:04:39
                               ETA: 01:20:09

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 46333 steps/s (collection: 1.994s, learning 0.128s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1328
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.7686
                       Mean reward: 2.18
               Mean episode length: 221.37
    Episode_Reward/reaching_object: 0.4579
     Episode_Reward/lifting_object: -0.0779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.12s
                      Time elapsed: 00:04:41
                               ETA: 01:20:00

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 46814 steps/s (collection: 1.985s, learning 0.115s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.3898
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.8618
                       Mean reward: 1.35
               Mean episode length: 214.57
    Episode_Reward/reaching_object: 0.4407
     Episode_Reward/lifting_object: -0.0828
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.10s
                      Time elapsed: 00:04:44
                               ETA: 01:19:50

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 45860 steps/s (collection: 2.042s, learning 0.102s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1169
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.8943
                       Mean reward: 1.80
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 0.4428
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.14s
                      Time elapsed: 00:04:46
                               ETA: 01:19:41

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 46274 steps/s (collection: 2.024s, learning 0.101s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.5773
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.9428
                       Mean reward: 1.61
               Mean episode length: 213.75
    Episode_Reward/reaching_object: 0.4333
     Episode_Reward/lifting_object: -0.0512
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.12s
                      Time elapsed: 00:04:48
                               ETA: 01:19:31

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 46029 steps/s (collection: 2.022s, learning 0.113s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.6022
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.9699
                       Mean reward: 2.05
               Mean episode length: 216.06
    Episode_Reward/reaching_object: 0.4462
     Episode_Reward/lifting_object: -0.0801
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.14s
                      Time elapsed: 00:04:50
                               ETA: 01:19:22

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 45526 steps/s (collection: 2.022s, learning 0.137s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.4025
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.0035
                       Mean reward: 2.14
               Mean episode length: 212.86
    Episode_Reward/reaching_object: 0.4546
     Episode_Reward/lifting_object: -0.0288
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.16s
                      Time elapsed: 00:04:52
                               ETA: 01:19:14

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 45440 steps/s (collection: 2.038s, learning 0.126s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.5803
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.0351
                       Mean reward: 2.21
               Mean episode length: 207.26
    Episode_Reward/reaching_object: 0.4660
     Episode_Reward/lifting_object: -0.0768
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.16s
                      Time elapsed: 00:04:54
                               ETA: 01:19:05

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 45885 steps/s (collection: 2.015s, learning 0.128s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.2392
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.0756
                       Mean reward: 1.76
               Mean episode length: 208.99
    Episode_Reward/reaching_object: 0.4499
     Episode_Reward/lifting_object: -0.0790
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.14s
                      Time elapsed: 00:04:56
                               ETA: 01:18:57

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 45682 steps/s (collection: 2.042s, learning 0.110s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1275
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.1614
                       Mean reward: 2.35
               Mean episode length: 209.60
    Episode_Reward/reaching_object: 0.4740
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.15s
                      Time elapsed: 00:04:59
                               ETA: 01:18:49

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 46347 steps/s (collection: 2.016s, learning 0.105s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3367
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.2278
                       Mean reward: 1.41
               Mean episode length: 206.57
    Episode_Reward/reaching_object: 0.4757
     Episode_Reward/lifting_object: -0.0696
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.12s
                      Time elapsed: 00:05:01
                               ETA: 01:18:40

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 44377 steps/s (collection: 2.118s, learning 0.097s)
             Mean action noise std: 1.49
          Mean value_function loss: 1.6755
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.2835
                       Mean reward: 0.66
               Mean episode length: 208.12
    Episode_Reward/reaching_object: 0.4770
     Episode_Reward/lifting_object: -0.3137
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.22s
                      Time elapsed: 00:05:03
                               ETA: 01:18:33

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 46096 steps/s (collection: 2.026s, learning 0.107s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.5837
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.3120
                       Mean reward: 2.16
               Mean episode length: 215.93
    Episode_Reward/reaching_object: 0.5329
     Episode_Reward/lifting_object: -0.1049
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.13s
                      Time elapsed: 00:05:05
                               ETA: 01:18:25

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 46548 steps/s (collection: 2.006s, learning 0.106s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0357
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.3463
                       Mean reward: 2.41
               Mean episode length: 227.16
    Episode_Reward/reaching_object: 0.5585
     Episode_Reward/lifting_object: -0.0389
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.11s
                      Time elapsed: 00:05:07
                               ETA: 01:18:16

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 46505 steps/s (collection: 2.006s, learning 0.108s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 47.4007
                       Mean reward: 2.92
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 0.5717
     Episode_Reward/lifting_object: -0.0080
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.11s
                      Time elapsed: 00:05:09
                               ETA: 01:18:08

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 45854 steps/s (collection: 2.030s, learning 0.113s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.8154
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.4414
                       Mean reward: 2.77
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 0.6021
     Episode_Reward/lifting_object: -0.0710
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.14s
                      Time elapsed: 00:05:11
                               ETA: 01:18:00

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 43594 steps/s (collection: 2.125s, learning 0.130s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.6116
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.4579
                       Mean reward: 1.59
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: -0.1288
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.25s
                      Time elapsed: 00:05:14
                               ETA: 01:17:54

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 45964 steps/s (collection: 2.024s, learning 0.115s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0894
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.4819
                       Mean reward: 2.46
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 0.6215
     Episode_Reward/lifting_object: -0.0195
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.14s
                      Time elapsed: 00:05:16
                               ETA: 01:17:46

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 45057 steps/s (collection: 2.076s, learning 0.106s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4682
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.5427
                       Mean reward: 2.88
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 0.5995
     Episode_Reward/lifting_object: -0.0254
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.18s
                      Time elapsed: 00:05:18
                               ETA: 01:17:39

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 45415 steps/s (collection: 2.063s, learning 0.101s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.3419
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.5650
                       Mean reward: 2.78
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 0.5739
     Episode_Reward/lifting_object: -0.0430
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.16s
                      Time elapsed: 00:05:20
                               ETA: 01:17:32

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 45209 steps/s (collection: 2.043s, learning 0.132s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4107
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.5890
                       Mean reward: 2.68
               Mean episode length: 214.97
    Episode_Reward/reaching_object: 0.5808
     Episode_Reward/lifting_object: -0.0849
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.17s
                      Time elapsed: 00:05:22
                               ETA: 01:17:25

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 45256 steps/s (collection: 2.049s, learning 0.123s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.8400
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.6532
                       Mean reward: 2.62
               Mean episode length: 216.92
    Episode_Reward/reaching_object: 0.5447
     Episode_Reward/lifting_object: -0.0705
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.17s
                      Time elapsed: 00:05:24
                               ETA: 01:17:18

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 45396 steps/s (collection: 2.027s, learning 0.139s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3360
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.6785
                       Mean reward: 2.60
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 0.5492
     Episode_Reward/lifting_object: 0.0011
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.17s
                      Time elapsed: 00:05:27
                               ETA: 01:17:11

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 43638 steps/s (collection: 2.154s, learning 0.099s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3153
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.7412
                       Mean reward: 2.06
               Mean episode length: 208.89
    Episode_Reward/reaching_object: 0.5418
     Episode_Reward/lifting_object: -0.1688
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.25s
                      Time elapsed: 00:05:29
                               ETA: 01:17:06

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 43566 steps/s (collection: 2.116s, learning 0.141s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.6975
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.7647
                       Mean reward: 2.36
               Mean episode length: 203.66
    Episode_Reward/reaching_object: 0.5484
     Episode_Reward/lifting_object: -0.0630
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.26s
                      Time elapsed: 00:05:31
                               ETA: 01:17:00

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 43205 steps/s (collection: 2.167s, learning 0.108s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.4878
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.7966
                       Mean reward: 1.56
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 0.5967
     Episode_Reward/lifting_object: -0.1014
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.28s
                      Time elapsed: 00:05:33
                               ETA: 01:16:55

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 44293 steps/s (collection: 2.099s, learning 0.120s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3285
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.8253
                       Mean reward: 3.07
               Mean episode length: 213.02
    Episode_Reward/reaching_object: 0.6257
     Episode_Reward/lifting_object: -0.0674
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.22s
                      Time elapsed: 00:05:36
                               ETA: 01:16:49

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 43600 steps/s (collection: 2.153s, learning 0.102s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.7627
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.8931
                       Mean reward: 3.22
               Mean episode length: 217.76
    Episode_Reward/reaching_object: 0.6500
     Episode_Reward/lifting_object: -0.0540
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.25s
                      Time elapsed: 00:05:38
                               ETA: 01:16:43

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 44556 steps/s (collection: 2.107s, learning 0.099s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2532
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.9328
                       Mean reward: 3.44
               Mean episode length: 220.28
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: -0.1047
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.21s
                      Time elapsed: 00:05:40
                               ETA: 01:16:37

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 45649 steps/s (collection: 2.047s, learning 0.107s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.0562
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 48.0103
                       Mean reward: 3.57
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 0.7336
     Episode_Reward/lifting_object: 0.0253
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.15s
                      Time elapsed: 00:05:42
                               ETA: 01:16:31

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 44135 steps/s (collection: 2.119s, learning 0.109s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3348
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.0960
                       Mean reward: 4.10
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 0.0437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.23s
                      Time elapsed: 00:05:44
                               ETA: 01:16:25

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 44899 steps/s (collection: 2.074s, learning 0.116s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2916
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.1163
                       Mean reward: 3.44
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 0.8072
     Episode_Reward/lifting_object: -0.0378
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.19s
                      Time elapsed: 00:05:47
                               ETA: 01:16:19

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 45039 steps/s (collection: 2.077s, learning 0.106s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1588
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.1435
                       Mean reward: 4.13
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 0.0401
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.18s
                      Time elapsed: 00:05:49
                               ETA: 01:16:13

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 44941 steps/s (collection: 2.084s, learning 0.103s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.7156
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.1601
                       Mean reward: 3.50
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: -0.0047
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.19s
                      Time elapsed: 00:05:51
                               ETA: 01:16:07

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 42503 steps/s (collection: 2.208s, learning 0.105s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1583
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.2022
                       Mean reward: 3.23
               Mean episode length: 219.91
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: -0.0295
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.31s
                      Time elapsed: 00:05:53
                               ETA: 01:16:03

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 44178 steps/s (collection: 2.101s, learning 0.124s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.4460
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.2551
                       Mean reward: 3.55
               Mean episode length: 227.71
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 0.0013
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.23s
                      Time elapsed: 00:05:56
                               ETA: 01:15:57

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 35357 steps/s (collection: 2.480s, learning 0.301s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2706
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.3399
                       Mean reward: 2.53
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: -0.1602
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.78s
                      Time elapsed: 00:05:58
                               ETA: 01:15:59

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 32029 steps/s (collection: 2.906s, learning 0.164s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.3985
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 48.3874
                       Mean reward: 4.03
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 0.7997
     Episode_Reward/lifting_object: -0.0024
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 3.07s
                      Time elapsed: 00:06:01
                               ETA: 01:16:04

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 34590 steps/s (collection: 2.680s, learning 0.162s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.5302
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.4195
                       Mean reward: 4.06
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 0.7939
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.84s
                      Time elapsed: 00:06:04
                               ETA: 01:16:06

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 34983 steps/s (collection: 2.625s, learning 0.185s)
             Mean action noise std: 1.56
          Mean value_function loss: 1.5057
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.4660
                       Mean reward: 3.53
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.8117
     Episode_Reward/lifting_object: -0.0995
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.81s
                      Time elapsed: 00:06:07
                               ETA: 01:16:08

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 35230 steps/s (collection: 2.645s, learning 0.146s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.6067
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.4893
                       Mean reward: 3.77
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.8179
     Episode_Reward/lifting_object: -0.0465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.79s
                      Time elapsed: 00:06:10
                               ETA: 01:16:10

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 39724 steps/s (collection: 2.286s, learning 0.189s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1702
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.5159
                       Mean reward: 3.90
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.8388
     Episode_Reward/lifting_object: -0.0944
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.47s
                      Time elapsed: 00:06:12
                               ETA: 01:16:07

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 37635 steps/s (collection: 2.416s, learning 0.196s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.7047
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.5683
                       Mean reward: 3.37
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.8705
     Episode_Reward/lifting_object: -0.1288
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.61s
                      Time elapsed: 00:06:15
                               ETA: 01:16:07

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 39648 steps/s (collection: 2.231s, learning 0.248s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1542
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.5864
                       Mean reward: 3.49
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: -0.0839
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.48s
                      Time elapsed: 00:06:17
                               ETA: 01:16:04

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 39623 steps/s (collection: 2.314s, learning 0.167s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4405
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.6240
                       Mean reward: 4.59
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: -0.0014
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.48s
                      Time elapsed: 00:06:20
                               ETA: 01:16:02

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 34991 steps/s (collection: 2.634s, learning 0.175s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1800
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.6445
                       Mean reward: 4.23
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.8880
     Episode_Reward/lifting_object: 0.0870
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.81s
                      Time elapsed: 00:06:23
                               ETA: 01:16:03

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 38136 steps/s (collection: 2.390s, learning 0.188s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3672
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.6857
                       Mean reward: 4.02
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.8739
     Episode_Reward/lifting_object: 0.0166
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.58s
                      Time elapsed: 00:06:25
                               ETA: 01:16:02

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 39314 steps/s (collection: 2.296s, learning 0.205s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.7039
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.7337
                       Mean reward: 3.46
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: -0.0488
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.50s
                      Time elapsed: 00:06:28
                               ETA: 01:16:00

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 37552 steps/s (collection: 2.414s, learning 0.204s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2957
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.7546
                       Mean reward: 4.25
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.8508
     Episode_Reward/lifting_object: -0.0174
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.62s
                      Time elapsed: 00:06:30
                               ETA: 01:15:59

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 36856 steps/s (collection: 2.466s, learning 0.202s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2314
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.8198
                       Mean reward: 4.26
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.8258
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.67s
                      Time elapsed: 00:06:33
                               ETA: 01:15:59

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 37348 steps/s (collection: 2.440s, learning 0.192s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.5635
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.8885
                       Mean reward: 3.28
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.8341
     Episode_Reward/lifting_object: -0.0158
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.63s
                      Time elapsed: 00:06:36
                               ETA: 01:15:58

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 40129 steps/s (collection: 2.332s, learning 0.118s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.6490
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.9010
                       Mean reward: 4.08
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: -0.0166
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.45s
                      Time elapsed: 00:06:38
                               ETA: 01:15:56

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 40030 steps/s (collection: 2.261s, learning 0.195s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1922
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.9154
                       Mean reward: 3.96
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.8514
     Episode_Reward/lifting_object: -0.0427
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.46s
                      Time elapsed: 00:06:41
                               ETA: 01:15:53

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 36150 steps/s (collection: 2.499s, learning 0.220s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1175
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 48.9528
                       Mean reward: 3.20
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.8326
     Episode_Reward/lifting_object: -0.0522
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.72s
                      Time elapsed: 00:06:43
                               ETA: 01:15:53

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 38686 steps/s (collection: 2.404s, learning 0.137s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0901
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.0159
                       Mean reward: 4.35
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.8304
     Episode_Reward/lifting_object: 0.0115
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.54s
                      Time elapsed: 00:06:46
                               ETA: 01:15:51

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 35069 steps/s (collection: 2.588s, learning 0.215s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.1356
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.0894
                       Mean reward: 4.40
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.8392
     Episode_Reward/lifting_object: 0.1018
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.80s
                      Time elapsed: 00:06:49
                               ETA: 01:15:52

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 37560 steps/s (collection: 2.468s, learning 0.149s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.0596
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.1495
                       Mean reward: 4.11
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.8390
     Episode_Reward/lifting_object: 0.0129
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.62s
                      Time elapsed: 00:06:51
                               ETA: 01:15:52

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 38461 steps/s (collection: 2.393s, learning 0.163s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3215
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.2050
                       Mean reward: 4.25
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 0.0399
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.56s
                      Time elapsed: 00:06:54
                               ETA: 01:15:50

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 39807 steps/s (collection: 2.259s, learning 0.210s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.0906
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.2531
                       Mean reward: 4.06
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.8173
     Episode_Reward/lifting_object: -0.0021
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.47s
                      Time elapsed: 00:06:56
                               ETA: 01:15:47

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 35014 steps/s (collection: 2.585s, learning 0.222s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3079
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.2995
                       Mean reward: 4.04
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 0.8232
     Episode_Reward/lifting_object: 0.0207
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.81s
                      Time elapsed: 00:06:59
                               ETA: 01:15:48

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 39650 steps/s (collection: 2.341s, learning 0.138s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2006
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.3123
                       Mean reward: 4.17
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.8051
     Episode_Reward/lifting_object: 0.0130
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.48s
                      Time elapsed: 00:07:02
                               ETA: 01:15:46

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 40677 steps/s (collection: 2.229s, learning 0.188s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.5502
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.3368
                       Mean reward: 3.66
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.8411
     Episode_Reward/lifting_object: 0.0060
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.42s
                      Time elapsed: 00:07:04
                               ETA: 01:15:43

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 40044 steps/s (collection: 2.296s, learning 0.159s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.1812
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.3531
                       Mean reward: 4.40
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.8073
     Episode_Reward/lifting_object: 0.0881
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.45s
                      Time elapsed: 00:07:06
                               ETA: 01:15:40

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 39838 steps/s (collection: 2.299s, learning 0.169s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.1858
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.4044
                       Mean reward: 3.88
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 0.7929
     Episode_Reward/lifting_object: 0.0449
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.47s
                      Time elapsed: 00:07:09
                               ETA: 01:15:37

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 41778 steps/s (collection: 2.216s, learning 0.137s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.6915
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.4480
                       Mean reward: 4.50
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.8468
     Episode_Reward/lifting_object: 0.0007
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.35s
                      Time elapsed: 00:07:11
                               ETA: 01:15:33

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 35056 steps/s (collection: 2.615s, learning 0.189s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2348
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.4662
                       Mean reward: 4.45
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.8623
     Episode_Reward/lifting_object: 0.0405
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.80s
                      Time elapsed: 00:07:14
                               ETA: 01:15:34

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 41504 steps/s (collection: 2.157s, learning 0.212s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.4667
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.5079
                       Mean reward: 4.26
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.8401
     Episode_Reward/lifting_object: 0.0417
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.37s
                      Time elapsed: 00:07:16
                               ETA: 01:15:31

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 36838 steps/s (collection: 2.483s, learning 0.185s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2680
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.5263
                       Mean reward: 4.27
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.8571
     Episode_Reward/lifting_object: 0.0264
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.67s
                      Time elapsed: 00:07:19
                               ETA: 01:15:30

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 40912 steps/s (collection: 2.215s, learning 0.188s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.8616
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.5684
                       Mean reward: 4.30
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 0.8323
     Episode_Reward/lifting_object: 0.0379
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.40s
                      Time elapsed: 00:07:22
                               ETA: 01:15:27

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 39786 steps/s (collection: 2.321s, learning 0.150s)
             Mean action noise std: 1.63
          Mean value_function loss: 5.1551
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.5962
                       Mean reward: 3.88
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.8306
     Episode_Reward/lifting_object: 0.0368
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.47s
                      Time elapsed: 00:07:24
                               ETA: 01:15:24

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 36926 steps/s (collection: 2.536s, learning 0.126s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.8092
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.6179
                       Mean reward: 4.72
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: -0.2218
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.66s
                      Time elapsed: 00:07:27
                               ETA: 01:15:23

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 38646 steps/s (collection: 2.377s, learning 0.167s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.3126
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.6735
                       Mean reward: 4.19
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 0.0183
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.54s
                      Time elapsed: 00:07:29
                               ETA: 01:15:22

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 38616 steps/s (collection: 2.405s, learning 0.141s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.1423
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.7219
                       Mean reward: 4.59
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.8720
     Episode_Reward/lifting_object: 0.0191
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.55s
                      Time elapsed: 00:07:32
                               ETA: 01:15:20

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 42422 steps/s (collection: 2.130s, learning 0.188s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.1952
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 49.7861
                       Mean reward: 4.73
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 0.8805
     Episode_Reward/lifting_object: 0.0783
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.32s
                      Time elapsed: 00:07:34
                               ETA: 01:15:15

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 39736 steps/s (collection: 2.290s, learning 0.184s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.2849
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.8454
                       Mean reward: 3.74
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.8444
     Episode_Reward/lifting_object: 0.0476
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.47s
                      Time elapsed: 00:07:37
                               ETA: 01:15:13

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 38317 steps/s (collection: 2.415s, learning 0.151s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.3306
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.8908
                       Mean reward: 4.26
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 0.8575
     Episode_Reward/lifting_object: 0.0827
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.57s
                      Time elapsed: 00:07:39
                               ETA: 01:15:11

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 39777 steps/s (collection: 2.299s, learning 0.172s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.3929
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.9626
                       Mean reward: 4.43
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.8622
     Episode_Reward/lifting_object: 0.0741
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.47s
                      Time elapsed: 00:07:42
                               ETA: 01:15:09

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 41412 steps/s (collection: 2.192s, learning 0.182s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4221
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.0342
                       Mean reward: 4.65
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.37s
                      Time elapsed: 00:07:44
                               ETA: 01:15:05

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 37575 steps/s (collection: 2.396s, learning 0.220s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.4799
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.1201
                       Mean reward: 4.78
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 0.8499
     Episode_Reward/lifting_object: 0.0875
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.62s
                      Time elapsed: 00:07:47
                               ETA: 01:15:04

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 42594 steps/s (collection: 2.146s, learning 0.162s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.7823
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.1476
                       Mean reward: 4.07
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.8344
     Episode_Reward/lifting_object: 0.1168
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.31s
                      Time elapsed: 00:07:49
                               ETA: 01:15:00

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 41587 steps/s (collection: 2.199s, learning 0.164s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.7985
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.1950
                       Mean reward: 4.60
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.8234
     Episode_Reward/lifting_object: 0.0808
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.36s
                      Time elapsed: 00:07:51
                               ETA: 01:14:56

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 39460 steps/s (collection: 2.318s, learning 0.174s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.4303
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.2122
                       Mean reward: 3.69
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 0.8336
     Episode_Reward/lifting_object: 0.0495
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.49s
                      Time elapsed: 00:07:54
                               ETA: 01:14:54

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 39531 steps/s (collection: 2.288s, learning 0.199s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.6282
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.2548
                       Mean reward: 4.31
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 0.0886
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.49s
                      Time elapsed: 00:07:56
                               ETA: 01:14:51

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 40148 steps/s (collection: 2.302s, learning 0.146s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.2122
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.2792
                       Mean reward: 3.30
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 0.7947
     Episode_Reward/lifting_object: 0.0393
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.45s
                      Time elapsed: 00:07:59
                               ETA: 01:14:48

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 37448 steps/s (collection: 2.495s, learning 0.130s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.4258
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.3154
                       Mean reward: 3.89
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 0.0369
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.63s
                      Time elapsed: 00:08:01
                               ETA: 01:14:47

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 40713 steps/s (collection: 2.233s, learning 0.182s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.9781
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.3799
                       Mean reward: 3.77
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: -0.0182
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.41s
                      Time elapsed: 00:08:04
                               ETA: 01:14:44

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 41813 steps/s (collection: 2.239s, learning 0.112s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.5374
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.3990
                       Mean reward: 3.77
               Mean episode length: 235.24
    Episode_Reward/reaching_object: 0.7887
     Episode_Reward/lifting_object: 0.1219
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.35s
                      Time elapsed: 00:08:06
                               ETA: 01:14:40

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 37125 steps/s (collection: 2.469s, learning 0.179s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.3060
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.4401
                       Mean reward: 3.76
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.8180
     Episode_Reward/lifting_object: 0.0478
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.65s
                      Time elapsed: 00:08:09
                               ETA: 01:14:39

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 38468 steps/s (collection: 2.443s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2330
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.4992
                       Mean reward: 4.74
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.8361
     Episode_Reward/lifting_object: 0.0073
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.56s
                      Time elapsed: 00:08:11
                               ETA: 01:14:38

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 40318 steps/s (collection: 2.231s, learning 0.207s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.2663
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.5915
                       Mean reward: 4.16
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 0.8219
     Episode_Reward/lifting_object: 0.0160
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.44s
                      Time elapsed: 00:08:14
                               ETA: 01:14:35

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 37645 steps/s (collection: 2.468s, learning 0.144s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.8281
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.6448
                       Mean reward: 5.19
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.8330
     Episode_Reward/lifting_object: 0.1431
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.61s
                      Time elapsed: 00:08:16
                               ETA: 01:14:33

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 39247 steps/s (collection: 2.350s, learning 0.155s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.5899
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.6807
                       Mean reward: 4.32
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 0.8192
     Episode_Reward/lifting_object: 0.0735
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.50s
                      Time elapsed: 00:08:19
                               ETA: 01:14:31

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 38171 steps/s (collection: 2.382s, learning 0.194s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.3001
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.7355
                       Mean reward: 4.22
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.8408
     Episode_Reward/lifting_object: 0.0965
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.58s
                      Time elapsed: 00:08:21
                               ETA: 01:14:29

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 40024 steps/s (collection: 2.342s, learning 0.114s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.2590
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.8167
                       Mean reward: 3.61
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.8327
     Episode_Reward/lifting_object: 0.1178
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.46s
                      Time elapsed: 00:08:24
                               ETA: 01:14:27

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 41179 steps/s (collection: 2.237s, learning 0.151s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.9746
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.8706
                       Mean reward: 3.51
               Mean episode length: 223.89
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: -0.0466
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.39s
                      Time elapsed: 00:08:26
                               ETA: 01:14:23

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 41919 steps/s (collection: 2.215s, learning 0.130s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.3960
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.8889
                       Mean reward: 4.63
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 0.8606
     Episode_Reward/lifting_object: 0.0751
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.35s
                      Time elapsed: 00:08:29
                               ETA: 01:14:20

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 41513 steps/s (collection: 2.201s, learning 0.167s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.8525
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.9415
                       Mean reward: 3.93
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 0.8400
     Episode_Reward/lifting_object: 0.0561
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.37s
                      Time elapsed: 00:08:31
                               ETA: 01:14:16

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 37234 steps/s (collection: 2.441s, learning 0.199s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.0447
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.0107
                       Mean reward: 3.42
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 0.0106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.64s
                      Time elapsed: 00:08:34
                               ETA: 01:14:15

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 42055 steps/s (collection: 2.206s, learning 0.132s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.7345
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.0617
                       Mean reward: 4.49
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 0.0476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.34s
                      Time elapsed: 00:08:36
                               ETA: 01:14:11

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 40040 steps/s (collection: 2.251s, learning 0.204s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.7612
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 51.1368
                       Mean reward: 3.15
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 0.8082
     Episode_Reward/lifting_object: 0.0274
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.46s
                      Time elapsed: 00:08:38
                               ETA: 01:14:09

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 39779 steps/s (collection: 2.349s, learning 0.122s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.3324
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 51.2124
                       Mean reward: 3.72
               Mean episode length: 235.75
    Episode_Reward/reaching_object: 0.8132
     Episode_Reward/lifting_object: 0.1103
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.47s
                      Time elapsed: 00:08:41
                               ETA: 01:14:06

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 39554 steps/s (collection: 2.330s, learning 0.155s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4102
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.2926
                       Mean reward: 3.90
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 0.8314
     Episode_Reward/lifting_object: 0.0756
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.49s
                      Time elapsed: 00:08:43
                               ETA: 01:14:04

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 42509 steps/s (collection: 2.158s, learning 0.154s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.5868
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.3424
                       Mean reward: 4.76
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 0.8362
     Episode_Reward/lifting_object: 0.1812
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.31s
                      Time elapsed: 00:08:46
                               ETA: 01:14:00

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 41898 steps/s (collection: 2.208s, learning 0.139s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.3357
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.3740
                       Mean reward: 4.49
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.8346
     Episode_Reward/lifting_object: 0.1144
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.35s
                      Time elapsed: 00:08:48
                               ETA: 01:13:56

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 41422 steps/s (collection: 2.192s, learning 0.181s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.3374
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 51.4504
                       Mean reward: 5.34
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 0.8319
     Episode_Reward/lifting_object: 0.1893
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.37s
                      Time elapsed: 00:08:50
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 41889 steps/s (collection: 2.153s, learning 0.194s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.8001
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.5126
                       Mean reward: 3.94
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 0.8145
     Episode_Reward/lifting_object: 0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.35s
                      Time elapsed: 00:08:53
                               ETA: 01:13:49

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 42074 steps/s (collection: 2.158s, learning 0.178s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.0025
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.5567
                       Mean reward: 4.88
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 0.8269
     Episode_Reward/lifting_object: 0.1677
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.34s
                      Time elapsed: 00:08:55
                               ETA: 01:13:45

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 43103 steps/s (collection: 2.126s, learning 0.155s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.4935
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.5793
                       Mean reward: 4.62
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.8784
     Episode_Reward/lifting_object: 0.1004
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.28s
                      Time elapsed: 00:08:57
                               ETA: 01:13:41

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 43041 steps/s (collection: 2.129s, learning 0.155s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3860
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.6324
                       Mean reward: 5.66
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.8600
     Episode_Reward/lifting_object: 0.1555
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.28s
                      Time elapsed: 00:09:00
                               ETA: 01:13:37

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 40411 steps/s (collection: 2.243s, learning 0.190s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.8145
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.6777
                       Mean reward: 4.80
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.8689
     Episode_Reward/lifting_object: 0.1934
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.43s
                      Time elapsed: 00:09:02
                               ETA: 01:13:34

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 40739 steps/s (collection: 2.271s, learning 0.142s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.7020
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.6981
                       Mean reward: 4.68
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.8553
     Episode_Reward/lifting_object: 0.1326
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.41s
                      Time elapsed: 00:09:04
                               ETA: 01:13:31

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 38793 steps/s (collection: 2.352s, learning 0.182s)
             Mean action noise std: 1.77
          Mean value_function loss: 2.0321
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.7351
                       Mean reward: 5.16
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 0.1619
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.53s
                      Time elapsed: 00:09:07
                               ETA: 01:13:29

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 40665 steps/s (collection: 2.282s, learning 0.135s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.4874
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.7497
                       Mean reward: 5.18
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.8623
     Episode_Reward/lifting_object: 0.0725
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.42s
                      Time elapsed: 00:09:09
                               ETA: 01:13:26

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 42780 steps/s (collection: 2.175s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.4272
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 51.7988
                       Mean reward: 5.01
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 0.8595
     Episode_Reward/lifting_object: 0.1638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.30s
                      Time elapsed: 00:09:12
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 40233 steps/s (collection: 2.246s, learning 0.197s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.6358
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.8573
                       Mean reward: 4.06
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 0.8452
     Episode_Reward/lifting_object: 0.1395
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.44s
                      Time elapsed: 00:09:14
                               ETA: 01:13:20

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 42689 steps/s (collection: 2.144s, learning 0.159s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.5903
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.8977
                       Mean reward: 5.10
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.8598
     Episode_Reward/lifting_object: 0.1696
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.30s
                      Time elapsed: 00:09:16
                               ETA: 01:13:16

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 41079 steps/s (collection: 2.213s, learning 0.180s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.0262
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.9316
                       Mean reward: 4.88
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 0.8797
     Episode_Reward/lifting_object: 0.2058
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.39s
                      Time elapsed: 00:09:19
                               ETA: 01:13:13

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 42406 steps/s (collection: 2.158s, learning 0.161s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.5066
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 51.9700
                       Mean reward: 5.36
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 0.1156
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.32s
                      Time elapsed: 00:09:21
                               ETA: 01:13:09

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 43778 steps/s (collection: 2.116s, learning 0.129s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.4293
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 52.0390
                       Mean reward: 4.85
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 0.8567
     Episode_Reward/lifting_object: 0.1594
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.25s
                      Time elapsed: 00:09:23
                               ETA: 01:13:05

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 42972 steps/s (collection: 2.127s, learning 0.161s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.4566
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 52.0990
                       Mean reward: 4.48
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 0.1258
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.29s
                      Time elapsed: 00:09:26
                               ETA: 01:13:01

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 42565 steps/s (collection: 2.162s, learning 0.148s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.5298
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 52.1591
                       Mean reward: 4.72
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 0.8496
     Episode_Reward/lifting_object: 0.1898
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.31s
                      Time elapsed: 00:09:28
                               ETA: 01:12:57

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 42154 steps/s (collection: 2.156s, learning 0.176s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.6952
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.2112
                       Mean reward: 5.10
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 0.8580
     Episode_Reward/lifting_object: 0.1898
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.33s
                      Time elapsed: 00:09:30
                               ETA: 01:12:54

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 43309 steps/s (collection: 2.129s, learning 0.141s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.4603
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.2813
                       Mean reward: 3.54
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 0.0713
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.27s
                      Time elapsed: 00:09:33
                               ETA: 01:12:50

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 41850 steps/s (collection: 2.167s, learning 0.182s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.7734
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 52.3283
                       Mean reward: 5.07
               Mean episode length: 227.72
    Episode_Reward/reaching_object: 0.8596
     Episode_Reward/lifting_object: 0.1667
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.35s
                      Time elapsed: 00:09:35
                               ETA: 01:12:46

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 44035 steps/s (collection: 2.089s, learning 0.143s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4583
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 52.3808
                       Mean reward: 4.95
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 0.8630
     Episode_Reward/lifting_object: 0.1807
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.23s
                      Time elapsed: 00:09:37
                               ETA: 01:12:42

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 43465 steps/s (collection: 2.137s, learning 0.125s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.6798
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.4510
                       Mean reward: 5.34
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 0.2609
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.26s
                      Time elapsed: 00:09:39
                               ETA: 01:12:38

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 41057 steps/s (collection: 2.219s, learning 0.175s)
             Mean action noise std: 1.82
          Mean value_function loss: 3.0629
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.4930
                       Mean reward: 4.88
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 0.8802
     Episode_Reward/lifting_object: 0.2326
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.39s
                      Time elapsed: 00:09:42
                               ETA: 01:12:35

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 40383 steps/s (collection: 2.261s, learning 0.174s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.0010
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.5109
                       Mean reward: 4.95
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 0.8531
     Episode_Reward/lifting_object: 0.2044
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.43s
                      Time elapsed: 00:09:44
                               ETA: 01:12:32

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 40992 steps/s (collection: 2.241s, learning 0.157s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.7906
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 52.5576
                       Mean reward: 6.43
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 0.8525
     Episode_Reward/lifting_object: 0.3406
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.40s
                      Time elapsed: 00:09:47
                               ETA: 01:12:29

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 41309 steps/s (collection: 2.238s, learning 0.142s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.0776
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.6121
                       Mean reward: 5.08
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 0.8052
     Episode_Reward/lifting_object: -0.0754
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.38s
                      Time elapsed: 00:09:49
                               ETA: 01:12:26

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 40397 steps/s (collection: 2.250s, learning 0.184s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.7906
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.6799
                       Mean reward: 5.68
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 0.8604
     Episode_Reward/lifting_object: 0.3345
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.43s
                      Time elapsed: 00:09:52
                               ETA: 01:12:23

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 43080 steps/s (collection: 2.155s, learning 0.127s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.8867
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 52.7259
                       Mean reward: 5.65
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 0.8607
     Episode_Reward/lifting_object: 0.2429
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.28s
                      Time elapsed: 00:09:54
                               ETA: 01:12:20

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 42417 steps/s (collection: 2.124s, learning 0.193s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.7341
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 52.7969
                       Mean reward: 4.98
               Mean episode length: 225.06
    Episode_Reward/reaching_object: 0.8240
     Episode_Reward/lifting_object: 0.2524
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.32s
                      Time elapsed: 00:09:56
                               ETA: 01:12:16

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 43751 steps/s (collection: 2.135s, learning 0.112s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.6344
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.8496
                       Mean reward: 5.26
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 0.8246
     Episode_Reward/lifting_object: 0.4869
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.25s
                      Time elapsed: 00:09:58
                               ETA: 01:12:12

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 40446 steps/s (collection: 2.193s, learning 0.238s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.8362
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 52.9069
                       Mean reward: 6.17
               Mean episode length: 224.93
    Episode_Reward/reaching_object: 0.8268
     Episode_Reward/lifting_object: 0.3687
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.43s
                      Time elapsed: 00:10:01
                               ETA: 01:12:09

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 39352 steps/s (collection: 2.368s, learning 0.130s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.8801
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.9713
                       Mean reward: 6.87
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.8514
     Episode_Reward/lifting_object: 0.3541
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.50s
                      Time elapsed: 00:10:03
                               ETA: 01:12:07

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 40911 steps/s (collection: 2.259s, learning 0.144s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.0120
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.0120
                       Mean reward: 5.09
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.8140
     Episode_Reward/lifting_object: 0.2135
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.40s
                      Time elapsed: 00:10:06
                               ETA: 01:12:04

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 40631 steps/s (collection: 2.244s, learning 0.176s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.8216
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.0498
                       Mean reward: 3.29
               Mean episode length: 214.92
    Episode_Reward/reaching_object: 0.7936
     Episode_Reward/lifting_object: 0.2046
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.42s
                      Time elapsed: 00:10:08
                               ETA: 01:12:01

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 39560 steps/s (collection: 2.299s, learning 0.186s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.2111
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.0643
                       Mean reward: 4.43
               Mean episode length: 212.75
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 0.1863
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.48s
                      Time elapsed: 00:10:11
                               ETA: 01:11:59

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 41974 steps/s (collection: 2.142s, learning 0.200s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.1978
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 53.1067
                       Mean reward: 4.74
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 0.7908
     Episode_Reward/lifting_object: 0.2767
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.34s
                      Time elapsed: 00:10:13
                               ETA: 01:11:56

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 41963 steps/s (collection: 2.197s, learning 0.146s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.0675
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.1683
                       Mean reward: 5.16
               Mean episode length: 209.59
    Episode_Reward/reaching_object: 0.7832
     Episode_Reward/lifting_object: 0.3825
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.34s
                      Time elapsed: 00:10:15
                               ETA: 01:11:52

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 39780 steps/s (collection: 2.356s, learning 0.116s)
             Mean action noise std: 1.88
          Mean value_function loss: 1.0432
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.2314
                       Mean reward: 5.86
               Mean episode length: 210.88
    Episode_Reward/reaching_object: 0.7860
     Episode_Reward/lifting_object: 0.3669
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.47s
                      Time elapsed: 00:10:18
                               ETA: 01:11:50

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 41214 steps/s (collection: 2.280s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 1.1665
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 53.2869
                       Mean reward: 5.75
               Mean episode length: 214.66
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 0.3997
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.39s
                      Time elapsed: 00:10:20
                               ETA: 01:11:47

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 40571 steps/s (collection: 2.252s, learning 0.171s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.1780
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.3341
                       Mean reward: 4.28
               Mean episode length: 203.63
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 0.3751
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.42s
                      Time elapsed: 00:10:23
                               ETA: 01:11:44

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 41723 steps/s (collection: 2.206s, learning 0.150s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.8451
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.3715
                       Mean reward: 6.75
               Mean episode length: 214.46
    Episode_Reward/reaching_object: 0.7970
     Episode_Reward/lifting_object: 0.4467
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.36s
                      Time elapsed: 00:10:25
                               ETA: 01:11:41

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 42612 steps/s (collection: 2.150s, learning 0.157s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.3143
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 53.4152
                       Mean reward: 3.96
               Mean episode length: 225.53
    Episode_Reward/reaching_object: 0.7899
     Episode_Reward/lifting_object: 0.5221
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.31s
                      Time elapsed: 00:10:27
                               ETA: 01:11:38

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 40636 steps/s (collection: 2.266s, learning 0.153s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.2996
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.4701
                       Mean reward: 6.29
               Mean episode length: 218.06
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 0.3568
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.42s
                      Time elapsed: 00:10:30
                               ETA: 01:11:35

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 41323 steps/s (collection: 2.235s, learning 0.144s)
             Mean action noise std: 1.90
          Mean value_function loss: 4.4576
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.5030
                       Mean reward: 5.96
               Mean episode length: 203.70
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 0.2674
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.38s
                      Time elapsed: 00:10:32
                               ETA: 01:11:32

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 37770 steps/s (collection: 2.379s, learning 0.224s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.7921
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.5400
                       Mean reward: 5.61
               Mean episode length: 201.91
    Episode_Reward/reaching_object: 0.7339
     Episode_Reward/lifting_object: 0.4063
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.60s
                      Time elapsed: 00:10:35
                               ETA: 01:11:30

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 34905 steps/s (collection: 2.655s, learning 0.161s)
             Mean action noise std: 1.90
          Mean value_function loss: 4.6039
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.5815
                       Mean reward: 6.62
               Mean episode length: 202.11
    Episode_Reward/reaching_object: 0.7420
     Episode_Reward/lifting_object: 0.4408
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.82s
                      Time elapsed: 00:10:37
                               ETA: 01:11:30

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 36041 steps/s (collection: 2.540s, learning 0.187s)
             Mean action noise std: 1.91
          Mean value_function loss: 3.4664
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.6103
                       Mean reward: 6.03
               Mean episode length: 192.57
    Episode_Reward/reaching_object: 0.7197
     Episode_Reward/lifting_object: 0.4600
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.73s
                      Time elapsed: 00:10:40
                               ETA: 01:11:29

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 36026 steps/s (collection: 2.515s, learning 0.213s)
             Mean action noise std: 1.91
          Mean value_function loss: 2.8839
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 53.6685
                       Mean reward: 4.99
               Mean episode length: 190.72
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 0.4225
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.73s
                      Time elapsed: 00:10:43
                               ETA: 01:11:29

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 39618 steps/s (collection: 2.291s, learning 0.190s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.2487
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.7373
                       Mean reward: 6.09
               Mean episode length: 197.26
    Episode_Reward/reaching_object: 0.6815
     Episode_Reward/lifting_object: 0.3599
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.48s
                      Time elapsed: 00:10:45
                               ETA: 01:11:26

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 41331 steps/s (collection: 2.199s, learning 0.180s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.3105
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.7839
                       Mean reward: 6.17
               Mean episode length: 188.17
    Episode_Reward/reaching_object: 0.6951
     Episode_Reward/lifting_object: 0.4483
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.38s
                      Time elapsed: 00:10:48
                               ETA: 01:11:23

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 38339 steps/s (collection: 2.393s, learning 0.171s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.3877
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.8204
                       Mean reward: 6.03
               Mean episode length: 194.22
    Episode_Reward/reaching_object: 0.7396
     Episode_Reward/lifting_object: 0.5828
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.56s
                      Time elapsed: 00:10:50
                               ETA: 01:11:22

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 41690 steps/s (collection: 2.249s, learning 0.109s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.7227
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 53.8567
                       Mean reward: 6.83
               Mean episode length: 207.84
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 0.4049
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.36s
                      Time elapsed: 00:10:53
                               ETA: 01:11:18

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 42465 steps/s (collection: 2.175s, learning 0.140s)
             Mean action noise std: 1.93
          Mean value_function loss: 4.0315
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.9048
                       Mean reward: 5.74
               Mean episode length: 201.28
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 0.5093
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.31s
                      Time elapsed: 00:10:55
                               ETA: 01:11:15

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 41963 steps/s (collection: 2.203s, learning 0.140s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.4372
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 53.9457
                       Mean reward: 6.17
               Mean episode length: 211.24
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 0.6144
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.34s
                      Time elapsed: 00:10:57
                               ETA: 01:11:12

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 41924 steps/s (collection: 2.209s, learning 0.136s)
             Mean action noise std: 1.94
          Mean value_function loss: 1.8023
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 54.0097
                       Mean reward: 5.86
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 0.5316
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.34s
                      Time elapsed: 00:11:00
                               ETA: 01:11:08

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 41938 steps/s (collection: 2.146s, learning 0.198s)
             Mean action noise std: 1.94
          Mean value_function loss: 2.6353
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.0566
                       Mean reward: 4.42
               Mean episode length: 210.33
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 0.5152
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.34s
                      Time elapsed: 00:11:02
                               ETA: 01:11:05

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 41712 steps/s (collection: 2.178s, learning 0.179s)
             Mean action noise std: 1.94
          Mean value_function loss: 7.1057
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.0929
                       Mean reward: 6.93
               Mean episode length: 204.93
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 0.4397
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.36s
                      Time elapsed: 00:11:04
                               ETA: 01:11:02

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 43319 steps/s (collection: 2.134s, learning 0.136s)
             Mean action noise std: 1.94
          Mean value_function loss: 1.6166
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 54.1181
                       Mean reward: 5.79
               Mean episode length: 196.18
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 0.6036
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.27s
                      Time elapsed: 00:11:07
                               ETA: 01:10:58

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 41399 steps/s (collection: 2.151s, learning 0.223s)
             Mean action noise std: 1.95
          Mean value_function loss: 2.1818
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.1669
                       Mean reward: 4.29
               Mean episode length: 193.73
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: 0.6365
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.37s
                      Time elapsed: 00:11:09
                               ETA: 01:10:55

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 41609 steps/s (collection: 2.191s, learning 0.171s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.8767
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.2250
                       Mean reward: 5.76
               Mean episode length: 206.61
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 0.5021
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.36s
                      Time elapsed: 00:11:11
                               ETA: 01:10:52

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.100s, learning 0.119s)
             Mean action noise std: 1.95
          Mean value_function loss: 2.3878
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.2729
                       Mean reward: 5.53
               Mean episode length: 207.16
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 0.7185
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.22s
                      Time elapsed: 00:11:14
                               ETA: 01:10:48

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 41143 steps/s (collection: 2.205s, learning 0.185s)
             Mean action noise std: 1.96
          Mean value_function loss: 1.7268
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.3120
                       Mean reward: 6.03
               Mean episode length: 185.65
    Episode_Reward/reaching_object: 0.7213
     Episode_Reward/lifting_object: 0.5399
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.39s
                      Time elapsed: 00:11:16
                               ETA: 01:10:45

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 42869 steps/s (collection: 2.164s, learning 0.129s)
             Mean action noise std: 1.96
          Mean value_function loss: 3.2168
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 54.3370
                       Mean reward: 6.14
               Mean episode length: 197.60
    Episode_Reward/reaching_object: 0.6968
     Episode_Reward/lifting_object: 0.6172
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.29s
                      Time elapsed: 00:11:18
                               ETA: 01:10:42

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 39420 steps/s (collection: 2.377s, learning 0.117s)
             Mean action noise std: 1.96
          Mean value_function loss: 3.5845
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.3531
                       Mean reward: 6.19
               Mean episode length: 189.12
    Episode_Reward/reaching_object: 0.6902
     Episode_Reward/lifting_object: 0.5684
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.49s
                      Time elapsed: 00:11:21
                               ETA: 01:10:40

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 41797 steps/s (collection: 2.192s, learning 0.160s)
             Mean action noise std: 1.96
          Mean value_function loss: 3.8299
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.3836
                       Mean reward: 6.66
               Mean episode length: 185.41
    Episode_Reward/reaching_object: 0.6932
     Episode_Reward/lifting_object: 0.6444
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.35s
                      Time elapsed: 00:11:23
                               ETA: 01:10:37

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 41895 steps/s (collection: 2.246s, learning 0.101s)
             Mean action noise std: 1.97
          Mean value_function loss: 2.7208
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 54.4126
                       Mean reward: 8.23
               Mean episode length: 185.60
    Episode_Reward/reaching_object: 0.6831
     Episode_Reward/lifting_object: 0.6790
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.35s
                      Time elapsed: 00:11:25
                               ETA: 01:10:33

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 41112 steps/s (collection: 2.225s, learning 0.166s)
             Mean action noise std: 1.97
          Mean value_function loss: 9.7149
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 54.4434
                       Mean reward: 6.17
               Mean episode length: 187.99
    Episode_Reward/reaching_object: 0.6471
     Episode_Reward/lifting_object: 0.6454
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.39s
                      Time elapsed: 00:11:28
                               ETA: 01:10:30

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 42366 steps/s (collection: 2.152s, learning 0.169s)
             Mean action noise std: 1.97
          Mean value_function loss: 3.7602
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.4623
                       Mean reward: 6.17
               Mean episode length: 193.24
    Episode_Reward/reaching_object: 0.6578
     Episode_Reward/lifting_object: 0.4910
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.32s
                      Time elapsed: 00:11:30
                               ETA: 01:10:27

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 41255 steps/s (collection: 2.235s, learning 0.148s)
             Mean action noise std: 1.97
          Mean value_function loss: 3.2755
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.4961
                       Mean reward: 7.88
               Mean episode length: 178.48
    Episode_Reward/reaching_object: 0.6529
     Episode_Reward/lifting_object: 0.7882
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.38s
                      Time elapsed: 00:11:33
                               ETA: 01:10:24

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 42551 steps/s (collection: 2.169s, learning 0.142s)
             Mean action noise std: 1.97
          Mean value_function loss: 4.7178
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.5242
                       Mean reward: 4.15
               Mean episode length: 182.63
    Episode_Reward/reaching_object: 0.6610
     Episode_Reward/lifting_object: 0.6942
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.31s
                      Time elapsed: 00:11:35
                               ETA: 01:10:21

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 41073 steps/s (collection: 2.189s, learning 0.204s)
             Mean action noise std: 1.98
          Mean value_function loss: 8.2705
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.5669
                       Mean reward: 10.69
               Mean episode length: 179.78
    Episode_Reward/reaching_object: 0.6889
     Episode_Reward/lifting_object: 1.1730
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.39s
                      Time elapsed: 00:11:37
                               ETA: 01:10:18

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 41203 steps/s (collection: 2.240s, learning 0.146s)
             Mean action noise std: 1.98
          Mean value_function loss: 3.9469
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.6129
                       Mean reward: 8.15
               Mean episode length: 190.99
    Episode_Reward/reaching_object: 0.6668
     Episode_Reward/lifting_object: 0.7026
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.39s
                      Time elapsed: 00:11:40
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 42449 steps/s (collection: 2.197s, learning 0.119s)
             Mean action noise std: 1.98
          Mean value_function loss: 3.0895
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.6563
                       Mean reward: 9.56
               Mean episode length: 195.56
    Episode_Reward/reaching_object: 0.6910
     Episode_Reward/lifting_object: 1.1046
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.32s
                      Time elapsed: 00:11:42
                               ETA: 01:10:12

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 37703 steps/s (collection: 2.398s, learning 0.209s)
             Mean action noise std: 1.99
          Mean value_function loss: 7.1847
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.6906
                       Mean reward: 7.10
               Mean episode length: 183.13
    Episode_Reward/reaching_object: 0.6397
     Episode_Reward/lifting_object: 0.7084
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.61s
                      Time elapsed: 00:11:45
                               ETA: 01:10:10

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 39466 steps/s (collection: 2.305s, learning 0.186s)
             Mean action noise std: 1.99
          Mean value_function loss: 5.3221
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 54.7491
                       Mean reward: 7.50
               Mean episode length: 190.24
    Episode_Reward/reaching_object: 0.6553
     Episode_Reward/lifting_object: 0.7866
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.49s
                      Time elapsed: 00:11:47
                               ETA: 01:10:08

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 41133 steps/s (collection: 2.288s, learning 0.102s)
             Mean action noise std: 2.00
          Mean value_function loss: 3.1476
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 54.8179
                       Mean reward: 9.51
               Mean episode length: 201.35
    Episode_Reward/reaching_object: 0.6852
     Episode_Reward/lifting_object: 0.8404
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.39s
                      Time elapsed: 00:11:49
                               ETA: 01:10:05

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 40405 steps/s (collection: 2.305s, learning 0.127s)
             Mean action noise std: 2.00
          Mean value_function loss: 4.4492
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 54.8799
                       Mean reward: 6.71
               Mean episode length: 194.41
    Episode_Reward/reaching_object: 0.6667
     Episode_Reward/lifting_object: 0.8520
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.43s
                      Time elapsed: 00:11:52
                               ETA: 01:10:03

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 39470 steps/s (collection: 2.365s, learning 0.126s)
             Mean action noise std: 2.00
          Mean value_function loss: 3.4659
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.9184
                       Mean reward: 7.13
               Mean episode length: 183.94
    Episode_Reward/reaching_object: 0.6446
     Episode_Reward/lifting_object: 0.7199
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.49s
                      Time elapsed: 00:11:54
                               ETA: 01:10:00

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 38577 steps/s (collection: 2.401s, learning 0.147s)
             Mean action noise std: 2.01
          Mean value_function loss: 3.3510
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.9478
                       Mean reward: 8.48
               Mean episode length: 174.11
    Episode_Reward/reaching_object: 0.6437
     Episode_Reward/lifting_object: 0.7676
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.55s
                      Time elapsed: 00:11:57
                               ETA: 01:09:58

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 36801 steps/s (collection: 2.446s, learning 0.225s)
             Mean action noise std: 2.01
          Mean value_function loss: 3.6453
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.9903
                       Mean reward: 8.40
               Mean episode length: 200.10
    Episode_Reward/reaching_object: 0.6735
     Episode_Reward/lifting_object: 0.7565
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.67s
                      Time elapsed: 00:12:00
                               ETA: 01:09:57

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 36780 steps/s (collection: 2.433s, learning 0.240s)
             Mean action noise std: 2.01
          Mean value_function loss: 6.9240
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.0306
                       Mean reward: 6.42
               Mean episode length: 177.79
    Episode_Reward/reaching_object: 0.6609
     Episode_Reward/lifting_object: 0.9957
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.67s
                      Time elapsed: 00:12:02
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 37304 steps/s (collection: 2.454s, learning 0.181s)
             Mean action noise std: 2.02
          Mean value_function loss: 3.9233
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 55.0812
                       Mean reward: 8.22
               Mean episode length: 174.90
    Episode_Reward/reaching_object: 0.6452
     Episode_Reward/lifting_object: 1.0277
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.64s
                      Time elapsed: 00:12:05
                               ETA: 01:09:55

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 36595 steps/s (collection: 2.519s, learning 0.167s)
             Mean action noise std: 2.02
          Mean value_function loss: 2.8946
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 55.1384
                       Mean reward: 7.22
               Mean episode length: 181.50
    Episode_Reward/reaching_object: 0.6442
     Episode_Reward/lifting_object: 0.8120
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.69s
                      Time elapsed: 00:12:08
                               ETA: 01:09:53

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 40214 steps/s (collection: 2.277s, learning 0.167s)
             Mean action noise std: 2.02
          Mean value_function loss: 5.1683
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.1817
                       Mean reward: 8.08
               Mean episode length: 192.34
    Episode_Reward/reaching_object: 0.6374
     Episode_Reward/lifting_object: 0.8936
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.44s
                      Time elapsed: 00:12:10
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 43469 steps/s (collection: 2.115s, learning 0.147s)
             Mean action noise std: 2.03
          Mean value_function loss: 4.4656
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.2218
                       Mean reward: 10.27
               Mean episode length: 198.10
    Episode_Reward/reaching_object: 0.6883
     Episode_Reward/lifting_object: 0.9892
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.26s
                      Time elapsed: 00:12:12
                               ETA: 01:09:47

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 41653 steps/s (collection: 2.165s, learning 0.195s)
             Mean action noise std: 2.03
          Mean value_function loss: 4.3447
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.2604
                       Mean reward: 9.41
               Mean episode length: 192.73
    Episode_Reward/reaching_object: 0.6703
     Episode_Reward/lifting_object: 1.0245
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.36s
                      Time elapsed: 00:12:15
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 40473 steps/s (collection: 2.264s, learning 0.165s)
             Mean action noise std: 2.03
          Mean value_function loss: 9.2913
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2998
                       Mean reward: 8.09
               Mean episode length: 182.73
    Episode_Reward/reaching_object: 0.6525
     Episode_Reward/lifting_object: 1.0802
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.43s
                      Time elapsed: 00:12:17
                               ETA: 01:09:42

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 42038 steps/s (collection: 2.181s, learning 0.157s)
             Mean action noise std: 2.04
          Mean value_function loss: 5.2370
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.3239
                       Mean reward: 8.56
               Mean episode length: 188.39
    Episode_Reward/reaching_object: 0.6610
     Episode_Reward/lifting_object: 1.2334
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.34s
                      Time elapsed: 00:12:19
                               ETA: 01:09:39

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 40289 steps/s (collection: 2.288s, learning 0.152s)
             Mean action noise std: 2.04
          Mean value_function loss: 3.3501
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.3567
                       Mean reward: 9.54
               Mean episode length: 184.16
    Episode_Reward/reaching_object: 0.7095
     Episode_Reward/lifting_object: 1.3175
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.44s
                      Time elapsed: 00:12:22
                               ETA: 01:09:36

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 43355 steps/s (collection: 2.145s, learning 0.122s)
             Mean action noise std: 2.04
          Mean value_function loss: 7.1820
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 55.3786
                       Mean reward: 6.01
               Mean episode length: 178.73
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 1.2277
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.27s
                      Time elapsed: 00:12:24
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 40796 steps/s (collection: 2.249s, learning 0.160s)
             Mean action noise std: 2.04
          Mean value_function loss: 5.5577
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.3983
                       Mean reward: 3.26
               Mean episode length: 182.07
    Episode_Reward/reaching_object: 0.6682
     Episode_Reward/lifting_object: 0.9058
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.41s
                      Time elapsed: 00:12:27
                               ETA: 01:09:30

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 42256 steps/s (collection: 2.166s, learning 0.161s)
             Mean action noise std: 2.04
          Mean value_function loss: 4.1784
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.4327
                       Mean reward: 8.55
               Mean episode length: 182.93
    Episode_Reward/reaching_object: 0.6528
     Episode_Reward/lifting_object: 0.9881
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.33s
                      Time elapsed: 00:12:29
                               ETA: 01:09:27

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 40402 steps/s (collection: 2.223s, learning 0.210s)
             Mean action noise std: 2.05
          Mean value_function loss: 4.5293
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 55.4804
                       Mean reward: 11.03
               Mean episode length: 186.39
    Episode_Reward/reaching_object: 0.6694
     Episode_Reward/lifting_object: 1.2630
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.43s
                      Time elapsed: 00:12:31
                               ETA: 01:09:24

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 40815 steps/s (collection: 2.248s, learning 0.160s)
             Mean action noise std: 2.05
          Mean value_function loss: 5.2048
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.5225
                       Mean reward: 9.48
               Mean episode length: 185.83
    Episode_Reward/reaching_object: 0.6834
     Episode_Reward/lifting_object: 1.2471
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.41s
                      Time elapsed: 00:12:34
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 42336 steps/s (collection: 2.208s, learning 0.114s)
             Mean action noise std: 2.05
          Mean value_function loss: 4.7001
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 55.5550
                       Mean reward: 8.07
               Mean episode length: 181.85
    Episode_Reward/reaching_object: 0.6256
     Episode_Reward/lifting_object: 1.1487
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.32s
                      Time elapsed: 00:12:36
                               ETA: 01:09:18

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 41582 steps/s (collection: 2.206s, learning 0.159s)
             Mean action noise std: 2.06
          Mean value_function loss: 11.2544
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.5993
                       Mean reward: 8.75
               Mean episode length: 172.42
    Episode_Reward/reaching_object: 0.6674
     Episode_Reward/lifting_object: 1.1122
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.36s
                      Time elapsed: 00:12:38
                               ETA: 01:09:15

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 38483 steps/s (collection: 2.384s, learning 0.170s)
             Mean action noise std: 2.06
          Mean value_function loss: 8.6722
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.6441
                       Mean reward: 9.32
               Mean episode length: 170.46
    Episode_Reward/reaching_object: 0.6372
     Episode_Reward/lifting_object: 1.3518
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.55s
                      Time elapsed: 00:12:41
                               ETA: 01:09:13

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 37296 steps/s (collection: 2.461s, learning 0.175s)
             Mean action noise std: 2.06
          Mean value_function loss: 5.4614
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 55.6821
                       Mean reward: 9.87
               Mean episode length: 176.05
    Episode_Reward/reaching_object: 0.6168
     Episode_Reward/lifting_object: 1.2523
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.64s
                      Time elapsed: 00:12:44
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 39033 steps/s (collection: 2.350s, learning 0.168s)
             Mean action noise std: 2.07
          Mean value_function loss: 4.5613
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.7193
                       Mean reward: 6.63
               Mean episode length: 164.10
    Episode_Reward/reaching_object: 0.5871
     Episode_Reward/lifting_object: 1.2151
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.52s
                      Time elapsed: 00:12:46
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 38191 steps/s (collection: 2.372s, learning 0.202s)
             Mean action noise std: 2.07
          Mean value_function loss: 6.8628
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.7426
                       Mean reward: 6.59
               Mean episode length: 167.41
    Episode_Reward/reaching_object: 0.6166
     Episode_Reward/lifting_object: 1.1480
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.57s
                      Time elapsed: 00:12:49
                               ETA: 01:09:08

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 39531 steps/s (collection: 2.280s, learning 0.207s)
             Mean action noise std: 2.07
          Mean value_function loss: 5.4097
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.7751
                       Mean reward: 9.74
               Mean episode length: 172.23
    Episode_Reward/reaching_object: 0.6241
     Episode_Reward/lifting_object: 1.3387
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.49s
                      Time elapsed: 00:12:51
                               ETA: 01:09:05

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 40553 steps/s (collection: 2.257s, learning 0.167s)
             Mean action noise std: 2.07
          Mean value_function loss: 7.9983
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8135
                       Mean reward: 13.56
               Mean episode length: 179.60
    Episode_Reward/reaching_object: 0.6641
     Episode_Reward/lifting_object: 1.5055
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.42s
                      Time elapsed: 00:12:54
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 40428 steps/s (collection: 2.274s, learning 0.157s)
             Mean action noise std: 2.08
          Mean value_function loss: 7.7749
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.8487
                       Mean reward: 10.42
               Mean episode length: 166.17
    Episode_Reward/reaching_object: 0.6203
     Episode_Reward/lifting_object: 1.5425
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.43s
                      Time elapsed: 00:12:56
                               ETA: 01:09:00

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 40589 steps/s (collection: 2.252s, learning 0.170s)
             Mean action noise std: 2.08
          Mean value_function loss: 18.9169
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 55.8694
                       Mean reward: 8.40
               Mean episode length: 175.58
    Episode_Reward/reaching_object: 0.6372
     Episode_Reward/lifting_object: 1.2833
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.42s
                      Time elapsed: 00:12:58
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 42048 steps/s (collection: 2.223s, learning 0.115s)
             Mean action noise std: 2.08
          Mean value_function loss: 3.9823
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.8744
                       Mean reward: 10.84
               Mean episode length: 195.40
    Episode_Reward/reaching_object: 0.6758
     Episode_Reward/lifting_object: 1.3588
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.34s
                      Time elapsed: 00:13:01
                               ETA: 01:08:54

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 42618 steps/s (collection: 2.200s, learning 0.107s)
             Mean action noise std: 2.08
          Mean value_function loss: 4.2348
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 55.8892
                       Mean reward: 8.85
               Mean episode length: 183.02
    Episode_Reward/reaching_object: 0.6742
     Episode_Reward/lifting_object: 1.6220
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.31s
                      Time elapsed: 00:13:03
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 42533 steps/s (collection: 2.157s, learning 0.155s)
             Mean action noise std: 2.08
          Mean value_function loss: 7.5720
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9162
                       Mean reward: 10.01
               Mean episode length: 180.52
    Episode_Reward/reaching_object: 0.6893
     Episode_Reward/lifting_object: 1.6622
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.31s
                      Time elapsed: 00:13:05
                               ETA: 01:08:48

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 42724 steps/s (collection: 2.154s, learning 0.147s)
             Mean action noise std: 2.09
          Mean value_function loss: 7.1559
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.9593
                       Mean reward: 12.84
               Mean episode length: 194.13
    Episode_Reward/reaching_object: 0.7235
     Episode_Reward/lifting_object: 1.5408
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.30s
                      Time elapsed: 00:13:08
                               ETA: 01:08:45

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 39296 steps/s (collection: 2.312s, learning 0.190s)
             Mean action noise std: 2.09
          Mean value_function loss: 6.4929
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 55.9985
                       Mean reward: 9.67
               Mean episode length: 175.90
    Episode_Reward/reaching_object: 0.6743
     Episode_Reward/lifting_object: 1.5738
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.50s
                      Time elapsed: 00:13:10
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 41250 steps/s (collection: 2.222s, learning 0.161s)
             Mean action noise std: 2.09
          Mean value_function loss: 7.5840
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.0118
                       Mean reward: 12.12
               Mean episode length: 183.41
    Episode_Reward/reaching_object: 0.6530
     Episode_Reward/lifting_object: 1.5300
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.38s
                      Time elapsed: 00:13:13
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 42103 steps/s (collection: 2.189s, learning 0.146s)
             Mean action noise std: 2.09
          Mean value_function loss: 13.9612
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.0361
                       Mean reward: 8.76
               Mean episode length: 185.78
    Episode_Reward/reaching_object: 0.6897
     Episode_Reward/lifting_object: 1.2431
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.33s
                      Time elapsed: 00:13:15
                               ETA: 01:08:37

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 40785 steps/s (collection: 2.225s, learning 0.186s)
             Mean action noise std: 2.10
          Mean value_function loss: 6.9753
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.0774
                       Mean reward: 12.26
               Mean episode length: 189.57
    Episode_Reward/reaching_object: 0.7027
     Episode_Reward/lifting_object: 1.5541
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.41s
                      Time elapsed: 00:13:17
                               ETA: 01:08:34

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 42438 steps/s (collection: 2.188s, learning 0.128s)
             Mean action noise std: 2.10
          Mean value_function loss: 6.1028
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.1124
                       Mean reward: 10.51
               Mean episode length: 196.27
    Episode_Reward/reaching_object: 0.7064
     Episode_Reward/lifting_object: 1.5183
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.32s
                      Time elapsed: 00:13:20
                               ETA: 01:08:31

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 41752 steps/s (collection: 2.227s, learning 0.128s)
             Mean action noise std: 2.10
          Mean value_function loss: 7.7299
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1374
                       Mean reward: 9.69
               Mean episode length: 192.48
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 1.3453
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.35s
                      Time elapsed: 00:13:22
                               ETA: 01:08:28

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 42446 steps/s (collection: 2.155s, learning 0.161s)
             Mean action noise std: 2.10
          Mean value_function loss: 5.0055
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.1717
                       Mean reward: 14.63
               Mean episode length: 207.71
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 1.8188
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.32s
                      Time elapsed: 00:13:24
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 38509 steps/s (collection: 2.378s, learning 0.175s)
             Mean action noise std: 2.11
          Mean value_function loss: 7.5007
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.2050
                       Mean reward: 13.94
               Mean episode length: 197.23
    Episode_Reward/reaching_object: 0.7060
     Episode_Reward/lifting_object: 1.6799
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.55s
                      Time elapsed: 00:13:27
                               ETA: 01:08:23

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 39305 steps/s (collection: 2.348s, learning 0.153s)
             Mean action noise std: 2.11
          Mean value_function loss: 6.6897
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 56.2382
                       Mean reward: 12.88
               Mean episode length: 202.93
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: 1.7145
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.50s
                      Time elapsed: 00:13:29
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 39903 steps/s (collection: 2.241s, learning 0.222s)
             Mean action noise std: 2.11
          Mean value_function loss: 8.1479
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.2821
                       Mean reward: 13.98
               Mean episode length: 193.28
    Episode_Reward/reaching_object: 0.7416
     Episode_Reward/lifting_object: 2.0114
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.46s
                      Time elapsed: 00:13:32
                               ETA: 01:08:18

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 38911 steps/s (collection: 2.353s, learning 0.174s)
             Mean action noise std: 2.11
          Mean value_function loss: 17.2169
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 56.3119
                       Mean reward: 12.02
               Mean episode length: 199.95
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 1.8298
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.53s
                      Time elapsed: 00:13:34
                               ETA: 01:08:16

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 36054 steps/s (collection: 2.487s, learning 0.240s)
             Mean action noise std: 2.11
          Mean value_function loss: 11.6077
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.3155
                       Mean reward: 12.55
               Mean episode length: 198.35
    Episode_Reward/reaching_object: 0.7142
     Episode_Reward/lifting_object: 1.7641
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.73s
                      Time elapsed: 00:13:37
                               ETA: 01:08:15

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 17142 steps/s (collection: 5.604s, learning 0.131s)
             Mean action noise std: 2.12
          Mean value_function loss: 7.2009
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.3347
                       Mean reward: 9.56
               Mean episode length: 190.92
    Episode_Reward/reaching_object: 0.7048
     Episode_Reward/lifting_object: 1.3542
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.73s
                      Time elapsed: 00:13:43
                               ETA: 01:08:29

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 13583 steps/s (collection: 7.082s, learning 0.155s)
             Mean action noise std: 2.12
          Mean value_function loss: 20.2671
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 56.3623
                       Mean reward: 11.44
               Mean episode length: 190.76
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 1.4086
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 7.24s
                      Time elapsed: 00:13:50
                               ETA: 01:08:50

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 13407 steps/s (collection: 7.194s, learning 0.138s)
             Mean action noise std: 2.12
          Mean value_function loss: 10.4686
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.3741
                       Mean reward: 9.13
               Mean episode length: 197.44
    Episode_Reward/reaching_object: 0.7262
     Episode_Reward/lifting_object: 1.3994
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 7.33s
                      Time elapsed: 00:13:57
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 13158 steps/s (collection: 7.284s, learning 0.187s)
             Mean action noise std: 2.12
          Mean value_function loss: 13.6623
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.4014
                       Mean reward: 1.14
               Mean episode length: 181.81
    Episode_Reward/reaching_object: 0.6903
     Episode_Reward/lifting_object: 1.2969
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.47s
                      Time elapsed: 00:14:05
                               ETA: 01:09:34

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 13104 steps/s (collection: 7.355s, learning 0.146s)
             Mean action noise std: 2.13
          Mean value_function loss: 6.5114
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 56.4481
                       Mean reward: 16.18
               Mean episode length: 194.23
    Episode_Reward/reaching_object: 0.7072
     Episode_Reward/lifting_object: 2.0353
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 7.50s
                      Time elapsed: 00:14:12
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 13098 steps/s (collection: 7.318s, learning 0.187s)
             Mean action noise std: 2.13
          Mean value_function loss: 10.1778
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.4857
                       Mean reward: 12.14
               Mean episode length: 192.05
    Episode_Reward/reaching_object: 0.6935
     Episode_Reward/lifting_object: 1.7694
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 7.51s
                      Time elapsed: 00:14:20
                               ETA: 01:10:18

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 13936 steps/s (collection: 6.912s, learning 0.142s)
             Mean action noise std: 2.13
          Mean value_function loss: 9.7294
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.5101
                       Mean reward: 13.12
               Mean episode length: 189.81
    Episode_Reward/reaching_object: 0.6980
     Episode_Reward/lifting_object: 2.2132
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 7.05s
                      Time elapsed: 00:14:27
                               ETA: 01:10:37

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 13724 steps/s (collection: 7.035s, learning 0.128s)
             Mean action noise std: 2.13
          Mean value_function loss: 9.5157
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.5481
                       Mean reward: 14.41
               Mean episode length: 186.87
    Episode_Reward/reaching_object: 0.7184
     Episode_Reward/lifting_object: 2.0184
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.16s
                      Time elapsed: 00:14:34
                               ETA: 01:10:57

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 11179 steps/s (collection: 8.640s, learning 0.153s)
             Mean action noise std: 2.14
          Mean value_function loss: 14.9567
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.5853
                       Mean reward: 14.38
               Mean episode length: 188.99
    Episode_Reward/reaching_object: 0.6831
     Episode_Reward/lifting_object: 1.5196
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.79s
                      Time elapsed: 00:14:43
                               ETA: 01:11:25

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 43036 steps/s (collection: 2.165s, learning 0.120s)
             Mean action noise std: 2.14
          Mean value_function loss: 9.8114
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.6280
                       Mean reward: 11.53
               Mean episode length: 187.81
    Episode_Reward/reaching_object: 0.7011
     Episode_Reward/lifting_object: 1.4787
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.28s
                      Time elapsed: 00:14:45
                               ETA: 01:11:21

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 41128 steps/s (collection: 2.206s, learning 0.184s)
             Mean action noise std: 2.14
          Mean value_function loss: 6.2249
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.6578
                       Mean reward: 13.23
               Mean episode length: 194.16
    Episode_Reward/reaching_object: 0.7153
     Episode_Reward/lifting_object: 2.1848
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.39s
                      Time elapsed: 00:14:48
                               ETA: 01:11:17

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 40745 steps/s (collection: 2.274s, learning 0.139s)
             Mean action noise std: 2.15
          Mean value_function loss: 7.8475
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.6834
                       Mean reward: 19.40
               Mean episode length: 210.36
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 2.3708
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.41s
                      Time elapsed: 00:14:50
                               ETA: 01:11:14

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 42372 steps/s (collection: 2.198s, learning 0.122s)
             Mean action noise std: 2.15
          Mean value_function loss: 7.0844
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.7159
                       Mean reward: 13.93
               Mean episode length: 200.20
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 2.0829
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.32s
                      Time elapsed: 00:14:52
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 43260 steps/s (collection: 2.131s, learning 0.142s)
             Mean action noise std: 2.15
          Mean value_function loss: 6.4284
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.7398
                       Mean reward: 16.59
               Mean episode length: 215.50
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 2.0381
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.27s
                      Time elapsed: 00:14:55
                               ETA: 01:11:06

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 43180 steps/s (collection: 2.160s, learning 0.116s)
             Mean action noise std: 2.15
          Mean value_function loss: 9.1204
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.7712
                       Mean reward: 14.18
               Mean episode length: 200.37
    Episode_Reward/reaching_object: 0.7107
     Episode_Reward/lifting_object: 1.9940
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.28s
                      Time elapsed: 00:14:57
                               ETA: 01:11:02

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 40416 steps/s (collection: 2.293s, learning 0.140s)
             Mean action noise std: 2.15
          Mean value_function loss: 7.5800
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.8027
                       Mean reward: 9.94
               Mean episode length: 189.53
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 1.7582
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.43s
                      Time elapsed: 00:14:59
                               ETA: 01:10:59

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 37892 steps/s (collection: 2.442s, learning 0.153s)
             Mean action noise std: 2.16
          Mean value_function loss: 5.7766
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.8228
                       Mean reward: 13.02
               Mean episode length: 203.35
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: 2.2547
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.59s
                      Time elapsed: 00:15:02
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 39140 steps/s (collection: 2.371s, learning 0.141s)
             Mean action noise std: 2.16
          Mean value_function loss: 10.9162
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.8445
                       Mean reward: 12.12
               Mean episode length: 199.47
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 2.0310
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.51s
                      Time elapsed: 00:15:04
                               ETA: 01:10:53

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 37519 steps/s (collection: 2.460s, learning 0.160s)
             Mean action noise std: 2.16
          Mean value_function loss: 6.4391
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.8665
                       Mean reward: 12.70
               Mean episode length: 200.73
    Episode_Reward/reaching_object: 0.7092
     Episode_Reward/lifting_object: 2.1863
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.62s
                      Time elapsed: 00:15:07
                               ETA: 01:10:51

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 35820 steps/s (collection: 2.575s, learning 0.170s)
             Mean action noise std: 2.16
          Mean value_function loss: 9.4011
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.8913
                       Mean reward: 13.42
               Mean episode length: 205.63
    Episode_Reward/reaching_object: 0.7201
     Episode_Reward/lifting_object: 2.0330
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.74s
                      Time elapsed: 00:15:10
                               ETA: 01:10:49

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 38745 steps/s (collection: 2.359s, learning 0.178s)
             Mean action noise std: 2.16
          Mean value_function loss: 9.2944
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.9150
                       Mean reward: 11.80
               Mean episode length: 198.57
    Episode_Reward/reaching_object: 0.7365
     Episode_Reward/lifting_object: 2.0118
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.54s
                      Time elapsed: 00:15:12
                               ETA: 01:10:46

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 40440 steps/s (collection: 2.282s, learning 0.148s)
             Mean action noise std: 2.17
          Mean value_function loss: 7.8426
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.9282
                       Mean reward: 14.13
               Mean episode length: 198.91
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 2.2804
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.43s
                      Time elapsed: 00:15:15
                               ETA: 01:10:43

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 38839 steps/s (collection: 2.367s, learning 0.164s)
             Mean action noise std: 2.17
          Mean value_function loss: 10.1364
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.9395
                       Mean reward: 17.22
               Mean episode length: 204.48
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 2.3254
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.53s
                      Time elapsed: 00:15:17
                               ETA: 01:10:40

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 37709 steps/s (collection: 2.396s, learning 0.211s)
             Mean action noise std: 2.17
          Mean value_function loss: 8.8471
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.9636
                       Mean reward: 16.19
               Mean episode length: 203.33
    Episode_Reward/reaching_object: 0.7380
     Episode_Reward/lifting_object: 2.1582
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.61s
                      Time elapsed: 00:15:20
                               ETA: 01:10:38

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 39942 steps/s (collection: 2.331s, learning 0.130s)
             Mean action noise std: 2.17
          Mean value_function loss: 9.2412
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 56.9953
                       Mean reward: 13.44
               Mean episode length: 204.95
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 2.2736
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.46s
                      Time elapsed: 00:15:22
                               ETA: 01:10:35

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 42852 steps/s (collection: 2.135s, learning 0.159s)
             Mean action noise std: 2.17
          Mean value_function loss: 8.7181
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.0262
                       Mean reward: 17.10
               Mean episode length: 200.92
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 2.4800
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.29s
                      Time elapsed: 00:15:25
                               ETA: 01:10:31

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 43492 steps/s (collection: 2.116s, learning 0.145s)
             Mean action noise std: 2.18
          Mean value_function loss: 7.9493
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.0462
                       Mean reward: 14.29
               Mean episode length: 198.45
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: 2.6192
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.26s
                      Time elapsed: 00:15:27
                               ETA: 01:10:27

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 42083 steps/s (collection: 2.154s, learning 0.182s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.6160
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.0600
                       Mean reward: 14.40
               Mean episode length: 195.06
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 2.4977
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.34s
                      Time elapsed: 00:15:29
                               ETA: 01:10:23

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 43069 steps/s (collection: 2.107s, learning 0.175s)
             Mean action noise std: 2.18
          Mean value_function loss: 13.9077
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.0740
                       Mean reward: 17.81
               Mean episode length: 193.99
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 2.4994
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.28s
                      Time elapsed: 00:15:31
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 43454 steps/s (collection: 2.133s, learning 0.130s)
             Mean action noise std: 2.18
          Mean value_function loss: 10.9298
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.0964
                       Mean reward: 12.82
               Mean episode length: 198.78
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 2.4577
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.26s
                      Time elapsed: 00:15:34
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 42569 steps/s (collection: 2.164s, learning 0.146s)
             Mean action noise std: 2.18
          Mean value_function loss: 10.7624
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.1239
                       Mean reward: 14.45
               Mean episode length: 195.02
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 2.8762
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.31s
                      Time elapsed: 00:15:36
                               ETA: 01:10:11

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 41874 steps/s (collection: 2.208s, learning 0.140s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.2876
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.1518
                       Mean reward: 19.85
               Mean episode length: 196.73
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 2.8566
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.35s
                      Time elapsed: 00:15:38
                               ETA: 01:10:08

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 39278 steps/s (collection: 2.269s, learning 0.234s)
             Mean action noise std: 2.19
          Mean value_function loss: 12.6983
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.1802
                       Mean reward: 19.30
               Mean episode length: 191.94
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 2.8669
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.50s
                      Time elapsed: 00:15:41
                               ETA: 01:10:05

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 41048 steps/s (collection: 2.248s, learning 0.147s)
             Mean action noise std: 2.19
          Mean value_function loss: 23.6293
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.2066
                       Mean reward: 18.12
               Mean episode length: 191.10
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 2.8425
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.39s
                      Time elapsed: 00:15:43
                               ETA: 01:10:02

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 41806 steps/s (collection: 2.217s, learning 0.134s)
             Mean action noise std: 2.19
          Mean value_function loss: 16.1114
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.2227
                       Mean reward: 15.01
               Mean episode length: 187.28
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 2.6656
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.35s
                      Time elapsed: 00:15:46
                               ETA: 01:09:58

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 41230 steps/s (collection: 2.191s, learning 0.194s)
             Mean action noise std: 2.19
          Mean value_function loss: 20.4055
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 57.2348
                       Mean reward: 16.20
               Mean episode length: 185.01
    Episode_Reward/reaching_object: 0.7279
     Episode_Reward/lifting_object: 2.8211
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.38s
                      Time elapsed: 00:15:48
                               ETA: 01:09:55

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 41017 steps/s (collection: 2.256s, learning 0.141s)
             Mean action noise std: 2.19
          Mean value_function loss: 23.4290
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.2517
                       Mean reward: 17.80
               Mean episode length: 185.93
    Episode_Reward/reaching_object: 0.7182
     Episode_Reward/lifting_object: 2.7864
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.40s
                      Time elapsed: 00:15:50
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 40993 steps/s (collection: 2.209s, learning 0.189s)
             Mean action noise std: 2.19
          Mean value_function loss: 13.8404
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.2774
                       Mean reward: 21.13
               Mean episode length: 183.74
    Episode_Reward/reaching_object: 0.7070
     Episode_Reward/lifting_object: 3.0396
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.40s
                      Time elapsed: 00:15:53
                               ETA: 01:09:48

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 39739 steps/s (collection: 2.289s, learning 0.185s)
             Mean action noise std: 2.20
          Mean value_function loss: 14.0885
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.2950
                       Mean reward: 17.80
               Mean episode length: 169.05
    Episode_Reward/reaching_object: 0.7049
     Episode_Reward/lifting_object: 2.6150
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.47s
                      Time elapsed: 00:15:55
                               ETA: 01:09:45

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 37373 steps/s (collection: 2.482s, learning 0.148s)
             Mean action noise std: 2.20
          Mean value_function loss: 11.9203
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.3134
                       Mean reward: 25.14
               Mean episode length: 192.23
    Episode_Reward/reaching_object: 0.7128
     Episode_Reward/lifting_object: 2.9414
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.63s
                      Time elapsed: 00:15:58
                               ETA: 01:09:43

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 38651 steps/s (collection: 2.364s, learning 0.179s)
             Mean action noise std: 2.20
          Mean value_function loss: 16.3576
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.3270
                       Mean reward: 19.73
               Mean episode length: 190.38
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 2.8601
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.54s
                      Time elapsed: 00:16:00
                               ETA: 01:09:40

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 37181 steps/s (collection: 2.516s, learning 0.128s)
             Mean action noise std: 2.20
          Mean value_function loss: 19.4140
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.3469
                       Mean reward: 20.88
               Mean episode length: 189.27
    Episode_Reward/reaching_object: 0.7246
     Episode_Reward/lifting_object: 3.0180
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.64s
                      Time elapsed: 00:16:03
                               ETA: 01:09:38

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 37426 steps/s (collection: 2.484s, learning 0.142s)
             Mean action noise std: 2.20
          Mean value_function loss: 15.0499
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.3731
                       Mean reward: 18.67
               Mean episode length: 173.75
    Episode_Reward/reaching_object: 0.7161
     Episode_Reward/lifting_object: 2.6925
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.63s
                      Time elapsed: 00:16:06
                               ETA: 01:09:35

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 40636 steps/s (collection: 2.231s, learning 0.188s)
             Mean action noise std: 2.20
          Mean value_function loss: 21.9611
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.3888
                       Mean reward: 16.95
               Mean episode length: 192.22
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 3.4295
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.42s
                      Time elapsed: 00:16:08
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 40880 steps/s (collection: 2.266s, learning 0.139s)
             Mean action noise std: 2.21
          Mean value_function loss: 24.3604
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.4067
                       Mean reward: 19.11
               Mean episode length: 183.11
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 3.5122
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.40s
                      Time elapsed: 00:16:11
                               ETA: 01:09:29

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 40405 steps/s (collection: 2.248s, learning 0.185s)
             Mean action noise std: 2.21
          Mean value_function loss: 15.9972
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.4222
                       Mean reward: 17.36
               Mean episode length: 185.85
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 3.3063
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.43s
                      Time elapsed: 00:16:13
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 39514 steps/s (collection: 2.344s, learning 0.144s)
             Mean action noise std: 2.21
          Mean value_function loss: 22.8663
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.4381
                       Mean reward: 23.97
               Mean episode length: 181.40
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 4.0567
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.49s
                      Time elapsed: 00:16:15
                               ETA: 01:09:23

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 40461 steps/s (collection: 2.272s, learning 0.158s)
             Mean action noise std: 2.21
          Mean value_function loss: 21.7381
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.4607
                       Mean reward: 17.77
               Mean episode length: 171.25
    Episode_Reward/reaching_object: 0.7192
     Episode_Reward/lifting_object: 3.1166
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.43s
                      Time elapsed: 00:16:18
                               ETA: 01:09:20

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 39454 steps/s (collection: 2.341s, learning 0.150s)
             Mean action noise std: 2.21
          Mean value_function loss: 21.3255
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.4819
                       Mean reward: 21.92
               Mean episode length: 173.13
    Episode_Reward/reaching_object: 0.7372
     Episode_Reward/lifting_object: 3.6499
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.49s
                      Time elapsed: 00:16:20
                               ETA: 01:09:17

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 40319 steps/s (collection: 2.300s, learning 0.139s)
             Mean action noise std: 2.21
          Mean value_function loss: 24.8730
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.4950
                       Mean reward: 15.78
               Mean episode length: 166.87
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 3.1190
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.44s
                      Time elapsed: 00:16:23
                               ETA: 01:09:14

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 38923 steps/s (collection: 2.383s, learning 0.143s)
             Mean action noise std: 2.21
          Mean value_function loss: 20.4963
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 57.5095
                       Mean reward: 21.38
               Mean episode length: 163.44
    Episode_Reward/reaching_object: 0.7164
     Episode_Reward/lifting_object: 3.7143
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.53s
                      Time elapsed: 00:16:25
                               ETA: 01:09:11

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 41301 steps/s (collection: 2.260s, learning 0.121s)
             Mean action noise std: 2.22
          Mean value_function loss: 20.1254
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 57.5312
                       Mean reward: 18.44
               Mean episode length: 170.86
    Episode_Reward/reaching_object: 0.7109
     Episode_Reward/lifting_object: 3.5131
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.38s
                      Time elapsed: 00:16:28
                               ETA: 01:09:08

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 36889 steps/s (collection: 2.499s, learning 0.166s)
             Mean action noise std: 2.22
          Mean value_function loss: 26.8138
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.5411
                       Mean reward: 22.88
               Mean episode length: 170.48
    Episode_Reward/reaching_object: 0.7308
     Episode_Reward/lifting_object: 3.9636
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.66s
                      Time elapsed: 00:16:30
                               ETA: 01:09:05

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 36245 steps/s (collection: 2.501s, learning 0.211s)
             Mean action noise std: 2.22
          Mean value_function loss: 36.4963
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.5508
                       Mean reward: 19.53
               Mean episode length: 163.74
    Episode_Reward/reaching_object: 0.6982
     Episode_Reward/lifting_object: 3.6661
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.71s
                      Time elapsed: 00:16:33
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 39111 steps/s (collection: 2.349s, learning 0.165s)
             Mean action noise std: 2.22
          Mean value_function loss: 30.6020
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.5634
                       Mean reward: 21.32
               Mean episode length: 169.68
    Episode_Reward/reaching_object: 0.6971
     Episode_Reward/lifting_object: 3.3986
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.51s
                      Time elapsed: 00:16:36
                               ETA: 01:09:01

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 37917 steps/s (collection: 2.432s, learning 0.161s)
             Mean action noise std: 2.22
          Mean value_function loss: 25.0887
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.5847
                       Mean reward: 28.03
               Mean episode length: 171.21
    Episode_Reward/reaching_object: 0.7048
     Episode_Reward/lifting_object: 4.2981
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.59s
                      Time elapsed: 00:16:38
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 36211 steps/s (collection: 2.510s, learning 0.205s)
             Mean action noise std: 2.22
          Mean value_function loss: 35.5754
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.6158
                       Mean reward: 21.52
               Mean episode length: 170.19
    Episode_Reward/reaching_object: 0.6941
     Episode_Reward/lifting_object: 3.7350
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.71s
                      Time elapsed: 00:16:41
                               ETA: 01:08:56

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 37202 steps/s (collection: 2.511s, learning 0.132s)
             Mean action noise std: 2.23
          Mean value_function loss: 29.9153
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 57.6390
                       Mean reward: 21.44
               Mean episode length: 164.47
    Episode_Reward/reaching_object: 0.7004
     Episode_Reward/lifting_object: 3.5340
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.64s
                      Time elapsed: 00:16:44
                               ETA: 01:08:54

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 38588 steps/s (collection: 2.380s, learning 0.167s)
             Mean action noise std: 2.23
          Mean value_function loss: 24.2404
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 57.6420
                       Mean reward: 30.69
               Mean episode length: 170.19
    Episode_Reward/reaching_object: 0.6990
     Episode_Reward/lifting_object: 4.0544
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.55s
                      Time elapsed: 00:16:46
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 37855 steps/s (collection: 2.410s, learning 0.187s)
             Mean action noise std: 2.23
          Mean value_function loss: 30.9278
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 57.6438
                       Mean reward: 27.26
               Mean episode length: 152.88
    Episode_Reward/reaching_object: 0.6755
     Episode_Reward/lifting_object: 4.5768
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.60s
                      Time elapsed: 00:16:49
                               ETA: 01:08:49

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 36067 steps/s (collection: 2.504s, learning 0.221s)
             Mean action noise std: 2.23
          Mean value_function loss: 24.8061
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.6499
                       Mean reward: 23.91
               Mean episode length: 176.88
    Episode_Reward/reaching_object: 0.7157
     Episode_Reward/lifting_object: 4.2773
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.73s
                      Time elapsed: 00:16:51
                               ETA: 01:08:47

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 36503 steps/s (collection: 2.461s, learning 0.232s)
             Mean action noise std: 2.23
          Mean value_function loss: 28.2559
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.6613
                       Mean reward: 22.15
               Mean episode length: 156.11
    Episode_Reward/reaching_object: 0.6980
     Episode_Reward/lifting_object: 4.5168
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.69s
                      Time elapsed: 00:16:54
                               ETA: 01:08:45

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 40075 steps/s (collection: 2.324s, learning 0.129s)
             Mean action noise std: 2.23
          Mean value_function loss: 26.7514
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.6773
                       Mean reward: 24.06
               Mean episode length: 150.55
    Episode_Reward/reaching_object: 0.6837
     Episode_Reward/lifting_object: 4.2557
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.45s
                      Time elapsed: 00:16:57
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 41352 steps/s (collection: 2.240s, learning 0.138s)
             Mean action noise std: 2.23
          Mean value_function loss: 28.6598
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.6944
                       Mean reward: 27.86
               Mean episode length: 154.84
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 4.8296
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.38s
                      Time elapsed: 00:16:59
                               ETA: 01:08:39

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 39309 steps/s (collection: 2.343s, learning 0.158s)
             Mean action noise std: 2.23
          Mean value_function loss: 27.5362
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.7114
                       Mean reward: 30.38
               Mean episode length: 152.27
    Episode_Reward/reaching_object: 0.7066
     Episode_Reward/lifting_object: 5.0031
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.50s
                      Time elapsed: 00:17:01
                               ETA: 01:08:36

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 38893 steps/s (collection: 2.380s, learning 0.147s)
             Mean action noise std: 2.23
          Mean value_function loss: 22.0665
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.7277
                       Mean reward: 24.08
               Mean episode length: 159.89
    Episode_Reward/reaching_object: 0.6845
     Episode_Reward/lifting_object: 4.5388
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.53s
                      Time elapsed: 00:17:04
                               ETA: 01:08:33

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 38922 steps/s (collection: 2.362s, learning 0.164s)
             Mean action noise std: 2.23
          Mean value_function loss: 25.2509
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 57.7370
                       Mean reward: 26.29
               Mean episode length: 161.63
    Episode_Reward/reaching_object: 0.6688
     Episode_Reward/lifting_object: 4.6125
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.53s
                      Time elapsed: 00:17:07
                               ETA: 01:08:30

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 37927 steps/s (collection: 2.411s, learning 0.181s)
             Mean action noise std: 2.24
          Mean value_function loss: 30.4333
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.7521
                       Mean reward: 24.07
               Mean episode length: 147.15
    Episode_Reward/reaching_object: 0.6684
     Episode_Reward/lifting_object: 4.7375
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.59s
                      Time elapsed: 00:17:09
                               ETA: 01:08:28

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 38041 steps/s (collection: 2.450s, learning 0.135s)
             Mean action noise std: 2.24
          Mean value_function loss: 34.4988
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 57.7649
                       Mean reward: 31.19
               Mean episode length: 153.65
    Episode_Reward/reaching_object: 0.6949
     Episode_Reward/lifting_object: 5.3713
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.58s
                      Time elapsed: 00:17:12
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 37600 steps/s (collection: 2.429s, learning 0.185s)
             Mean action noise std: 2.24
          Mean value_function loss: 35.0863
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.7687
                       Mean reward: 28.33
               Mean episode length: 173.33
    Episode_Reward/reaching_object: 0.7116
     Episode_Reward/lifting_object: 4.9781
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.61s
                      Time elapsed: 00:17:14
                               ETA: 01:08:23

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 39585 steps/s (collection: 2.366s, learning 0.117s)
             Mean action noise std: 2.24
          Mean value_function loss: 36.7299
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.7772
                       Mean reward: 30.42
               Mean episode length: 167.72
    Episode_Reward/reaching_object: 0.6832
     Episode_Reward/lifting_object: 5.0583
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.48s
                      Time elapsed: 00:17:17
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 38465 steps/s (collection: 2.376s, learning 0.180s)
             Mean action noise std: 2.24
          Mean value_function loss: 29.8735
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.7889
                       Mean reward: 28.33
               Mean episode length: 153.13
    Episode_Reward/reaching_object: 0.6979
     Episode_Reward/lifting_object: 5.3554
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.56s
                      Time elapsed: 00:17:19
                               ETA: 01:08:17

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 37216 steps/s (collection: 2.477s, learning 0.165s)
             Mean action noise std: 2.24
          Mean value_function loss: 44.4781
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.8033
                       Mean reward: 32.50
               Mean episode length: 158.37
    Episode_Reward/reaching_object: 0.6992
     Episode_Reward/lifting_object: 5.2936
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.64s
                      Time elapsed: 00:17:22
                               ETA: 01:08:15

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 38741 steps/s (collection: 2.379s, learning 0.159s)
             Mean action noise std: 2.24
          Mean value_function loss: 43.7487
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 57.8188
                       Mean reward: 28.79
               Mean episode length: 164.74
    Episode_Reward/reaching_object: 0.6992
     Episode_Reward/lifting_object: 5.0754
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.54s
                      Time elapsed: 00:17:25
                               ETA: 01:08:12

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 37132 steps/s (collection: 2.433s, learning 0.215s)
             Mean action noise std: 2.24
          Mean value_function loss: 32.2613
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 57.8257
                       Mean reward: 33.22
               Mean episode length: 158.57
    Episode_Reward/reaching_object: 0.7041
     Episode_Reward/lifting_object: 5.3453
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.65s
                      Time elapsed: 00:17:27
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 40722 steps/s (collection: 2.288s, learning 0.126s)
             Mean action noise std: 2.24
          Mean value_function loss: 46.6568
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 57.8307
                       Mean reward: 36.02
               Mean episode length: 174.57
    Episode_Reward/reaching_object: 0.7067
     Episode_Reward/lifting_object: 5.4957
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.41s
                      Time elapsed: 00:17:30
                               ETA: 01:08:07

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 38898 steps/s (collection: 2.346s, learning 0.182s)
             Mean action noise std: 2.24
          Mean value_function loss: 26.9415
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.8393
                       Mean reward: 31.40
               Mean episode length: 164.95
    Episode_Reward/reaching_object: 0.7093
     Episode_Reward/lifting_object: 5.9344
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.53s
                      Time elapsed: 00:17:32
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 37051 steps/s (collection: 2.454s, learning 0.199s)
             Mean action noise std: 2.25
          Mean value_function loss: 30.7426
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.8565
                       Mean reward: 24.62
               Mean episode length: 158.59
    Episode_Reward/reaching_object: 0.7353
     Episode_Reward/lifting_object: 5.4790
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.65s
                      Time elapsed: 00:17:35
                               ETA: 01:08:02

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 34494 steps/s (collection: 2.667s, learning 0.183s)
             Mean action noise std: 2.25
          Mean value_function loss: 37.9848
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.8749
                       Mean reward: 31.06
               Mean episode length: 159.67
    Episode_Reward/reaching_object: 0.6930
     Episode_Reward/lifting_object: 5.4104
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.85s
                      Time elapsed: 00:17:38
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 36132 steps/s (collection: 2.501s, learning 0.220s)
             Mean action noise std: 2.25
          Mean value_function loss: 43.3053
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 57.8834
                       Mean reward: 30.22
               Mean episode length: 162.09
    Episode_Reward/reaching_object: 0.7149
     Episode_Reward/lifting_object: 5.8919
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.72s
                      Time elapsed: 00:17:40
                               ETA: 01:07:59

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 36242 steps/s (collection: 2.532s, learning 0.180s)
             Mean action noise std: 2.25
          Mean value_function loss: 37.3645
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.8996
                       Mean reward: 34.30
               Mean episode length: 167.12
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 6.0712
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.71s
                      Time elapsed: 00:17:43
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 33587 steps/s (collection: 2.715s, learning 0.212s)
             Mean action noise std: 2.25
          Mean value_function loss: 51.7861
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.9166
                       Mean reward: 39.26
               Mean episode length: 167.40
    Episode_Reward/reaching_object: 0.7236
     Episode_Reward/lifting_object: 5.4010
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.93s
                      Time elapsed: 00:17:46
                               ETA: 01:07:55

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 36592 steps/s (collection: 2.509s, learning 0.178s)
             Mean action noise std: 2.25
          Mean value_function loss: 43.8715
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.9371
                       Mean reward: 32.03
               Mean episode length: 154.24
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 6.0556
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.69s
                      Time elapsed: 00:17:49
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 37653 steps/s (collection: 2.466s, learning 0.145s)
             Mean action noise std: 2.25
          Mean value_function loss: 62.7143
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 57.9493
                       Mean reward: 33.57
               Mean episode length: 148.27
    Episode_Reward/reaching_object: 0.6817
     Episode_Reward/lifting_object: 6.1564
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.61s
                      Time elapsed: 00:17:51
                               ETA: 01:07:51

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 38957 steps/s (collection: 2.402s, learning 0.122s)
             Mean action noise std: 2.25
          Mean value_function loss: 56.6825
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.9564
                       Mean reward: 32.43
               Mean episode length: 147.69
    Episode_Reward/reaching_object: 0.6988
     Episode_Reward/lifting_object: 6.1647
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.52s
                      Time elapsed: 00:17:54
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 41436 steps/s (collection: 2.243s, learning 0.129s)
             Mean action noise std: 2.25
          Mean value_function loss: 86.5643
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 57.9661
                       Mean reward: 37.59
               Mean episode length: 163.96
    Episode_Reward/reaching_object: 0.7039
     Episode_Reward/lifting_object: 5.5462
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.37s
                      Time elapsed: 00:17:56
                               ETA: 01:07:45

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 38765 steps/s (collection: 2.358s, learning 0.178s)
             Mean action noise std: 2.26
          Mean value_function loss: 40.2689
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 57.9731
                       Mean reward: 41.24
               Mean episode length: 162.10
    Episode_Reward/reaching_object: 0.6950
     Episode_Reward/lifting_object: 6.2616
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.54s
                      Time elapsed: 00:17:59
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 40503 steps/s (collection: 2.262s, learning 0.165s)
             Mean action noise std: 2.26
          Mean value_function loss: 47.9115
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 57.9804
                       Mean reward: 35.98
               Mean episode length: 147.05
    Episode_Reward/reaching_object: 0.7082
     Episode_Reward/lifting_object: 6.7370
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.43s
                      Time elapsed: 00:18:01
                               ETA: 01:07:39

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 38372 steps/s (collection: 2.397s, learning 0.165s)
             Mean action noise std: 2.26
          Mean value_function loss: 37.2749
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 57.9870
                       Mean reward: 38.35
               Mean episode length: 143.22
    Episode_Reward/reaching_object: 0.6954
     Episode_Reward/lifting_object: 7.0221
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.56s
                      Time elapsed: 00:18:04
                               ETA: 01:07:36

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 37567 steps/s (collection: 2.395s, learning 0.222s)
             Mean action noise std: 2.26
          Mean value_function loss: 47.7392
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.9924
                       Mean reward: 39.71
               Mean episode length: 147.76
    Episode_Reward/reaching_object: 0.7076
     Episode_Reward/lifting_object: 7.3372
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.62s
                      Time elapsed: 00:18:06
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 37148 steps/s (collection: 2.443s, learning 0.204s)
             Mean action noise std: 2.26
          Mean value_function loss: 39.8872
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.0004
                       Mean reward: 38.09
               Mean episode length: 141.78
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 7.1486
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.65s
                      Time elapsed: 00:18:09
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 36923 steps/s (collection: 2.468s, learning 0.194s)
             Mean action noise std: 2.26
          Mean value_function loss: 51.4801
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.0116
                       Mean reward: 43.56
               Mean episode length: 156.77
    Episode_Reward/reaching_object: 0.7025
     Episode_Reward/lifting_object: 6.9559
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.66s
                      Time elapsed: 00:18:12
                               ETA: 01:07:29

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 39950 steps/s (collection: 2.349s, learning 0.112s)
             Mean action noise std: 2.26
          Mean value_function loss: 48.9153
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.0263
                       Mean reward: 44.16
               Mean episode length: 139.38
    Episode_Reward/reaching_object: 0.6917
     Episode_Reward/lifting_object: 7.5472
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.46s
                      Time elapsed: 00:18:14
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 36939 steps/s (collection: 2.538s, learning 0.124s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.5495
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.0417
                       Mean reward: 50.11
               Mean episode length: 149.84
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 7.7115
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.66s
                      Time elapsed: 00:18:17
                               ETA: 01:07:24

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 40148 steps/s (collection: 2.333s, learning 0.115s)
             Mean action noise std: 2.26
          Mean value_function loss: 91.0876
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 58.0498
                       Mean reward: 43.94
               Mean episode length: 154.10
    Episode_Reward/reaching_object: 0.6873
     Episode_Reward/lifting_object: 7.0876
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.45s
                      Time elapsed: 00:18:19
                               ETA: 01:07:21

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 40559 steps/s (collection: 2.265s, learning 0.158s)
             Mean action noise std: 2.26
          Mean value_function loss: 133.6766
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 58.0512
                       Mean reward: 32.91
               Mean episode length: 146.98
    Episode_Reward/reaching_object: 0.7038
     Episode_Reward/lifting_object: 6.8537
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.42s
                      Time elapsed: 00:18:22
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 40977 steps/s (collection: 2.298s, learning 0.101s)
             Mean action noise std: 2.26
          Mean value_function loss: 211.7145
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.0515
                       Mean reward: 39.28
               Mean episode length: 137.92
    Episode_Reward/reaching_object: 0.6751
     Episode_Reward/lifting_object: 7.3639
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.40s
                      Time elapsed: 00:18:24
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 39958 steps/s (collection: 2.315s, learning 0.145s)
             Mean action noise std: 2.26
          Mean value_function loss: 67.8061
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.0545
                       Mean reward: 39.64
               Mean episode length: 156.94
    Episode_Reward/reaching_object: 0.6837
     Episode_Reward/lifting_object: 7.1456
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.46s
                      Time elapsed: 00:18:26
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 38520 steps/s (collection: 2.377s, learning 0.175s)
             Mean action noise std: 2.26
          Mean value_function loss: 58.2216
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.0634
                       Mean reward: 42.25
               Mean episode length: 148.70
    Episode_Reward/reaching_object: 0.6832
     Episode_Reward/lifting_object: 7.5427
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.55s
                      Time elapsed: 00:18:29
                               ETA: 01:07:09

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 38889 steps/s (collection: 2.351s, learning 0.177s)
             Mean action noise std: 2.27
          Mean value_function loss: 57.6447
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.0820
                       Mean reward: 16.42
               Mean episode length: 130.39
    Episode_Reward/reaching_object: 0.6623
     Episode_Reward/lifting_object: 6.1400
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.53s
                      Time elapsed: 00:18:32
                               ETA: 01:07:07

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 37708 steps/s (collection: 2.446s, learning 0.161s)
             Mean action noise std: 2.27
          Mean value_function loss: 56.8052
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.0976
                       Mean reward: 44.75
               Mean episode length: 144.91
    Episode_Reward/reaching_object: 0.6654
     Episode_Reward/lifting_object: 7.2146
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.61s
                      Time elapsed: 00:18:34
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 38562 steps/s (collection: 2.390s, learning 0.159s)
             Mean action noise std: 2.27
          Mean value_function loss: 85.1169
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.1122
                       Mean reward: 31.61
               Mean episode length: 135.75
    Episode_Reward/reaching_object: 0.6486
     Episode_Reward/lifting_object: 6.8922
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.55s
                      Time elapsed: 00:18:37
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 37426 steps/s (collection: 2.428s, learning 0.198s)
             Mean action noise std: 2.27
          Mean value_function loss: 84.0960
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.1284
                       Mean reward: 40.18
               Mean episode length: 136.44
    Episode_Reward/reaching_object: 0.6762
     Episode_Reward/lifting_object: 6.8458
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.63s
                      Time elapsed: 00:18:39
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 38462 steps/s (collection: 2.408s, learning 0.148s)
             Mean action noise std: 2.27
          Mean value_function loss: 67.8400
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.1411
                       Mean reward: 33.73
               Mean episode length: 130.65
    Episode_Reward/reaching_object: 0.6539
     Episode_Reward/lifting_object: 7.3211
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.56s
                      Time elapsed: 00:18:42
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 38083 steps/s (collection: 2.376s, learning 0.206s)
             Mean action noise std: 2.27
          Mean value_function loss: 63.0887
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.1486
                       Mean reward: 43.62
               Mean episode length: 142.60
    Episode_Reward/reaching_object: 0.6423
     Episode_Reward/lifting_object: 7.1434
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.58s
                      Time elapsed: 00:18:44
                               ETA: 01:06:54

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 40627 steps/s (collection: 2.308s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 57.4007
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.1580
                       Mean reward: 43.84
               Mean episode length: 131.77
    Episode_Reward/reaching_object: 0.6110
     Episode_Reward/lifting_object: 7.4495
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.42s
                      Time elapsed: 00:18:47
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 39955 steps/s (collection: 2.277s, learning 0.183s)
             Mean action noise std: 2.27
          Mean value_function loss: 84.4878
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.1674
                       Mean reward: 40.44
               Mean episode length: 135.27
    Episode_Reward/reaching_object: 0.6181
     Episode_Reward/lifting_object: 7.2174
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.46s
                      Time elapsed: 00:18:49
                               ETA: 01:06:48

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 38528 steps/s (collection: 2.389s, learning 0.163s)
             Mean action noise std: 2.27
          Mean value_function loss: 78.8317
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.1775
                       Mean reward: 23.55
               Mean episode length: 137.00
    Episode_Reward/reaching_object: 0.6253
     Episode_Reward/lifting_object: 6.5176
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.55s
                      Time elapsed: 00:18:52
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 39081 steps/s (collection: 2.330s, learning 0.186s)
             Mean action noise std: 2.27
          Mean value_function loss: 50.0993
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.1889
                       Mean reward: 41.73
               Mean episode length: 126.87
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 7.4466
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.52s
                      Time elapsed: 00:18:54
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 39204 steps/s (collection: 2.384s, learning 0.124s)
             Mean action noise std: 2.28
          Mean value_function loss: 70.3885
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 58.1983
                       Mean reward: 44.62
               Mean episode length: 146.88
    Episode_Reward/reaching_object: 0.6323
     Episode_Reward/lifting_object: 7.6289
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.51s
                      Time elapsed: 00:18:57
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 38597 steps/s (collection: 2.411s, learning 0.136s)
             Mean action noise std: 2.28
          Mean value_function loss: 67.1338
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 58.2043
                       Mean reward: 39.47
               Mean episode length: 136.85
    Episode_Reward/reaching_object: 0.6493
     Episode_Reward/lifting_object: 7.9581
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.55s
                      Time elapsed: 00:18:59
                               ETA: 01:06:37

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 39079 steps/s (collection: 2.334s, learning 0.181s)
             Mean action noise std: 2.28
          Mean value_function loss: 91.5134
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.2130
                       Mean reward: 34.66
               Mean episode length: 124.03
    Episode_Reward/reaching_object: 0.6330
     Episode_Reward/lifting_object: 7.7060
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.52s
                      Time elapsed: 00:19:02
                               ETA: 01:06:34

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 37027 steps/s (collection: 2.522s, learning 0.133s)
             Mean action noise std: 2.28
          Mean value_function loss: 61.3660
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.2212
                       Mean reward: 43.86
               Mean episode length: 132.00
    Episode_Reward/reaching_object: 0.6161
     Episode_Reward/lifting_object: 7.9755
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.65s
                      Time elapsed: 00:19:05
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 40079 steps/s (collection: 2.338s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 60.9143
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.2279
                       Mean reward: 49.98
               Mean episode length: 138.82
    Episode_Reward/reaching_object: 0.6240
     Episode_Reward/lifting_object: 8.0772
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.45s
                      Time elapsed: 00:19:07
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 37798 steps/s (collection: 2.422s, learning 0.179s)
             Mean action noise std: 2.28
          Mean value_function loss: 75.3391
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.2395
                       Mean reward: 40.86
               Mean episode length: 121.72
    Episode_Reward/reaching_object: 0.6284
     Episode_Reward/lifting_object: 8.7909
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.60s
                      Time elapsed: 00:19:10
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 38913 steps/s (collection: 2.371s, learning 0.156s)
             Mean action noise std: 2.28
          Mean value_function loss: 108.8171
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.2479
                       Mean reward: 32.69
               Mean episode length: 128.71
    Episode_Reward/reaching_object: 0.6301
     Episode_Reward/lifting_object: 7.8292
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.53s
                      Time elapsed: 00:19:12
                               ETA: 01:06:24

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 36129 steps/s (collection: 2.556s, learning 0.165s)
             Mean action noise std: 2.28
          Mean value_function loss: 90.8164
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.2532
                       Mean reward: 35.32
               Mean episode length: 128.35
    Episode_Reward/reaching_object: 0.6130
     Episode_Reward/lifting_object: 7.9722
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.72s
                      Time elapsed: 00:19:15
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 38526 steps/s (collection: 2.354s, learning 0.198s)
             Mean action noise std: 2.28
          Mean value_function loss: 129.3677
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.2635
                       Mean reward: 41.72
               Mean episode length: 131.77
    Episode_Reward/reaching_object: 0.6538
     Episode_Reward/lifting_object: 9.0887
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.55s
                      Time elapsed: 00:19:18
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 36436 steps/s (collection: 2.537s, learning 0.161s)
             Mean action noise std: 2.28
          Mean value_function loss: 74.0069
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.2782
                       Mean reward: 35.42
               Mean episode length: 134.35
    Episode_Reward/reaching_object: 0.6365
     Episode_Reward/lifting_object: 7.9271
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.70s
                      Time elapsed: 00:19:20
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 38386 steps/s (collection: 2.408s, learning 0.153s)
             Mean action noise std: 2.28
          Mean value_function loss: 92.7342
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.2870
                       Mean reward: 54.28
               Mean episode length: 148.02
    Episode_Reward/reaching_object: 0.6442
     Episode_Reward/lifting_object: 9.3079
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.56s
                      Time elapsed: 00:19:23
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 39662 steps/s (collection: 2.330s, learning 0.149s)
             Mean action noise std: 2.28
          Mean value_function loss: 72.3188
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.2938
                       Mean reward: 46.50
               Mean episode length: 139.76
    Episode_Reward/reaching_object: 0.6510
     Episode_Reward/lifting_object: 9.3838
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.48s
                      Time elapsed: 00:19:25
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 39155 steps/s (collection: 2.365s, learning 0.146s)
             Mean action noise std: 2.28
          Mean value_function loss: 67.5378
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.2986
                       Mean reward: 50.43
               Mean episode length: 135.82
    Episode_Reward/reaching_object: 0.6818
     Episode_Reward/lifting_object: 10.8405
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.51s
                      Time elapsed: 00:19:28
                               ETA: 01:06:09

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 38423 steps/s (collection: 2.436s, learning 0.122s)
             Mean action noise std: 2.29
          Mean value_function loss: 77.2946
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.3058
                       Mean reward: 58.72
               Mean episode length: 140.92
    Episode_Reward/reaching_object: 0.6438
     Episode_Reward/lifting_object: 10.5394
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.56s
                      Time elapsed: 00:19:30
                               ETA: 01:06:06

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 40110 steps/s (collection: 2.328s, learning 0.123s)
             Mean action noise std: 2.29
          Mean value_function loss: 114.1855
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.3161
                       Mean reward: 59.57
               Mean episode length: 137.77
    Episode_Reward/reaching_object: 0.6626
     Episode_Reward/lifting_object: 10.2320
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.45s
                      Time elapsed: 00:19:33
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 39084 steps/s (collection: 2.344s, learning 0.171s)
             Mean action noise std: 2.29
          Mean value_function loss: 81.3002
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.3237
                       Mean reward: 55.97
               Mean episode length: 135.80
    Episode_Reward/reaching_object: 0.6493
     Episode_Reward/lifting_object: 9.8626
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.52s
                      Time elapsed: 00:19:35
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 40593 steps/s (collection: 2.274s, learning 0.148s)
             Mean action noise std: 2.29
          Mean value_function loss: 103.3981
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.3279
                       Mean reward: 47.07
               Mean episode length: 131.74
    Episode_Reward/reaching_object: 0.6604
     Episode_Reward/lifting_object: 10.3191
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.42s
                      Time elapsed: 00:19:38
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 38750 steps/s (collection: 2.384s, learning 0.153s)
             Mean action noise std: 2.29
          Mean value_function loss: 104.7816
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.3312
                       Mean reward: 54.47
               Mean episode length: 132.04
    Episode_Reward/reaching_object: 0.6428
     Episode_Reward/lifting_object: 10.4653
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.54s
                      Time elapsed: 00:19:40
                               ETA: 01:05:55

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 38333 steps/s (collection: 2.381s, learning 0.184s)
             Mean action noise std: 2.29
          Mean value_function loss: 106.7553
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.3385
                       Mean reward: 64.75
               Mean episode length: 137.96
    Episode_Reward/reaching_object: 0.6379
     Episode_Reward/lifting_object: 10.4768
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.56s
                      Time elapsed: 00:19:43
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 39546 steps/s (collection: 2.339s, learning 0.147s)
             Mean action noise std: 2.29
          Mean value_function loss: 81.7530
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 58.3476
                       Mean reward: 60.97
               Mean episode length: 137.03
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: 11.3741
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.49s
                      Time elapsed: 00:19:45
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 37916 steps/s (collection: 2.419s, learning 0.173s)
             Mean action noise std: 2.29
          Mean value_function loss: 81.2236
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.3533
                       Mean reward: 54.75
               Mean episode length: 129.76
    Episode_Reward/reaching_object: 0.6345
     Episode_Reward/lifting_object: 11.2149
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.59s
                      Time elapsed: 00:19:48
                               ETA: 01:05:47

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 37961 steps/s (collection: 2.385s, learning 0.204s)
             Mean action noise std: 2.29
          Mean value_function loss: 80.1168
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.3583
                       Mean reward: 66.17
               Mean episode length: 136.89
    Episode_Reward/reaching_object: 0.6620
     Episode_Reward/lifting_object: 12.0004
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.59s
                      Time elapsed: 00:19:50
                               ETA: 01:05:45

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 36601 steps/s (collection: 2.491s, learning 0.195s)
             Mean action noise std: 2.29
          Mean value_function loss: 85.1978
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.3664
                       Mean reward: 58.26
               Mean episode length: 122.95
    Episode_Reward/reaching_object: 0.6455
     Episode_Reward/lifting_object: 11.8119
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.69s
                      Time elapsed: 00:19:53
                               ETA: 01:05:42

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 35987 steps/s (collection: 2.595s, learning 0.136s)
             Mean action noise std: 2.29
          Mean value_function loss: 92.0967
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.3700
                       Mean reward: 64.85
               Mean episode length: 131.03
    Episode_Reward/reaching_object: 0.6445
     Episode_Reward/lifting_object: 11.8965
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.73s
                      Time elapsed: 00:19:56
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 33095 steps/s (collection: 2.758s, learning 0.212s)
             Mean action noise std: 2.29
          Mean value_function loss: 85.0936
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.3748
                       Mean reward: 65.71
               Mean episode length: 121.57
    Episode_Reward/reaching_object: 0.6548
     Episode_Reward/lifting_object: 11.9970
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.97s
                      Time elapsed: 00:19:59
                               ETA: 01:05:39

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 32758 steps/s (collection: 2.818s, learning 0.183s)
             Mean action noise std: 2.29
          Mean value_function loss: 73.8924
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.3806
                       Mean reward: 64.48
               Mean episode length: 128.66
    Episode_Reward/reaching_object: 0.6360
     Episode_Reward/lifting_object: 12.2460
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 3.00s
                      Time elapsed: 00:20:02
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 33845 steps/s (collection: 2.741s, learning 0.163s)
             Mean action noise std: 2.29
          Mean value_function loss: 95.4466
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.3850
                       Mean reward: 73.14
               Mean episode length: 130.36
    Episode_Reward/reaching_object: 0.6244
     Episode_Reward/lifting_object: 12.0602
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.90s
                      Time elapsed: 00:20:05
                               ETA: 01:05:37

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 35723 steps/s (collection: 2.589s, learning 0.163s)
             Mean action noise std: 2.29
          Mean value_function loss: 86.1253
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 58.3912
                       Mean reward: 75.10
               Mean episode length: 135.15
    Episode_Reward/reaching_object: 0.6463
     Episode_Reward/lifting_object: 13.1596
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.75s
                      Time elapsed: 00:20:08
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 39253 steps/s (collection: 2.402s, learning 0.102s)
             Mean action noise std: 2.29
          Mean value_function loss: 103.9778
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.3960
                       Mean reward: 70.17
               Mean episode length: 135.91
    Episode_Reward/reaching_object: 0.6574
     Episode_Reward/lifting_object: 13.5750
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.50s
                      Time elapsed: 00:20:10
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 37833 steps/s (collection: 2.380s, learning 0.218s)
             Mean action noise std: 2.29
          Mean value_function loss: 94.4378
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 58.4022
                       Mean reward: 67.59
               Mean episode length: 124.10
    Episode_Reward/reaching_object: 0.6378
     Episode_Reward/lifting_object: 13.0870
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.60s
                      Time elapsed: 00:20:13
                               ETA: 01:05:29

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 35423 steps/s (collection: 2.623s, learning 0.152s)
             Mean action noise std: 2.29
          Mean value_function loss: 88.0787
               Mean surrogate loss: 0.0183
                 Mean entropy loss: 58.4049
                       Mean reward: 72.10
               Mean episode length: 131.26
    Episode_Reward/reaching_object: 0.6568
     Episode_Reward/lifting_object: 13.5691
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.78s
                      Time elapsed: 00:20:15
                               ETA: 01:05:27

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 37142 steps/s (collection: 2.481s, learning 0.166s)
             Mean action noise std: 2.29
          Mean value_function loss: 98.1343
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.4053
                       Mean reward: 82.66
               Mean episode length: 129.49
    Episode_Reward/reaching_object: 0.6558
     Episode_Reward/lifting_object: 14.4018
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.65s
                      Time elapsed: 00:20:18
                               ETA: 01:05:25

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 39713 steps/s (collection: 2.336s, learning 0.139s)
             Mean action noise std: 2.29
          Mean value_function loss: 100.9642
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.4062
                       Mean reward: 61.78
               Mean episode length: 121.99
    Episode_Reward/reaching_object: 0.6375
     Episode_Reward/lifting_object: 13.6534
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.48s
                      Time elapsed: 00:20:21
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 37758 steps/s (collection: 2.486s, learning 0.118s)
             Mean action noise std: 2.29
          Mean value_function loss: 89.0983
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.4067
                       Mean reward: 81.78
               Mean episode length: 130.39
    Episode_Reward/reaching_object: 0.6531
     Episode_Reward/lifting_object: 13.6683
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.60s
                      Time elapsed: 00:20:23
                               ETA: 01:05:20

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 37389 steps/s (collection: 2.461s, learning 0.168s)
             Mean action noise std: 2.29
          Mean value_function loss: 107.9250
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 58.4077
                       Mean reward: 86.23
               Mean episode length: 137.36
    Episode_Reward/reaching_object: 0.6480
     Episode_Reward/lifting_object: 14.3361
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.63s
                      Time elapsed: 00:20:26
                               ETA: 01:05:17

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 36730 steps/s (collection: 2.506s, learning 0.170s)
             Mean action noise std: 2.29
          Mean value_function loss: 88.9938
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 58.4104
                       Mean reward: 69.20
               Mean episode length: 117.51
    Episode_Reward/reaching_object: 0.6421
     Episode_Reward/lifting_object: 14.6181
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.68s
                      Time elapsed: 00:20:28
                               ETA: 01:05:15

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 35853 steps/s (collection: 2.577s, learning 0.165s)
             Mean action noise std: 2.29
          Mean value_function loss: 118.8315
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 58.4120
                       Mean reward: 80.17
               Mean episode length: 133.06
    Episode_Reward/reaching_object: 0.6216
     Episode_Reward/lifting_object: 13.7829
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.74s
                      Time elapsed: 00:20:31
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 34996 steps/s (collection: 2.642s, learning 0.167s)
             Mean action noise std: 2.29
          Mean value_function loss: 112.1615
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.4130
                       Mean reward: 74.94
               Mean episode length: 116.64
    Episode_Reward/reaching_object: 0.6276
     Episode_Reward/lifting_object: 14.4428
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.81s
                      Time elapsed: 00:20:34
                               ETA: 01:05:11

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 37732 steps/s (collection: 2.453s, learning 0.152s)
             Mean action noise std: 2.29
          Mean value_function loss: 102.1435
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 58.4142
                       Mean reward: 75.37
               Mean episode length: 117.41
    Episode_Reward/reaching_object: 0.6007
     Episode_Reward/lifting_object: 13.5427
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.61s
                      Time elapsed: 00:20:37
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 38213 steps/s (collection: 2.384s, learning 0.188s)
             Mean action noise std: 2.29
          Mean value_function loss: 110.2278
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 58.4145
                       Mean reward: 74.31
               Mean episode length: 125.36
    Episode_Reward/reaching_object: 0.6181
     Episode_Reward/lifting_object: 13.9169
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.57s
                      Time elapsed: 00:20:39
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 38002 steps/s (collection: 2.414s, learning 0.173s)
             Mean action noise std: 2.29
          Mean value_function loss: 105.8552
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 58.4147
                       Mean reward: 72.74
               Mean episode length: 121.20
    Episode_Reward/reaching_object: 0.6284
     Episode_Reward/lifting_object: 14.0974
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.59s
                      Time elapsed: 00:20:42
                               ETA: 01:05:04

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 38853 steps/s (collection: 2.410s, learning 0.120s)
             Mean action noise std: 2.30
          Mean value_function loss: 125.6177
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 58.4156
                       Mean reward: 66.81
               Mean episode length: 123.11
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 13.5545
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.53s
                      Time elapsed: 00:20:44
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 38649 steps/s (collection: 2.377s, learning 0.167s)
             Mean action noise std: 2.30
          Mean value_function loss: 91.2432
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 58.4173
                       Mean reward: 75.14
               Mean episode length: 117.93
    Episode_Reward/reaching_object: 0.6235
     Episode_Reward/lifting_object: 14.4222
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.54s
                      Time elapsed: 00:20:47
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 36857 steps/s (collection: 2.557s, learning 0.110s)
             Mean action noise std: 2.30
          Mean value_function loss: 92.6984
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 58.4183
                       Mean reward: 63.76
               Mean episode length: 110.85
    Episode_Reward/reaching_object: 0.6078
     Episode_Reward/lifting_object: 14.2630
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.67s
                      Time elapsed: 00:20:49
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 34296 steps/s (collection: 2.655s, learning 0.212s)
             Mean action noise std: 2.30
          Mean value_function loss: 97.5099
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 58.4189
                       Mean reward: 77.99
               Mean episode length: 122.09
    Episode_Reward/reaching_object: 0.6322
     Episode_Reward/lifting_object: 14.8215
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.87s
                      Time elapsed: 00:20:52
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 34471 steps/s (collection: 2.676s, learning 0.176s)
             Mean action noise std: 2.30
          Mean value_function loss: 112.3090
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 58.4194
                       Mean reward: 76.52
               Mean episode length: 118.99
    Episode_Reward/reaching_object: 0.6275
     Episode_Reward/lifting_object: 14.5898
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.85s
                      Time elapsed: 00:20:55
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 35056 steps/s (collection: 2.638s, learning 0.166s)
             Mean action noise std: 2.30
          Mean value_function loss: 117.9586
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 58.4197
                       Mean reward: 70.30
               Mean episode length: 114.52
    Episode_Reward/reaching_object: 0.6117
     Episode_Reward/lifting_object: 14.3822
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.80s
                      Time elapsed: 00:20:58
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 34971 steps/s (collection: 2.677s, learning 0.134s)
             Mean action noise std: 2.30
          Mean value_function loss: 117.5331
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.4207
                       Mean reward: 80.07
               Mean episode length: 128.86
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 14.7023
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.81s
                      Time elapsed: 00:21:01
                               ETA: 01:04:49

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 36006 steps/s (collection: 2.551s, learning 0.180s)
             Mean action noise std: 2.30
          Mean value_function loss: 127.6878
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4227
                       Mean reward: 71.36
               Mean episode length: 115.03
    Episode_Reward/reaching_object: 0.6041
     Episode_Reward/lifting_object: 13.9325
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.73s
                      Time elapsed: 00:21:04
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 35929 steps/s (collection: 2.597s, learning 0.139s)
             Mean action noise std: 2.30
          Mean value_function loss: 119.9582
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.4244
                       Mean reward: 80.57
               Mean episode length: 118.96
    Episode_Reward/reaching_object: 0.6474
     Episode_Reward/lifting_object: 16.1464
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.74s
                      Time elapsed: 00:21:06
                               ETA: 01:04:45

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 38506 steps/s (collection: 2.408s, learning 0.145s)
             Mean action noise std: 2.30
          Mean value_function loss: 141.8789
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.4281
                       Mean reward: 78.94
               Mean episode length: 112.37
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: 16.3247
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.55s
                      Time elapsed: 00:21:09
                               ETA: 01:04:42

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 39442 steps/s (collection: 2.390s, learning 0.102s)
             Mean action noise std: 2.30
          Mean value_function loss: 147.9940
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.4327
                       Mean reward: 85.63
               Mean episode length: 127.82
    Episode_Reward/reaching_object: 0.6300
     Episode_Reward/lifting_object: 15.4812
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.49s
                      Time elapsed: 00:21:11
                               ETA: 01:04:39

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 40098 steps/s (collection: 2.299s, learning 0.152s)
             Mean action noise std: 2.30
          Mean value_function loss: 112.7081
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.4361
                       Mean reward: 80.73
               Mean episode length: 132.00
    Episode_Reward/reaching_object: 0.6530
     Episode_Reward/lifting_object: 16.3457
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.45s
                      Time elapsed: 00:21:14
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 39465 steps/s (collection: 2.358s, learning 0.133s)
             Mean action noise std: 2.30
          Mean value_function loss: 175.9816
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.4420
                       Mean reward: 66.63
               Mean episode length: 120.59
    Episode_Reward/reaching_object: 0.6379
     Episode_Reward/lifting_object: 14.3968
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.49s
                      Time elapsed: 00:21:16
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 37607 steps/s (collection: 2.425s, learning 0.189s)
             Mean action noise std: 2.30
          Mean value_function loss: 141.7561
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.4484
                       Mean reward: 84.79
               Mean episode length: 129.61
    Episode_Reward/reaching_object: 0.6560
     Episode_Reward/lifting_object: 16.4299
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.61s
                      Time elapsed: 00:21:19
                               ETA: 01:04:31

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 39386 steps/s (collection: 2.367s, learning 0.129s)
             Mean action noise std: 2.30
          Mean value_function loss: 132.5849
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.4525
                       Mean reward: 88.17
               Mean episode length: 132.99
    Episode_Reward/reaching_object: 0.6416
     Episode_Reward/lifting_object: 16.6032
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.50s
                      Time elapsed: 00:21:21
                               ETA: 01:04:28

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 36518 steps/s (collection: 2.550s, learning 0.142s)
             Mean action noise std: 2.30
          Mean value_function loss: 126.9762
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.4566
                       Mean reward: 80.37
               Mean episode length: 122.67
    Episode_Reward/reaching_object: 0.6365
     Episode_Reward/lifting_object: 16.2302
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.69s
                      Time elapsed: 00:21:24
                               ETA: 01:04:26

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 38348 steps/s (collection: 2.412s, learning 0.151s)
             Mean action noise std: 2.30
          Mean value_function loss: 125.6629
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.4587
                       Mean reward: 92.37
               Mean episode length: 133.77
    Episode_Reward/reaching_object: 0.6458
     Episode_Reward/lifting_object: 16.6687
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.56s
                      Time elapsed: 00:21:27
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 37892 steps/s (collection: 2.455s, learning 0.139s)
             Mean action noise std: 2.30
          Mean value_function loss: 140.0213
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.4605
                       Mean reward: 74.55
               Mean episode length: 111.50
    Episode_Reward/reaching_object: 0.6164
     Episode_Reward/lifting_object: 16.8007
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.59s
                      Time elapsed: 00:21:29
                               ETA: 01:04:21

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 37829 steps/s (collection: 2.458s, learning 0.140s)
             Mean action noise std: 2.30
          Mean value_function loss: 157.1309
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.4635
                       Mean reward: 82.46
               Mean episode length: 120.99
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 16.6892
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.60s
                      Time elapsed: 00:21:32
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 35337 steps/s (collection: 2.606s, learning 0.176s)
             Mean action noise std: 2.30
          Mean value_function loss: 154.3167
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.4673
                       Mean reward: 93.12
               Mean episode length: 125.86
    Episode_Reward/reaching_object: 0.6350
     Episode_Reward/lifting_object: 17.6954
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.78s
                      Time elapsed: 00:21:35
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 30496 steps/s (collection: 2.955s, learning 0.268s)
             Mean action noise std: 2.30
          Mean value_function loss: 137.8922
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.4713
                       Mean reward: 92.78
               Mean episode length: 126.57
    Episode_Reward/reaching_object: 0.6462
     Episode_Reward/lifting_object: 17.8553
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 3.22s
                      Time elapsed: 00:21:38
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 32365 steps/s (collection: 2.776s, learning 0.261s)
             Mean action noise std: 2.30
          Mean value_function loss: 157.4617
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.4764
                       Mean reward: 90.68
               Mean episode length: 124.26
    Episode_Reward/reaching_object: 0.6089
     Episode_Reward/lifting_object: 17.2731
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 3.04s
                      Time elapsed: 00:21:41
                               ETA: 01:04:15

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 34603 steps/s (collection: 2.620s, learning 0.221s)
             Mean action noise std: 2.30
          Mean value_function loss: 162.4940
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.4804
                       Mean reward: 85.94
               Mean episode length: 118.86
    Episode_Reward/reaching_object: 0.6200
     Episode_Reward/lifting_object: 17.8311
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.84s
                      Time elapsed: 00:21:44
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 33480 steps/s (collection: 2.757s, learning 0.179s)
             Mean action noise std: 2.30
          Mean value_function loss: 171.1646
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.4863
                       Mean reward: 95.15
               Mean episode length: 116.71
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 18.8633
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.94s
                      Time elapsed: 00:21:47
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 37442 steps/s (collection: 2.488s, learning 0.137s)
             Mean action noise std: 2.30
          Mean value_function loss: 170.4175
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.4923
                       Mean reward: 94.40
               Mean episode length: 115.53
    Episode_Reward/reaching_object: 0.6192
     Episode_Reward/lifting_object: 18.6535
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.63s
                      Time elapsed: 00:21:49
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 38520 steps/s (collection: 2.370s, learning 0.182s)
             Mean action noise std: 2.30
          Mean value_function loss: 187.9329
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.4962
                       Mean reward: 103.38
               Mean episode length: 123.07
    Episode_Reward/reaching_object: 0.6318
     Episode_Reward/lifting_object: 19.1559
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.55s
                      Time elapsed: 00:21:52
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 35225 steps/s (collection: 2.593s, learning 0.198s)
             Mean action noise std: 2.30
          Mean value_function loss: 149.9430
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.4990
                       Mean reward: 68.65
               Mean episode length: 128.41
    Episode_Reward/reaching_object: 0.6297
     Episode_Reward/lifting_object: 18.3276
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.79s
                      Time elapsed: 00:21:55
                               ETA: 01:04:04

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 38132 steps/s (collection: 2.463s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 200.5450
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.5018
                       Mean reward: 110.49
               Mean episode length: 119.17
    Episode_Reward/reaching_object: 0.6606
     Episode_Reward/lifting_object: 21.2248
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.58s
                      Time elapsed: 00:21:57
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 37876 steps/s (collection: 2.431s, learning 0.164s)
             Mean action noise std: 2.30
          Mean value_function loss: 160.4044
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.5065
                       Mean reward: 101.37
               Mean episode length: 125.40
    Episode_Reward/reaching_object: 0.6276
     Episode_Reward/lifting_object: 19.2971
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.60s
                      Time elapsed: 00:22:00
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 38443 steps/s (collection: 2.432s, learning 0.126s)
             Mean action noise std: 2.30
          Mean value_function loss: 270.8919
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.5102
                       Mean reward: 93.26
               Mean episode length: 118.11
    Episode_Reward/reaching_object: 0.6311
     Episode_Reward/lifting_object: 19.5525
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.56s
                      Time elapsed: 00:22:02
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 38581 steps/s (collection: 2.395s, learning 0.153s)
             Mean action noise std: 2.30
          Mean value_function loss: 317.4507
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.5128
                       Mean reward: 91.87
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.6316
     Episode_Reward/lifting_object: 18.1237
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.55s
                      Time elapsed: 00:22:05
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 37109 steps/s (collection: 2.499s, learning 0.150s)
             Mean action noise std: 2.31
          Mean value_function loss: 158.8586
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 58.5173
                       Mean reward: 104.20
               Mean episode length: 119.50
    Episode_Reward/reaching_object: 0.6247
     Episode_Reward/lifting_object: 20.5125
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.65s
                      Time elapsed: 00:22:08
                               ETA: 01:03:52

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 35597 steps/s (collection: 2.509s, learning 0.253s)
             Mean action noise std: 2.31
          Mean value_function loss: 166.9370
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.5217
                       Mean reward: 111.64
               Mean episode length: 127.44
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 19.2953
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.76s
                      Time elapsed: 00:22:10
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 37503 steps/s (collection: 2.446s, learning 0.176s)
             Mean action noise std: 2.31
          Mean value_function loss: 180.8005
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.5264
                       Mean reward: 114.14
               Mean episode length: 117.11
    Episode_Reward/reaching_object: 0.6293
     Episode_Reward/lifting_object: 20.4595
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.62s
                      Time elapsed: 00:22:13
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 37587 steps/s (collection: 2.470s, learning 0.145s)
             Mean action noise std: 2.31
          Mean value_function loss: 176.5612
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.5302
                       Mean reward: 110.39
               Mean episode length: 120.36
    Episode_Reward/reaching_object: 0.6238
     Episode_Reward/lifting_object: 20.2062
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 33.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.62s
                      Time elapsed: 00:22:16
                               ETA: 01:03:45

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 38094 steps/s (collection: 2.413s, learning 0.167s)
             Mean action noise std: 2.31
          Mean value_function loss: 191.0609
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.5321
                       Mean reward: 91.84
               Mean episode length: 117.43
    Episode_Reward/reaching_object: 0.6222
     Episode_Reward/lifting_object: 20.4063
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.58s
                      Time elapsed: 00:22:18
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 37301 steps/s (collection: 2.463s, learning 0.173s)
             Mean action noise std: 2.31
          Mean value_function loss: 180.0927
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 58.5339
                       Mean reward: 116.14
               Mean episode length: 115.73
    Episode_Reward/reaching_object: 0.6197
     Episode_Reward/lifting_object: 20.8924
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.64s
                      Time elapsed: 00:22:21
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 36760 steps/s (collection: 2.483s, learning 0.191s)
             Mean action noise std: 2.31
          Mean value_function loss: 168.6389
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 58.5354
                       Mean reward: 107.57
               Mean episode length: 112.99
    Episode_Reward/reaching_object: 0.6230
     Episode_Reward/lifting_object: 21.1193
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.67s
                      Time elapsed: 00:22:23
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 37402 steps/s (collection: 2.453s, learning 0.175s)
             Mean action noise std: 2.31
          Mean value_function loss: 229.7887
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 58.5358
                       Mean reward: 108.82
               Mean episode length: 119.19
    Episode_Reward/reaching_object: 0.6203
     Episode_Reward/lifting_object: 21.3974
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.63s
                      Time elapsed: 00:22:26
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 38202 steps/s (collection: 2.393s, learning 0.180s)
             Mean action noise std: 2.31
          Mean value_function loss: 206.4489
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.5363
                       Mean reward: 118.40
               Mean episode length: 117.53
    Episode_Reward/reaching_object: 0.6277
     Episode_Reward/lifting_object: 21.9087
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.57s
                      Time elapsed: 00:22:29
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 35832 steps/s (collection: 2.533s, learning 0.211s)
             Mean action noise std: 2.31
          Mean value_function loss: 183.1823
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.5386
                       Mean reward: 117.18
               Mean episode length: 124.38
    Episode_Reward/reaching_object: 0.6144
     Episode_Reward/lifting_object: 21.4026
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.74s
                      Time elapsed: 00:22:31
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 35601 steps/s (collection: 2.585s, learning 0.176s)
             Mean action noise std: 2.31
          Mean value_function loss: 233.3557
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.5409
                       Mean reward: 108.92
               Mean episode length: 104.91
    Episode_Reward/reaching_object: 0.5946
     Episode_Reward/lifting_object: 21.5676
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.76s
                      Time elapsed: 00:22:34
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 38067 steps/s (collection: 2.430s, learning 0.153s)
             Mean action noise std: 2.31
          Mean value_function loss: 200.5196
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.5428
                       Mean reward: 102.85
               Mean episode length: 121.93
    Episode_Reward/reaching_object: 0.6232
     Episode_Reward/lifting_object: 21.2632
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.58s
                      Time elapsed: 00:22:37
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 35159 steps/s (collection: 2.632s, learning 0.164s)
             Mean action noise std: 2.31
          Mean value_function loss: 223.0882
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.5455
                       Mean reward: 112.64
               Mean episode length: 122.98
    Episode_Reward/reaching_object: 0.6417
     Episode_Reward/lifting_object: 22.7879
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.80s
                      Time elapsed: 00:22:40
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 36941 steps/s (collection: 2.519s, learning 0.142s)
             Mean action noise std: 2.31
          Mean value_function loss: 236.4692
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.5496
                       Mean reward: 118.20
               Mean episode length: 122.94
    Episode_Reward/reaching_object: 0.6324
     Episode_Reward/lifting_object: 22.4324
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.66s
                      Time elapsed: 00:22:42
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 37106 steps/s (collection: 2.493s, learning 0.156s)
             Mean action noise std: 2.31
          Mean value_function loss: 210.7087
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 58.5524
                       Mean reward: 113.76
               Mean episode length: 114.47
    Episode_Reward/reaching_object: 0.6316
     Episode_Reward/lifting_object: 22.8932
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.65s
                      Time elapsed: 00:22:45
                               ETA: 01:03:19

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 38548 steps/s (collection: 2.397s, learning 0.153s)
             Mean action noise std: 2.31
          Mean value_function loss: 212.9914
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.5544
                       Mean reward: 122.37
               Mean episode length: 117.66
    Episode_Reward/reaching_object: 0.6083
     Episode_Reward/lifting_object: 22.8008
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.55s
                      Time elapsed: 00:22:47
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 39411 steps/s (collection: 2.391s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 261.8334
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.5579
                       Mean reward: 129.07
               Mean episode length: 117.63
    Episode_Reward/reaching_object: 0.6110
     Episode_Reward/lifting_object: 23.5411
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.49s
                      Time elapsed: 00:22:50
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 36092 steps/s (collection: 2.560s, learning 0.164s)
             Mean action noise std: 2.31
          Mean value_function loss: 231.9377
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.5598
                       Mean reward: 117.56
               Mean episode length: 108.08
    Episode_Reward/reaching_object: 0.5932
     Episode_Reward/lifting_object: 22.5271
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.72s
                      Time elapsed: 00:22:53
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 35993 steps/s (collection: 2.535s, learning 0.196s)
             Mean action noise std: 2.31
          Mean value_function loss: 281.8975
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.5606
                       Mean reward: 114.66
               Mean episode length: 106.75
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 23.4969
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.73s
                      Time elapsed: 00:22:55
                               ETA: 01:03:09

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 35944 steps/s (collection: 2.560s, learning 0.175s)
             Mean action noise std: 2.31
          Mean value_function loss: 227.0896
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.5622
                       Mean reward: 129.15
               Mean episode length: 108.66
    Episode_Reward/reaching_object: 0.6120
     Episode_Reward/lifting_object: 24.6896
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.73s
                      Time elapsed: 00:22:58
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 39745 steps/s (collection: 2.326s, learning 0.147s)
             Mean action noise std: 2.31
          Mean value_function loss: 239.9482
               Mean surrogate loss: 0.0144
                 Mean entropy loss: 58.5635
                       Mean reward: 144.66
               Mean episode length: 116.80
    Episode_Reward/reaching_object: 0.6091
     Episode_Reward/lifting_object: 25.5135
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.47s
                      Time elapsed: 00:23:01
                               ETA: 01:03:04

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 37520 steps/s (collection: 2.440s, learning 0.180s)
             Mean action noise std: 2.31
          Mean value_function loss: 269.0849
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 58.5638
                       Mean reward: 119.08
               Mean episode length: 98.16
    Episode_Reward/reaching_object: 0.5899
     Episode_Reward/lifting_object: 24.2512
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.62s
                      Time elapsed: 00:23:03
                               ETA: 01:03:01

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 40116 steps/s (collection: 2.342s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 226.6365
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.5643
                       Mean reward: 124.43
               Mean episode length: 110.79
    Episode_Reward/reaching_object: 0.5926
     Episode_Reward/lifting_object: 24.4519
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.45s
                      Time elapsed: 00:23:06
                               ETA: 01:02:58

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 38531 steps/s (collection: 2.392s, learning 0.159s)
             Mean action noise std: 2.31
          Mean value_function loss: 224.5563
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 58.5649
                       Mean reward: 116.95
               Mean episode length: 101.97
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 24.9974
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.55s
                      Time elapsed: 00:23:08
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 37711 steps/s (collection: 2.430s, learning 0.177s)
             Mean action noise std: 2.31
          Mean value_function loss: 242.9146
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.5656
                       Mean reward: 134.58
               Mean episode length: 111.68
    Episode_Reward/reaching_object: 0.6145
     Episode_Reward/lifting_object: 25.8107
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.61s
                      Time elapsed: 00:23:11
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 38876 steps/s (collection: 2.377s, learning 0.152s)
             Mean action noise std: 2.31
          Mean value_function loss: 236.6658
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.5674
                       Mean reward: 116.80
               Mean episode length: 108.23
    Episode_Reward/reaching_object: 0.5875
     Episode_Reward/lifting_object: 24.8902
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.53s
                      Time elapsed: 00:23:13
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 38448 steps/s (collection: 2.397s, learning 0.160s)
             Mean action noise std: 2.31
          Mean value_function loss: 240.5016
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.5689
                       Mean reward: 152.02
               Mean episode length: 114.18
    Episode_Reward/reaching_object: 0.6108
     Episode_Reward/lifting_object: 26.4064
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.56s
                      Time elapsed: 00:23:16
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 38580 steps/s (collection: 2.433s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 226.6359
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.5706
                       Mean reward: 135.62
               Mean episode length: 108.29
    Episode_Reward/reaching_object: 0.6116
     Episode_Reward/lifting_object: 27.1919
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.55s
                      Time elapsed: 00:23:18
                               ETA: 01:02:45

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 36305 steps/s (collection: 2.505s, learning 0.203s)
             Mean action noise std: 2.31
          Mean value_function loss: 266.9018
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 58.5720
                       Mean reward: 138.37
               Mean episode length: 111.77
    Episode_Reward/reaching_object: 0.6167
     Episode_Reward/lifting_object: 26.6696
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.71s
                      Time elapsed: 00:23:21
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 33956 steps/s (collection: 2.697s, learning 0.198s)
             Mean action noise std: 2.31
          Mean value_function loss: 274.8793
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.5735
                       Mean reward: 142.78
               Mean episode length: 110.01
    Episode_Reward/reaching_object: 0.6076
     Episode_Reward/lifting_object: 27.4034
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.89s
                      Time elapsed: 00:23:24
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 35665 steps/s (collection: 2.557s, learning 0.200s)
             Mean action noise std: 2.31
          Mean value_function loss: 249.6816
               Mean surrogate loss: 0.0181
                 Mean entropy loss: 58.5746
                       Mean reward: 139.26
               Mean episode length: 110.55
    Episode_Reward/reaching_object: 0.6111
     Episode_Reward/lifting_object: 26.5685
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.76s
                      Time elapsed: 00:23:27
                               ETA: 01:02:39

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 34534 steps/s (collection: 2.670s, learning 0.177s)
             Mean action noise std: 2.31
          Mean value_function loss: 251.1910
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 58.5749
                       Mean reward: 142.40
               Mean episode length: 113.74
    Episode_Reward/reaching_object: 0.6313
     Episode_Reward/lifting_object: 28.9569
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.85s
                      Time elapsed: 00:23:30
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 36778 steps/s (collection: 2.502s, learning 0.171s)
             Mean action noise std: 2.31
          Mean value_function loss: 251.9704
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 58.5754
                       Mean reward: 128.38
               Mean episode length: 103.51
    Episode_Reward/reaching_object: 0.6101
     Episode_Reward/lifting_object: 27.6558
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.67s
                      Time elapsed: 00:23:32
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 37315 steps/s (collection: 2.479s, learning 0.156s)
             Mean action noise std: 2.31
          Mean value_function loss: 259.7727
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 58.5759
                       Mean reward: 139.54
               Mean episode length: 102.76
    Episode_Reward/reaching_object: 0.5959
     Episode_Reward/lifting_object: 26.8903
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.63s
                      Time elapsed: 00:23:35
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 36679 steps/s (collection: 2.522s, learning 0.159s)
             Mean action noise std: 2.31
          Mean value_function loss: 266.4297
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 58.5760
                       Mean reward: 154.88
               Mean episode length: 121.32
    Episode_Reward/reaching_object: 0.6296
     Episode_Reward/lifting_object: 29.8739
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.68s
                      Time elapsed: 00:23:38
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 38572 steps/s (collection: 2.354s, learning 0.194s)
             Mean action noise std: 2.31
          Mean value_function loss: 252.0521
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.5761
                       Mean reward: 143.43
               Mean episode length: 111.21
    Episode_Reward/reaching_object: 0.6259
     Episode_Reward/lifting_object: 28.8183
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.55s
                      Time elapsed: 00:23:40
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 39041 steps/s (collection: 2.381s, learning 0.137s)
             Mean action noise std: 2.31
          Mean value_function loss: 265.4577
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 58.5761
                       Mean reward: 150.24
               Mean episode length: 111.96
    Episode_Reward/reaching_object: 0.6191
     Episode_Reward/lifting_object: 28.5808
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.52s
                      Time elapsed: 00:23:43
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 37376 steps/s (collection: 2.494s, learning 0.136s)
             Mean action noise std: 2.31
          Mean value_function loss: 253.3376
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 58.5762
                       Mean reward: 151.28
               Mean episode length: 106.00
    Episode_Reward/reaching_object: 0.6307
     Episode_Reward/lifting_object: 30.1294
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.63s
                      Time elapsed: 00:23:45
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 38601 steps/s (collection: 2.387s, learning 0.160s)
             Mean action noise std: 2.31
          Mean value_function loss: 251.0726
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.5765
                       Mean reward: 147.74
               Mean episode length: 99.22
    Episode_Reward/reaching_object: 0.6110
     Episode_Reward/lifting_object: 30.5143
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.55s
                      Time elapsed: 00:23:48
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 38146 steps/s (collection: 2.429s, learning 0.148s)
             Mean action noise std: 2.31
          Mean value_function loss: 264.9244
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.5769
                       Mean reward: 157.36
               Mean episode length: 123.61
    Episode_Reward/reaching_object: 0.6214
     Episode_Reward/lifting_object: 31.2772
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.58s
                      Time elapsed: 00:23:50
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 39299 steps/s (collection: 2.354s, learning 0.148s)
             Mean action noise std: 2.31
          Mean value_function loss: 243.2294
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 58.5772
                       Mean reward: 136.25
               Mean episode length: 106.36
    Episode_Reward/reaching_object: 0.6253
     Episode_Reward/lifting_object: 31.3172
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.50s
                      Time elapsed: 00:23:53
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 37803 steps/s (collection: 2.444s, learning 0.157s)
             Mean action noise std: 2.31
          Mean value_function loss: 298.3230
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 58.5774
                       Mean reward: 163.90
               Mean episode length: 119.19
    Episode_Reward/reaching_object: 0.6272
     Episode_Reward/lifting_object: 31.7858
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.60s
                      Time elapsed: 00:23:56
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 37962 steps/s (collection: 2.451s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 304.1122
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.5779
                       Mean reward: 172.57
               Mean episode length: 118.43
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 30.4162
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.59s
                      Time elapsed: 00:23:58
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 36478 steps/s (collection: 2.510s, learning 0.185s)
             Mean action noise std: 2.31
          Mean value_function loss: 287.6483
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 58.5790
                       Mean reward: 163.35
               Mean episode length: 110.86
    Episode_Reward/reaching_object: 0.6207
     Episode_Reward/lifting_object: 30.2907
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.69s
                      Time elapsed: 00:24:01
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 36242 steps/s (collection: 2.534s, learning 0.178s)
             Mean action noise std: 2.31
          Mean value_function loss: 285.4485
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.5799
                       Mean reward: 160.94
               Mean episode length: 118.77
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 31.2735
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 35.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.71s
                      Time elapsed: 00:24:04
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 34654 steps/s (collection: 2.677s, learning 0.160s)
             Mean action noise std: 2.31
          Mean value_function loss: 305.5912
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.5804
                       Mean reward: 151.61
               Mean episode length: 115.62
    Episode_Reward/reaching_object: 0.6241
     Episode_Reward/lifting_object: 30.9311
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.84s
                      Time elapsed: 00:24:06
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 35025 steps/s (collection: 2.605s, learning 0.202s)
             Mean action noise std: 2.31
          Mean value_function loss: 288.5686
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.5798
                       Mean reward: 164.09
               Mean episode length: 108.86
    Episode_Reward/reaching_object: 0.6290
     Episode_Reward/lifting_object: 31.6133
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.81s
                      Time elapsed: 00:24:09
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 35122 steps/s (collection: 2.688s, learning 0.111s)
             Mean action noise std: 2.31
          Mean value_function loss: 275.2856
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 58.5802
                       Mean reward: 182.85
               Mean episode length: 123.41
    Episode_Reward/reaching_object: 0.6469
     Episode_Reward/lifting_object: 33.1596
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.80s
                      Time elapsed: 00:24:12
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 38138 steps/s (collection: 2.435s, learning 0.142s)
             Mean action noise std: 2.31
          Mean value_function loss: 296.2668
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.5812
                       Mean reward: 165.56
               Mean episode length: 114.28
    Episode_Reward/reaching_object: 0.6438
     Episode_Reward/lifting_object: 32.6739
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.58s
                      Time elapsed: 00:24:15
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 38854 steps/s (collection: 2.395s, learning 0.135s)
             Mean action noise std: 2.31
          Mean value_function loss: 325.6047
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.5819
                       Mean reward: 183.08
               Mean episode length: 121.54
    Episode_Reward/reaching_object: 0.6510
     Episode_Reward/lifting_object: 33.3829
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.53s
                      Time elapsed: 00:24:17
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 38217 steps/s (collection: 2.413s, learning 0.159s)
             Mean action noise std: 2.31
          Mean value_function loss: 339.3012
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 58.5824
                       Mean reward: 181.94
               Mean episode length: 116.92
    Episode_Reward/reaching_object: 0.6528
     Episode_Reward/lifting_object: 33.3572
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.57s
                      Time elapsed: 00:24:20
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 34990 steps/s (collection: 2.578s, learning 0.232s)
             Mean action noise std: 2.31
          Mean value_function loss: 315.8873
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 58.5825
                       Mean reward: 186.56
               Mean episode length: 112.39
    Episode_Reward/reaching_object: 0.6865
     Episode_Reward/lifting_object: 36.4340
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.81s
                      Time elapsed: 00:24:22
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 37432 steps/s (collection: 2.488s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 367.4030
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.5826
                       Mean reward: 209.26
               Mean episode length: 119.55
    Episode_Reward/reaching_object: 0.6910
     Episode_Reward/lifting_object: 38.7647
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.63s
                      Time elapsed: 00:24:25
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 38611 steps/s (collection: 2.406s, learning 0.140s)
             Mean action noise std: 2.31
          Mean value_function loss: 356.2146
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.5831
                       Mean reward: 221.40
               Mean episode length: 120.10
    Episode_Reward/reaching_object: 0.6951
     Episode_Reward/lifting_object: 41.1310
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.55s
                      Time elapsed: 00:24:28
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 36814 steps/s (collection: 2.493s, learning 0.178s)
             Mean action noise std: 2.31
          Mean value_function loss: 343.1979
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.5830
                       Mean reward: 238.93
               Mean episode length: 127.62
    Episode_Reward/reaching_object: 0.6916
     Episode_Reward/lifting_object: 40.9840
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.67s
                      Time elapsed: 00:24:30
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 38816 steps/s (collection: 2.428s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 382.6836
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.5824
                       Mean reward: 213.34
               Mean episode length: 117.15
    Episode_Reward/reaching_object: 0.6844
     Episode_Reward/lifting_object: 40.2080
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.0417
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.53s
                      Time elapsed: 00:24:33
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 39106 steps/s (collection: 2.351s, learning 0.163s)
             Mean action noise std: 2.31
          Mean value_function loss: 399.9115
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.5822
                       Mean reward: 204.39
               Mean episode length: 117.30
    Episode_Reward/reaching_object: 0.7046
     Episode_Reward/lifting_object: 41.9856
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.51s
                      Time elapsed: 00:24:35
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 35435 steps/s (collection: 2.603s, learning 0.172s)
             Mean action noise std: 2.31
          Mean value_function loss: 359.5750
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.5825
                       Mean reward: 215.88
               Mean episode length: 113.58
    Episode_Reward/reaching_object: 0.7046
     Episode_Reward/lifting_object: 43.4935
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.77s
                      Time elapsed: 00:24:38
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 36980 steps/s (collection: 2.535s, learning 0.123s)
             Mean action noise std: 2.31
          Mean value_function loss: 376.4536
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.5835
                       Mean reward: 216.17
               Mean episode length: 111.78
    Episode_Reward/reaching_object: 0.7042
     Episode_Reward/lifting_object: 41.8869
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.66s
                      Time elapsed: 00:24:41
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 37274 steps/s (collection: 2.512s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 393.4704
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.5833
                       Mean reward: 216.08
               Mean episode length: 120.16
    Episode_Reward/reaching_object: 0.6998
     Episode_Reward/lifting_object: 41.7547
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.64s
                      Time elapsed: 00:24:43
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 39715 steps/s (collection: 2.352s, learning 0.123s)
             Mean action noise std: 2.31
          Mean value_function loss: 374.3554
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 58.5837
                       Mean reward: 219.63
               Mean episode length: 116.71
    Episode_Reward/reaching_object: 0.7048
     Episode_Reward/lifting_object: 43.6727
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.48s
                      Time elapsed: 00:24:46
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 38186 steps/s (collection: 2.451s, learning 0.124s)
             Mean action noise std: 2.31
          Mean value_function loss: 366.5758
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.5849
                       Mean reward: 211.49
               Mean episode length: 119.11
    Episode_Reward/reaching_object: 0.7082
     Episode_Reward/lifting_object: 44.0720
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.57s
                      Time elapsed: 00:24:48
                               ETA: 01:01:23

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 39463 steps/s (collection: 2.374s, learning 0.117s)
             Mean action noise std: 2.31
          Mean value_function loss: 387.1513
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 58.5871
                       Mean reward: 244.84
               Mean episode length: 131.20
    Episode_Reward/reaching_object: 0.7349
     Episode_Reward/lifting_object: 43.7990
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.49s
                      Time elapsed: 00:24:51
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 34146 steps/s (collection: 2.710s, learning 0.169s)
             Mean action noise std: 2.31
          Mean value_function loss: 379.3002
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 58.5883
                       Mean reward: 255.32
               Mean episode length: 127.45
    Episode_Reward/reaching_object: 0.7229
     Episode_Reward/lifting_object: 44.4809
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.88s
                      Time elapsed: 00:24:54
                               ETA: 01:01:18

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 29745 steps/s (collection: 3.147s, learning 0.158s)
             Mean action noise std: 2.31
          Mean value_function loss: 344.1550
               Mean surrogate loss: 0.0174
                 Mean entropy loss: 58.5885
                       Mean reward: 231.81
               Mean episode length: 127.88
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 47.5629
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 3.30s
                      Time elapsed: 00:24:57
                               ETA: 01:01:18

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 35468 steps/s (collection: 2.630s, learning 0.142s)
             Mean action noise std: 2.31
          Mean value_function loss: 323.7617
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.5885
                       Mean reward: 250.51
               Mean episode length: 137.18
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 49.4702
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.77s
                      Time elapsed: 00:25:00
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 33561 steps/s (collection: 2.761s, learning 0.168s)
             Mean action noise std: 2.31
          Mean value_function loss: 349.9333
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 58.5886
                       Mean reward: 296.12
               Mean episode length: 154.03
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 47.9790
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.93s
                      Time elapsed: 00:25:03
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 37720 steps/s (collection: 2.429s, learning 0.178s)
             Mean action noise std: 2.31
          Mean value_function loss: 464.0782
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 58.5886
                       Mean reward: 258.21
               Mean episode length: 136.38
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: 48.0362
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.61s
                      Time elapsed: 00:25:05
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 34758 steps/s (collection: 2.690s, learning 0.139s)
             Mean action noise std: 2.31
          Mean value_function loss: 539.5369
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 58.5887
                       Mean reward: 239.00
               Mean episode length: 138.43
    Episode_Reward/reaching_object: 0.8077
     Episode_Reward/lifting_object: 48.9384
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.83s
                      Time elapsed: 00:25:08
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 37439 steps/s (collection: 2.493s, learning 0.133s)
             Mean action noise std: 2.31
          Mean value_function loss: 367.2408
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 58.5887
                       Mean reward: 304.41
               Mean episode length: 153.29
    Episode_Reward/reaching_object: 0.8190
     Episode_Reward/lifting_object: 50.0717
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.63s
                      Time elapsed: 00:25:11
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 37008 steps/s (collection: 2.477s, learning 0.180s)
             Mean action noise std: 2.31
          Mean value_function loss: 374.0929
               Mean surrogate loss: 0.0157
                 Mean entropy loss: 58.5887
                       Mean reward: 270.18
               Mean episode length: 144.10
    Episode_Reward/reaching_object: 0.8179
     Episode_Reward/lifting_object: 51.2551
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.66s
                      Time elapsed: 00:25:14
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 35324 steps/s (collection: 2.649s, learning 0.134s)
             Mean action noise std: 2.31
          Mean value_function loss: 360.5094
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 58.5887
                       Mean reward: 217.33
               Mean episode length: 115.63
    Episode_Reward/reaching_object: 0.7903
     Episode_Reward/lifting_object: 48.6266
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.78s
                      Time elapsed: 00:25:16
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 37034 steps/s (collection: 2.498s, learning 0.156s)
             Mean action noise std: 2.31
          Mean value_function loss: 376.4510
               Mean surrogate loss: 0.0193
                 Mean entropy loss: 58.5887
                       Mean reward: 267.26
               Mean episode length: 139.03
    Episode_Reward/reaching_object: 0.8067
     Episode_Reward/lifting_object: 50.9954
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.65s
                      Time elapsed: 00:25:19
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 36703 steps/s (collection: 2.525s, learning 0.154s)
             Mean action noise std: 2.31
          Mean value_function loss: 386.1503
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 58.5887
                       Mean reward: 307.47
               Mean episode length: 149.91
    Episode_Reward/reaching_object: 0.8395
     Episode_Reward/lifting_object: 54.8481
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.68s
                      Time elapsed: 00:25:22
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 34098 steps/s (collection: 2.644s, learning 0.239s)
             Mean action noise std: 2.31
          Mean value_function loss: 376.3949
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 58.5888
                       Mean reward: 275.10
               Mean episode length: 139.97
    Episode_Reward/reaching_object: 0.8190
     Episode_Reward/lifting_object: 55.4621
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.88s
                      Time elapsed: 00:25:25
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 36770 steps/s (collection: 2.536s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 418.4097
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.5888
                       Mean reward: 308.71
               Mean episode length: 140.43
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 55.6395
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.67s
                      Time elapsed: 00:25:27
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 38380 steps/s (collection: 2.418s, learning 0.144s)
             Mean action noise std: 2.31
          Mean value_function loss: 388.5789
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.5889
                       Mean reward: 321.50
               Mean episode length: 136.91
    Episode_Reward/reaching_object: 0.8224
     Episode_Reward/lifting_object: 56.4779
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.56s
                      Time elapsed: 00:25:30
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 38666 steps/s (collection: 2.416s, learning 0.127s)
             Mean action noise std: 2.31
          Mean value_function loss: 390.8725
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 58.5889
                       Mean reward: 291.45
               Mean episode length: 137.04
    Episode_Reward/reaching_object: 0.8030
     Episode_Reward/lifting_object: 57.1425
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.54s
                      Time elapsed: 00:25:32
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 35146 steps/s (collection: 2.632s, learning 0.165s)
             Mean action noise std: 2.31
          Mean value_function loss: 367.0657
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 58.5889
                       Mean reward: 248.33
               Mean episode length: 125.46
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 53.3302
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.80s
                      Time elapsed: 00:25:35
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 33967 steps/s (collection: 2.722s, learning 0.172s)
             Mean action noise std: 2.31
          Mean value_function loss: 374.0403
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.5888
                       Mean reward: 283.86
               Mean episode length: 132.17
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 53.7628
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.89s
                      Time elapsed: 00:25:38
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 36617 steps/s (collection: 2.484s, learning 0.201s)
             Mean action noise std: 2.31
          Mean value_function loss: 366.4204
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 58.5882
                       Mean reward: 217.28
               Mean episode length: 115.27
    Episode_Reward/reaching_object: 0.7008
     Episode_Reward/lifting_object: 46.7830
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.68s
                      Time elapsed: 00:25:41
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 38060 steps/s (collection: 2.414s, learning 0.169s)
             Mean action noise std: 2.31
          Mean value_function loss: 400.1565
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 58.5878
                       Mean reward: 194.95
               Mean episode length: 117.02
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: 44.0132
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.58s
                      Time elapsed: 00:25:43
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 38971 steps/s (collection: 2.392s, learning 0.130s)
             Mean action noise std: 2.31
          Mean value_function loss: 431.4318
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 58.5872
                       Mean reward: 209.13
               Mean episode length: 115.00
    Episode_Reward/reaching_object: 0.6769
     Episode_Reward/lifting_object: 43.6915
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.52s
                      Time elapsed: 00:25:46
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 34701 steps/s (collection: 2.646s, learning 0.187s)
             Mean action noise std: 2.31
          Mean value_function loss: 425.7242
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 58.5872
                       Mean reward: 215.99
               Mean episode length: 109.29
    Episode_Reward/reaching_object: 0.6940
     Episode_Reward/lifting_object: 47.1345
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.83s
                      Time elapsed: 00:25:49
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 36097 steps/s (collection: 2.572s, learning 0.151s)
             Mean action noise std: 2.31
          Mean value_function loss: 427.6257
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 58.5873
                       Mean reward: 233.55
               Mean episode length: 117.25
    Episode_Reward/reaching_object: 0.7310
     Episode_Reward/lifting_object: 50.0953
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.72s
                      Time elapsed: 00:25:51
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 35266 steps/s (collection: 2.647s, learning 0.140s)
             Mean action noise std: 2.31
          Mean value_function loss: 431.8052
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.5874
                       Mean reward: 271.19
               Mean episode length: 128.15
    Episode_Reward/reaching_object: 0.7307
     Episode_Reward/lifting_object: 50.8500
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.79s
                      Time elapsed: 00:25:54
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 36179 steps/s (collection: 2.597s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 426.4691
               Mean surrogate loss: 0.0232
                 Mean entropy loss: 58.5875
                       Mean reward: 303.98
               Mean episode length: 143.61
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 54.6916
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.72s
                      Time elapsed: 00:25:57
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 34897 steps/s (collection: 2.606s, learning 0.211s)
             Mean action noise std: 2.31
          Mean value_function loss: 431.7815
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 58.5876
                       Mean reward: 273.33
               Mean episode length: 123.97
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 55.0337
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.82s
                      Time elapsed: 00:26:00
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 31607 steps/s (collection: 2.950s, learning 0.160s)
             Mean action noise std: 2.31
          Mean value_function loss: 445.6761
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 58.5876
                       Mean reward: 306.55
               Mean episode length: 127.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 56.4268
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 3.11s
                      Time elapsed: 00:26:03
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 33779 steps/s (collection: 2.711s, learning 0.199s)
             Mean action noise std: 2.31
          Mean value_function loss: 448.5419
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 58.5877
                       Mean reward: 294.36
               Mean episode length: 124.65
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 58.1212
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.91s
                      Time elapsed: 00:26:06
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 32241 steps/s (collection: 2.858s, learning 0.191s)
             Mean action noise std: 2.31
          Mean value_function loss: 417.4953
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 58.5878
                       Mean reward: 323.76
               Mean episode length: 138.91
    Episode_Reward/reaching_object: 0.7977
     Episode_Reward/lifting_object: 59.1522
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 3.05s
                      Time elapsed: 00:26:09
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 34513 steps/s (collection: 2.556s, learning 0.293s)
             Mean action noise std: 2.31
          Mean value_function loss: 442.2286
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.5886
                       Mean reward: 322.09
               Mean episode length: 135.99
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 56.9795
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.85s
                      Time elapsed: 00:26:12
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 28466 steps/s (collection: 3.234s, learning 0.220s)
             Mean action noise std: 2.31
          Mean value_function loss: 438.6210
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 58.5899
                       Mean reward: 324.73
               Mean episode length: 137.71
    Episode_Reward/reaching_object: 0.8526
     Episode_Reward/lifting_object: 60.9579
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 3.45s
                      Time elapsed: 00:26:15
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 33737 steps/s (collection: 2.763s, learning 0.151s)
             Mean action noise std: 2.31
          Mean value_function loss: 441.6438
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.5902
                       Mean reward: 357.78
               Mean episode length: 145.20
    Episode_Reward/reaching_object: 0.8965
     Episode_Reward/lifting_object: 66.9231
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.91s
                      Time elapsed: 00:26:18
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 35943 steps/s (collection: 2.566s, learning 0.169s)
             Mean action noise std: 2.31
          Mean value_function loss: 435.9458
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 58.5896
                       Mean reward: 364.37
               Mean episode length: 145.96
    Episode_Reward/reaching_object: 0.9339
     Episode_Reward/lifting_object: 71.8337
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.73s
                      Time elapsed: 00:26:21
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 35760 steps/s (collection: 2.584s, learning 0.165s)
             Mean action noise std: 2.31
          Mean value_function loss: 451.1700
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 58.5898
                       Mean reward: 356.55
               Mean episode length: 143.73
    Episode_Reward/reaching_object: 0.9238
     Episode_Reward/lifting_object: 71.3521
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.75s
                      Time elapsed: 00:26:23
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 37243 steps/s (collection: 2.487s, learning 0.152s)
             Mean action noise std: 2.31
          Mean value_function loss: 433.6133
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.5899
                       Mean reward: 368.24
               Mean episode length: 144.95
    Episode_Reward/reaching_object: 0.9402
     Episode_Reward/lifting_object: 72.4778
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.64s
                      Time elapsed: 00:26:26
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 32443 steps/s (collection: 2.803s, learning 0.227s)
             Mean action noise std: 2.31
          Mean value_function loss: 472.0741
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 58.5904
                       Mean reward: 357.44
               Mean episode length: 139.93
    Episode_Reward/reaching_object: 0.9350
     Episode_Reward/lifting_object: 73.0510
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 3.03s
                      Time elapsed: 00:26:29
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 35849 steps/s (collection: 2.550s, learning 0.193s)
             Mean action noise std: 2.31
          Mean value_function loss: 430.8368
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 58.5915
                       Mean reward: 436.39
               Mean episode length: 164.62
    Episode_Reward/reaching_object: 0.9458
     Episode_Reward/lifting_object: 72.7791
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.74s
                      Time elapsed: 00:26:32
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 33707 steps/s (collection: 2.702s, learning 0.215s)
             Mean action noise std: 2.31
          Mean value_function loss: 417.8094
               Mean surrogate loss: 0.0180
                 Mean entropy loss: 58.5920
                       Mean reward: 372.78
               Mean episode length: 152.73
    Episode_Reward/reaching_object: 0.9632
     Episode_Reward/lifting_object: 73.5190
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.92s
                      Time elapsed: 00:26:35
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 29733 steps/s (collection: 3.082s, learning 0.224s)
             Mean action noise std: 2.31
          Mean value_function loss: 471.3158
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.5921
                       Mean reward: 384.59
               Mean episode length: 149.57
    Episode_Reward/reaching_object: 0.9877
     Episode_Reward/lifting_object: 78.8816
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 3.31s
                      Time elapsed: 00:26:38
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 27884 steps/s (collection: 3.357s, learning 0.168s)
             Mean action noise std: 2.31
          Mean value_function loss: 440.5693
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.5920
                       Mean reward: 361.13
               Mean episode length: 139.70
    Episode_Reward/reaching_object: 0.9461
     Episode_Reward/lifting_object: 75.5754
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 3.53s
                      Time elapsed: 00:26:42
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 35077 steps/s (collection: 2.636s, learning 0.167s)
             Mean action noise std: 2.31
          Mean value_function loss: 457.6393
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 58.5920
                       Mean reward: 418.93
               Mean episode length: 151.63
    Episode_Reward/reaching_object: 0.9287
     Episode_Reward/lifting_object: 74.5143
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.80s
                      Time elapsed: 00:26:44
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 33011 steps/s (collection: 2.840s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 450.5136
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 58.5920
                       Mean reward: 339.91
               Mean episode length: 133.75
    Episode_Reward/reaching_object: 0.8916
     Episode_Reward/lifting_object: 69.5753
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.98s
                      Time elapsed: 00:26:47
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 26337 steps/s (collection: 3.479s, learning 0.253s)
             Mean action noise std: 2.31
          Mean value_function loss: 479.5012
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.5922
                       Mean reward: 347.85
               Mean episode length: 135.24
    Episode_Reward/reaching_object: 0.8573
     Episode_Reward/lifting_object: 64.9360
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 3.73s
                      Time elapsed: 00:26:51
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 33374 steps/s (collection: 2.775s, learning 0.170s)
             Mean action noise std: 2.31
          Mean value_function loss: 118110202.4000
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.5929
                       Mean reward: -12512.71
               Mean episode length: 130.94
    Episode_Reward/reaching_object: 0.8647
     Episode_Reward/lifting_object: 64.7481
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.9464
          Episode_Reward/joint_vel: -333.8212
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.95s
                      Time elapsed: 00:26:54
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 30702 steps/s (collection: 3.037s, learning 0.165s)
             Mean action noise std: 2.31
          Mean value_function loss: 444.3628
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.5929
                       Mean reward: 327.75
               Mean episode length: 136.37
    Episode_Reward/reaching_object: 0.8976
     Episode_Reward/lifting_object: 68.7754
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 3.20s
                      Time elapsed: 00:26:57
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 34214 steps/s (collection: 2.757s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 528.8472
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.5927
                       Mean reward: 358.41
               Mean episode length: 133.78
    Episode_Reward/reaching_object: 0.8960
     Episode_Reward/lifting_object: 70.5668
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.87s
                      Time elapsed: 00:27:00
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 36553 steps/s (collection: 2.509s, learning 0.180s)
             Mean action noise std: 2.31
          Mean value_function loss: 482.7216
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 58.5936
                       Mean reward: 308.17
               Mean episode length: 128.91
    Episode_Reward/reaching_object: 0.8919
     Episode_Reward/lifting_object: 70.1215
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.69s
                      Time elapsed: 00:27:03
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 38323 steps/s (collection: 2.421s, learning 0.144s)
             Mean action noise std: 2.31
          Mean value_function loss: 456.2830
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 58.5939
                       Mean reward: 360.49
               Mean episode length: 140.11
    Episode_Reward/reaching_object: 0.9035
     Episode_Reward/lifting_object: 71.6161
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.57s
                      Time elapsed: 00:27:05
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 36484 steps/s (collection: 2.574s, learning 0.121s)
             Mean action noise std: 2.31
          Mean value_function loss: 431.7463
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 58.5939
                       Mean reward: 412.54
               Mean episode length: 148.44
    Episode_Reward/reaching_object: 0.9314
     Episode_Reward/lifting_object: 73.2922
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.69s
                      Time elapsed: 00:27:08
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 37145 steps/s (collection: 2.479s, learning 0.167s)
             Mean action noise std: 2.31
          Mean value_function loss: 430.8648
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.5940
                       Mean reward: 323.34
               Mean episode length: 137.33
    Episode_Reward/reaching_object: 0.9688
     Episode_Reward/lifting_object: 76.9551
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.65s
                      Time elapsed: 00:27:11
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 35618 steps/s (collection: 2.615s, learning 0.145s)
             Mean action noise std: 2.31
          Mean value_function loss: 416.6220
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 58.5940
                       Mean reward: 439.97
               Mean episode length: 159.45
    Episode_Reward/reaching_object: 1.0834
     Episode_Reward/lifting_object: 86.8776
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.76s
                      Time elapsed: 00:27:13
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 30917 steps/s (collection: 2.996s, learning 0.184s)
             Mean action noise std: 2.31
          Mean value_function loss: 433.7821
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.5940
                       Mean reward: 447.59
               Mean episode length: 161.84
    Episode_Reward/reaching_object: 1.1384
     Episode_Reward/lifting_object: 92.8570
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 3.18s
                      Time elapsed: 00:27:17
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 35237 steps/s (collection: 2.520s, learning 0.270s)
             Mean action noise std: 2.31
          Mean value_function loss: 439.3843
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.5935
                       Mean reward: 542.67
               Mean episode length: 189.55
    Episode_Reward/reaching_object: 1.1478
     Episode_Reward/lifting_object: 93.0079
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.79s
                      Time elapsed: 00:27:19
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 29595 steps/s (collection: 3.123s, learning 0.199s)
             Mean action noise std: 2.31
          Mean value_function loss: 518.1295
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 58.5929
                       Mean reward: 511.58
               Mean episode length: 184.02
    Episode_Reward/reaching_object: 1.1334
     Episode_Reward/lifting_object: 91.2655
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 3.32s
                      Time elapsed: 00:27:23
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 36124 steps/s (collection: 2.555s, learning 0.167s)
             Mean action noise std: 2.31
          Mean value_function loss: 428.2188
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 58.5934
                       Mean reward: 425.16
               Mean episode length: 168.50
    Episode_Reward/reaching_object: 1.1146
     Episode_Reward/lifting_object: 87.6666
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.72s
                      Time elapsed: 00:27:26
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 36561 steps/s (collection: 2.518s, learning 0.171s)
             Mean action noise std: 2.31
          Mean value_function loss: 438.6420
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 58.5937
                       Mean reward: 513.53
               Mean episode length: 185.52
    Episode_Reward/reaching_object: 1.2090
     Episode_Reward/lifting_object: 97.1972
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.69s
                      Time elapsed: 00:27:28
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 36235 steps/s (collection: 2.499s, learning 0.214s)
             Mean action noise std: 2.31
          Mean value_function loss: 448.1952
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.5947
                       Mean reward: 441.55
               Mean episode length: 162.87
    Episode_Reward/reaching_object: 1.1580
     Episode_Reward/lifting_object: 92.4141
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.71s
                      Time elapsed: 00:27:31
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 31522 steps/s (collection: 2.946s, learning 0.173s)
             Mean action noise std: 2.31
          Mean value_function loss: 498.6086
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 58.5952
                       Mean reward: 460.51
               Mean episode length: 168.36
    Episode_Reward/reaching_object: 1.1753
     Episode_Reward/lifting_object: 92.7918
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 3.12s
                      Time elapsed: 00:27:34
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 29264 steps/s (collection: 3.122s, learning 0.237s)
             Mean action noise std: 2.31
          Mean value_function loss: 437.0604
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.5953
                       Mean reward: 436.79
               Mean episode length: 171.32
    Episode_Reward/reaching_object: 1.1821
     Episode_Reward/lifting_object: 92.2516
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 3.36s
                      Time elapsed: 00:27:37
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 28042 steps/s (collection: 3.251s, learning 0.254s)
             Mean action noise std: 2.31
          Mean value_function loss: 487.0435
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.5956
                       Mean reward: 420.65
               Mean episode length: 160.21
    Episode_Reward/reaching_object: 1.0592
     Episode_Reward/lifting_object: 80.8131
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 3.51s
                      Time elapsed: 00:27:41
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 36217 steps/s (collection: 2.607s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 469.5581
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 58.5954
                       Mean reward: 445.77
               Mean episode length: 169.44
    Episode_Reward/reaching_object: 1.1330
     Episode_Reward/lifting_object: 88.4456
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.71s
                      Time elapsed: 00:27:44
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 38763 steps/s (collection: 2.397s, learning 0.139s)
             Mean action noise std: 2.31
          Mean value_function loss: 452.6304
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 58.5955
                       Mean reward: 517.70
               Mean episode length: 182.97
    Episode_Reward/reaching_object: 1.1668
     Episode_Reward/lifting_object: 92.2057
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.54s
                      Time elapsed: 00:27:46
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 38626 steps/s (collection: 2.411s, learning 0.134s)
             Mean action noise std: 2.31
          Mean value_function loss: 504.0008
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.5958
                       Mean reward: 481.18
               Mean episode length: 180.68
    Episode_Reward/reaching_object: 1.1171
     Episode_Reward/lifting_object: 87.6996
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.54s
                      Time elapsed: 00:27:49
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 36201 steps/s (collection: 2.613s, learning 0.102s)
             Mean action noise std: 2.31
          Mean value_function loss: 458.8282
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 58.5965
                       Mean reward: 414.17
               Mean episode length: 149.20
    Episode_Reward/reaching_object: 1.1284
     Episode_Reward/lifting_object: 90.1834
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.72s
                      Time elapsed: 00:27:51
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 39698 steps/s (collection: 2.358s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 487.8409
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 58.5966
                       Mean reward: 483.91
               Mean episode length: 170.52
    Episode_Reward/reaching_object: 1.1760
     Episode_Reward/lifting_object: 95.7035
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.48s
                      Time elapsed: 00:27:54
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 39915 steps/s (collection: 2.358s, learning 0.105s)
             Mean action noise std: 2.31
          Mean value_function loss: 582.5609
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 58.5966
                       Mean reward: 459.07
               Mean episode length: 174.12
    Episode_Reward/reaching_object: 1.1940
     Episode_Reward/lifting_object: 96.3622
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.46s
                      Time elapsed: 00:27:56
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 40158 steps/s (collection: 2.329s, learning 0.119s)
             Mean action noise std: 2.31
          Mean value_function loss: 438.9872
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 58.5967
                       Mean reward: 550.30
               Mean episode length: 175.40
    Episode_Reward/reaching_object: 1.2440
     Episode_Reward/lifting_object: 104.0905
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.45s
                      Time elapsed: 00:27:59
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 39581 steps/s (collection: 2.383s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 435.2254
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.5970
                       Mean reward: 536.63
               Mean episode length: 174.85
    Episode_Reward/reaching_object: 1.1956
     Episode_Reward/lifting_object: 100.5147
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.48s
                      Time elapsed: 00:28:01
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 39913 steps/s (collection: 2.339s, learning 0.124s)
             Mean action noise std: 2.31
          Mean value_function loss: 441.6946
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 58.5971
                       Mean reward: 545.48
               Mean episode length: 179.22
    Episode_Reward/reaching_object: 1.1816
     Episode_Reward/lifting_object: 99.9691
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.46s
                      Time elapsed: 00:28:04
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 38510 steps/s (collection: 2.437s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 458.0765
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.5972
                       Mean reward: 517.61
               Mean episode length: 172.69
    Episode_Reward/reaching_object: 1.1444
     Episode_Reward/lifting_object: 96.1164
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.55s
                      Time elapsed: 00:28:06
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 39011 steps/s (collection: 2.397s, learning 0.123s)
             Mean action noise std: 2.31
          Mean value_function loss: 471.7036
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 58.5972
                       Mean reward: 539.16
               Mean episode length: 177.76
    Episode_Reward/reaching_object: 1.1721
     Episode_Reward/lifting_object: 100.2477
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.52s
                      Time elapsed: 00:28:09
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 37299 steps/s (collection: 2.497s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 475.4833
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 58.5972
                       Mean reward: 504.25
               Mean episode length: 165.24
    Episode_Reward/reaching_object: 1.1314
     Episode_Reward/lifting_object: 95.6023
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.64s
                      Time elapsed: 00:28:11
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 38562 steps/s (collection: 2.441s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 497.5418
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 58.5972
                       Mean reward: 458.90
               Mean episode length: 153.22
    Episode_Reward/reaching_object: 1.0943
     Episode_Reward/lifting_object: 93.1063
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.55s
                      Time elapsed: 00:28:14
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 38529 steps/s (collection: 2.441s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 483.9645
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 58.5972
                       Mean reward: 564.09
               Mean episode length: 175.76
    Episode_Reward/reaching_object: 1.0973
     Episode_Reward/lifting_object: 93.9568
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.55s
                      Time elapsed: 00:28:17
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 39187 steps/s (collection: 2.401s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 492.6684
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 58.5972
                       Mean reward: 470.09
               Mean episode length: 157.36
    Episode_Reward/reaching_object: 1.0823
     Episode_Reward/lifting_object: 91.9766
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.51s
                      Time elapsed: 00:28:19
                               ETA: 00:58:44

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 39519 steps/s (collection: 2.380s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 487.7992
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 58.5973
                       Mean reward: 427.63
               Mean episode length: 148.52
    Episode_Reward/reaching_object: 1.0729
     Episode_Reward/lifting_object: 91.4764
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.49s
                      Time elapsed: 00:28:22
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 39201 steps/s (collection: 2.410s, learning 0.098s)
             Mean action noise std: 2.31
          Mean value_function loss: 455.3483
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.5979
                       Mean reward: 489.94
               Mean episode length: 165.85
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: 88.0600
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.51s
                      Time elapsed: 00:28:24
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 39727 steps/s (collection: 2.332s, learning 0.143s)
             Mean action noise std: 2.31
          Mean value_function loss: 474.1663
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.5983
                       Mean reward: 410.49
               Mean episode length: 141.44
    Episode_Reward/reaching_object: 1.0854
     Episode_Reward/lifting_object: 91.8949
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.47s
                      Time elapsed: 00:28:27
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 38893 steps/s (collection: 2.406s, learning 0.122s)
             Mean action noise std: 2.31
          Mean value_function loss: 508.1153
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.5989
                       Mean reward: 468.52
               Mean episode length: 155.32
    Episode_Reward/reaching_object: 1.0932
     Episode_Reward/lifting_object: 93.3896
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.53s
                      Time elapsed: 00:28:29
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 39302 steps/s (collection: 2.383s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 475.3438
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.5996
                       Mean reward: 484.39
               Mean episode length: 160.56
    Episode_Reward/reaching_object: 1.1094
     Episode_Reward/lifting_object: 95.8023
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.50s
                      Time elapsed: 00:28:32
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 26088 steps/s (collection: 3.522s, learning 0.247s)
             Mean action noise std: 2.32
          Mean value_function loss: 511.7138
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.6005
                       Mean reward: 452.12
               Mean episode length: 152.61
    Episode_Reward/reaching_object: 1.0839
     Episode_Reward/lifting_object: 92.3922
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 3.77s
                      Time elapsed: 00:28:35
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 31634 steps/s (collection: 2.971s, learning 0.136s)
             Mean action noise std: 2.32
          Mean value_function loss: 487.0935
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 58.6016
                       Mean reward: 495.01
               Mean episode length: 163.86
    Episode_Reward/reaching_object: 1.1139
     Episode_Reward/lifting_object: 95.6034
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 3.11s
                      Time elapsed: 00:28:38
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 36883 steps/s (collection: 2.505s, learning 0.161s)
             Mean action noise std: 2.32
          Mean value_function loss: 424.8098
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.6018
                       Mean reward: 459.01
               Mean episode length: 162.91
    Episode_Reward/reaching_object: 1.1714
     Episode_Reward/lifting_object: 98.8376
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.67s
                      Time elapsed: 00:28:41
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 37210 steps/s (collection: 2.464s, learning 0.178s)
             Mean action noise std: 2.32
          Mean value_function loss: 384.9503
               Mean surrogate loss: 0.0157
                 Mean entropy loss: 58.6024
                       Mean reward: 498.48
               Mean episode length: 170.86
    Episode_Reward/reaching_object: 1.2147
     Episode_Reward/lifting_object: 102.5726
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.64s
                      Time elapsed: 00:28:44
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 35338 steps/s (collection: 2.653s, learning 0.129s)
             Mean action noise std: 2.32
          Mean value_function loss: 386.4385
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 58.6026
                       Mean reward: 532.71
               Mean episode length: 174.52
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 106.2844
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.78s
                      Time elapsed: 00:28:47
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 32543 steps/s (collection: 2.854s, learning 0.167s)
             Mean action noise std: 2.32
          Mean value_function loss: 382.6508
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6025
                       Mean reward: 533.15
               Mean episode length: 182.09
    Episode_Reward/reaching_object: 1.3135
     Episode_Reward/lifting_object: 111.0263
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 3.02s
                      Time elapsed: 00:28:50
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 35434 steps/s (collection: 2.663s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 437.2729
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.6023
                       Mean reward: 564.58
               Mean episode length: 189.48
    Episode_Reward/reaching_object: 1.3055
     Episode_Reward/lifting_object: 109.5985
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.77s
                      Time elapsed: 00:28:52
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 30044 steps/s (collection: 3.116s, learning 0.156s)
             Mean action noise std: 2.32
          Mean value_function loss: 404.0821
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 58.6025
                       Mean reward: 535.32
               Mean episode length: 182.80
    Episode_Reward/reaching_object: 1.2948
     Episode_Reward/lifting_object: 107.3355
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 3.27s
                      Time elapsed: 00:28:56
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 33205 steps/s (collection: 2.784s, learning 0.177s)
             Mean action noise std: 2.32
          Mean value_function loss: 429.9468
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6027
                       Mean reward: 523.16
               Mean episode length: 180.54
    Episode_Reward/reaching_object: 1.2979
     Episode_Reward/lifting_object: 106.4266
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.96s
                      Time elapsed: 00:28:59
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 35977 steps/s (collection: 2.524s, learning 0.208s)
             Mean action noise std: 2.32
          Mean value_function loss: 405.1680
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6034
                       Mean reward: 558.49
               Mean episode length: 190.19
    Episode_Reward/reaching_object: 1.3552
     Episode_Reward/lifting_object: 113.6735
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.73s
                      Time elapsed: 00:29:01
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 21419 steps/s (collection: 4.454s, learning 0.135s)
             Mean action noise std: 2.32
          Mean value_function loss: 419.0038
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 58.6048
                       Mean reward: 598.66
               Mean episode length: 191.80
    Episode_Reward/reaching_object: 1.3021
     Episode_Reward/lifting_object: 108.4426
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 4.59s
                      Time elapsed: 00:29:06
                               ETA: 00:58:12

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 12458 steps/s (collection: 7.733s, learning 0.157s)
             Mean action noise std: 2.32
          Mean value_function loss: 407.7675
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.6047
                       Mean reward: 532.03
               Mean episode length: 183.77
    Episode_Reward/reaching_object: 1.3226
     Episode_Reward/lifting_object: 110.6766
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.89s
                      Time elapsed: 00:29:14
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 12863 steps/s (collection: 7.481s, learning 0.161s)
             Mean action noise std: 2.32
          Mean value_function loss: 392.5343
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.6053
                       Mean reward: 522.73
               Mean episode length: 175.88
    Episode_Reward/reaching_object: 1.3094
     Episode_Reward/lifting_object: 108.6303
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.64s
                      Time elapsed: 00:29:21
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 12193 steps/s (collection: 7.823s, learning 0.239s)
             Mean action noise std: 2.32
          Mean value_function loss: 393.2201
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.6066
                       Mean reward: 545.80
               Mean episode length: 183.29
    Episode_Reward/reaching_object: 1.2808
     Episode_Reward/lifting_object: 108.0841
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 8.06s
                      Time elapsed: 00:29:29
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 11643 steps/s (collection: 8.309s, learning 0.134s)
             Mean action noise std: 2.32
          Mean value_function loss: 408.6356
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.6078
                       Mean reward: 589.88
               Mean episode length: 189.67
    Episode_Reward/reaching_object: 1.2884
     Episode_Reward/lifting_object: 109.5801
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 8.44s
                      Time elapsed: 00:29:38
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 12323 steps/s (collection: 7.798s, learning 0.179s)
             Mean action noise std: 2.32
          Mean value_function loss: 413.8693
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 58.6095
                       Mean reward: 572.09
               Mean episode length: 187.54
    Episode_Reward/reaching_object: 1.2926
     Episode_Reward/lifting_object: 110.3684
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.98s
                      Time elapsed: 00:29:46
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 12806 steps/s (collection: 7.553s, learning 0.123s)
             Mean action noise std: 2.32
          Mean value_function loss: 395.3142
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 58.6100
                       Mean reward: 604.80
               Mean episode length: 191.75
    Episode_Reward/reaching_object: 1.3656
     Episode_Reward/lifting_object: 118.7622
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.68s
                      Time elapsed: 00:29:54
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 10418 steps/s (collection: 9.303s, learning 0.133s)
             Mean action noise std: 2.32
          Mean value_function loss: 418.5579
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6105
                       Mean reward: 599.34
               Mean episode length: 193.72
    Episode_Reward/reaching_object: 1.3503
     Episode_Reward/lifting_object: 116.5829
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 9.44s
                      Time elapsed: 00:30:03
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 11111 steps/s (collection: 8.565s, learning 0.282s)
             Mean action noise std: 2.32
          Mean value_function loss: 378.5486
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.6108
                       Mean reward: 630.90
               Mean episode length: 195.37
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 122.8896
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 8.85s
                      Time elapsed: 00:30:12
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 12596 steps/s (collection: 7.453s, learning 0.351s)
             Mean action noise std: 2.32
          Mean value_function loss: 378.3071
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 58.6112
                       Mean reward: 563.47
               Mean episode length: 182.31
    Episode_Reward/reaching_object: 1.3650
     Episode_Reward/lifting_object: 118.0137
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 7.80s
                      Time elapsed: 00:30:20
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 27232 steps/s (collection: 3.343s, learning 0.266s)
             Mean action noise std: 2.32
          Mean value_function loss: 406.3535
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.6115
                       Mean reward: 580.07
               Mean episode length: 186.88
    Episode_Reward/reaching_object: 1.3097
     Episode_Reward/lifting_object: 111.9266
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 3.61s
                      Time elapsed: 00:30:23
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 28023 steps/s (collection: 3.240s, learning 0.268s)
             Mean action noise std: 2.32
          Mean value_function loss: 406.4504
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 58.6115
                       Mean reward: 569.98
               Mean episode length: 183.40
    Episode_Reward/reaching_object: 1.3134
     Episode_Reward/lifting_object: 112.0556
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 3.51s
                      Time elapsed: 00:30:27
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 27629 steps/s (collection: 3.335s, learning 0.223s)
             Mean action noise std: 2.32
          Mean value_function loss: 399.3276
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.6115
                       Mean reward: 595.10
               Mean episode length: 188.74
    Episode_Reward/reaching_object: 1.3575
     Episode_Reward/lifting_object: 117.2714
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 3.56s
                      Time elapsed: 00:30:30
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 23891 steps/s (collection: 3.845s, learning 0.270s)
             Mean action noise std: 2.32
          Mean value_function loss: 397.1024
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.6121
                       Mean reward: 573.66
               Mean episode length: 180.90
    Episode_Reward/reaching_object: 1.3316
     Episode_Reward/lifting_object: 115.1579
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 4.11s
                      Time elapsed: 00:30:34
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 24757 steps/s (collection: 3.736s, learning 0.235s)
             Mean action noise std: 2.32
          Mean value_function loss: 390.4656
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 58.6131
                       Mean reward: 648.88
               Mean episode length: 200.89
    Episode_Reward/reaching_object: 1.3469
     Episode_Reward/lifting_object: 117.2310
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 3.97s
                      Time elapsed: 00:30:38
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 27415 steps/s (collection: 3.323s, learning 0.262s)
             Mean action noise std: 2.32
          Mean value_function loss: 394.8041
               Mean surrogate loss: 0.0151
                 Mean entropy loss: 58.6146
                       Mean reward: 631.80
               Mean episode length: 200.80
    Episode_Reward/reaching_object: 1.4158
     Episode_Reward/lifting_object: 122.7791
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 3.59s
                      Time elapsed: 00:30:42
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 25425 steps/s (collection: 3.612s, learning 0.255s)
             Mean action noise std: 2.32
          Mean value_function loss: 372.6599
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 58.6148
                       Mean reward: 594.67
               Mean episode length: 187.11
    Episode_Reward/reaching_object: 1.3569
     Episode_Reward/lifting_object: 117.8721
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 3.87s
                      Time elapsed: 00:30:46
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 28064 steps/s (collection: 3.240s, learning 0.263s)
             Mean action noise std: 2.32
          Mean value_function loss: 385.3580
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 58.6149
                       Mean reward: 577.57
               Mean episode length: 184.64
    Episode_Reward/reaching_object: 1.3622
     Episode_Reward/lifting_object: 117.7559
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 3.50s
                      Time elapsed: 00:30:49
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 25662 steps/s (collection: 3.524s, learning 0.307s)
             Mean action noise std: 2.32
          Mean value_function loss: 396.8729
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 58.6151
                       Mean reward: 613.26
               Mean episode length: 190.80
    Episode_Reward/reaching_object: 1.4141
     Episode_Reward/lifting_object: 124.3628
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 3.83s
                      Time elapsed: 00:30:53
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 24592 steps/s (collection: 3.661s, learning 0.336s)
             Mean action noise std: 2.32
          Mean value_function loss: 392.1541
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 58.6153
                       Mean reward: 609.96
               Mean episode length: 189.17
    Episode_Reward/reaching_object: 1.4586
     Episode_Reward/lifting_object: 129.2151
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 4.00s
                      Time elapsed: 00:30:57
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 25935 steps/s (collection: 3.574s, learning 0.216s)
             Mean action noise std: 2.32
          Mean value_function loss: 376.0745
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.6154
                       Mean reward: 657.90
               Mean episode length: 199.43
    Episode_Reward/reaching_object: 1.3698
     Episode_Reward/lifting_object: 120.2312
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 3.79s
                      Time elapsed: 00:31:01
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 26552 steps/s (collection: 3.424s, learning 0.278s)
             Mean action noise std: 2.32
          Mean value_function loss: 378.4108
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 58.6155
                       Mean reward: 646.69
               Mean episode length: 196.82
    Episode_Reward/reaching_object: 1.4080
     Episode_Reward/lifting_object: 124.7396
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 3.70s
                      Time elapsed: 00:31:05
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 27277 steps/s (collection: 3.374s, learning 0.230s)
             Mean action noise std: 2.32
          Mean value_function loss: 364.7421
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.6158
                       Mean reward: 630.98
               Mean episode length: 196.30
    Episode_Reward/reaching_object: 1.3988
     Episode_Reward/lifting_object: 123.8268
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 3.60s
                      Time elapsed: 00:31:08
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 22630 steps/s (collection: 3.995s, learning 0.349s)
             Mean action noise std: 2.32
          Mean value_function loss: 389.8918
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.6170
                       Mean reward: 648.03
               Mean episode length: 197.85
    Episode_Reward/reaching_object: 1.3753
     Episode_Reward/lifting_object: 120.6596
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 4.34s
                      Time elapsed: 00:31:13
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 25907 steps/s (collection: 3.604s, learning 0.191s)
             Mean action noise std: 2.32
          Mean value_function loss: 407.3647
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.6185
                       Mean reward: 635.18
               Mean episode length: 193.55
    Episode_Reward/reaching_object: 1.3983
     Episode_Reward/lifting_object: 124.2869
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 3.79s
                      Time elapsed: 00:31:16
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 25322 steps/s (collection: 3.666s, learning 0.217s)
             Mean action noise std: 2.32
          Mean value_function loss: 385.9891
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6194
                       Mean reward: 627.07
               Mean episode length: 194.05
    Episode_Reward/reaching_object: 1.3970
     Episode_Reward/lifting_object: 123.1852
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 3.88s
                      Time elapsed: 00:31:20
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 25870 steps/s (collection: 3.462s, learning 0.337s)
             Mean action noise std: 2.32
          Mean value_function loss: 374.5278
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6210
                       Mean reward: 622.96
               Mean episode length: 191.28
    Episode_Reward/reaching_object: 1.3641
     Episode_Reward/lifting_object: 120.2088
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 3.80s
                      Time elapsed: 00:31:24
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 26078 steps/s (collection: 3.582s, learning 0.187s)
             Mean action noise std: 2.32
          Mean value_function loss: 361.6570
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.6216
                       Mean reward: 608.27
               Mean episode length: 188.21
    Episode_Reward/reaching_object: 1.4277
     Episode_Reward/lifting_object: 127.1964
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 3.77s
                      Time elapsed: 00:31:28
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 28128 steps/s (collection: 3.269s, learning 0.226s)
             Mean action noise std: 2.32
          Mean value_function loss: 408.9970
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 58.6218
                       Mean reward: 603.91
               Mean episode length: 183.30
    Episode_Reward/reaching_object: 1.4396
     Episode_Reward/lifting_object: 129.1264
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 3.49s
                      Time elapsed: 00:31:31
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 24699 steps/s (collection: 3.650s, learning 0.330s)
             Mean action noise std: 2.32
          Mean value_function loss: 377.2893
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 58.6220
                       Mean reward: 697.31
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 1.4595
     Episode_Reward/lifting_object: 130.4859
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 3.98s
                      Time elapsed: 00:31:35
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 24032 steps/s (collection: 3.698s, learning 0.392s)
             Mean action noise std: 2.32
          Mean value_function loss: 390.4473
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 58.6225
                       Mean reward: 549.48
               Mean episode length: 171.36
    Episode_Reward/reaching_object: 1.3830
     Episode_Reward/lifting_object: 122.6322
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 4.09s
                      Time elapsed: 00:31:39
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 22858 steps/s (collection: 4.030s, learning 0.271s)
             Mean action noise std: 2.32
          Mean value_function loss: 370.5692
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 58.6227
                       Mean reward: 619.70
               Mean episode length: 192.81
    Episode_Reward/reaching_object: 1.3721
     Episode_Reward/lifting_object: 121.6179
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 4.30s
                      Time elapsed: 00:31:44
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 21523 steps/s (collection: 4.188s, learning 0.379s)
             Mean action noise std: 2.32
          Mean value_function loss: 374.1926
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.6231
                       Mean reward: 613.07
               Mean episode length: 187.54
    Episode_Reward/reaching_object: 1.3706
     Episode_Reward/lifting_object: 121.1392
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 4.57s
                      Time elapsed: 00:31:48
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 23338 steps/s (collection: 3.917s, learning 0.295s)
             Mean action noise std: 2.32
          Mean value_function loss: 369.8360
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.6247
                       Mean reward: 654.85
               Mean episode length: 199.16
    Episode_Reward/reaching_object: 1.4126
     Episode_Reward/lifting_object: 124.1626
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 4.21s
                      Time elapsed: 00:31:53
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 20694 steps/s (collection: 4.337s, learning 0.414s)
             Mean action noise std: 2.32
          Mean value_function loss: 351.9512
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 58.6259
                       Mean reward: 637.67
               Mean episode length: 191.93
    Episode_Reward/reaching_object: 1.4033
     Episode_Reward/lifting_object: 124.4846
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 4.75s
                      Time elapsed: 00:31:57
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 22682 steps/s (collection: 4.094s, learning 0.240s)
             Mean action noise std: 2.32
          Mean value_function loss: 339.4114
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 58.6263
                       Mean reward: 669.98
               Mean episode length: 204.75
    Episode_Reward/reaching_object: 1.4837
     Episode_Reward/lifting_object: 131.1208
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 4.33s
                      Time elapsed: 00:32:02
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 22676 steps/s (collection: 4.028s, learning 0.307s)
             Mean action noise std: 2.32
          Mean value_function loss: 351.3243
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 58.6263
                       Mean reward: 649.55
               Mean episode length: 203.79
    Episode_Reward/reaching_object: 1.3912
     Episode_Reward/lifting_object: 120.2043
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 4.33s
                      Time elapsed: 00:32:06
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 26768 steps/s (collection: 3.393s, learning 0.280s)
             Mean action noise std: 2.32
          Mean value_function loss: 342.9641
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.6263
                       Mean reward: 647.51
               Mean episode length: 196.51
    Episode_Reward/reaching_object: 1.4371
     Episode_Reward/lifting_object: 125.4215
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 3.67s
                      Time elapsed: 00:32:10
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 23405 steps/s (collection: 3.909s, learning 0.291s)
             Mean action noise std: 2.32
          Mean value_function loss: 340.4075
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.6263
                       Mean reward: 702.13
               Mean episode length: 213.26
    Episode_Reward/reaching_object: 1.4669
     Episode_Reward/lifting_object: 127.8908
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 4.20s
                      Time elapsed: 00:32:14
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 22402 steps/s (collection: 4.072s, learning 0.316s)
             Mean action noise std: 2.32
          Mean value_function loss: 347.6879
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.6260
                       Mean reward: 681.00
               Mean episode length: 205.81
    Episode_Reward/reaching_object: 1.5129
     Episode_Reward/lifting_object: 133.4613
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 4.39s
                      Time elapsed: 00:32:18
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 23203 steps/s (collection: 3.888s, learning 0.348s)
             Mean action noise std: 2.32
          Mean value_function loss: 361.4083
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 58.6260
                       Mean reward: 700.58
               Mean episode length: 210.36
    Episode_Reward/reaching_object: 1.4560
     Episode_Reward/lifting_object: 126.2610
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 4.24s
                      Time elapsed: 00:32:22
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 24654 steps/s (collection: 3.604s, learning 0.383s)
             Mean action noise std: 2.32
          Mean value_function loss: 334.9986
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 58.6261
                       Mean reward: 615.01
               Mean episode length: 187.64
    Episode_Reward/reaching_object: 1.5138
     Episode_Reward/lifting_object: 133.3536
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 3.99s
                      Time elapsed: 00:32:26
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 22593 steps/s (collection: 4.081s, learning 0.270s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.3547
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 58.6262
                       Mean reward: 616.03
               Mean episode length: 190.12
    Episode_Reward/reaching_object: 1.4154
     Episode_Reward/lifting_object: 122.4045
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 4.35s
                      Time elapsed: 00:32:31
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 23026 steps/s (collection: 3.977s, learning 0.293s)
             Mean action noise std: 2.32
          Mean value_function loss: 344.4009
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.6265
                       Mean reward: 652.71
               Mean episode length: 196.57
    Episode_Reward/reaching_object: 1.4552
     Episode_Reward/lifting_object: 129.0300
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 4.27s
                      Time elapsed: 00:32:35
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 24661 steps/s (collection: 3.724s, learning 0.262s)
             Mean action noise std: 2.32
          Mean value_function loss: 343.7706
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 58.6266
                       Mean reward: 674.45
               Mean episode length: 203.02
    Episode_Reward/reaching_object: 1.4314
     Episode_Reward/lifting_object: 127.1351
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 3.99s
                      Time elapsed: 00:32:39
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 26104 steps/s (collection: 3.459s, learning 0.306s)
             Mean action noise std: 2.32
          Mean value_function loss: 438.8594
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.6268
                       Mean reward: 686.88
               Mean episode length: 205.55
    Episode_Reward/reaching_object: 1.4625
     Episode_Reward/lifting_object: 130.6169
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 3.77s
                      Time elapsed: 00:32:43
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 26049 steps/s (collection: 3.606s, learning 0.168s)
             Mean action noise std: 2.32
          Mean value_function loss: 376.2560
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.6288
                       Mean reward: 607.05
               Mean episode length: 187.77
    Episode_Reward/reaching_object: 1.4442
     Episode_Reward/lifting_object: 128.0485
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 3.77s
                      Time elapsed: 00:32:47
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 29561 steps/s (collection: 3.118s, learning 0.208s)
             Mean action noise std: 2.32
          Mean value_function loss: 345.3557
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 58.6314
                       Mean reward: 604.34
               Mean episode length: 182.19
    Episode_Reward/reaching_object: 1.4360
     Episode_Reward/lifting_object: 127.7653
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 3.33s
                      Time elapsed: 00:32:50
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 28145 steps/s (collection: 3.218s, learning 0.274s)
             Mean action noise std: 2.32
          Mean value_function loss: 416.6354
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 58.6325
                       Mean reward: 547.93
               Mean episode length: 170.96
    Episode_Reward/reaching_object: 1.4196
     Episode_Reward/lifting_object: 125.6277
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 3.49s
                      Time elapsed: 00:32:53
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 22582 steps/s (collection: 3.943s, learning 0.411s)
             Mean action noise std: 2.32
          Mean value_function loss: 369.6568
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.6335
                       Mean reward: 651.67
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 1.4572
     Episode_Reward/lifting_object: 129.8205
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 4.35s
                      Time elapsed: 00:32:58
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 22227 steps/s (collection: 4.271s, learning 0.151s)
             Mean action noise std: 2.32
          Mean value_function loss: 371.9652
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.6363
                       Mean reward: 680.39
               Mean episode length: 203.19
    Episode_Reward/reaching_object: 1.4697
     Episode_Reward/lifting_object: 131.1870
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 4.42s
                      Time elapsed: 00:33:02
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 29738 steps/s (collection: 3.096s, learning 0.209s)
             Mean action noise std: 2.32
          Mean value_function loss: 357.9417
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.6379
                       Mean reward: 721.11
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 1.5357
     Episode_Reward/lifting_object: 137.6801
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 3.31s
                      Time elapsed: 00:33:05
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 28061 steps/s (collection: 3.238s, learning 0.266s)
             Mean action noise std: 2.32
          Mean value_function loss: 321.4012
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.6396
                       Mean reward: 695.75
               Mean episode length: 210.81
    Episode_Reward/reaching_object: 1.4840
     Episode_Reward/lifting_object: 132.3042
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 3.50s
                      Time elapsed: 00:33:09
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 27443 steps/s (collection: 3.253s, learning 0.329s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.3979
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.6420
                       Mean reward: 700.06
               Mean episode length: 207.99
    Episode_Reward/reaching_object: 1.5198
     Episode_Reward/lifting_object: 135.9069
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 3.58s
                      Time elapsed: 00:33:13
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 26161 steps/s (collection: 3.444s, learning 0.313s)
             Mean action noise std: 2.32
          Mean value_function loss: 296.5699
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 58.6433
                       Mean reward: 709.43
               Mean episode length: 212.86
    Episode_Reward/reaching_object: 1.5639
     Episode_Reward/lifting_object: 139.5592
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 3.76s
                      Time elapsed: 00:33:16
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 27163 steps/s (collection: 3.437s, learning 0.182s)
             Mean action noise std: 2.32
          Mean value_function loss: 307.0625
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 58.6434
                       Mean reward: 732.70
               Mean episode length: 217.14
    Episode_Reward/reaching_object: 1.5667
     Episode_Reward/lifting_object: 139.7734
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 3.62s
                      Time elapsed: 00:33:20
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 24269 steps/s (collection: 3.786s, learning 0.264s)
             Mean action noise std: 2.32
          Mean value_function loss: 315.1323
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 58.6435
                       Mean reward: 679.01
               Mean episode length: 200.70
    Episode_Reward/reaching_object: 1.4999
     Episode_Reward/lifting_object: 134.5777
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 4.05s
                      Time elapsed: 00:33:24
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 23481 steps/s (collection: 3.956s, learning 0.231s)
             Mean action noise std: 2.32
          Mean value_function loss: 329.6017
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 58.6435
                       Mean reward: 629.35
               Mean episode length: 193.92
    Episode_Reward/reaching_object: 1.5150
     Episode_Reward/lifting_object: 134.2546
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 4.19s
                      Time elapsed: 00:33:28
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 27440 steps/s (collection: 3.342s, learning 0.240s)
             Mean action noise std: 2.32
          Mean value_function loss: 325.7721
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 58.6435
                       Mean reward: 689.68
               Mean episode length: 205.43
    Episode_Reward/reaching_object: 1.5097
     Episode_Reward/lifting_object: 134.0862
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 3.58s
                      Time elapsed: 00:33:32
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 26502 steps/s (collection: 3.425s, learning 0.285s)
             Mean action noise std: 2.32
          Mean value_function loss: 316.1715
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.6436
                       Mean reward: 695.74
               Mean episode length: 206.96
    Episode_Reward/reaching_object: 1.5015
     Episode_Reward/lifting_object: 134.2024
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 3.71s
                      Time elapsed: 00:33:35
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 24876 steps/s (collection: 3.660s, learning 0.292s)
             Mean action noise std: 2.32
          Mean value_function loss: 373.1933
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.6444
                       Mean reward: 704.20
               Mean episode length: 207.06
    Episode_Reward/reaching_object: 1.5267
     Episode_Reward/lifting_object: 136.3734
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 3.95s
                      Time elapsed: 00:33:39
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 26802 steps/s (collection: 3.374s, learning 0.293s)
             Mean action noise std: 2.32
          Mean value_function loss: 366.5204
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.6462
                       Mean reward: 673.61
               Mean episode length: 204.88
    Episode_Reward/reaching_object: 1.5186
     Episode_Reward/lifting_object: 134.4877
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 3.67s
                      Time elapsed: 00:33:43
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 25881 steps/s (collection: 3.518s, learning 0.280s)
             Mean action noise std: 2.32
          Mean value_function loss: 392.1286
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6480
                       Mean reward: 615.84
               Mean episode length: 190.02
    Episode_Reward/reaching_object: 1.4692
     Episode_Reward/lifting_object: 130.1913
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 3.80s
                      Time elapsed: 00:33:47
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 24096 steps/s (collection: 3.746s, learning 0.333s)
             Mean action noise std: 2.32
          Mean value_function loss: 487.5583
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.6497
                       Mean reward: 631.31
               Mean episode length: 197.56
    Episode_Reward/reaching_object: 1.4837
     Episode_Reward/lifting_object: 130.5430
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 4.08s
                      Time elapsed: 00:33:51
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 23313 steps/s (collection: 3.899s, learning 0.317s)
             Mean action noise std: 2.32
          Mean value_function loss: 534.5266
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.6517
                       Mean reward: 649.34
               Mean episode length: 206.43
    Episode_Reward/reaching_object: 1.4745
     Episode_Reward/lifting_object: 128.6291
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 4.22s
                      Time elapsed: 00:33:55
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 25317 steps/s (collection: 3.632s, learning 0.251s)
             Mean action noise std: 2.32
          Mean value_function loss: 420.9879
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.6536
                       Mean reward: 682.06
               Mean episode length: 207.52
    Episode_Reward/reaching_object: 1.4291
     Episode_Reward/lifting_object: 125.1241
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 3.88s
                      Time elapsed: 00:33:59
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 26580 steps/s (collection: 3.438s, learning 0.261s)
             Mean action noise std: 2.32
          Mean value_function loss: 354.9621
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6548
                       Mean reward: 618.14
               Mean episode length: 193.96
    Episode_Reward/reaching_object: 1.4376
     Episode_Reward/lifting_object: 126.3605
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 3.70s
                      Time elapsed: 00:34:03
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 22940 steps/s (collection: 3.976s, learning 0.309s)
             Mean action noise std: 2.32
          Mean value_function loss: 363.6667
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.6563
                       Mean reward: 615.87
               Mean episode length: 191.32
    Episode_Reward/reaching_object: 1.4037
     Episode_Reward/lifting_object: 123.6355
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 4.29s
                      Time elapsed: 00:34:07
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 22844 steps/s (collection: 4.088s, learning 0.215s)
             Mean action noise std: 2.32
          Mean value_function loss: 368.1650
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.6576
                       Mean reward: 602.78
               Mean episode length: 191.50
    Episode_Reward/reaching_object: 1.4129
     Episode_Reward/lifting_object: 124.3830
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 4.30s
                      Time elapsed: 00:34:11
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 22559 steps/s (collection: 4.128s, learning 0.229s)
             Mean action noise std: 2.32
          Mean value_function loss: 364.0256
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.6578
                       Mean reward: 670.06
               Mean episode length: 200.78
    Episode_Reward/reaching_object: 1.4880
     Episode_Reward/lifting_object: 132.0265
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 4.36s
                      Time elapsed: 00:34:16
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 26317 steps/s (collection: 3.450s, learning 0.286s)
             Mean action noise std: 2.32
          Mean value_function loss: 361.9887
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.6586
                       Mean reward: 646.90
               Mean episode length: 197.05
    Episode_Reward/reaching_object: 1.4729
     Episode_Reward/lifting_object: 130.6698
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 3.74s
                      Time elapsed: 00:34:19
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 24010 steps/s (collection: 3.849s, learning 0.245s)
             Mean action noise std: 2.32
          Mean value_function loss: 348.1457
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6603
                       Mean reward: 662.14
               Mean episode length: 201.13
    Episode_Reward/reaching_object: 1.4755
     Episode_Reward/lifting_object: 130.4256
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 4.09s
                      Time elapsed: 00:34:24
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 23039 steps/s (collection: 4.017s, learning 0.250s)
             Mean action noise std: 2.32
          Mean value_function loss: 346.2881
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 58.6623
                       Mean reward: 675.17
               Mean episode length: 207.90
    Episode_Reward/reaching_object: 1.5077
     Episode_Reward/lifting_object: 133.7499
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 4.27s
                      Time elapsed: 00:34:28
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 23298 steps/s (collection: 3.932s, learning 0.287s)
             Mean action noise std: 2.32
          Mean value_function loss: 342.5650
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.6633
                       Mean reward: 647.34
               Mean episode length: 194.65
    Episode_Reward/reaching_object: 1.5360
     Episode_Reward/lifting_object: 138.6812
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 4.22s
                      Time elapsed: 00:34:32
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 27977 steps/s (collection: 3.314s, learning 0.200s)
             Mean action noise std: 2.32
          Mean value_function loss: 311.1531
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.6645
                       Mean reward: 695.96
               Mean episode length: 215.32
    Episode_Reward/reaching_object: 1.5196
     Episode_Reward/lifting_object: 136.0121
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 3.51s
                      Time elapsed: 00:34:36
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 24912 steps/s (collection: 3.673s, learning 0.273s)
             Mean action noise std: 2.32
          Mean value_function loss: 315.6616
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.6676
                       Mean reward: 723.70
               Mean episode length: 215.14
    Episode_Reward/reaching_object: 1.5118
     Episode_Reward/lifting_object: 135.8253
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 3.95s
                      Time elapsed: 00:34:39
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 23592 steps/s (collection: 3.863s, learning 0.304s)
             Mean action noise std: 2.32
          Mean value_function loss: 336.1073
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6702
                       Mean reward: 646.34
               Mean episode length: 194.20
    Episode_Reward/reaching_object: 1.5388
     Episode_Reward/lifting_object: 138.4024
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 4.17s
                      Time elapsed: 00:34:44
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 25944 steps/s (collection: 3.566s, learning 0.223s)
             Mean action noise std: 2.32
          Mean value_function loss: 313.6289
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.6713
                       Mean reward: 729.96
               Mean episode length: 213.97
    Episode_Reward/reaching_object: 1.5096
     Episode_Reward/lifting_object: 134.9637
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 3.79s
                      Time elapsed: 00:34:47
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 23270 steps/s (collection: 3.928s, learning 0.297s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.0377
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 58.6716
                       Mean reward: 687.23
               Mean episode length: 206.98
    Episode_Reward/reaching_object: 1.5153
     Episode_Reward/lifting_object: 136.4085
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 4.22s
                      Time elapsed: 00:34:52
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 22637 steps/s (collection: 4.082s, learning 0.260s)
             Mean action noise std: 2.32
          Mean value_function loss: 302.3594
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 58.6718
                       Mean reward: 714.00
               Mean episode length: 212.38
    Episode_Reward/reaching_object: 1.5696
     Episode_Reward/lifting_object: 142.1020
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 4.34s
                      Time elapsed: 00:34:56
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 22712 steps/s (collection: 3.972s, learning 0.356s)
             Mean action noise std: 2.32
          Mean value_function loss: 334.4610
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.6721
                       Mean reward: 639.38
               Mean episode length: 195.60
    Episode_Reward/reaching_object: 1.5238
     Episode_Reward/lifting_object: 138.2622
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 4.33s
                      Time elapsed: 00:35:00
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 21648 steps/s (collection: 4.241s, learning 0.299s)
             Mean action noise std: 2.32
          Mean value_function loss: 333.4797
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.6729
                       Mean reward: 683.90
               Mean episode length: 203.10
    Episode_Reward/reaching_object: 1.5285
     Episode_Reward/lifting_object: 138.3292
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 4.54s
                      Time elapsed: 00:35:05
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 21698 steps/s (collection: 4.234s, learning 0.296s)
             Mean action noise std: 2.32
          Mean value_function loss: 324.8459
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.6735
                       Mean reward: 741.29
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.5444
     Episode_Reward/lifting_object: 139.9615
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 4.53s
                      Time elapsed: 00:35:09
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 23257 steps/s (collection: 3.947s, learning 0.280s)
             Mean action noise std: 2.32
          Mean value_function loss: 330.3500
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.6744
                       Mean reward: 711.20
               Mean episode length: 212.78
    Episode_Reward/reaching_object: 1.5766
     Episode_Reward/lifting_object: 144.2556
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 4.23s
                      Time elapsed: 00:35:14
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 23831 steps/s (collection: 3.880s, learning 0.245s)
             Mean action noise std: 2.32
          Mean value_function loss: 299.4665
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.6764
                       Mean reward: 737.62
               Mean episode length: 213.87
    Episode_Reward/reaching_object: 1.5385
     Episode_Reward/lifting_object: 140.1852
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 4.12s
                      Time elapsed: 00:35:18
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 32503 steps/s (collection: 2.897s, learning 0.127s)
             Mean action noise std: 2.32
          Mean value_function loss: 324.2256
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 58.6783
                       Mean reward: 725.02
               Mean episode length: 216.33
    Episode_Reward/reaching_object: 1.4938
     Episode_Reward/lifting_object: 134.7573
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 3.02s
                      Time elapsed: 00:35:21
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 31695 steps/s (collection: 2.944s, learning 0.158s)
             Mean action noise std: 2.32
          Mean value_function loss: 325.3676
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.6806
                       Mean reward: 680.66
               Mean episode length: 205.89
    Episode_Reward/reaching_object: 1.4746
     Episode_Reward/lifting_object: 131.5587
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 3.10s
                      Time elapsed: 00:35:24
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 30295 steps/s (collection: 2.978s, learning 0.267s)
             Mean action noise std: 2.32
          Mean value_function loss: 315.2326
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6845
                       Mean reward: 687.14
               Mean episode length: 205.39
    Episode_Reward/reaching_object: 1.5038
     Episode_Reward/lifting_object: 135.3138
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 3.24s
                      Time elapsed: 00:35:27
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 33599 steps/s (collection: 2.778s, learning 0.148s)
             Mean action noise std: 2.33
          Mean value_function loss: 297.9814
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.6888
                       Mean reward: 738.17
               Mean episode length: 220.86
    Episode_Reward/reaching_object: 1.5099
     Episode_Reward/lifting_object: 137.4456
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.93s
                      Time elapsed: 00:35:30
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 31857 steps/s (collection: 2.852s, learning 0.234s)
             Mean action noise std: 2.33
          Mean value_function loss: 279.0562
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.6924
                       Mean reward: 714.61
               Mean episode length: 208.20
    Episode_Reward/reaching_object: 1.5832
     Episode_Reward/lifting_object: 145.5521
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 3.09s
                      Time elapsed: 00:35:33
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 28916 steps/s (collection: 3.129s, learning 0.270s)
             Mean action noise std: 2.33
          Mean value_function loss: 283.8641
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.6960
                       Mean reward: 702.94
               Mean episode length: 205.31
    Episode_Reward/reaching_object: 1.5447
     Episode_Reward/lifting_object: 143.2631
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 3.40s
                      Time elapsed: 00:35:37
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 28704 steps/s (collection: 3.162s, learning 0.263s)
             Mean action noise std: 2.33
          Mean value_function loss: 302.5406
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.6990
                       Mean reward: 668.82
               Mean episode length: 199.98
    Episode_Reward/reaching_object: 1.5053
     Episode_Reward/lifting_object: 138.7493
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 3.42s
                      Time elapsed: 00:35:40
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 30029 steps/s (collection: 2.932s, learning 0.342s)
             Mean action noise std: 2.33
          Mean value_function loss: 257.3055
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.7022
                       Mean reward: 781.39
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 1.5736
     Episode_Reward/lifting_object: 145.6318
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 3.27s
                      Time elapsed: 00:35:43
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 30315 steps/s (collection: 2.963s, learning 0.280s)
             Mean action noise std: 2.33
          Mean value_function loss: 275.6117
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.7046
                       Mean reward: 686.98
               Mean episode length: 201.19
    Episode_Reward/reaching_object: 1.5567
     Episode_Reward/lifting_object: 145.5472
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 3.24s
                      Time elapsed: 00:35:46
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 25185 steps/s (collection: 3.607s, learning 0.297s)
             Mean action noise std: 2.33
          Mean value_function loss: 309.3973
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.7062
                       Mean reward: 696.35
               Mean episode length: 207.61
    Episode_Reward/reaching_object: 1.5517
     Episode_Reward/lifting_object: 143.8417
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 3.90s
                      Time elapsed: 00:35:50
                               ETA: 00:58:24

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 26262 steps/s (collection: 3.478s, learning 0.265s)
             Mean action noise std: 2.33
          Mean value_function loss: 282.6616
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.7079
                       Mean reward: 756.04
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 1.5987
     Episode_Reward/lifting_object: 149.8882
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 3.74s
                      Time elapsed: 00:35:54
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 23538 steps/s (collection: 3.809s, learning 0.367s)
             Mean action noise std: 2.33
          Mean value_function loss: 273.4740
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.7123
                       Mean reward: 769.84
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 1.5276
     Episode_Reward/lifting_object: 142.0786
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 4.18s
                      Time elapsed: 00:35:58
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 27196 steps/s (collection: 3.347s, learning 0.267s)
             Mean action noise std: 2.33
          Mean value_function loss: 265.5242
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.7155
                       Mean reward: 766.51
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 1.6418
     Episode_Reward/lifting_object: 154.2379
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 3.61s
                      Time elapsed: 00:36:02
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 27127 steps/s (collection: 3.458s, learning 0.166s)
             Mean action noise std: 2.33
          Mean value_function loss: 259.8037
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.7186
                       Mean reward: 755.98
               Mean episode length: 214.44
    Episode_Reward/reaching_object: 1.5570
     Episode_Reward/lifting_object: 145.6358
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 3.62s
                      Time elapsed: 00:36:06
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 26291 steps/s (collection: 3.439s, learning 0.300s)
             Mean action noise std: 2.33
          Mean value_function loss: 259.6038
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.7251
                       Mean reward: 743.20
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 1.5932
     Episode_Reward/lifting_object: 151.4221
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 3.74s
                      Time elapsed: 00:36:09
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 22257 steps/s (collection: 4.172s, learning 0.244s)
             Mean action noise std: 2.33
          Mean value_function loss: 264.9359
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.7326
                       Mean reward: 724.02
               Mean episode length: 206.94
    Episode_Reward/reaching_object: 1.5932
     Episode_Reward/lifting_object: 151.4227
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 4.42s
                      Time elapsed: 00:36:14
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 23970 steps/s (collection: 3.856s, learning 0.245s)
             Mean action noise std: 2.33
          Mean value_function loss: 244.6802
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.7387
                       Mean reward: 789.53
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.5941
     Episode_Reward/lifting_object: 151.2308
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 4.10s
                      Time elapsed: 00:36:18
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 24829 steps/s (collection: 3.579s, learning 0.380s)
             Mean action noise std: 2.33
          Mean value_function loss: 259.2010
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.7411
                       Mean reward: 790.68
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 1.6285
     Episode_Reward/lifting_object: 155.5219
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 3.96s
                      Time elapsed: 00:36:22
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 25450 steps/s (collection: 3.567s, learning 0.296s)
             Mean action noise std: 2.33
          Mean value_function loss: 268.8514
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.7438
                       Mean reward: 768.24
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 1.5891
     Episode_Reward/lifting_object: 150.2305
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 3.86s
                      Time elapsed: 00:36:26
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 38010 steps/s (collection: 2.486s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 269.3957
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.7457
                       Mean reward: 737.78
               Mean episode length: 211.28
    Episode_Reward/reaching_object: 1.5825
     Episode_Reward/lifting_object: 150.8638
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.59s
                      Time elapsed: 00:36:28
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 40021 steps/s (collection: 2.349s, learning 0.107s)
             Mean action noise std: 2.33
          Mean value_function loss: 251.8083
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.7475
                       Mean reward: 789.62
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.6268
     Episode_Reward/lifting_object: 155.8035
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.46s
                      Time elapsed: 00:36:31
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 37484 steps/s (collection: 2.475s, learning 0.148s)
             Mean action noise std: 2.33
          Mean value_function loss: 266.6257
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 58.7482
                       Mean reward: 730.99
               Mean episode length: 211.32
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 148.0575
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.62s
                      Time elapsed: 00:36:33
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 38140 steps/s (collection: 2.440s, learning 0.138s)
             Mean action noise std: 2.33
          Mean value_function loss: 230.4280
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.7484
                       Mean reward: 763.14
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 1.5984
     Episode_Reward/lifting_object: 151.8633
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.58s
                      Time elapsed: 00:36:36
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 36783 steps/s (collection: 2.515s, learning 0.158s)
             Mean action noise std: 2.33
          Mean value_function loss: 233.1689
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.7491
                       Mean reward: 788.93
               Mean episode length: 222.60
    Episode_Reward/reaching_object: 1.6069
     Episode_Reward/lifting_object: 153.8699
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.67s
                      Time elapsed: 00:36:39
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 40130 steps/s (collection: 2.337s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 252.8410
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.7521
                       Mean reward: 756.43
               Mean episode length: 213.17
    Episode_Reward/reaching_object: 1.6005
     Episode_Reward/lifting_object: 152.3280
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.45s
                      Time elapsed: 00:36:41
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 41026 steps/s (collection: 2.286s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 283.8125
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.7574
                       Mean reward: 744.80
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.5627
     Episode_Reward/lifting_object: 147.9113
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.40s
                      Time elapsed: 00:36:43
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 39182 steps/s (collection: 2.361s, learning 0.148s)
             Mean action noise std: 2.33
          Mean value_function loss: 300.1124
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 58.7620
                       Mean reward: 681.81
               Mean episode length: 199.82
    Episode_Reward/reaching_object: 1.5505
     Episode_Reward/lifting_object: 146.7334
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.51s
                      Time elapsed: 00:36:46
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 40564 steps/s (collection: 2.316s, learning 0.107s)
             Mean action noise std: 2.33
          Mean value_function loss: 313.3331
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.7659
                       Mean reward: 763.04
               Mean episode length: 217.47
    Episode_Reward/reaching_object: 1.5787
     Episode_Reward/lifting_object: 150.6245
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.42s
                      Time elapsed: 00:36:48
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 40691 steps/s (collection: 2.302s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 302.8188
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.7706
                       Mean reward: 721.05
               Mean episode length: 208.67
    Episode_Reward/reaching_object: 1.5145
     Episode_Reward/lifting_object: 142.8209
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.42s
                      Time elapsed: 00:36:51
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 40970 steps/s (collection: 2.270s, learning 0.130s)
             Mean action noise std: 2.33
          Mean value_function loss: 264.8286
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.7747
                       Mean reward: 772.70
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 1.6161
     Episode_Reward/lifting_object: 154.4054
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.40s
                      Time elapsed: 00:36:53
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 41464 steps/s (collection: 2.249s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 260.1922
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 58.7814
                       Mean reward: 800.17
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 153.7282
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.37s
                      Time elapsed: 00:36:55
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 40433 steps/s (collection: 2.299s, learning 0.132s)
             Mean action noise std: 2.33
          Mean value_function loss: 283.4277
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.7842
                       Mean reward: 738.86
               Mean episode length: 210.34
    Episode_Reward/reaching_object: 1.5402
     Episode_Reward/lifting_object: 146.6505
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.43s
                      Time elapsed: 00:36:58
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 40732 steps/s (collection: 2.282s, learning 0.131s)
             Mean action noise std: 2.34
          Mean value_function loss: 255.4145
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.7912
                       Mean reward: 758.27
               Mean episode length: 218.91
    Episode_Reward/reaching_object: 1.5894
     Episode_Reward/lifting_object: 151.2020
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.41s
                      Time elapsed: 00:37:00
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 38127 steps/s (collection: 2.438s, learning 0.140s)
             Mean action noise std: 2.34
          Mean value_function loss: 300.5350
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.7976
                       Mean reward: 765.10
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 1.5300
     Episode_Reward/lifting_object: 144.9860
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.58s
                      Time elapsed: 00:37:03
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 37373 steps/s (collection: 2.503s, learning 0.128s)
             Mean action noise std: 2.34
          Mean value_function loss: 257.7263
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.8018
                       Mean reward: 714.52
               Mean episode length: 205.41
    Episode_Reward/reaching_object: 1.5240
     Episode_Reward/lifting_object: 143.1195
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.63s
                      Time elapsed: 00:37:06
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 34712 steps/s (collection: 2.659s, learning 0.173s)
             Mean action noise std: 2.34
          Mean value_function loss: 255.5978
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.8039
                       Mean reward: 739.21
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 1.5710
     Episode_Reward/lifting_object: 150.0492
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.83s
                      Time elapsed: 00:37:08
                               ETA: 00:57:18

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 34126 steps/s (collection: 2.658s, learning 0.223s)
             Mean action noise std: 2.34
          Mean value_function loss: 243.2879
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.8061
                       Mean reward: 781.04
               Mean episode length: 223.08
    Episode_Reward/reaching_object: 1.5875
     Episode_Reward/lifting_object: 153.4155
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.88s
                      Time elapsed: 00:37:11
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 30082 steps/s (collection: 3.051s, learning 0.217s)
             Mean action noise std: 2.34
          Mean value_function loss: 256.0758
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.8087
                       Mean reward: 817.70
               Mean episode length: 227.44
    Episode_Reward/reaching_object: 1.6099
     Episode_Reward/lifting_object: 156.5417
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 3.27s
                      Time elapsed: 00:37:15
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 27727 steps/s (collection: 3.191s, learning 0.354s)
             Mean action noise std: 2.34
          Mean value_function loss: 249.8008
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 58.8129
                       Mean reward: 807.36
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 1.6170
     Episode_Reward/lifting_object: 157.7618
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 3.55s
                      Time elapsed: 00:37:18
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 29243 steps/s (collection: 3.156s, learning 0.205s)
             Mean action noise std: 2.34
          Mean value_function loss: 270.9127
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.8145
                       Mean reward: 764.67
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 1.5863
     Episode_Reward/lifting_object: 155.2317
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 3.36s
                      Time elapsed: 00:37:21
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 30423 steps/s (collection: 2.949s, learning 0.282s)
             Mean action noise std: 2.34
          Mean value_function loss: 256.1182
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.8158
                       Mean reward: 778.68
               Mean episode length: 218.60
    Episode_Reward/reaching_object: 1.5520
     Episode_Reward/lifting_object: 151.8523
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 3.23s
                      Time elapsed: 00:37:25
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 26983 steps/s (collection: 3.444s, learning 0.199s)
             Mean action noise std: 2.34
          Mean value_function loss: 247.4749
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.8196
                       Mean reward: 759.90
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 1.5793
     Episode_Reward/lifting_object: 153.9853
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 3.64s
                      Time elapsed: 00:37:28
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 25161 steps/s (collection: 3.569s, learning 0.338s)
             Mean action noise std: 2.34
          Mean value_function loss: 217.0073
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.8252
                       Mean reward: 754.58
               Mean episode length: 219.38
    Episode_Reward/reaching_object: 1.5899
     Episode_Reward/lifting_object: 155.2145
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 3.91s
                      Time elapsed: 00:37:32
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 30737 steps/s (collection: 2.984s, learning 0.214s)
             Mean action noise std: 2.34
          Mean value_function loss: 253.7276
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.8303
                       Mean reward: 784.61
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.5913
     Episode_Reward/lifting_object: 154.5238
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 3.20s
                      Time elapsed: 00:37:35
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 26245 steps/s (collection: 3.454s, learning 0.292s)
             Mean action noise std: 2.34
          Mean value_function loss: 227.1281
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 58.8356
                       Mean reward: 811.56
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.5798
     Episode_Reward/lifting_object: 154.5836
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 3.75s
                      Time elapsed: 00:37:39
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 33262 steps/s (collection: 2.820s, learning 0.135s)
             Mean action noise std: 2.34
          Mean value_function loss: 217.6325
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.8379
                       Mean reward: 780.43
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 1.5863
     Episode_Reward/lifting_object: 154.4233
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.96s
                      Time elapsed: 00:37:42
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 36285 steps/s (collection: 2.537s, learning 0.172s)
             Mean action noise std: 2.34
          Mean value_function loss: 229.9565
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.8412
                       Mean reward: 758.47
               Mean episode length: 214.49
    Episode_Reward/reaching_object: 1.5968
     Episode_Reward/lifting_object: 155.9216
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.71s
                      Time elapsed: 00:37:45
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 38529 steps/s (collection: 2.429s, learning 0.123s)
             Mean action noise std: 2.34
          Mean value_function loss: 260.5191
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.8451
                       Mean reward: 760.40
               Mean episode length: 214.87
    Episode_Reward/reaching_object: 1.6035
     Episode_Reward/lifting_object: 156.7753
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.55s
                      Time elapsed: 00:37:47
                               ETA: 00:56:51

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 38490 steps/s (collection: 2.419s, learning 0.135s)
             Mean action noise std: 2.34
          Mean value_function loss: 361.8724
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 58.8525
                       Mean reward: 779.35
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 1.5671
     Episode_Reward/lifting_object: 153.0181
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.55s
                      Time elapsed: 00:37:50
                               ETA: 00:56:48

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 39399 steps/s (collection: 2.381s, learning 0.114s)
             Mean action noise std: 2.34
          Mean value_function loss: 264.7252
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.8673
                       Mean reward: 798.05
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.5855
     Episode_Reward/lifting_object: 155.0034
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.50s
                      Time elapsed: 00:37:52
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 37445 steps/s (collection: 2.486s, learning 0.139s)
             Mean action noise std: 2.34
          Mean value_function loss: 217.8798
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.8774
                       Mean reward: 851.14
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.6316
     Episode_Reward/lifting_object: 160.5279
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.63s
                      Time elapsed: 00:37:55
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 39264 steps/s (collection: 2.375s, learning 0.129s)
             Mean action noise std: 2.34
          Mean value_function loss: 241.7114
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.8819
                       Mean reward: 810.00
               Mean episode length: 223.80
    Episode_Reward/reaching_object: 1.6224
     Episode_Reward/lifting_object: 159.5731
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.50s
                      Time elapsed: 00:37:58
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 38536 steps/s (collection: 2.414s, learning 0.136s)
             Mean action noise std: 2.34
          Mean value_function loss: 280.8893
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.8879
                       Mean reward: 761.49
               Mean episode length: 212.48
    Episode_Reward/reaching_object: 1.5441
     Episode_Reward/lifting_object: 150.4312
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.55s
                      Time elapsed: 00:38:00
                               ETA: 00:56:35

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 36805 steps/s (collection: 2.548s, learning 0.122s)
             Mean action noise std: 2.35
          Mean value_function loss: 262.6931
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.8916
                       Mean reward: 778.91
               Mean episode length: 216.73
    Episode_Reward/reaching_object: 1.5508
     Episode_Reward/lifting_object: 152.3047
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.67s
                      Time elapsed: 00:38:03
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 38484 steps/s (collection: 2.399s, learning 0.156s)
             Mean action noise std: 2.35
          Mean value_function loss: 298.6779
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.8941
                       Mean reward: 736.18
               Mean episode length: 207.87
    Episode_Reward/reaching_object: 1.5551
     Episode_Reward/lifting_object: 152.6061
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.55s
                      Time elapsed: 00:38:05
                               ETA: 00:56:29

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 38570 steps/s (collection: 2.431s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 318.9459
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.8964
                       Mean reward: 765.89
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 1.5327
     Episode_Reward/lifting_object: 150.5158
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.55s
                      Time elapsed: 00:38:08
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 36679 steps/s (collection: 2.550s, learning 0.130s)
             Mean action noise std: 2.35
          Mean value_function loss: 300.8150
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.9021
                       Mean reward: 772.64
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 148.4954
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.68s
                      Time elapsed: 00:38:11
                               ETA: 00:56:22

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 39381 steps/s (collection: 2.358s, learning 0.138s)
             Mean action noise std: 2.35
          Mean value_function loss: 287.7327
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.9049
                       Mean reward: 760.98
               Mean episode length: 214.24
    Episode_Reward/reaching_object: 1.5318
     Episode_Reward/lifting_object: 150.2878
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.50s
                      Time elapsed: 00:38:13
                               ETA: 00:56:19

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 38522 steps/s (collection: 2.422s, learning 0.130s)
             Mean action noise std: 2.35
          Mean value_function loss: 280.5871
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.9100
                       Mean reward: 804.20
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 1.5938
     Episode_Reward/lifting_object: 156.6167
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.55s
                      Time elapsed: 00:38:16
                               ETA: 00:56:16

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 37458 steps/s (collection: 2.487s, learning 0.137s)
             Mean action noise std: 2.35
          Mean value_function loss: 303.2105
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.9158
                       Mean reward: 712.33
               Mean episode length: 203.49
    Episode_Reward/reaching_object: 1.5620
     Episode_Reward/lifting_object: 153.2923
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.62s
                      Time elapsed: 00:38:18
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 39194 steps/s (collection: 2.391s, learning 0.117s)
             Mean action noise std: 2.35
          Mean value_function loss: 283.7137
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.9184
                       Mean reward: 754.07
               Mean episode length: 210.76
    Episode_Reward/reaching_object: 1.5507
     Episode_Reward/lifting_object: 151.3455
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.51s
                      Time elapsed: 00:38:21
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 37926 steps/s (collection: 2.432s, learning 0.160s)
             Mean action noise std: 2.35
          Mean value_function loss: 224.3836
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 58.9242
                       Mean reward: 785.30
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 1.5824
     Episode_Reward/lifting_object: 155.2354
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.59s
                      Time elapsed: 00:38:23
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 38282 steps/s (collection: 2.445s, learning 0.123s)
             Mean action noise std: 2.35
          Mean value_function loss: 232.4593
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.9354
                       Mean reward: 765.23
               Mean episode length: 214.01
    Episode_Reward/reaching_object: 1.6005
     Episode_Reward/lifting_object: 155.9935
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.57s
                      Time elapsed: 00:38:26
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 36714 steps/s (collection: 2.543s, learning 0.134s)
             Mean action noise std: 2.35
          Mean value_function loss: 207.4834
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.9393
                       Mean reward: 765.37
               Mean episode length: 215.77
    Episode_Reward/reaching_object: 1.6265
     Episode_Reward/lifting_object: 158.6104
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.68s
                      Time elapsed: 00:38:29
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 37891 steps/s (collection: 2.433s, learning 0.161s)
             Mean action noise std: 2.35
          Mean value_function loss: 228.0718
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.9415
                       Mean reward: 771.85
               Mean episode length: 216.50
    Episode_Reward/reaching_object: 1.6223
     Episode_Reward/lifting_object: 158.3612
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.59s
                      Time elapsed: 00:38:31
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 35871 steps/s (collection: 2.626s, learning 0.114s)
             Mean action noise std: 2.35
          Mean value_function loss: 245.3450
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.9461
                       Mean reward: 781.18
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 1.5933
     Episode_Reward/lifting_object: 154.9735
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.74s
                      Time elapsed: 00:38:34
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 35710 steps/s (collection: 2.636s, learning 0.117s)
             Mean action noise std: 2.35
          Mean value_function loss: 238.6131
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.9489
                       Mean reward: 768.36
               Mean episode length: 216.85
    Episode_Reward/reaching_object: 1.6158
     Episode_Reward/lifting_object: 155.9650
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.75s
                      Time elapsed: 00:38:37
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 39838 steps/s (collection: 2.330s, learning 0.137s)
             Mean action noise std: 2.35
          Mean value_function loss: 228.5387
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 58.9496
                       Mean reward: 833.20
               Mean episode length: 230.52
    Episode_Reward/reaching_object: 1.6326
     Episode_Reward/lifting_object: 159.0525
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.47s
                      Time elapsed: 00:38:39
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 38850 steps/s (collection: 2.411s, learning 0.120s)
             Mean action noise std: 2.35
          Mean value_function loss: 221.4007
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.9503
                       Mean reward: 718.43
               Mean episode length: 203.90
    Episode_Reward/reaching_object: 1.6231
     Episode_Reward/lifting_object: 157.8362
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.53s
                      Time elapsed: 00:38:42
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 39338 steps/s (collection: 2.381s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 266.7598
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.9517
                       Mean reward: 811.07
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 1.6322
     Episode_Reward/lifting_object: 158.8379
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.50s
                      Time elapsed: 00:38:44
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 40285 steps/s (collection: 2.322s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 245.7140
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.9556
                       Mean reward: 811.15
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 1.6318
     Episode_Reward/lifting_object: 158.8242
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.44s
                      Time elapsed: 00:38:47
                               ETA: 00:55:37

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 40708 steps/s (collection: 2.301s, learning 0.114s)
             Mean action noise std: 2.35
          Mean value_function loss: 242.5531
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 58.9604
                       Mean reward: 793.88
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.6238
     Episode_Reward/lifting_object: 157.7182
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.41s
                      Time elapsed: 00:38:49
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 39695 steps/s (collection: 2.347s, learning 0.129s)
             Mean action noise std: 2.35
          Mean value_function loss: 270.8688
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.9674
                       Mean reward: 736.70
               Mean episode length: 207.01
    Episode_Reward/reaching_object: 1.5547
     Episode_Reward/lifting_object: 150.1816
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.48s
                      Time elapsed: 00:38:51
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 39772 steps/s (collection: 2.353s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 242.7885
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.9702
                       Mean reward: 819.74
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.6115
     Episode_Reward/lifting_object: 157.0257
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.47s
                      Time elapsed: 00:38:54
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 38832 steps/s (collection: 2.407s, learning 0.125s)
             Mean action noise std: 2.35
          Mean value_function loss: 226.8513
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.9745
                       Mean reward: 783.77
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 1.6062
     Episode_Reward/lifting_object: 156.9431
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.53s
                      Time elapsed: 00:38:56
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 39517 steps/s (collection: 2.366s, learning 0.121s)
             Mean action noise std: 2.35
          Mean value_function loss: 260.5788
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.9790
                       Mean reward: 738.68
               Mean episode length: 209.13
    Episode_Reward/reaching_object: 1.5853
     Episode_Reward/lifting_object: 154.8469
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.49s
                      Time elapsed: 00:38:59
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 36889 steps/s (collection: 2.538s, learning 0.127s)
             Mean action noise std: 2.36
          Mean value_function loss: 249.4897
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.9864
                       Mean reward: 812.76
               Mean episode length: 223.36
    Episode_Reward/reaching_object: 1.6143
     Episode_Reward/lifting_object: 158.2251
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.66s
                      Time elapsed: 00:39:02
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 36921 steps/s (collection: 2.511s, learning 0.152s)
             Mean action noise std: 2.36
          Mean value_function loss: 242.7959
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.9926
                       Mean reward: 778.10
               Mean episode length: 215.16
    Episode_Reward/reaching_object: 1.5851
     Episode_Reward/lifting_object: 155.5642
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.66s
                      Time elapsed: 00:39:04
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 36807 steps/s (collection: 2.541s, learning 0.130s)
             Mean action noise std: 2.36
          Mean value_function loss: 241.7191
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.9987
                       Mean reward: 838.17
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 1.6113
     Episode_Reward/lifting_object: 158.2233
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.67s
                      Time elapsed: 00:39:07
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 36455 steps/s (collection: 2.550s, learning 0.147s)
             Mean action noise std: 2.36
          Mean value_function loss: 257.0976
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.0097
                       Mean reward: 823.09
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 1.5813
     Episode_Reward/lifting_object: 155.7151
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.70s
                      Time elapsed: 00:39:10
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 36943 steps/s (collection: 2.524s, learning 0.137s)
             Mean action noise std: 2.36
          Mean value_function loss: 310.3270
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.0178
                       Mean reward: 768.26
               Mean episode length: 215.15
    Episode_Reward/reaching_object: 1.5182
     Episode_Reward/lifting_object: 148.7055
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.66s
                      Time elapsed: 00:39:12
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 36449 steps/s (collection: 2.573s, learning 0.124s)
             Mean action noise std: 2.36
          Mean value_function loss: 256.8342
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.0247
                       Mean reward: 788.49
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 1.5842
     Episode_Reward/lifting_object: 155.6737
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.70s
                      Time elapsed: 00:39:15
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 37288 steps/s (collection: 2.522s, learning 0.115s)
             Mean action noise std: 2.36
          Mean value_function loss: 283.8070
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.0268
                       Mean reward: 759.55
               Mean episode length: 213.07
    Episode_Reward/reaching_object: 1.5245
     Episode_Reward/lifting_object: 149.2481
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.64s
                      Time elapsed: 00:39:18
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 37730 steps/s (collection: 2.477s, learning 0.128s)
             Mean action noise std: 2.36
          Mean value_function loss: 255.3448
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.0300
                       Mean reward: 728.02
               Mean episode length: 205.63
    Episode_Reward/reaching_object: 1.5547
     Episode_Reward/lifting_object: 152.5441
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.61s
                      Time elapsed: 00:39:20
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 37766 steps/s (collection: 2.486s, learning 0.117s)
             Mean action noise std: 2.36
          Mean value_function loss: 273.3056
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.0335
                       Mean reward: 748.90
               Mean episode length: 212.36
    Episode_Reward/reaching_object: 1.5626
     Episode_Reward/lifting_object: 152.9097
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.60s
                      Time elapsed: 00:39:23
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 37380 steps/s (collection: 2.484s, learning 0.146s)
             Mean action noise std: 2.36
          Mean value_function loss: 252.7881
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.0395
                       Mean reward: 750.49
               Mean episode length: 209.57
    Episode_Reward/reaching_object: 1.5459
     Episode_Reward/lifting_object: 151.9033
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.63s
                      Time elapsed: 00:39:26
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 37392 steps/s (collection: 2.500s, learning 0.129s)
             Mean action noise std: 2.36
          Mean value_function loss: 283.5904
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 59.0484
                       Mean reward: 805.28
               Mean episode length: 220.39
    Episode_Reward/reaching_object: 1.6062
     Episode_Reward/lifting_object: 158.4796
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.63s
                      Time elapsed: 00:39:28
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 36587 steps/s (collection: 2.525s, learning 0.162s)
             Mean action noise std: 2.36
          Mean value_function loss: 294.2006
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.0565
                       Mean reward: 783.79
               Mean episode length: 218.42
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 153.9790
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.69s
                      Time elapsed: 00:39:31
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 38175 steps/s (collection: 2.460s, learning 0.115s)
             Mean action noise std: 2.36
          Mean value_function loss: 278.2986
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.0631
                       Mean reward: 809.96
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.5928
     Episode_Reward/lifting_object: 157.3998
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.58s
                      Time elapsed: 00:39:33
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 36771 steps/s (collection: 2.555s, learning 0.118s)
             Mean action noise std: 2.36
          Mean value_function loss: 306.1236
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.0688
                       Mean reward: 834.26
               Mean episode length: 228.69
    Episode_Reward/reaching_object: 1.5850
     Episode_Reward/lifting_object: 157.1601
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.67s
                      Time elapsed: 00:39:36
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 37645 steps/s (collection: 2.462s, learning 0.149s)
             Mean action noise std: 2.36
          Mean value_function loss: 264.8738
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.0757
                       Mean reward: 795.97
               Mean episode length: 218.91
    Episode_Reward/reaching_object: 1.5869
     Episode_Reward/lifting_object: 156.9368
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.61s
                      Time elapsed: 00:39:39
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 37095 steps/s (collection: 2.517s, learning 0.133s)
             Mean action noise std: 2.36
          Mean value_function loss: 255.5105
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.0834
                       Mean reward: 771.69
               Mean episode length: 213.11
    Episode_Reward/reaching_object: 1.6141
     Episode_Reward/lifting_object: 160.2208
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.65s
                      Time elapsed: 00:39:41
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 37020 steps/s (collection: 2.521s, learning 0.134s)
             Mean action noise std: 2.37
          Mean value_function loss: 266.8166
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 59.0880
                       Mean reward: 799.73
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.5890
     Episode_Reward/lifting_object: 156.0345
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.66s
                      Time elapsed: 00:39:44
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 37215 steps/s (collection: 2.497s, learning 0.145s)
             Mean action noise std: 2.37
          Mean value_function loss: 259.9662
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.0951
                       Mean reward: 772.83
               Mean episode length: 215.63
    Episode_Reward/reaching_object: 1.5770
     Episode_Reward/lifting_object: 154.9062
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.64s
                      Time elapsed: 00:39:47
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 37408 steps/s (collection: 2.501s, learning 0.126s)
             Mean action noise std: 2.37
          Mean value_function loss: 303.7605
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.1022
                       Mean reward: 737.17
               Mean episode length: 205.20
    Episode_Reward/reaching_object: 1.5569
     Episode_Reward/lifting_object: 154.2674
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.63s
                      Time elapsed: 00:39:49
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 36852 steps/s (collection: 2.526s, learning 0.142s)
             Mean action noise std: 2.37
          Mean value_function loss: 281.1626
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.1064
                       Mean reward: 810.10
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.5692
     Episode_Reward/lifting_object: 155.7441
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.67s
                      Time elapsed: 00:39:52
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 37813 steps/s (collection: 2.474s, learning 0.126s)
             Mean action noise std: 2.37
          Mean value_function loss: 327.4593
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1101
                       Mean reward: 734.82
               Mean episode length: 205.38
    Episode_Reward/reaching_object: 1.5159
     Episode_Reward/lifting_object: 149.3641
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.60s
                      Time elapsed: 00:39:55
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 37339 steps/s (collection: 2.514s, learning 0.119s)
             Mean action noise std: 2.37
          Mean value_function loss: 270.3214
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.1162
                       Mean reward: 804.72
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.5746
     Episode_Reward/lifting_object: 156.0432
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.63s
                      Time elapsed: 00:39:57
                               ETA: 00:54:13

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 37340 steps/s (collection: 2.489s, learning 0.143s)
             Mean action noise std: 2.37
          Mean value_function loss: 239.9735
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.1230
                       Mean reward: 797.24
               Mean episode length: 220.03
    Episode_Reward/reaching_object: 1.5834
     Episode_Reward/lifting_object: 156.5737
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.63s
                      Time elapsed: 00:40:00
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 37646 steps/s (collection: 2.471s, learning 0.140s)
             Mean action noise std: 2.37
          Mean value_function loss: 273.8252
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.1290
                       Mean reward: 779.77
               Mean episode length: 218.32
    Episode_Reward/reaching_object: 1.5631
     Episode_Reward/lifting_object: 154.4627
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.61s
                      Time elapsed: 00:40:02
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 37042 steps/s (collection: 2.515s, learning 0.139s)
             Mean action noise std: 2.37
          Mean value_function loss: 310.8332
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.1320
                       Mean reward: 778.30
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 1.5865
     Episode_Reward/lifting_object: 156.6796
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.65s
                      Time elapsed: 00:40:05
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 37306 steps/s (collection: 2.520s, learning 0.116s)
             Mean action noise std: 2.37
          Mean value_function loss: 233.3845
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.1350
                       Mean reward: 830.34
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.5986
     Episode_Reward/lifting_object: 158.4760
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.64s
                      Time elapsed: 00:40:08
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 37205 steps/s (collection: 2.509s, learning 0.133s)
             Mean action noise std: 2.37
          Mean value_function loss: 251.5253
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 59.1417
                       Mean reward: 765.07
               Mean episode length: 213.75
    Episode_Reward/reaching_object: 1.5460
     Episode_Reward/lifting_object: 152.9335
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.64s
                      Time elapsed: 00:40:10
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 37304 steps/s (collection: 2.493s, learning 0.142s)
             Mean action noise std: 2.37
          Mean value_function loss: 289.2883
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.1542
                       Mean reward: 759.81
               Mean episode length: 211.04
    Episode_Reward/reaching_object: 1.5427
     Episode_Reward/lifting_object: 152.4498
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.64s
                      Time elapsed: 00:40:13
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 36663 steps/s (collection: 2.544s, learning 0.137s)
             Mean action noise std: 2.37
          Mean value_function loss: 295.1820
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.1600
                       Mean reward: 807.92
               Mean episode length: 221.18
    Episode_Reward/reaching_object: 1.5998
     Episode_Reward/lifting_object: 158.3128
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.68s
                      Time elapsed: 00:40:16
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 35741 steps/s (collection: 2.603s, learning 0.147s)
             Mean action noise std: 2.37
          Mean value_function loss: 276.3150
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.1673
                       Mean reward: 776.27
               Mean episode length: 217.81
    Episode_Reward/reaching_object: 1.5717
     Episode_Reward/lifting_object: 155.1493
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.75s
                      Time elapsed: 00:40:18
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 37432 steps/s (collection: 2.498s, learning 0.129s)
             Mean action noise std: 2.37
          Mean value_function loss: 309.4613
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.1736
                       Mean reward: 778.58
               Mean episode length: 217.17
    Episode_Reward/reaching_object: 1.5948
     Episode_Reward/lifting_object: 157.8935
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.63s
                      Time elapsed: 00:40:21
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 37564 steps/s (collection: 2.499s, learning 0.118s)
             Mean action noise std: 2.37
          Mean value_function loss: 257.8179
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.1817
                       Mean reward: 803.38
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 1.5628
     Episode_Reward/lifting_object: 154.6577
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.62s
                      Time elapsed: 00:40:24
                               ETA: 00:53:42

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 36096 steps/s (collection: 2.572s, learning 0.152s)
             Mean action noise std: 2.38
          Mean value_function loss: 257.5323
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.1898
                       Mean reward: 777.91
               Mean episode length: 217.19
    Episode_Reward/reaching_object: 1.5900
     Episode_Reward/lifting_object: 157.1291
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.72s
                      Time elapsed: 00:40:26
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 37558 steps/s (collection: 2.474s, learning 0.143s)
             Mean action noise std: 2.38
          Mean value_function loss: 272.2694
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.1973
                       Mean reward: 826.47
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 1.5462
     Episode_Reward/lifting_object: 150.4671
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.62s
                      Time elapsed: 00:40:29
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 37435 steps/s (collection: 2.503s, learning 0.123s)
             Mean action noise std: 2.38
          Mean value_function loss: 256.3941
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.2027
                       Mean reward: 834.26
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 1.5894
     Episode_Reward/lifting_object: 157.1843
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.63s
                      Time elapsed: 00:40:32
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 36218 steps/s (collection: 2.550s, learning 0.164s)
             Mean action noise std: 2.38
          Mean value_function loss: 305.4441
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 59.2044
                       Mean reward: 784.80
               Mean episode length: 220.38
    Episode_Reward/reaching_object: 1.5643
     Episode_Reward/lifting_object: 154.0877
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.71s
                      Time elapsed: 00:40:34
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 37788 steps/s (collection: 2.483s, learning 0.119s)
             Mean action noise std: 2.38
          Mean value_function loss: 283.6654
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 59.2067
                       Mean reward: 767.22
               Mean episode length: 214.17
    Episode_Reward/reaching_object: 1.5553
     Episode_Reward/lifting_object: 153.8864
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.60s
                      Time elapsed: 00:40:37
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 37078 steps/s (collection: 2.511s, learning 0.140s)
             Mean action noise std: 2.38
          Mean value_function loss: 232.9734
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.2101
                       Mean reward: 841.92
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 1.6400
     Episode_Reward/lifting_object: 163.4213
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.65s
                      Time elapsed: 00:40:40
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 36518 steps/s (collection: 2.546s, learning 0.146s)
             Mean action noise std: 2.38
          Mean value_function loss: 288.9194
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.2184
                       Mean reward: 751.62
               Mean episode length: 206.99
    Episode_Reward/reaching_object: 1.5437
     Episode_Reward/lifting_object: 153.7574
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.69s
                      Time elapsed: 00:40:42
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 37086 steps/s (collection: 2.505s, learning 0.145s)
             Mean action noise std: 2.38
          Mean value_function loss: 311.3080
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.2279
                       Mean reward: 722.10
               Mean episode length: 205.17
    Episode_Reward/reaching_object: 1.5545
     Episode_Reward/lifting_object: 153.5648
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.65s
                      Time elapsed: 00:40:45
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 36978 steps/s (collection: 2.499s, learning 0.160s)
             Mean action noise std: 2.38
          Mean value_function loss: 266.1769
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.2347
                       Mean reward: 762.21
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 1.5755
     Episode_Reward/lifting_object: 156.1382
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.66s
                      Time elapsed: 00:40:48
                               ETA: 00:53:15

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 37350 steps/s (collection: 2.501s, learning 0.131s)
             Mean action noise std: 2.38
          Mean value_function loss: 254.5802
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.2431
                       Mean reward: 812.48
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.6006
     Episode_Reward/lifting_object: 158.9175
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.63s
                      Time elapsed: 00:40:50
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 37427 steps/s (collection: 2.499s, learning 0.128s)
             Mean action noise std: 2.38
          Mean value_function loss: 239.7064
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.2514
                       Mean reward: 731.42
               Mean episode length: 204.24
    Episode_Reward/reaching_object: 1.5689
     Episode_Reward/lifting_object: 155.9717
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.63s
                      Time elapsed: 00:40:53
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 38048 steps/s (collection: 2.460s, learning 0.124s)
             Mean action noise std: 2.38
          Mean value_function loss: 284.3921
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.2639
                       Mean reward: 766.33
               Mean episode length: 212.30
    Episode_Reward/reaching_object: 1.5551
     Episode_Reward/lifting_object: 153.8414
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.58s
                      Time elapsed: 00:40:55
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 37581 steps/s (collection: 2.489s, learning 0.127s)
             Mean action noise std: 2.38
          Mean value_function loss: 227.9678
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.2786
                       Mean reward: 807.71
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 1.5926
     Episode_Reward/lifting_object: 158.6480
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.62s
                      Time elapsed: 00:40:58
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 37151 steps/s (collection: 2.489s, learning 0.157s)
             Mean action noise std: 2.38
          Mean value_function loss: 253.4132
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.2856
                       Mean reward: 811.35
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.5646
     Episode_Reward/lifting_object: 155.6817
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.65s
                      Time elapsed: 00:41:01
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 35881 steps/s (collection: 2.592s, learning 0.148s)
             Mean action noise std: 2.39
          Mean value_function loss: 288.3299
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.2928
                       Mean reward: 786.65
               Mean episode length: 217.00
    Episode_Reward/reaching_object: 1.5620
     Episode_Reward/lifting_object: 155.8923
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.74s
                      Time elapsed: 00:41:03
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 36245 steps/s (collection: 2.546s, learning 0.166s)
             Mean action noise std: 2.39
          Mean value_function loss: 244.7488
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.3020
                       Mean reward: 775.52
               Mean episode length: 214.16
    Episode_Reward/reaching_object: 1.5672
     Episode_Reward/lifting_object: 156.0447
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.71s
                      Time elapsed: 00:41:06
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 34566 steps/s (collection: 2.688s, learning 0.156s)
             Mean action noise std: 2.39
          Mean value_function loss: 238.8551
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.3134
                       Mean reward: 833.72
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 1.6136
     Episode_Reward/lifting_object: 161.1949
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.84s
                      Time elapsed: 00:41:09
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 36023 steps/s (collection: 2.573s, learning 0.156s)
             Mean action noise std: 2.39
          Mean value_function loss: 225.6842
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 59.3202
                       Mean reward: 828.05
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 1.5992
     Episode_Reward/lifting_object: 160.5416
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.73s
                      Time elapsed: 00:41:12
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 36581 steps/s (collection: 2.561s, learning 0.126s)
             Mean action noise std: 2.39
          Mean value_function loss: 255.5168
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.3234
                       Mean reward: 845.91
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 1.6031
     Episode_Reward/lifting_object: 160.3313
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.69s
                      Time elapsed: 00:41:14
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 37388 steps/s (collection: 2.494s, learning 0.135s)
             Mean action noise std: 2.39
          Mean value_function loss: 240.0157
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.3291
                       Mean reward: 801.17
               Mean episode length: 220.23
    Episode_Reward/reaching_object: 1.5893
     Episode_Reward/lifting_object: 158.9851
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.63s
                      Time elapsed: 00:41:17
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 36248 steps/s (collection: 2.541s, learning 0.171s)
             Mean action noise std: 2.39
          Mean value_function loss: 235.0852
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.3393
                       Mean reward: 770.97
               Mean episode length: 212.84
    Episode_Reward/reaching_object: 1.5825
     Episode_Reward/lifting_object: 159.0352
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.71s
                      Time elapsed: 00:41:20
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 36730 steps/s (collection: 2.526s, learning 0.150s)
             Mean action noise std: 2.39
          Mean value_function loss: 199.4187
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.3470
                       Mean reward: 821.43
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.6187
     Episode_Reward/lifting_object: 162.6501
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.68s
                      Time elapsed: 00:41:22
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 37424 steps/s (collection: 2.487s, learning 0.140s)
             Mean action noise std: 2.39
          Mean value_function loss: 247.8518
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.3548
                       Mean reward: 780.79
               Mean episode length: 218.38
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 156.0827
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.63s
                      Time elapsed: 00:41:25
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 36757 steps/s (collection: 2.552s, learning 0.122s)
             Mean action noise std: 2.39
          Mean value_function loss: 241.5323
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.3626
                       Mean reward: 818.91
               Mean episode length: 223.21
    Episode_Reward/reaching_object: 1.6050
     Episode_Reward/lifting_object: 161.0245
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.67s
                      Time elapsed: 00:41:28
                               ETA: 00:52:30

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 36548 steps/s (collection: 2.551s, learning 0.139s)
             Mean action noise std: 2.39
          Mean value_function loss: 228.7657
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 59.3666
                       Mean reward: 833.01
               Mean episode length: 227.99
    Episode_Reward/reaching_object: 1.6176
     Episode_Reward/lifting_object: 161.9388
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.69s
                      Time elapsed: 00:41:30
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 36742 steps/s (collection: 2.545s, learning 0.130s)
             Mean action noise std: 2.39
          Mean value_function loss: 250.9835
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.3697
                       Mean reward: 797.72
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.5722
     Episode_Reward/lifting_object: 156.6249
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.68s
                      Time elapsed: 00:41:33
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 36880 steps/s (collection: 2.551s, learning 0.115s)
             Mean action noise std: 2.39
          Mean value_function loss: 235.3898
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.3796
                       Mean reward: 861.26
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.6231
     Episode_Reward/lifting_object: 162.8069
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.67s
                      Time elapsed: 00:41:36
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 38087 steps/s (collection: 2.449s, learning 0.132s)
             Mean action noise std: 2.40
          Mean value_function loss: 211.6438
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.3906
                       Mean reward: 786.83
               Mean episode length: 217.93
    Episode_Reward/reaching_object: 1.5999
     Episode_Reward/lifting_object: 159.7495
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.58s
                      Time elapsed: 00:41:38
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 37645 steps/s (collection: 2.495s, learning 0.116s)
             Mean action noise std: 2.40
          Mean value_function loss: 221.6110
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.4029
                       Mean reward: 829.11
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 1.5866
     Episode_Reward/lifting_object: 158.1234
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.61s
                      Time elapsed: 00:41:41
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 37155 steps/s (collection: 2.495s, learning 0.151s)
             Mean action noise std: 2.40
          Mean value_function loss: 223.2523
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.4129
                       Mean reward: 797.41
               Mean episode length: 218.90
    Episode_Reward/reaching_object: 1.6654
     Episode_Reward/lifting_object: 166.8655
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.65s
                      Time elapsed: 00:41:44
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 36650 steps/s (collection: 2.536s, learning 0.146s)
             Mean action noise std: 2.40
          Mean value_function loss: 219.8973
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.4215
                       Mean reward: 779.88
               Mean episode length: 214.66
    Episode_Reward/reaching_object: 1.5697
     Episode_Reward/lifting_object: 156.6407
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.68s
                      Time elapsed: 00:41:46
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 37723 steps/s (collection: 2.466s, learning 0.140s)
             Mean action noise std: 2.40
          Mean value_function loss: 228.8920
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.4353
                       Mean reward: 780.58
               Mean episode length: 215.50
    Episode_Reward/reaching_object: 1.6001
     Episode_Reward/lifting_object: 160.2110
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.61s
                      Time elapsed: 00:41:49
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 37538 steps/s (collection: 2.481s, learning 0.137s)
             Mean action noise std: 2.40
          Mean value_function loss: 235.5422
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.4472
                       Mean reward: 815.80
               Mean episode length: 224.45
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 156.5167
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.62s
                      Time elapsed: 00:41:51
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 37781 steps/s (collection: 2.461s, learning 0.141s)
             Mean action noise std: 2.40
          Mean value_function loss: 224.1252
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.4529
                       Mean reward: 792.97
               Mean episode length: 217.20
    Episode_Reward/reaching_object: 1.5858
     Episode_Reward/lifting_object: 158.7831
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.60s
                      Time elapsed: 00:41:54
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 37439 steps/s (collection: 2.487s, learning 0.139s)
             Mean action noise std: 2.40
          Mean value_function loss: 247.2732
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.4577
                       Mean reward: 857.79
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.5990
     Episode_Reward/lifting_object: 160.3412
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.63s
                      Time elapsed: 00:41:57
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 37250 steps/s (collection: 2.507s, learning 0.132s)
             Mean action noise std: 2.40
          Mean value_function loss: 281.5274
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 59.4648
                       Mean reward: 756.88
               Mean episode length: 208.21
    Episode_Reward/reaching_object: 1.5799
     Episode_Reward/lifting_object: 158.5073
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.64s
                      Time elapsed: 00:41:59
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 37033 steps/s (collection: 2.501s, learning 0.153s)
             Mean action noise std: 2.40
          Mean value_function loss: 234.8009
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.4735
                       Mean reward: 751.20
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 1.6107
     Episode_Reward/lifting_object: 162.7358
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.65s
                      Time elapsed: 00:42:02
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 37335 steps/s (collection: 2.491s, learning 0.142s)
             Mean action noise std: 2.40
          Mean value_function loss: 319.5781
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.4831
                       Mean reward: 762.20
               Mean episode length: 210.44
    Episode_Reward/reaching_object: 1.5514
     Episode_Reward/lifting_object: 156.6040
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.63s
                      Time elapsed: 00:42:05
                               ETA: 00:51:47

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 36827 steps/s (collection: 2.519s, learning 0.150s)
             Mean action noise std: 2.41
          Mean value_function loss: 250.9572
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.4955
                       Mean reward: 757.64
               Mean episode length: 209.44
    Episode_Reward/reaching_object: 1.5462
     Episode_Reward/lifting_object: 155.7726
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.67s
                      Time elapsed: 00:42:07
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 36031 steps/s (collection: 2.593s, learning 0.136s)
             Mean action noise std: 2.41
          Mean value_function loss: 232.1837
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.5120
                       Mean reward: 784.75
               Mean episode length: 213.56
    Episode_Reward/reaching_object: 1.5461
     Episode_Reward/lifting_object: 156.3149
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.73s
                      Time elapsed: 00:42:10
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 37096 steps/s (collection: 2.514s, learning 0.136s)
             Mean action noise std: 2.41
          Mean value_function loss: 220.7040
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 59.5207
                       Mean reward: 815.72
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.5768
     Episode_Reward/lifting_object: 159.3239
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.65s
                      Time elapsed: 00:42:13
                               ETA: 00:51:38

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 36997 steps/s (collection: 2.510s, learning 0.147s)
             Mean action noise std: 2.41
          Mean value_function loss: 324.2188
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.5332
                       Mean reward: 777.61
               Mean episode length: 214.14
    Episode_Reward/reaching_object: 1.5612
     Episode_Reward/lifting_object: 157.5041
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.66s
                      Time elapsed: 00:42:15
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 34861 steps/s (collection: 2.652s, learning 0.168s)
             Mean action noise std: 2.41
          Mean value_function loss: 258.9535
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.5537
                       Mean reward: 755.89
               Mean episode length: 209.28
    Episode_Reward/reaching_object: 1.5428
     Episode_Reward/lifting_object: 154.6108
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.82s
                      Time elapsed: 00:42:18
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 37862 steps/s (collection: 2.433s, learning 0.163s)
             Mean action noise std: 2.41
          Mean value_function loss: 246.5938
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.5655
                       Mean reward: 784.22
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 1.5888
     Episode_Reward/lifting_object: 161.5156
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.60s
                      Time elapsed: 00:42:21
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 37012 steps/s (collection: 2.529s, learning 0.127s)
             Mean action noise std: 2.41
          Mean value_function loss: 347.2863
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 59.5708
                       Mean reward: 770.51
               Mean episode length: 212.04
    Episode_Reward/reaching_object: 1.5509
     Episode_Reward/lifting_object: 156.0699
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.66s
                      Time elapsed: 00:42:23
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 37287 steps/s (collection: 2.507s, learning 0.129s)
             Mean action noise std: 2.41
          Mean value_function loss: 270.1977
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.5761
                       Mean reward: 813.73
               Mean episode length: 220.62
    Episode_Reward/reaching_object: 1.5702
     Episode_Reward/lifting_object: 158.4439
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.64s
                      Time elapsed: 00:42:26
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 37957 steps/s (collection: 2.465s, learning 0.124s)
             Mean action noise std: 2.41
          Mean value_function loss: 269.9769
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.5814
                       Mean reward: 796.15
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 1.5737
     Episode_Reward/lifting_object: 158.9413
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.59s
                      Time elapsed: 00:42:29
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 37123 steps/s (collection: 2.502s, learning 0.146s)
             Mean action noise std: 2.41
          Mean value_function loss: 258.6995
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.5886
                       Mean reward: 795.88
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 158.8918
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.65s
                      Time elapsed: 00:42:31
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 36886 steps/s (collection: 2.529s, learning 0.136s)
             Mean action noise std: 2.42
          Mean value_function loss: 273.6120
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.5981
                       Mean reward: 820.02
               Mean episode length: 224.04
    Episode_Reward/reaching_object: 1.5846
     Episode_Reward/lifting_object: 159.8811
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.67s
                      Time elapsed: 00:42:34
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 37007 steps/s (collection: 2.540s, learning 0.117s)
             Mean action noise std: 2.42
          Mean value_function loss: 244.4731
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.6047
                       Mean reward: 764.54
               Mean episode length: 210.15
    Episode_Reward/reaching_object: 1.5168
     Episode_Reward/lifting_object: 151.7522
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.66s
                      Time elapsed: 00:42:37
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 36395 steps/s (collection: 2.548s, learning 0.153s)
             Mean action noise std: 2.42
          Mean value_function loss: 189.9148
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.6134
                       Mean reward: 820.43
               Mean episode length: 222.99
    Episode_Reward/reaching_object: 1.5669
     Episode_Reward/lifting_object: 158.4245
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.70s
                      Time elapsed: 00:42:39
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 34601 steps/s (collection: 2.666s, learning 0.175s)
             Mean action noise std: 2.42
          Mean value_function loss: 265.0181
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.6218
                       Mean reward: 764.67
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 1.5606
     Episode_Reward/lifting_object: 157.4818
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.84s
                      Time elapsed: 00:42:42
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 31339 steps/s (collection: 2.965s, learning 0.172s)
             Mean action noise std: 2.42
          Mean value_function loss: 227.2830
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.6339
                       Mean reward: 811.75
               Mean episode length: 220.07
    Episode_Reward/reaching_object: 1.5849
     Episode_Reward/lifting_object: 160.5048
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 3.14s
                      Time elapsed: 00:42:45
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 35345 steps/s (collection: 2.640s, learning 0.141s)
             Mean action noise std: 2.42
          Mean value_function loss: 230.7405
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 59.6434
                       Mean reward: 808.52
               Mean episode length: 220.83
    Episode_Reward/reaching_object: 1.5626
     Episode_Reward/lifting_object: 157.1789
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.78s
                      Time elapsed: 00:42:48
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 35767 steps/s (collection: 2.603s, learning 0.145s)
             Mean action noise std: 2.42
          Mean value_function loss: 221.9898
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.6470
                       Mean reward: 822.18
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 1.5724
     Episode_Reward/lifting_object: 158.1306
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.75s
                      Time elapsed: 00:42:51
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 35171 steps/s (collection: 2.664s, learning 0.131s)
             Mean action noise std: 2.42
          Mean value_function loss: 204.8275
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6561
                       Mean reward: 811.12
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 1.6235
     Episode_Reward/lifting_object: 164.3533
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.79s
                      Time elapsed: 00:42:54
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 37657 steps/s (collection: 2.472s, learning 0.138s)
             Mean action noise std: 2.42
          Mean value_function loss: 224.0568
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.6656
                       Mean reward: 790.95
               Mean episode length: 216.92
    Episode_Reward/reaching_object: 1.5758
     Episode_Reward/lifting_object: 158.2699
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.61s
                      Time elapsed: 00:42:56
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 38303 steps/s (collection: 2.446s, learning 0.121s)
             Mean action noise std: 2.42
          Mean value_function loss: 239.1823
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.6726
                       Mean reward: 815.85
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 1.5891
     Episode_Reward/lifting_object: 158.9061
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.57s
                      Time elapsed: 00:42:59
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 37234 steps/s (collection: 2.481s, learning 0.159s)
             Mean action noise std: 2.42
          Mean value_function loss: 276.5919
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.6854
                       Mean reward: 808.71
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.5927
     Episode_Reward/lifting_object: 160.6942
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.64s
                      Time elapsed: 00:43:01
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 37831 steps/s (collection: 2.464s, learning 0.135s)
             Mean action noise std: 2.43
          Mean value_function loss: 230.8831
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.6966
                       Mean reward: 778.75
               Mean episode length: 214.81
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 157.9738
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.60s
                      Time elapsed: 00:43:04
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 37392 steps/s (collection: 2.474s, learning 0.155s)
             Mean action noise std: 2.43
          Mean value_function loss: 206.4564
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 59.7042
                       Mean reward: 833.87
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 1.6287
     Episode_Reward/lifting_object: 164.3801
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.63s
                      Time elapsed: 00:43:07
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 37548 steps/s (collection: 2.483s, learning 0.135s)
             Mean action noise std: 2.43
          Mean value_function loss: 232.5842
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.7152
                       Mean reward: 835.62
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.5732
     Episode_Reward/lifting_object: 157.8150
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.62s
                      Time elapsed: 00:43:09
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 37427 steps/s (collection: 2.470s, learning 0.156s)
             Mean action noise std: 2.43
          Mean value_function loss: 243.3210
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.7281
                       Mean reward: 847.87
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.6442
     Episode_Reward/lifting_object: 166.0789
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.63s
                      Time elapsed: 00:43:12
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 36977 steps/s (collection: 2.507s, learning 0.151s)
             Mean action noise std: 2.43
          Mean value_function loss: 219.4173
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.7420
                       Mean reward: 810.17
               Mean episode length: 221.62
    Episode_Reward/reaching_object: 1.6118
     Episode_Reward/lifting_object: 162.1624
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.66s
                      Time elapsed: 00:43:15
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 36229 steps/s (collection: 2.576s, learning 0.137s)
             Mean action noise std: 2.43
          Mean value_function loss: 243.7899
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.7520
                       Mean reward: 813.38
               Mean episode length: 221.98
    Episode_Reward/reaching_object: 1.5822
     Episode_Reward/lifting_object: 159.3745
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.71s
                      Time elapsed: 00:43:17
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 36658 steps/s (collection: 2.526s, learning 0.155s)
             Mean action noise std: 2.43
          Mean value_function loss: 226.9076
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.7637
                       Mean reward: 791.73
               Mean episode length: 217.81
    Episode_Reward/reaching_object: 1.6051
     Episode_Reward/lifting_object: 161.6116
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.68s
                      Time elapsed: 00:43:20
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 36760 steps/s (collection: 2.538s, learning 0.136s)
             Mean action noise std: 2.43
          Mean value_function loss: 207.0848
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.7766
                       Mean reward: 830.91
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.6129
     Episode_Reward/lifting_object: 162.7095
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.67s
                      Time elapsed: 00:43:23
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 36591 steps/s (collection: 2.527s, learning 0.160s)
             Mean action noise std: 2.44
          Mean value_function loss: 199.5696
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.7895
                       Mean reward: 827.47
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 1.6461
     Episode_Reward/lifting_object: 165.9077
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.69s
                      Time elapsed: 00:43:25
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 37266 steps/s (collection: 2.517s, learning 0.121s)
             Mean action noise std: 2.44
          Mean value_function loss: 224.5585
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.8001
                       Mean reward: 801.24
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 1.6160
     Episode_Reward/lifting_object: 162.8409
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.64s
                      Time elapsed: 00:43:28
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 35909 steps/s (collection: 2.598s, learning 0.140s)
             Mean action noise std: 2.44
          Mean value_function loss: 272.7243
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.8058
                       Mean reward: 821.16
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 157.9797
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.74s
                      Time elapsed: 00:43:31
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 36022 steps/s (collection: 2.570s, learning 0.159s)
             Mean action noise std: 2.44
          Mean value_function loss: 222.1371
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.8152
                       Mean reward: 805.25
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 1.5778
     Episode_Reward/lifting_object: 157.9439
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.73s
                      Time elapsed: 00:43:33
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 35896 steps/s (collection: 2.598s, learning 0.140s)
             Mean action noise std: 2.44
          Mean value_function loss: 204.1222
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.8257
                       Mean reward: 817.27
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.6020
     Episode_Reward/lifting_object: 160.5820
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.74s
                      Time elapsed: 00:43:36
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 36550 steps/s (collection: 2.528s, learning 0.161s)
             Mean action noise std: 2.44
          Mean value_function loss: 209.5139
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.8415
                       Mean reward: 813.99
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 1.6292
     Episode_Reward/lifting_object: 164.1640
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.69s
                      Time elapsed: 00:43:39
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 37419 steps/s (collection: 2.492s, learning 0.135s)
             Mean action noise std: 2.44
          Mean value_function loss: 175.2966
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.8589
                       Mean reward: 856.67
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 1.6490
     Episode_Reward/lifting_object: 165.6703
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.63s
                      Time elapsed: 00:43:41
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 36468 steps/s (collection: 2.537s, learning 0.159s)
             Mean action noise std: 2.44
          Mean value_function loss: 201.5785
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.8821
                       Mean reward: 778.76
               Mean episode length: 215.77
    Episode_Reward/reaching_object: 1.6178
     Episode_Reward/lifting_object: 162.4086
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.70s
                      Time elapsed: 00:43:44
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 35568 steps/s (collection: 2.591s, learning 0.172s)
             Mean action noise std: 2.45
          Mean value_function loss: 210.6522
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.9046
                       Mean reward: 852.69
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.6275
     Episode_Reward/lifting_object: 164.2026
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.76s
                      Time elapsed: 00:43:47
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 36634 steps/s (collection: 2.541s, learning 0.142s)
             Mean action noise std: 2.45
          Mean value_function loss: 240.8300
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.9185
                       Mean reward: 799.58
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 1.5707
     Episode_Reward/lifting_object: 157.1929
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.68s
                      Time elapsed: 00:43:50
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 35665 steps/s (collection: 2.608s, learning 0.148s)
             Mean action noise std: 2.45
          Mean value_function loss: 209.7127
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.9343
                       Mean reward: 823.56
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.6204
     Episode_Reward/lifting_object: 163.0306
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.76s
                      Time elapsed: 00:43:52
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 36399 steps/s (collection: 2.541s, learning 0.159s)
             Mean action noise std: 2.45
          Mean value_function loss: 193.8487
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.9586
                       Mean reward: 805.83
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 1.6555
     Episode_Reward/lifting_object: 167.0546
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.70s
                      Time elapsed: 00:43:55
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 36505 steps/s (collection: 2.556s, learning 0.136s)
             Mean action noise std: 2.46
          Mean value_function loss: 201.0690
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.9872
                       Mean reward: 816.38
               Mean episode length: 222.86
    Episode_Reward/reaching_object: 1.6316
     Episode_Reward/lifting_object: 164.2336
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.69s
                      Time elapsed: 00:43:58
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 36140 steps/s (collection: 2.542s, learning 0.178s)
             Mean action noise std: 2.46
          Mean value_function loss: 172.6402
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.0083
                       Mean reward: 849.00
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.6829
     Episode_Reward/lifting_object: 169.4835
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.72s
                      Time elapsed: 00:44:00
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 37205 steps/s (collection: 2.488s, learning 0.154s)
             Mean action noise std: 2.46
          Mean value_function loss: 241.6475
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.0267
                       Mean reward: 818.57
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.6208
     Episode_Reward/lifting_object: 162.2814
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.64s
                      Time elapsed: 00:44:03
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 36050 steps/s (collection: 2.569s, learning 0.157s)
             Mean action noise std: 2.46
          Mean value_function loss: 212.4345
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.0405
                       Mean reward: 816.01
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 1.5984
     Episode_Reward/lifting_object: 159.5991
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.73s
                      Time elapsed: 00:44:06
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 37065 steps/s (collection: 2.529s, learning 0.123s)
             Mean action noise std: 2.46
          Mean value_function loss: 200.5837
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.0508
                       Mean reward: 819.18
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 1.6215
     Episode_Reward/lifting_object: 162.3398
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.65s
                      Time elapsed: 00:44:09
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 37504 steps/s (collection: 2.487s, learning 0.134s)
             Mean action noise std: 2.46
          Mean value_function loss: 261.8032
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.0666
                       Mean reward: 814.16
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 1.6290
     Episode_Reward/lifting_object: 161.6825
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.62s
                      Time elapsed: 00:44:11
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 37020 steps/s (collection: 2.504s, learning 0.151s)
             Mean action noise std: 2.46
          Mean value_function loss: 220.0367
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.0856
                       Mean reward: 814.91
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.6007
     Episode_Reward/lifting_object: 160.2495
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.66s
                      Time elapsed: 00:44:14
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 35456 steps/s (collection: 2.627s, learning 0.146s)
             Mean action noise std: 2.47
          Mean value_function loss: 240.9143
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.0955
                       Mean reward: 827.80
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 1.6004
     Episode_Reward/lifting_object: 160.1150
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.77s
                      Time elapsed: 00:44:17
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 38233 steps/s (collection: 2.454s, learning 0.117s)
             Mean action noise std: 2.47
          Mean value_function loss: 247.2875
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.1074
                       Mean reward: 822.97
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.6405
     Episode_Reward/lifting_object: 163.8737
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.57s
                      Time elapsed: 00:44:19
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 38835 steps/s (collection: 2.385s, learning 0.147s)
             Mean action noise std: 2.47
          Mean value_function loss: 247.1867
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.1221
                       Mean reward: 728.51
               Mean episode length: 203.52
    Episode_Reward/reaching_object: 1.5870
     Episode_Reward/lifting_object: 157.7103
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.53s
                      Time elapsed: 00:44:22
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 39410 steps/s (collection: 2.370s, learning 0.124s)
             Mean action noise std: 2.47
          Mean value_function loss: 201.1438
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.1312
                       Mean reward: 871.26
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.6466
     Episode_Reward/lifting_object: 165.0617
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.49s
                      Time elapsed: 00:44:24
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 39742 steps/s (collection: 2.336s, learning 0.137s)
             Mean action noise std: 2.47
          Mean value_function loss: 180.9220
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.1369
                       Mean reward: 813.77
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.6127
     Episode_Reward/lifting_object: 161.1964
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.47s
                      Time elapsed: 00:44:27
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 39674 steps/s (collection: 2.353s, learning 0.125s)
             Mean action noise std: 2.47
          Mean value_function loss: 177.8511
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.1429
                       Mean reward: 844.20
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.6471
     Episode_Reward/lifting_object: 165.4941
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.48s
                      Time elapsed: 00:44:29
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 38326 steps/s (collection: 2.408s, learning 0.157s)
             Mean action noise std: 2.47
          Mean value_function loss: 209.7578
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.1517
                       Mean reward: 820.55
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 1.6211
     Episode_Reward/lifting_object: 162.7737
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.56s
                      Time elapsed: 00:44:32
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 38764 steps/s (collection: 2.416s, learning 0.120s)
             Mean action noise std: 2.47
          Mean value_function loss: 196.5048
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.1635
                       Mean reward: 788.76
               Mean episode length: 216.31
    Episode_Reward/reaching_object: 1.5939
     Episode_Reward/lifting_object: 159.2353
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.54s
                      Time elapsed: 00:44:34
                               ETA: 00:49:01

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 39194 steps/s (collection: 2.382s, learning 0.126s)
             Mean action noise std: 2.47
          Mean value_function loss: 204.9387
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.1728
                       Mean reward: 821.79
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 1.6504
     Episode_Reward/lifting_object: 165.7142
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.51s
                      Time elapsed: 00:44:37
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 39321 steps/s (collection: 2.380s, learning 0.120s)
             Mean action noise std: 2.47
          Mean value_function loss: 210.4296
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.1889
                       Mean reward: 841.11
               Mean episode length: 228.82
    Episode_Reward/reaching_object: 1.6360
     Episode_Reward/lifting_object: 164.3891
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.50s
                      Time elapsed: 00:44:39
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 40603 steps/s (collection: 2.304s, learning 0.117s)
             Mean action noise std: 2.48
          Mean value_function loss: 190.6981
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.2013
                       Mean reward: 804.33
               Mean episode length: 220.23
    Episode_Reward/reaching_object: 1.6038
     Episode_Reward/lifting_object: 161.5319
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.42s
                      Time elapsed: 00:44:42
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 39924 steps/s (collection: 2.331s, learning 0.131s)
             Mean action noise std: 2.48
          Mean value_function loss: 222.8231
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.2167
                       Mean reward: 804.15
               Mean episode length: 218.28
    Episode_Reward/reaching_object: 1.6056
     Episode_Reward/lifting_object: 161.2370
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.46s
                      Time elapsed: 00:44:44
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 39979 steps/s (collection: 2.336s, learning 0.123s)
             Mean action noise std: 2.48
          Mean value_function loss: 246.6766
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.2295
                       Mean reward: 819.21
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 1.5950
     Episode_Reward/lifting_object: 160.7668
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.46s
                      Time elapsed: 00:44:47
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 38846 steps/s (collection: 2.412s, learning 0.118s)
             Mean action noise std: 2.48
          Mean value_function loss: 238.0682
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.2423
                       Mean reward: 781.05
               Mean episode length: 213.62
    Episode_Reward/reaching_object: 1.5936
     Episode_Reward/lifting_object: 161.1100
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.53s
                      Time elapsed: 00:44:49
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 38099 steps/s (collection: 2.435s, learning 0.145s)
             Mean action noise std: 2.48
          Mean value_function loss: 208.0457
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 60.2490
                       Mean reward: 804.53
               Mean episode length: 218.86
    Episode_Reward/reaching_object: 1.5891
     Episode_Reward/lifting_object: 160.1754
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.58s
                      Time elapsed: 00:44:52
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 39684 steps/s (collection: 2.346s, learning 0.132s)
             Mean action noise std: 2.48
          Mean value_function loss: 227.9438
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.2597
                       Mean reward: 781.21
               Mean episode length: 214.93
    Episode_Reward/reaching_object: 1.5592
     Episode_Reward/lifting_object: 157.3732
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.48s
                      Time elapsed: 00:44:54
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 38957 steps/s (collection: 2.400s, learning 0.123s)
             Mean action noise std: 2.48
          Mean value_function loss: 218.1586
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.2691
                       Mean reward: 840.80
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 1.6258
     Episode_Reward/lifting_object: 164.1706
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.52s
                      Time elapsed: 00:44:57
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 39545 steps/s (collection: 2.356s, learning 0.130s)
             Mean action noise std: 2.48
          Mean value_function loss: 203.7156
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.2774
                       Mean reward: 905.19
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 1.6543
     Episode_Reward/lifting_object: 167.7893
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.49s
                      Time elapsed: 00:44:59
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 39620 steps/s (collection: 2.367s, learning 0.114s)
             Mean action noise std: 2.49
          Mean value_function loss: 202.5778
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.2894
                       Mean reward: 816.93
               Mean episode length: 222.57
    Episode_Reward/reaching_object: 1.6164
     Episode_Reward/lifting_object: 163.5360
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.48s
                      Time elapsed: 00:45:02
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 38140 steps/s (collection: 2.414s, learning 0.163s)
             Mean action noise std: 2.49
          Mean value_function loss: 237.7536
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.3047
                       Mean reward: 749.96
               Mean episode length: 206.33
    Episode_Reward/reaching_object: 1.5884
     Episode_Reward/lifting_object: 160.5313
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.58s
                      Time elapsed: 00:45:04
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 37733 steps/s (collection: 2.476s, learning 0.129s)
             Mean action noise std: 2.49
          Mean value_function loss: 189.7079
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 60.3241
                       Mean reward: 812.93
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 1.6075
     Episode_Reward/lifting_object: 162.4696
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.61s
                      Time elapsed: 00:45:07
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 37685 steps/s (collection: 2.486s, learning 0.123s)
             Mean action noise std: 2.49
          Mean value_function loss: 206.7811
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.3432
                       Mean reward: 838.30
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.6126
     Episode_Reward/lifting_object: 163.3203
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.61s
                      Time elapsed: 00:45:09
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 37186 steps/s (collection: 2.496s, learning 0.148s)
             Mean action noise std: 2.49
          Mean value_function loss: 226.3074
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.3616
                       Mean reward: 856.60
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 1.6054
     Episode_Reward/lifting_object: 162.4029
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.64s
                      Time elapsed: 00:45:12
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 36393 steps/s (collection: 2.571s, learning 0.130s)
             Mean action noise std: 2.49
          Mean value_function loss: 205.3745
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.3779
                       Mean reward: 818.65
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 1.6382
     Episode_Reward/lifting_object: 165.9796
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.70s
                      Time elapsed: 00:45:15
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 36616 steps/s (collection: 2.540s, learning 0.145s)
             Mean action noise std: 2.49
          Mean value_function loss: 261.7726
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.3888
                       Mean reward: 791.77
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 1.5778
     Episode_Reward/lifting_object: 159.2169
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.68s
                      Time elapsed: 00:45:17
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 34809 steps/s (collection: 2.689s, learning 0.135s)
             Mean action noise std: 2.50
          Mean value_function loss: 198.4237
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.4051
                       Mean reward: 811.74
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 1.6198
     Episode_Reward/lifting_object: 164.1685
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.82s
                      Time elapsed: 00:45:20
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 36073 steps/s (collection: 2.598s, learning 0.127s)
             Mean action noise std: 2.50
          Mean value_function loss: 218.3814
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.4223
                       Mean reward: 809.58
               Mean episode length: 220.37
    Episode_Reward/reaching_object: 1.5909
     Episode_Reward/lifting_object: 161.0251
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.73s
                      Time elapsed: 00:45:23
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 37028 steps/s (collection: 2.514s, learning 0.141s)
             Mean action noise std: 2.50
          Mean value_function loss: 184.9129
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.4431
                       Mean reward: 858.89
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 1.6440
     Episode_Reward/lifting_object: 166.5788
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.65s
                      Time elapsed: 00:45:26
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 36554 steps/s (collection: 2.543s, learning 0.147s)
             Mean action noise std: 2.50
          Mean value_function loss: 192.9964
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.4677
                       Mean reward: 816.53
               Mean episode length: 223.76
    Episode_Reward/reaching_object: 1.5959
     Episode_Reward/lifting_object: 160.6686
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.69s
                      Time elapsed: 00:45:28
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 36457 steps/s (collection: 2.524s, learning 0.172s)
             Mean action noise std: 2.50
          Mean value_function loss: 178.7349
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.4805
                       Mean reward: 811.12
               Mean episode length: 220.57
    Episode_Reward/reaching_object: 1.6357
     Episode_Reward/lifting_object: 164.9551
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.70s
                      Time elapsed: 00:45:31
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 36355 steps/s (collection: 2.579s, learning 0.125s)
             Mean action noise std: 2.51
          Mean value_function loss: 186.6822
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.5018
                       Mean reward: 823.43
               Mean episode length: 221.19
    Episode_Reward/reaching_object: 1.6355
     Episode_Reward/lifting_object: 166.3264
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.70s
                      Time elapsed: 00:45:34
                               ETA: 00:47:51

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 36158 steps/s (collection: 2.598s, learning 0.121s)
             Mean action noise std: 2.51
          Mean value_function loss: 162.6657
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.5219
                       Mean reward: 851.63
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 1.6402
     Episode_Reward/lifting_object: 165.1332
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.72s
                      Time elapsed: 00:45:36
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 36263 steps/s (collection: 2.559s, learning 0.152s)
             Mean action noise std: 2.51
          Mean value_function loss: 159.6556
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.5287
                       Mean reward: 860.10
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 1.6734
     Episode_Reward/lifting_object: 169.1099
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.71s
                      Time elapsed: 00:45:39
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 37198 steps/s (collection: 2.521s, learning 0.122s)
             Mean action noise std: 2.51
          Mean value_function loss: 200.0561
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.5328
                       Mean reward: 846.32
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.6919
     Episode_Reward/lifting_object: 170.9849
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.64s
                      Time elapsed: 00:45:42
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 36112 steps/s (collection: 2.583s, learning 0.139s)
             Mean action noise std: 2.51
          Mean value_function loss: 181.7937
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 60.5368
                       Mean reward: 848.95
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.6840
     Episode_Reward/lifting_object: 169.2079
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.72s
                      Time elapsed: 00:45:45
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 36744 steps/s (collection: 2.548s, learning 0.127s)
             Mean action noise std: 2.51
          Mean value_function loss: 230.5955
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.5414
                       Mean reward: 872.85
               Mean episode length: 236.26
    Episode_Reward/reaching_object: 1.6426
     Episode_Reward/lifting_object: 165.0038
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.68s
                      Time elapsed: 00:45:47
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 36660 steps/s (collection: 2.554s, learning 0.128s)
             Mean action noise std: 2.51
          Mean value_function loss: 181.7828
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.5483
                       Mean reward: 831.04
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 1.6485
     Episode_Reward/lifting_object: 165.2299
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.68s
                      Time elapsed: 00:45:50
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 35325 steps/s (collection: 2.636s, learning 0.147s)
             Mean action noise std: 2.51
          Mean value_function loss: 175.2136
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 60.5530
                       Mean reward: 843.99
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 1.6518
     Episode_Reward/lifting_object: 165.8964
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.78s
                      Time elapsed: 00:45:53
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 37156 steps/s (collection: 2.512s, learning 0.133s)
             Mean action noise std: 2.51
          Mean value_function loss: 187.4059
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.5553
                       Mean reward: 821.11
               Mean episode length: 223.48
    Episode_Reward/reaching_object: 1.6436
     Episode_Reward/lifting_object: 163.9258
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.65s
                      Time elapsed: 00:45:55
                               ETA: 00:47:28

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 36322 steps/s (collection: 2.572s, learning 0.135s)
             Mean action noise std: 2.51
          Mean value_function loss: 190.6877
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 60.5575
                       Mean reward: 837.24
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 1.6347
     Episode_Reward/lifting_object: 164.1321
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.71s
                      Time elapsed: 00:45:58
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 35475 steps/s (collection: 2.631s, learning 0.140s)
             Mean action noise std: 2.51
          Mean value_function loss: 184.2425
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 60.5591
                       Mean reward: 847.85
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.6415
     Episode_Reward/lifting_object: 164.5620
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.77s
                      Time elapsed: 00:46:01
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 37253 steps/s (collection: 2.488s, learning 0.150s)
             Mean action noise std: 2.51
          Mean value_function loss: 190.1021
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 60.5608
                       Mean reward: 771.68
               Mean episode length: 213.13
    Episode_Reward/reaching_object: 1.6091
     Episode_Reward/lifting_object: 160.9154
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.64s
                      Time elapsed: 00:46:03
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 37309 steps/s (collection: 2.511s, learning 0.124s)
             Mean action noise std: 2.51
          Mean value_function loss: 177.0682
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 60.5627
                       Mean reward: 850.16
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.6508
     Episode_Reward/lifting_object: 165.8527
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.63s
                      Time elapsed: 00:46:06
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 34963 steps/s (collection: 2.669s, learning 0.142s)
             Mean action noise std: 2.51
          Mean value_function loss: 156.1748
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 60.5645
                       Mean reward: 919.80
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 1.6807
     Episode_Reward/lifting_object: 169.0388
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.81s
                      Time elapsed: 00:46:09
                               ETA: 00:47:13

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 37650 steps/s (collection: 2.486s, learning 0.125s)
             Mean action noise std: 2.51
          Mean value_function loss: 162.3082
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 60.5663
                       Mean reward: 856.92
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.6622
     Episode_Reward/lifting_object: 166.5877
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.61s
                      Time elapsed: 00:46:12
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 37188 steps/s (collection: 2.500s, learning 0.143s)
             Mean action noise std: 2.51
          Mean value_function loss: 192.8484
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 60.5695
                       Mean reward: 825.24
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 1.6794
     Episode_Reward/lifting_object: 168.2272
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.64s
                      Time elapsed: 00:46:14
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 36232 steps/s (collection: 2.569s, learning 0.144s)
             Mean action noise std: 2.51
          Mean value_function loss: 183.5401
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.5739
                       Mean reward: 809.85
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 1.6514
     Episode_Reward/lifting_object: 166.0166
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.71s
                      Time elapsed: 00:46:17
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 35405 steps/s (collection: 2.606s, learning 0.170s)
             Mean action noise std: 2.51
          Mean value_function loss: 189.6881
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.5831
                       Mean reward: 838.03
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 1.6411
     Episode_Reward/lifting_object: 164.9973
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.78s
                      Time elapsed: 00:46:20
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 36215 steps/s (collection: 2.580s, learning 0.134s)
             Mean action noise std: 2.52
          Mean value_function loss: 195.1587
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.5989
                       Mean reward: 790.84
               Mean episode length: 216.04
    Episode_Reward/reaching_object: 1.6299
     Episode_Reward/lifting_object: 163.7094
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.71s
                      Time elapsed: 00:46:22
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 36862 steps/s (collection: 2.516s, learning 0.151s)
             Mean action noise std: 2.52
          Mean value_function loss: 174.6370
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.6123
                       Mean reward: 829.66
               Mean episode length: 225.25
    Episode_Reward/reaching_object: 1.6500
     Episode_Reward/lifting_object: 166.4055
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.67s
                      Time elapsed: 00:46:25
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 36584 steps/s (collection: 2.539s, learning 0.148s)
             Mean action noise std: 2.52
          Mean value_function loss: 164.7907
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.6300
                       Mean reward: 819.40
               Mean episode length: 224.17
    Episode_Reward/reaching_object: 1.6676
     Episode_Reward/lifting_object: 168.0235
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.69s
                      Time elapsed: 00:46:28
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 37012 steps/s (collection: 2.493s, learning 0.163s)
             Mean action noise std: 2.52
          Mean value_function loss: 172.7906
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.6421
                       Mean reward: 827.83
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 1.6285
     Episode_Reward/lifting_object: 163.4222
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.66s
                      Time elapsed: 00:46:30
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 37014 steps/s (collection: 2.537s, learning 0.119s)
             Mean action noise std: 2.52
          Mean value_function loss: 170.3005
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.6593
                       Mean reward: 818.09
               Mean episode length: 222.33
    Episode_Reward/reaching_object: 1.6530
     Episode_Reward/lifting_object: 166.1226
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.66s
                      Time elapsed: 00:46:33
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 37368 steps/s (collection: 2.509s, learning 0.122s)
             Mean action noise std: 2.53
          Mean value_function loss: 172.6428
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.6806
                       Mean reward: 835.34
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.6706
     Episode_Reward/lifting_object: 168.1924
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.63s
                      Time elapsed: 00:46:36
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 36993 steps/s (collection: 2.530s, learning 0.128s)
             Mean action noise std: 2.53
          Mean value_function loss: 192.5181
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.6950
                       Mean reward: 852.67
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 1.6478
     Episode_Reward/lifting_object: 166.0950
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.66s
                      Time elapsed: 00:46:38
                               ETA: 00:46:41

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 12191 steps/s (collection: 7.934s, learning 0.130s)
             Mean action noise std: 2.53
          Mean value_function loss: 162.6127
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.7140
                       Mean reward: 845.55
               Mean episode length: 227.42
    Episode_Reward/reaching_object: 1.6481
     Episode_Reward/lifting_object: 166.4076
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 8.06s
                      Time elapsed: 00:46:46
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 12538 steps/s (collection: 7.694s, learning 0.146s)
             Mean action noise std: 2.53
          Mean value_function loss: 157.4620
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.7355
                       Mean reward: 863.63
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.6803
     Episode_Reward/lifting_object: 169.4999
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.84s
                      Time elapsed: 00:46:54
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 12344 steps/s (collection: 7.843s, learning 0.120s)
             Mean action noise std: 2.53
          Mean value_function loss: 183.7652
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.7533
                       Mean reward: 801.14
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 1.6370
     Episode_Reward/lifting_object: 164.3549
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.96s
                      Time elapsed: 00:47:02
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 12471 steps/s (collection: 7.742s, learning 0.140s)
             Mean action noise std: 2.54
          Mean value_function loss: 151.3718
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.7743
                       Mean reward: 819.17
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 1.6691
     Episode_Reward/lifting_object: 167.8627
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.88s
                      Time elapsed: 00:47:10
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 12586 steps/s (collection: 7.653s, learning 0.157s)
             Mean action noise std: 2.54
          Mean value_function loss: 166.6029
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.8020
                       Mean reward: 871.89
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.6721
     Episode_Reward/lifting_object: 168.4510
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.81s
                      Time elapsed: 00:47:18
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 12006 steps/s (collection: 8.050s, learning 0.138s)
             Mean action noise std: 2.54
          Mean value_function loss: 184.0253
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 60.8213
                       Mean reward: 830.63
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 1.6707
     Episode_Reward/lifting_object: 167.7048
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 8.19s
                      Time elapsed: 00:47:26
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 12253 steps/s (collection: 7.883s, learning 0.140s)
             Mean action noise std: 2.54
          Mean value_function loss: 178.4912
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 60.8238
                       Mean reward: 843.91
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.6751
     Episode_Reward/lifting_object: 168.7132
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 8.02s
                      Time elapsed: 00:47:34
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 12302 steps/s (collection: 7.865s, learning 0.125s)
             Mean action noise std: 2.54
          Mean value_function loss: 189.8739
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.8326
                       Mean reward: 807.18
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 1.6667
     Episode_Reward/lifting_object: 167.2043
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.99s
                      Time elapsed: 00:47:42
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 14454 steps/s (collection: 6.707s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 211.2400
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.8545
                       Mean reward: 836.47
               Mean episode length: 225.27
    Episode_Reward/reaching_object: 1.6395
     Episode_Reward/lifting_object: 164.5158
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.80s
                      Time elapsed: 00:47:49
                               ETA: 00:47:01

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 35924 steps/s (collection: 2.621s, learning 0.115s)
             Mean action noise std: 2.55
          Mean value_function loss: 223.6655
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.8801
                       Mean reward: 815.44
               Mean episode length: 223.04
    Episode_Reward/reaching_object: 1.6521
     Episode_Reward/lifting_object: 165.4947
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.74s
                      Time elapsed: 00:47:52
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 35372 steps/s (collection: 2.662s, learning 0.117s)
             Mean action noise std: 2.55
          Mean value_function loss: 205.0883
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 60.8924
                       Mean reward: 825.51
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 1.6469
     Episode_Reward/lifting_object: 165.7687
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.78s
                      Time elapsed: 00:47:54
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 36776 steps/s (collection: 2.519s, learning 0.154s)
             Mean action noise std: 2.55
          Mean value_function loss: 192.1861
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 60.9117
                       Mean reward: 847.77
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 1.5886
     Episode_Reward/lifting_object: 159.9338
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.67s
                      Time elapsed: 00:47:57
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 36155 steps/s (collection: 2.558s, learning 0.161s)
             Mean action noise std: 2.55
          Mean value_function loss: 176.3604
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.9362
                       Mean reward: 871.75
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 1.6619
     Episode_Reward/lifting_object: 168.5067
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.72s
                      Time elapsed: 00:48:00
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 37566 steps/s (collection: 2.491s, learning 0.126s)
             Mean action noise std: 2.55
          Mean value_function loss: 183.9396
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.9526
                       Mean reward: 826.25
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 1.6237
     Episode_Reward/lifting_object: 163.7693
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.62s
                      Time elapsed: 00:48:02
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 36325 steps/s (collection: 2.574s, learning 0.132s)
             Mean action noise std: 2.55
          Mean value_function loss: 188.3805
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.9631
                       Mean reward: 843.05
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 1.6616
     Episode_Reward/lifting_object: 168.4754
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.71s
                      Time elapsed: 00:48:05
                               ETA: 00:46:43

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 36367 steps/s (collection: 2.553s, learning 0.151s)
             Mean action noise std: 2.56
          Mean value_function loss: 163.3818
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.9818
                       Mean reward: 828.60
               Mean episode length: 224.25
    Episode_Reward/reaching_object: 1.6714
     Episode_Reward/lifting_object: 168.9807
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.70s
                      Time elapsed: 00:48:08
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 36654 steps/s (collection: 2.541s, learning 0.141s)
             Mean action noise std: 2.56
          Mean value_function loss: 168.8842
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.0014
                       Mean reward: 843.38
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.6564
     Episode_Reward/lifting_object: 167.1762
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.68s
                      Time elapsed: 00:48:10
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 36683 steps/s (collection: 2.538s, learning 0.142s)
             Mean action noise std: 2.56
          Mean value_function loss: 154.1504
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.0189
                       Mean reward: 865.47
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.6747
     Episode_Reward/lifting_object: 169.0605
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.68s
                      Time elapsed: 00:48:13
                               ETA: 00:46:34

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 37553 steps/s (collection: 2.501s, learning 0.117s)
             Mean action noise std: 2.56
          Mean value_function loss: 147.8578
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.0386
                       Mean reward: 837.10
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.6331
     Episode_Reward/lifting_object: 164.5272
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.62s
                      Time elapsed: 00:48:16
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 36912 steps/s (collection: 2.517s, learning 0.146s)
             Mean action noise std: 2.56
          Mean value_function loss: 173.0526
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.0674
                       Mean reward: 864.51
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.6900
     Episode_Reward/lifting_object: 170.1728
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.66s
                      Time elapsed: 00:48:18
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 36517 steps/s (collection: 2.555s, learning 0.137s)
             Mean action noise std: 2.57
          Mean value_function loss: 168.8426
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.0861
                       Mean reward: 801.15
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 1.6349
     Episode_Reward/lifting_object: 164.5773
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.69s
                      Time elapsed: 00:48:21
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 37487 steps/s (collection: 2.477s, learning 0.146s)
             Mean action noise std: 2.57
          Mean value_function loss: 178.5580
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.0960
                       Mean reward: 813.93
               Mean episode length: 220.26
    Episode_Reward/reaching_object: 1.6258
     Episode_Reward/lifting_object: 163.6521
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.62s
                      Time elapsed: 00:48:24
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 37241 steps/s (collection: 2.504s, learning 0.136s)
             Mean action noise std: 2.57
          Mean value_function loss: 164.0103
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.1067
                       Mean reward: 838.03
               Mean episode length: 227.52
    Episode_Reward/reaching_object: 1.6483
     Episode_Reward/lifting_object: 165.2659
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.64s
                      Time elapsed: 00:48:26
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 36581 steps/s (collection: 2.524s, learning 0.164s)
             Mean action noise std: 2.57
          Mean value_function loss: 178.3225
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.1201
                       Mean reward: 846.38
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.6548
     Episode_Reward/lifting_object: 166.0679
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.69s
                      Time elapsed: 00:48:29
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 37329 steps/s (collection: 2.517s, learning 0.117s)
             Mean action noise std: 2.57
          Mean value_function loss: 163.8060
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.1429
                       Mean reward: 832.48
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.6691
     Episode_Reward/lifting_object: 168.0911
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.63s
                      Time elapsed: 00:48:32
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 37329 steps/s (collection: 2.503s, learning 0.130s)
             Mean action noise std: 2.58
          Mean value_function loss: 183.8389
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.1674
                       Mean reward: 837.74
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 1.6414
     Episode_Reward/lifting_object: 165.6227
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.63s
                      Time elapsed: 00:48:34
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 36241 steps/s (collection: 2.562s, learning 0.150s)
             Mean action noise std: 2.58
          Mean value_function loss: 170.2982
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.1914
                       Mean reward: 852.97
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 1.6683
     Episode_Reward/lifting_object: 167.7082
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.71s
                      Time elapsed: 00:48:37
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 35586 steps/s (collection: 2.610s, learning 0.152s)
             Mean action noise std: 2.58
          Mean value_function loss: 161.9797
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.2087
                       Mean reward: 844.17
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 1.6893
     Episode_Reward/lifting_object: 170.6433
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.76s
                      Time elapsed: 00:48:40
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 36467 steps/s (collection: 2.544s, learning 0.152s)
             Mean action noise std: 2.58
          Mean value_function loss: 204.1611
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.2261
                       Mean reward: 855.83
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 1.6810
     Episode_Reward/lifting_object: 169.1870
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.70s
                      Time elapsed: 00:48:43
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 36213 steps/s (collection: 2.576s, learning 0.138s)
             Mean action noise std: 2.58
          Mean value_function loss: 219.8473
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.2413
                       Mean reward: 818.83
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 1.6423
     Episode_Reward/lifting_object: 165.1611
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.71s
                      Time elapsed: 00:48:45
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 36398 steps/s (collection: 2.544s, learning 0.157s)
             Mean action noise std: 2.58
          Mean value_function loss: 217.4933
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.2570
                       Mean reward: 808.88
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.6582
     Episode_Reward/lifting_object: 166.9508
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.70s
                      Time elapsed: 00:48:48
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 37679 steps/s (collection: 2.486s, learning 0.123s)
             Mean action noise std: 2.58
          Mean value_function loss: 169.3452
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.2685
                       Mean reward: 905.80
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.6488
     Episode_Reward/lifting_object: 167.0733
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.61s
                      Time elapsed: 00:48:51
                               ETA: 00:45:52

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 37235 steps/s (collection: 2.484s, learning 0.156s)
             Mean action noise std: 2.59
          Mean value_function loss: 182.0641
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.2774
                       Mean reward: 803.98
               Mean episode length: 219.69
    Episode_Reward/reaching_object: 1.6313
     Episode_Reward/lifting_object: 164.5865
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.64s
                      Time elapsed: 00:48:53
                               ETA: 00:45:49

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 36422 steps/s (collection: 2.561s, learning 0.138s)
             Mean action noise std: 2.59
          Mean value_function loss: 188.6307
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.2955
                       Mean reward: 849.78
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 1.6859
     Episode_Reward/lifting_object: 170.7151
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.70s
                      Time elapsed: 00:48:56
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 37785 steps/s (collection: 2.462s, learning 0.140s)
             Mean action noise std: 2.59
          Mean value_function loss: 237.6602
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.3203
                       Mean reward: 809.89
               Mean episode length: 219.96
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 159.3750
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.60s
                      Time elapsed: 00:48:58
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 36879 steps/s (collection: 2.545s, learning 0.121s)
             Mean action noise std: 2.59
          Mean value_function loss: 242.6150
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.3377
                       Mean reward: 827.34
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.6106
     Episode_Reward/lifting_object: 162.8179
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.67s
                      Time elapsed: 00:49:01
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 37082 steps/s (collection: 2.510s, learning 0.141s)
             Mean action noise std: 2.59
          Mean value_function loss: 187.5488
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.3528
                       Mean reward: 839.32
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.6206
     Episode_Reward/lifting_object: 164.3663
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.65s
                      Time elapsed: 00:49:04
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 37872 steps/s (collection: 2.484s, learning 0.111s)
             Mean action noise std: 2.60
          Mean value_function loss: 183.3267
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.3729
                       Mean reward: 831.85
               Mean episode length: 226.29
    Episode_Reward/reaching_object: 1.6167
     Episode_Reward/lifting_object: 164.2495
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.60s
                      Time elapsed: 00:49:06
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 37027 steps/s (collection: 2.499s, learning 0.156s)
             Mean action noise std: 2.60
          Mean value_function loss: 214.6367
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.3863
                       Mean reward: 819.19
               Mean episode length: 222.35
    Episode_Reward/reaching_object: 1.6189
     Episode_Reward/lifting_object: 162.9852
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.65s
                      Time elapsed: 00:49:09
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 36219 steps/s (collection: 2.572s, learning 0.142s)
             Mean action noise std: 2.60
          Mean value_function loss: 153.2539
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 61.4002
                       Mean reward: 864.85
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.6686
     Episode_Reward/lifting_object: 169.9032
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.71s
                      Time elapsed: 00:49:12
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 38139 steps/s (collection: 2.427s, learning 0.150s)
             Mean action noise std: 2.60
          Mean value_function loss: 175.7076
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.4180
                       Mean reward: 829.03
               Mean episode length: 225.79
    Episode_Reward/reaching_object: 1.6382
     Episode_Reward/lifting_object: 166.2983
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.58s
                      Time elapsed: 00:49:14
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 37230 steps/s (collection: 2.488s, learning 0.152s)
             Mean action noise std: 2.60
          Mean value_function loss: 152.0130
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 61.4353
                       Mean reward: 880.40
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.6757
     Episode_Reward/lifting_object: 170.7696
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.64s
                      Time elapsed: 00:49:17
                               ETA: 00:45:21

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 38679 steps/s (collection: 2.418s, learning 0.123s)
             Mean action noise std: 2.60
          Mean value_function loss: 213.5918
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.4575
                       Mean reward: 803.68
               Mean episode length: 220.34
    Episode_Reward/reaching_object: 1.6297
     Episode_Reward/lifting_object: 164.8112
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.54s
                      Time elapsed: 00:49:20
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 37808 steps/s (collection: 2.455s, learning 0.145s)
             Mean action noise std: 2.61
          Mean value_function loss: 179.8108
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 61.4727
                       Mean reward: 838.00
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.6389
     Episode_Reward/lifting_object: 166.0670
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.60s
                      Time elapsed: 00:49:22
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 37711 steps/s (collection: 2.483s, learning 0.123s)
             Mean action noise std: 2.61
          Mean value_function loss: 206.1544
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 61.4892
                       Mean reward: 857.36
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 1.6230
     Episode_Reward/lifting_object: 164.9558
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.61s
                      Time elapsed: 00:49:25
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 36790 steps/s (collection: 2.556s, learning 0.116s)
             Mean action noise std: 2.61
          Mean value_function loss: 198.9744
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.5074
                       Mean reward: 788.06
               Mean episode length: 215.41
    Episode_Reward/reaching_object: 1.6187
     Episode_Reward/lifting_object: 163.9041
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.67s
                      Time elapsed: 00:49:27
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 37210 steps/s (collection: 2.507s, learning 0.135s)
             Mean action noise std: 2.61
          Mean value_function loss: 149.7243
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.5238
                       Mean reward: 822.91
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.6349
     Episode_Reward/lifting_object: 166.4905
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.64s
                      Time elapsed: 00:49:30
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 37248 steps/s (collection: 2.499s, learning 0.140s)
             Mean action noise std: 2.62
          Mean value_function loss: 183.7287
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.5490
                       Mean reward: 791.00
               Mean episode length: 216.47
    Episode_Reward/reaching_object: 1.6393
     Episode_Reward/lifting_object: 166.7089
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.64s
                      Time elapsed: 00:49:33
                               ETA: 00:45:03

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 35492 steps/s (collection: 2.637s, learning 0.133s)
             Mean action noise std: 2.62
          Mean value_function loss: 215.2301
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.5764
                       Mean reward: 817.09
               Mean episode length: 221.09
    Episode_Reward/reaching_object: 1.6241
     Episode_Reward/lifting_object: 164.8840
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.77s
                      Time elapsed: 00:49:35
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 36439 steps/s (collection: 2.557s, learning 0.141s)
             Mean action noise std: 2.62
          Mean value_function loss: 164.1807
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.5927
                       Mean reward: 868.04
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 1.6366
     Episode_Reward/lifting_object: 166.6693
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.70s
                      Time elapsed: 00:49:38
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 36314 steps/s (collection: 2.552s, learning 0.155s)
             Mean action noise std: 2.62
          Mean value_function loss: 156.1318
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.6099
                       Mean reward: 839.74
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.6359
     Episode_Reward/lifting_object: 167.1785
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.71s
                      Time elapsed: 00:49:41
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 36055 steps/s (collection: 2.561s, learning 0.166s)
             Mean action noise std: 2.62
          Mean value_function loss: 163.4309
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.6275
                       Mean reward: 818.30
               Mean episode length: 220.96
    Episode_Reward/reaching_object: 1.6630
     Episode_Reward/lifting_object: 169.7025
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.73s
                      Time elapsed: 00:49:44
                               ETA: 00:44:51

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 37114 steps/s (collection: 2.500s, learning 0.149s)
             Mean action noise std: 2.62
          Mean value_function loss: 163.4128
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.6483
                       Mean reward: 893.75
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 1.6843
     Episode_Reward/lifting_object: 171.8406
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.65s
                      Time elapsed: 00:49:46
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 35644 steps/s (collection: 2.598s, learning 0.160s)
             Mean action noise std: 2.63
          Mean value_function loss: 207.4080
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.6664
                       Mean reward: 771.68
               Mean episode length: 208.98
    Episode_Reward/reaching_object: 1.6128
     Episode_Reward/lifting_object: 164.2575
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.76s
                      Time elapsed: 00:49:49
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 36127 steps/s (collection: 2.569s, learning 0.152s)
             Mean action noise std: 2.63
          Mean value_function loss: 175.4320
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.6836
                       Mean reward: 862.26
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.6872
     Episode_Reward/lifting_object: 172.1202
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.72s
                      Time elapsed: 00:49:52
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 35889 steps/s (collection: 2.617s, learning 0.122s)
             Mean action noise std: 2.63
          Mean value_function loss: 220.8306
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.7029
                       Mean reward: 855.51
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.6599
     Episode_Reward/lifting_object: 168.5544
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.74s
                      Time elapsed: 00:49:54
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 36345 steps/s (collection: 2.536s, learning 0.169s)
             Mean action noise std: 2.63
          Mean value_function loss: 183.3579
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.7265
                       Mean reward: 852.61
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 1.6139
     Episode_Reward/lifting_object: 163.8396
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.70s
                      Time elapsed: 00:49:57
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 36904 steps/s (collection: 2.506s, learning 0.158s)
             Mean action noise std: 2.63
          Mean value_function loss: 204.9961
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.7409
                       Mean reward: 835.52
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.6363
     Episode_Reward/lifting_object: 166.5427
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.66s
                      Time elapsed: 00:50:00
                               ETA: 00:44:34

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 35991 steps/s (collection: 2.591s, learning 0.141s)
             Mean action noise std: 2.64
          Mean value_function loss: 129.1910
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.7562
                       Mean reward: 882.31
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.6371
     Episode_Reward/lifting_object: 165.9849
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.73s
                      Time elapsed: 00:50:03
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 36883 steps/s (collection: 2.511s, learning 0.154s)
             Mean action noise std: 2.64
          Mean value_function loss: 162.1682
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.7733
                       Mean reward: 835.30
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 1.6504
     Episode_Reward/lifting_object: 168.0356
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.67s
                      Time elapsed: 00:50:05
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 38095 steps/s (collection: 2.440s, learning 0.140s)
             Mean action noise std: 2.64
          Mean value_function loss: 141.0107
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 61.7897
                       Mean reward: 868.90
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.6899
     Episode_Reward/lifting_object: 172.1289
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.58s
                      Time elapsed: 00:50:08
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 37182 steps/s (collection: 2.492s, learning 0.152s)
             Mean action noise std: 2.64
          Mean value_function loss: 162.0100
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.8080
                       Mean reward: 839.81
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.6511
     Episode_Reward/lifting_object: 167.3766
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.64s
                      Time elapsed: 00:50:10
                               ETA: 00:44:22

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 36471 steps/s (collection: 2.536s, learning 0.159s)
             Mean action noise std: 2.64
          Mean value_function loss: 162.3807
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.8276
                       Mean reward: 878.16
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.6639
     Episode_Reward/lifting_object: 169.2785
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.70s
                      Time elapsed: 00:50:13
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 36718 steps/s (collection: 2.541s, learning 0.136s)
             Mean action noise std: 2.65
          Mean value_function loss: 161.6762
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.8445
                       Mean reward: 840.83
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.6975
     Episode_Reward/lifting_object: 173.1407
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.68s
                      Time elapsed: 00:50:16
                               ETA: 00:44:16

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 36530 steps/s (collection: 2.568s, learning 0.123s)
             Mean action noise std: 2.65
          Mean value_function loss: 184.4015
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.8678
                       Mean reward: 821.51
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 1.5990
     Episode_Reward/lifting_object: 162.0710
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.69s
                      Time elapsed: 00:50:19
                               ETA: 00:44:13

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 37510 steps/s (collection: 2.490s, learning 0.130s)
             Mean action noise std: 2.65
          Mean value_function loss: 192.2685
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.8895
                       Mean reward: 860.72
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.6486
     Episode_Reward/lifting_object: 167.7033
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.62s
                      Time elapsed: 00:50:21
                               ETA: 00:44:10

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 37307 steps/s (collection: 2.508s, learning 0.127s)
             Mean action noise std: 2.65
          Mean value_function loss: 160.7659
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.9124
                       Mean reward: 846.63
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 1.6877
     Episode_Reward/lifting_object: 172.1585
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.63s
                      Time elapsed: 00:50:24
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 37400 steps/s (collection: 2.491s, learning 0.137s)
             Mean action noise std: 2.65
          Mean value_function loss: 159.3897
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.9271
                       Mean reward: 828.46
               Mean episode length: 222.60
    Episode_Reward/reaching_object: 1.6747
     Episode_Reward/lifting_object: 171.0741
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.63s
                      Time elapsed: 00:50:26
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 37108 steps/s (collection: 2.511s, learning 0.139s)
             Mean action noise std: 2.66
          Mean value_function loss: 154.5214
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.9496
                       Mean reward: 840.15
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.6458
     Episode_Reward/lifting_object: 167.5632
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.65s
                      Time elapsed: 00:50:29
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 37382 steps/s (collection: 2.497s, learning 0.133s)
             Mean action noise std: 2.66
          Mean value_function loss: 166.7405
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.9765
                       Mean reward: 854.29
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.6664
     Episode_Reward/lifting_object: 169.1962
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.63s
                      Time elapsed: 00:50:32
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 36956 steps/s (collection: 2.522s, learning 0.138s)
             Mean action noise std: 2.66
          Mean value_function loss: 180.2332
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.9966
                       Mean reward: 850.56
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.6447
     Episode_Reward/lifting_object: 167.0156
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.66s
                      Time elapsed: 00:50:34
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 36737 steps/s (collection: 2.548s, learning 0.128s)
             Mean action noise std: 2.66
          Mean value_function loss: 190.4013
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.0132
                       Mean reward: 844.60
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.6489
     Episode_Reward/lifting_object: 167.9604
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.68s
                      Time elapsed: 00:50:37
                               ETA: 00:43:52

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 37419 steps/s (collection: 2.488s, learning 0.139s)
             Mean action noise std: 2.66
          Mean value_function loss: 193.3566
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.0297
                       Mean reward: 851.48
               Mean episode length: 229.08
    Episode_Reward/reaching_object: 1.6397
     Episode_Reward/lifting_object: 167.1557
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.63s
                      Time elapsed: 00:50:40
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 37589 steps/s (collection: 2.489s, learning 0.126s)
             Mean action noise std: 2.67
          Mean value_function loss: 191.4335
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.0452
                       Mean reward: 814.50
               Mean episode length: 219.72
    Episode_Reward/reaching_object: 1.6066
     Episode_Reward/lifting_object: 163.4846
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.62s
                      Time elapsed: 00:50:42
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 37790 steps/s (collection: 2.490s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 187.8980
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.0621
                       Mean reward: 854.10
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 1.6258
     Episode_Reward/lifting_object: 165.3047
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.60s
                      Time elapsed: 00:50:45
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 37991 steps/s (collection: 2.449s, learning 0.138s)
             Mean action noise std: 2.67
          Mean value_function loss: 163.6567
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.0783
                       Mean reward: 859.33
               Mean episode length: 232.05
    Episode_Reward/reaching_object: 1.6744
     Episode_Reward/lifting_object: 171.3051
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.59s
                      Time elapsed: 00:50:47
                               ETA: 00:43:40

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 37547 steps/s (collection: 2.486s, learning 0.132s)
             Mean action noise std: 2.67
          Mean value_function loss: 179.1774
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.1038
                       Mean reward: 851.92
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.6306
     Episode_Reward/lifting_object: 166.2474
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.62s
                      Time elapsed: 00:50:50
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 37642 steps/s (collection: 2.481s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 142.1588
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.1193
                       Mean reward: 848.36
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 1.6378
     Episode_Reward/lifting_object: 167.2291
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.61s
                      Time elapsed: 00:50:53
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 36416 steps/s (collection: 2.574s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 144.9352
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.1335
                       Mean reward: 856.35
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 1.6200
     Episode_Reward/lifting_object: 165.9123
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.70s
                      Time elapsed: 00:50:55
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 36873 steps/s (collection: 2.522s, learning 0.144s)
             Mean action noise std: 2.68
          Mean value_function loss: 173.1970
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.1509
                       Mean reward: 852.56
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.6519
     Episode_Reward/lifting_object: 169.1492
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.67s
                      Time elapsed: 00:50:58
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 36012 steps/s (collection: 2.591s, learning 0.139s)
             Mean action noise std: 2.68
          Mean value_function loss: 145.1235
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.1696
                       Mean reward: 867.28
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 1.6582
     Episode_Reward/lifting_object: 168.8631
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.73s
                      Time elapsed: 00:51:01
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 37547 steps/s (collection: 2.474s, learning 0.145s)
             Mean action noise std: 2.68
          Mean value_function loss: 144.2856
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.1904
                       Mean reward: 870.26
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 1.6814
     Episode_Reward/lifting_object: 171.6164
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.62s
                      Time elapsed: 00:51:03
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 36683 steps/s (collection: 2.540s, learning 0.140s)
             Mean action noise std: 2.69
          Mean value_function loss: 139.6101
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.2203
                       Mean reward: 854.11
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 1.6773
     Episode_Reward/lifting_object: 171.3649
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.68s
                      Time elapsed: 00:51:06
                               ETA: 00:43:19

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 36618 steps/s (collection: 2.524s, learning 0.160s)
             Mean action noise std: 2.69
          Mean value_function loss: 159.6511
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.2439
                       Mean reward: 848.21
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 1.6837
     Episode_Reward/lifting_object: 171.5927
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.68s
                      Time elapsed: 00:51:09
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 37652 steps/s (collection: 2.501s, learning 0.110s)
             Mean action noise std: 2.69
          Mean value_function loss: 145.5326
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.2621
                       Mean reward: 837.32
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 1.6389
     Episode_Reward/lifting_object: 166.3654
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.61s
                      Time elapsed: 00:51:11
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 36161 steps/s (collection: 2.582s, learning 0.136s)
             Mean action noise std: 2.69
          Mean value_function loss: 164.2902
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 62.2775
                       Mean reward: 828.38
               Mean episode length: 224.48
    Episode_Reward/reaching_object: 1.6564
     Episode_Reward/lifting_object: 168.1841
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.72s
                      Time elapsed: 00:51:14
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 36639 steps/s (collection: 2.516s, learning 0.167s)
             Mean action noise std: 2.69
          Mean value_function loss: 200.5170
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.2984
                       Mean reward: 856.99
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.6365
     Episode_Reward/lifting_object: 166.3167
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.68s
                      Time elapsed: 00:51:17
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 37110 steps/s (collection: 2.504s, learning 0.145s)
             Mean action noise std: 2.70
          Mean value_function loss: 127.9577
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.3210
                       Mean reward: 866.69
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 1.6816
     Episode_Reward/lifting_object: 171.6280
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.65s
                      Time elapsed: 00:51:19
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 37194 steps/s (collection: 2.495s, learning 0.148s)
             Mean action noise std: 2.70
          Mean value_function loss: 168.0582
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.3493
                       Mean reward: 831.50
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.6255
     Episode_Reward/lifting_object: 165.8100
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.64s
                      Time elapsed: 00:51:22
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 37291 steps/s (collection: 2.496s, learning 0.141s)
             Mean action noise std: 2.70
          Mean value_function loss: 181.6007
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.3637
                       Mean reward: 830.63
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 1.6382
     Episode_Reward/lifting_object: 166.9910
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.64s
                      Time elapsed: 00:51:25
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 37254 steps/s (collection: 2.517s, learning 0.122s)
             Mean action noise std: 2.70
          Mean value_function loss: 161.4978
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.3743
                       Mean reward: 847.51
               Mean episode length: 228.12
    Episode_Reward/reaching_object: 1.6344
     Episode_Reward/lifting_object: 166.7237
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.64s
                      Time elapsed: 00:51:27
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 37595 steps/s (collection: 2.477s, learning 0.138s)
             Mean action noise std: 2.70
          Mean value_function loss: 163.6774
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.3906
                       Mean reward: 814.25
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 1.6157
     Episode_Reward/lifting_object: 165.2512
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.61s
                      Time elapsed: 00:51:30
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 36590 steps/s (collection: 2.543s, learning 0.143s)
             Mean action noise std: 2.70
          Mean value_function loss: 187.6753
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 62.4005
                       Mean reward: 833.56
               Mean episode length: 224.15
    Episode_Reward/reaching_object: 1.6435
     Episode_Reward/lifting_object: 168.0296
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.69s
                      Time elapsed: 00:51:33
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 36895 steps/s (collection: 2.525s, learning 0.139s)
             Mean action noise std: 2.71
          Mean value_function loss: 172.3788
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.4139
                       Mean reward: 830.37
               Mean episode length: 223.74
    Episode_Reward/reaching_object: 1.6186
     Episode_Reward/lifting_object: 165.4056
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.66s
                      Time elapsed: 00:51:35
                               ETA: 00:42:46

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 36213 steps/s (collection: 2.585s, learning 0.129s)
             Mean action noise std: 2.71
          Mean value_function loss: 203.8212
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.4294
                       Mean reward: 847.74
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 1.6415
     Episode_Reward/lifting_object: 167.4438
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.71s
                      Time elapsed: 00:51:38
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 37115 steps/s (collection: 2.504s, learning 0.145s)
             Mean action noise std: 2.71
          Mean value_function loss: 172.5276
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.4450
                       Mean reward: 809.36
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 1.6139
     Episode_Reward/lifting_object: 165.4618
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.65s
                      Time elapsed: 00:51:41
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 36976 steps/s (collection: 2.511s, learning 0.148s)
             Mean action noise std: 2.71
          Mean value_function loss: 177.2933
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.4528
                       Mean reward: 865.08
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.6309
     Episode_Reward/lifting_object: 167.1173
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.66s
                      Time elapsed: 00:51:43
                               ETA: 00:42:37

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 35981 steps/s (collection: 2.598s, learning 0.134s)
             Mean action noise std: 2.71
          Mean value_function loss: 157.1755
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.4678
                       Mean reward: 834.24
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 1.6355
     Episode_Reward/lifting_object: 167.5187
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.73s
                      Time elapsed: 00:51:46
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 36546 steps/s (collection: 2.556s, learning 0.134s)
             Mean action noise std: 2.72
          Mean value_function loss: 148.4624
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.4897
                       Mean reward: 835.59
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 1.6637
     Episode_Reward/lifting_object: 171.0019
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.69s
                      Time elapsed: 00:51:49
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 35979 steps/s (collection: 2.607s, learning 0.126s)
             Mean action noise std: 2.72
          Mean value_function loss: 152.8344
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.5191
                       Mean reward: 861.91
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.6428
     Episode_Reward/lifting_object: 168.6798
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.73s
                      Time elapsed: 00:51:51
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 36345 steps/s (collection: 2.582s, learning 0.123s)
             Mean action noise std: 2.72
          Mean value_function loss: 171.2922
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.5431
                       Mean reward: 869.59
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 1.6642
     Episode_Reward/lifting_object: 170.6911
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.70s
                      Time elapsed: 00:51:54
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 36304 steps/s (collection: 2.571s, learning 0.136s)
             Mean action noise std: 2.72
          Mean value_function loss: 210.1159
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 62.5658
                       Mean reward: 841.47
               Mean episode length: 228.98
    Episode_Reward/reaching_object: 1.6090
     Episode_Reward/lifting_object: 164.9090
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.71s
                      Time elapsed: 00:51:57
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 36866 steps/s (collection: 2.526s, learning 0.140s)
             Mean action noise std: 2.72
          Mean value_function loss: 193.5685
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.5850
                       Mean reward: 817.07
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 1.6071
     Episode_Reward/lifting_object: 164.3557
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.67s
                      Time elapsed: 00:52:00
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 36154 steps/s (collection: 2.577s, learning 0.142s)
             Mean action noise std: 2.73
          Mean value_function loss: 166.7654
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.6026
                       Mean reward: 803.36
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 1.6106
     Episode_Reward/lifting_object: 163.3393
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.72s
                      Time elapsed: 00:52:02
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 36329 steps/s (collection: 2.580s, learning 0.126s)
             Mean action noise std: 2.73
          Mean value_function loss: 200.9466
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.6196
                       Mean reward: 797.29
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 1.6032
     Episode_Reward/lifting_object: 164.1111
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.71s
                      Time elapsed: 00:52:05
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 37441 steps/s (collection: 2.510s, learning 0.115s)
             Mean action noise std: 2.73
          Mean value_function loss: 163.5579
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.6392
                       Mean reward: 817.76
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 1.6447
     Episode_Reward/lifting_object: 168.2303
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.63s
                      Time elapsed: 00:52:08
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 35867 steps/s (collection: 2.581s, learning 0.160s)
             Mean action noise std: 2.73
          Mean value_function loss: 200.3140
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.6572
                       Mean reward: 827.29
               Mean episode length: 223.33
    Episode_Reward/reaching_object: 1.6098
     Episode_Reward/lifting_object: 163.3779
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.74s
                      Time elapsed: 00:52:10
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 37478 steps/s (collection: 2.471s, learning 0.152s)
             Mean action noise std: 2.74
          Mean value_function loss: 144.9075
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.6815
                       Mean reward: 857.70
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.6752
     Episode_Reward/lifting_object: 171.6909
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.62s
                      Time elapsed: 00:52:13
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 38400 steps/s (collection: 2.444s, learning 0.116s)
             Mean action noise std: 2.74
          Mean value_function loss: 167.1941
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.7049
                       Mean reward: 805.10
               Mean episode length: 220.14
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 163.6340
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.56s
                      Time elapsed: 00:52:16
                               ETA: 00:42:02

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 37269 steps/s (collection: 2.472s, learning 0.165s)
             Mean action noise std: 2.74
          Mean value_function loss: 140.3085
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.7210
                       Mean reward: 848.65
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.6388
     Episode_Reward/lifting_object: 168.0819
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.64s
                      Time elapsed: 00:52:18
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 38221 steps/s (collection: 2.446s, learning 0.126s)
             Mean action noise std: 2.74
          Mean value_function loss: 136.0319
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.7419
                       Mean reward: 866.17
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.6416
     Episode_Reward/lifting_object: 167.4364
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.57s
                      Time elapsed: 00:52:21
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 37712 steps/s (collection: 2.466s, learning 0.141s)
             Mean action noise std: 2.74
          Mean value_function loss: 180.1719
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.7643
                       Mean reward: 832.65
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.6554
     Episode_Reward/lifting_object: 168.6682
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.61s
                      Time elapsed: 00:52:23
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 38420 steps/s (collection: 2.436s, learning 0.123s)
             Mean action noise std: 2.75
          Mean value_function loss: 160.3487
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.7772
                       Mean reward: 862.03
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.6436
     Episode_Reward/lifting_object: 166.8369
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.56s
                      Time elapsed: 00:52:26
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 37283 steps/s (collection: 2.471s, learning 0.166s)
             Mean action noise std: 2.75
          Mean value_function loss: 184.2570
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.7965
                       Mean reward: 820.08
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 1.6166
     Episode_Reward/lifting_object: 163.6848
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.64s
                      Time elapsed: 00:52:29
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 36957 steps/s (collection: 2.516s, learning 0.144s)
             Mean action noise std: 2.75
          Mean value_function loss: 174.8261
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.8136
                       Mean reward: 869.56
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.6367
     Episode_Reward/lifting_object: 167.0589
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.66s
                      Time elapsed: 00:52:31
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 37291 steps/s (collection: 2.504s, learning 0.132s)
             Mean action noise std: 2.75
          Mean value_function loss: 167.2506
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.8274
                       Mean reward: 844.38
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.6183
     Episode_Reward/lifting_object: 164.3726
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.64s
                      Time elapsed: 00:52:34
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 36219 steps/s (collection: 2.561s, learning 0.153s)
             Mean action noise std: 2.75
          Mean value_function loss: 180.2234
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.8475
                       Mean reward: 865.80
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 1.6372
     Episode_Reward/lifting_object: 167.5715
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.71s
                      Time elapsed: 00:52:37
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 36496 steps/s (collection: 2.538s, learning 0.156s)
             Mean action noise std: 2.75
          Mean value_function loss: 138.4145
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.8612
                       Mean reward: 871.27
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.6524
     Episode_Reward/lifting_object: 169.1081
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.69s
                      Time elapsed: 00:52:39
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 37346 steps/s (collection: 2.511s, learning 0.121s)
             Mean action noise std: 2.76
          Mean value_function loss: 170.3870
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 62.8769
                       Mean reward: 810.29
               Mean episode length: 218.67
    Episode_Reward/reaching_object: 1.6097
     Episode_Reward/lifting_object: 165.1147
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.63s
                      Time elapsed: 00:52:42
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 36912 steps/s (collection: 2.529s, learning 0.134s)
             Mean action noise std: 2.76
          Mean value_function loss: 186.7985
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9069
                       Mean reward: 840.91
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 1.6055
     Episode_Reward/lifting_object: 163.8871
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.66s
                      Time elapsed: 00:52:45
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 38491 steps/s (collection: 2.420s, learning 0.134s)
             Mean action noise std: 2.76
          Mean value_function loss: 172.0196
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.9305
                       Mean reward: 839.19
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.5802
     Episode_Reward/lifting_object: 161.5148
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.55s
                      Time elapsed: 00:52:47
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 38016 steps/s (collection: 2.452s, learning 0.134s)
             Mean action noise std: 2.76
          Mean value_function loss: 171.9471
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.9449
                       Mean reward: 877.18
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.6693
     Episode_Reward/lifting_object: 171.3256
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.59s
                      Time elapsed: 00:52:50
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 37417 steps/s (collection: 2.467s, learning 0.160s)
             Mean action noise std: 2.76
          Mean value_function loss: 176.9742
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.9541
                       Mean reward: 838.51
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 1.5779
     Episode_Reward/lifting_object: 161.5286
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.63s
                      Time elapsed: 00:52:52
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 36419 steps/s (collection: 2.559s, learning 0.140s)
             Mean action noise std: 2.77
          Mean value_function loss: 173.0771
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.9645
                       Mean reward: 813.01
               Mean episode length: 221.96
    Episode_Reward/reaching_object: 1.6392
     Episode_Reward/lifting_object: 167.4282
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.70s
                      Time elapsed: 00:52:55
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 38134 steps/s (collection: 2.448s, learning 0.130s)
             Mean action noise std: 2.77
          Mean value_function loss: 157.9740
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9788
                       Mean reward: 867.41
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.6623
     Episode_Reward/lifting_object: 170.6849
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.58s
                      Time elapsed: 00:52:58
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 35452 steps/s (collection: 2.613s, learning 0.160s)
             Mean action noise std: 2.77
          Mean value_function loss: 161.7663
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 62.9974
                       Mean reward: 843.24
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 1.6442
     Episode_Reward/lifting_object: 168.5440
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.77s
                      Time elapsed: 00:53:00
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 37530 steps/s (collection: 2.482s, learning 0.137s)
             Mean action noise std: 2.77
          Mean value_function loss: 164.7802
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.0180
                       Mean reward: 813.81
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 1.6084
     Episode_Reward/lifting_object: 164.6163
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.62s
                      Time elapsed: 00:53:03
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 38017 steps/s (collection: 2.467s, learning 0.119s)
             Mean action noise std: 2.77
          Mean value_function loss: 154.9425
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.0301
                       Mean reward: 858.29
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 1.6305
     Episode_Reward/lifting_object: 168.0535
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.59s
                      Time elapsed: 00:53:06
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 37793 steps/s (collection: 2.452s, learning 0.150s)
             Mean action noise std: 2.77
          Mean value_function loss: 148.4199
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.0434
                       Mean reward: 862.55
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 1.6410
     Episode_Reward/lifting_object: 168.3627
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.60s
                      Time elapsed: 00:53:08
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 37433 steps/s (collection: 2.511s, learning 0.116s)
             Mean action noise std: 2.78
          Mean value_function loss: 158.3088
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 63.0653
                       Mean reward: 863.16
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.6360
     Episode_Reward/lifting_object: 168.0297
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.63s
                      Time elapsed: 00:53:11
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 37488 steps/s (collection: 2.471s, learning 0.151s)
             Mean action noise std: 2.78
          Mean value_function loss: 171.4027
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.0900
                       Mean reward: 820.01
               Mean episode length: 221.49
    Episode_Reward/reaching_object: 1.6150
     Episode_Reward/lifting_object: 165.6032
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.62s
                      Time elapsed: 00:53:13
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 38449 steps/s (collection: 2.432s, learning 0.125s)
             Mean action noise std: 2.78
          Mean value_function loss: 164.2729
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.1016
                       Mean reward: 818.12
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 1.6550
     Episode_Reward/lifting_object: 169.8900
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.56s
                      Time elapsed: 00:53:16
                               ETA: 00:40:53

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 37087 steps/s (collection: 2.501s, learning 0.150s)
             Mean action noise std: 2.78
          Mean value_function loss: 181.0763
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.1188
                       Mean reward: 837.99
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 1.6305
     Episode_Reward/lifting_object: 167.3182
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.65s
                      Time elapsed: 00:53:19
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 36898 steps/s (collection: 2.521s, learning 0.144s)
             Mean action noise std: 2.78
          Mean value_function loss: 173.7646
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.1349
                       Mean reward: 786.46
               Mean episode length: 216.30
    Episode_Reward/reaching_object: 1.6136
     Episode_Reward/lifting_object: 165.2158
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.66s
                      Time elapsed: 00:53:21
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 36751 steps/s (collection: 2.556s, learning 0.119s)
             Mean action noise std: 2.78
          Mean value_function loss: 191.2127
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.1404
                       Mean reward: 785.88
               Mean episode length: 216.16
    Episode_Reward/reaching_object: 1.6254
     Episode_Reward/lifting_object: 166.4571
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.67s
                      Time elapsed: 00:53:24
                               ETA: 00:40:44

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 36147 steps/s (collection: 2.564s, learning 0.156s)
             Mean action noise std: 2.79
          Mean value_function loss: 195.0680
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.1543
                       Mean reward: 849.67
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 1.5933
     Episode_Reward/lifting_object: 163.6957
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.72s
                      Time elapsed: 00:53:27
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 36130 steps/s (collection: 2.574s, learning 0.147s)
             Mean action noise std: 2.79
          Mean value_function loss: 197.2121
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.1681
                       Mean reward: 806.75
               Mean episode length: 219.07
    Episode_Reward/reaching_object: 1.6036
     Episode_Reward/lifting_object: 164.4790
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.72s
                      Time elapsed: 00:53:29
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 35034 steps/s (collection: 2.661s, learning 0.144s)
             Mean action noise std: 2.79
          Mean value_function loss: 156.4363
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 63.1843
                       Mean reward: 868.54
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.5916
     Episode_Reward/lifting_object: 164.1626
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.81s
                      Time elapsed: 00:53:32
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 35581 steps/s (collection: 2.607s, learning 0.156s)
             Mean action noise std: 2.79
          Mean value_function loss: 172.6865
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.2037
                       Mean reward: 824.54
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 1.5770
     Episode_Reward/lifting_object: 162.1029
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.76s
                      Time elapsed: 00:53:35
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 34492 steps/s (collection: 2.697s, learning 0.153s)
             Mean action noise std: 2.79
          Mean value_function loss: 166.2474
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.2143
                       Mean reward: 830.06
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 1.6335
     Episode_Reward/lifting_object: 168.6563
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.85s
                      Time elapsed: 00:53:38
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 35415 steps/s (collection: 2.625s, learning 0.150s)
             Mean action noise std: 2.79
          Mean value_function loss: 145.2474
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.2268
                       Mean reward: 834.48
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.6443
     Episode_Reward/lifting_object: 169.2229
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.78s
                      Time elapsed: 00:53:41
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 36188 steps/s (collection: 2.548s, learning 0.169s)
             Mean action noise std: 2.80
          Mean value_function loss: 176.2330
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.2458
                       Mean reward: 853.68
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.6106
     Episode_Reward/lifting_object: 165.9537
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.72s
                      Time elapsed: 00:53:43
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 35677 steps/s (collection: 2.611s, learning 0.145s)
             Mean action noise std: 2.80
          Mean value_function loss: 150.7497
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 63.2618
                       Mean reward: 865.61
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.6337
     Episode_Reward/lifting_object: 168.6560
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.76s
                      Time elapsed: 00:53:46
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 34988 steps/s (collection: 2.684s, learning 0.126s)
             Mean action noise std: 2.80
          Mean value_function loss: 147.9274
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.2823
                       Mean reward: 825.64
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.6145
     Episode_Reward/lifting_object: 166.1625
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.81s
                      Time elapsed: 00:53:49
                               ETA: 00:40:19

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 35112 steps/s (collection: 2.659s, learning 0.140s)
             Mean action noise std: 2.80
          Mean value_function loss: 193.0225
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.3058
                       Mean reward: 797.47
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.6129
     Episode_Reward/lifting_object: 166.5573
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.80s
                      Time elapsed: 00:53:52
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 36093 steps/s (collection: 2.577s, learning 0.147s)
             Mean action noise std: 2.81
          Mean value_function loss: 183.2789
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.3393
                       Mean reward: 836.22
               Mean episode length: 224.83
    Episode_Reward/reaching_object: 1.6057
     Episode_Reward/lifting_object: 164.7382
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.72s
                      Time elapsed: 00:53:54
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 36065 steps/s (collection: 2.597s, learning 0.129s)
             Mean action noise std: 2.81
          Mean value_function loss: 157.1254
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.3675
                       Mean reward: 841.99
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 1.6211
     Episode_Reward/lifting_object: 166.8512
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.73s
                      Time elapsed: 00:53:57
                               ETA: 00:40:10

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 35419 steps/s (collection: 2.636s, learning 0.139s)
             Mean action noise std: 2.81
          Mean value_function loss: 195.9531
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.3867
                       Mean reward: 810.40
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 161.2979
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.78s
                      Time elapsed: 00:54:00
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 35742 steps/s (collection: 2.590s, learning 0.160s)
             Mean action noise std: 2.81
          Mean value_function loss: 206.3659
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.4027
                       Mean reward: 791.85
               Mean episode length: 215.75
    Episode_Reward/reaching_object: 1.6020
     Episode_Reward/lifting_object: 164.8530
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.75s
                      Time elapsed: 00:54:03
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 38790 steps/s (collection: 2.420s, learning 0.114s)
             Mean action noise std: 2.82
          Mean value_function loss: 203.1990
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.4221
                       Mean reward: 786.81
               Mean episode length: 214.21
    Episode_Reward/reaching_object: 1.5516
     Episode_Reward/lifting_object: 160.4504
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.53s
                      Time elapsed: 00:54:05
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 38845 steps/s (collection: 2.360s, learning 0.171s)
             Mean action noise std: 2.82
          Mean value_function loss: 187.4685
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.4425
                       Mean reward: 828.48
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.5996
     Episode_Reward/lifting_object: 166.2161
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.53s
                      Time elapsed: 00:54:08
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 37280 steps/s (collection: 2.504s, learning 0.132s)
             Mean action noise std: 2.82
          Mean value_function loss: 233.5464
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.4649
                       Mean reward: 786.15
               Mean episode length: 214.05
    Episode_Reward/reaching_object: 1.5433
     Episode_Reward/lifting_object: 159.7428
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.64s
                      Time elapsed: 00:54:10
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 36852 steps/s (collection: 2.538s, learning 0.129s)
             Mean action noise std: 2.82
          Mean value_function loss: 227.5469
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.4860
                       Mean reward: 860.32
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.5510
     Episode_Reward/lifting_object: 161.4300
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.67s
                      Time elapsed: 00:54:13
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 37171 steps/s (collection: 2.510s, learning 0.135s)
             Mean action noise std: 2.82
          Mean value_function loss: 239.6414
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.5039
                       Mean reward: 782.55
               Mean episode length: 214.23
    Episode_Reward/reaching_object: 1.5075
     Episode_Reward/lifting_object: 155.8292
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.64s
                      Time elapsed: 00:54:16
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 37102 steps/s (collection: 2.523s, learning 0.127s)
             Mean action noise std: 2.82
          Mean value_function loss: 186.5998
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 63.5119
                       Mean reward: 824.90
               Mean episode length: 222.57
    Episode_Reward/reaching_object: 1.5644
     Episode_Reward/lifting_object: 163.8316
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.65s
                      Time elapsed: 00:54:18
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 36997 steps/s (collection: 2.492s, learning 0.165s)
             Mean action noise std: 2.82
          Mean value_function loss: 190.3302
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 63.5134
                       Mean reward: 878.24
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 1.5742
     Episode_Reward/lifting_object: 165.1443
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.66s
                      Time elapsed: 00:54:21
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 36355 steps/s (collection: 2.549s, learning 0.155s)
             Mean action noise std: 2.83
          Mean value_function loss: 226.2161
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 63.5160
                       Mean reward: 855.07
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 1.5690
     Episode_Reward/lifting_object: 164.2656
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.70s
                      Time elapsed: 00:54:24
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 37239 steps/s (collection: 2.494s, learning 0.145s)
             Mean action noise std: 2.83
          Mean value_function loss: 209.7210
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 63.5175
                       Mean reward: 818.64
               Mean episode length: 223.56
    Episode_Reward/reaching_object: 1.5638
     Episode_Reward/lifting_object: 163.2772
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.64s
                      Time elapsed: 00:54:26
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 36793 steps/s (collection: 2.513s, learning 0.159s)
             Mean action noise std: 2.83
          Mean value_function loss: 177.6945
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.5201
                       Mean reward: 842.49
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 163.8338
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.67s
                      Time elapsed: 00:54:29
                               ETA: 00:39:35

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 37486 steps/s (collection: 2.500s, learning 0.123s)
             Mean action noise std: 2.83
          Mean value_function loss: 207.5082
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.5254
                       Mean reward: 849.82
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 1.5118
     Episode_Reward/lifting_object: 157.8082
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.62s
                      Time elapsed: 00:54:32
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 37000 steps/s (collection: 2.502s, learning 0.155s)
             Mean action noise std: 2.83
          Mean value_function loss: 210.2059
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.5354
                       Mean reward: 808.51
               Mean episode length: 218.39
    Episode_Reward/reaching_object: 1.5580
     Episode_Reward/lifting_object: 164.3698
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.66s
                      Time elapsed: 00:54:34
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 37322 steps/s (collection: 2.496s, learning 0.138s)
             Mean action noise std: 2.83
          Mean value_function loss: 230.8967
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.5456
                       Mean reward: 835.36
               Mean episode length: 224.51
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 160.3888
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.63s
                      Time elapsed: 00:54:37
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 37394 steps/s (collection: 2.495s, learning 0.134s)
             Mean action noise std: 2.83
          Mean value_function loss: 195.5879
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.5579
                       Mean reward: 844.65
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.5420
     Episode_Reward/lifting_object: 162.0643
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.63s
                      Time elapsed: 00:54:40
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 38488 steps/s (collection: 2.436s, learning 0.119s)
             Mean action noise std: 2.83
          Mean value_function loss: 195.8358
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.5709
                       Mean reward: 820.76
               Mean episode length: 223.18
    Episode_Reward/reaching_object: 1.5579
     Episode_Reward/lifting_object: 164.1600
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.55s
                      Time elapsed: 00:54:42
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 37740 steps/s (collection: 2.479s, learning 0.126s)
             Mean action noise std: 2.83
          Mean value_function loss: 180.3654
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.5821
                       Mean reward: 841.30
               Mean episode length: 227.00
    Episode_Reward/reaching_object: 1.5661
     Episode_Reward/lifting_object: 164.5581
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.60s
                      Time elapsed: 00:54:45
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 36770 steps/s (collection: 2.536s, learning 0.137s)
             Mean action noise std: 2.84
          Mean value_function loss: 207.8013
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.5982
                       Mean reward: 838.39
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.5597
     Episode_Reward/lifting_object: 163.7292
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.67s
                      Time elapsed: 00:54:47
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 35962 steps/s (collection: 2.573s, learning 0.161s)
             Mean action noise std: 2.84
          Mean value_function loss: 224.0388
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.6197
                       Mean reward: 816.30
               Mean episode length: 220.07
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 164.4008
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.73s
                      Time elapsed: 00:54:50
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 36867 steps/s (collection: 2.524s, learning 0.142s)
             Mean action noise std: 2.84
          Mean value_function loss: 179.8988
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.6298
                       Mean reward: 817.79
               Mean episode length: 220.87
    Episode_Reward/reaching_object: 1.5796
     Episode_Reward/lifting_object: 165.7332
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.67s
                      Time elapsed: 00:54:53
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 36864 steps/s (collection: 2.519s, learning 0.147s)
             Mean action noise std: 2.84
          Mean value_function loss: 180.8037
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.6365
                       Mean reward: 866.03
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 1.5962
     Episode_Reward/lifting_object: 166.3994
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.67s
                      Time elapsed: 00:54:55
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 36580 steps/s (collection: 2.552s, learning 0.135s)
             Mean action noise std: 2.84
          Mean value_function loss: 204.0505
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.6449
                       Mean reward: 831.72
               Mean episode length: 223.99
    Episode_Reward/reaching_object: 1.5513
     Episode_Reward/lifting_object: 162.0907
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.69s
                      Time elapsed: 00:54:58
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 36299 steps/s (collection: 2.580s, learning 0.129s)
             Mean action noise std: 2.84
          Mean value_function loss: 197.3605
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.6574
                       Mean reward: 832.64
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.5715
     Episode_Reward/lifting_object: 164.4550
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.71s
                      Time elapsed: 00:55:01
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 34971 steps/s (collection: 2.668s, learning 0.143s)
             Mean action noise std: 2.84
          Mean value_function loss: 169.0855
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.6780
                       Mean reward: 822.50
               Mean episode length: 223.64
    Episode_Reward/reaching_object: 1.5747
     Episode_Reward/lifting_object: 164.7054
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.81s
                      Time elapsed: 00:55:04
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 36544 steps/s (collection: 2.565s, learning 0.125s)
             Mean action noise std: 2.85
          Mean value_function loss: 156.7728
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.6919
                       Mean reward: 876.73
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.6185
     Episode_Reward/lifting_object: 169.7976
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.69s
                      Time elapsed: 00:55:06
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 36596 steps/s (collection: 2.543s, learning 0.143s)
             Mean action noise std: 2.85
          Mean value_function loss: 155.0400
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 63.7044
                       Mean reward: 837.38
               Mean episode length: 229.05
    Episode_Reward/reaching_object: 1.6086
     Episode_Reward/lifting_object: 167.6652
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.69s
                      Time elapsed: 00:55:09
                               ETA: 00:38:51

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 38396 steps/s (collection: 2.448s, learning 0.112s)
             Mean action noise std: 2.85
          Mean value_function loss: 200.9946
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.7207
                       Mean reward: 850.96
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 1.5416
     Episode_Reward/lifting_object: 161.0907
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.56s
                      Time elapsed: 00:55:12
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 38922 steps/s (collection: 2.386s, learning 0.140s)
             Mean action noise std: 2.85
          Mean value_function loss: 205.1557
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.7402
                       Mean reward: 791.18
               Mean episode length: 217.92
    Episode_Reward/reaching_object: 1.5848
     Episode_Reward/lifting_object: 165.6981
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.53s
                      Time elapsed: 00:55:14
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 38143 steps/s (collection: 2.445s, learning 0.132s)
             Mean action noise std: 2.85
          Mean value_function loss: 211.8618
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.7551
                       Mean reward: 806.77
               Mean episode length: 219.03
    Episode_Reward/reaching_object: 1.5521
     Episode_Reward/lifting_object: 162.0459
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.58s
                      Time elapsed: 00:55:17
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 38192 steps/s (collection: 2.441s, learning 0.132s)
             Mean action noise std: 2.86
          Mean value_function loss: 173.1789
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.7723
                       Mean reward: 791.55
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.6024
     Episode_Reward/lifting_object: 167.5515
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.57s
                      Time elapsed: 00:55:19
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 39233 steps/s (collection: 2.380s, learning 0.126s)
             Mean action noise std: 2.86
          Mean value_function loss: 160.9575
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.7875
                       Mean reward: 840.74
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.5638
     Episode_Reward/lifting_object: 162.6070
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.51s
                      Time elapsed: 00:55:22
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 39387 steps/s (collection: 2.352s, learning 0.144s)
             Mean action noise std: 2.86
          Mean value_function loss: 169.1756
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.8059
                       Mean reward: 863.99
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.5522
     Episode_Reward/lifting_object: 162.1098
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.50s
                      Time elapsed: 00:55:24
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 39228 steps/s (collection: 2.372s, learning 0.134s)
             Mean action noise std: 2.86
          Mean value_function loss: 144.1803
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.8228
                       Mean reward: 823.11
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 1.6118
     Episode_Reward/lifting_object: 168.0089
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.51s
                      Time elapsed: 00:55:27
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 39094 steps/s (collection: 2.359s, learning 0.155s)
             Mean action noise std: 2.86
          Mean value_function loss: 130.8220
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.8411
                       Mean reward: 857.19
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 1.5810
     Episode_Reward/lifting_object: 164.8532
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.51s
                      Time elapsed: 00:55:29
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 38631 steps/s (collection: 2.401s, learning 0.143s)
             Mean action noise std: 2.86
          Mean value_function loss: 139.4712
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.8568
                       Mean reward: 873.07
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 1.6235
     Episode_Reward/lifting_object: 169.0044
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.54s
                      Time elapsed: 00:55:32
                               ETA: 00:38:24

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 36253 steps/s (collection: 2.539s, learning 0.173s)
             Mean action noise std: 2.87
          Mean value_function loss: 176.0470
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.8656
                       Mean reward: 883.73
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.5997
     Episode_Reward/lifting_object: 167.0169
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.71s
                      Time elapsed: 00:55:35
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 37429 steps/s (collection: 2.479s, learning 0.147s)
             Mean action noise std: 2.87
          Mean value_function loss: 197.7201
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.8768
                       Mean reward: 862.34
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 1.5611
     Episode_Reward/lifting_object: 162.0965
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.63s
                      Time elapsed: 00:55:37
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 36656 steps/s (collection: 2.546s, learning 0.136s)
             Mean action noise std: 2.87
          Mean value_function loss: 159.1739
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.8906
                       Mean reward: 826.64
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 1.6197
     Episode_Reward/lifting_object: 169.4910
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.68s
                      Time elapsed: 00:55:40
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 37907 steps/s (collection: 2.437s, learning 0.156s)
             Mean action noise std: 2.87
          Mean value_function loss: 193.5205
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.9084
                       Mean reward: 836.39
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.5892
     Episode_Reward/lifting_object: 165.5592
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.59s
                      Time elapsed: 00:55:42
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 37572 steps/s (collection: 2.487s, learning 0.129s)
             Mean action noise std: 2.87
          Mean value_function loss: 153.7069
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.9229
                       Mean reward: 876.89
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 1.6010
     Episode_Reward/lifting_object: 167.6069
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.62s
                      Time elapsed: 00:55:45
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 37551 steps/s (collection: 2.472s, learning 0.146s)
             Mean action noise std: 2.88
          Mean value_function loss: 160.5340
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.9400
                       Mean reward: 842.03
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 1.5614
     Episode_Reward/lifting_object: 163.1329
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.62s
                      Time elapsed: 00:55:48
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 38766 steps/s (collection: 2.390s, learning 0.146s)
             Mean action noise std: 2.88
          Mean value_function loss: 139.5135
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.9593
                       Mean reward: 827.65
               Mean episode length: 224.18
    Episode_Reward/reaching_object: 1.5959
     Episode_Reward/lifting_object: 167.2516
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.54s
                      Time elapsed: 00:55:50
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 38416 steps/s (collection: 2.415s, learning 0.144s)
             Mean action noise std: 2.88
          Mean value_function loss: 180.7758
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.9809
                       Mean reward: 825.98
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 1.5918
     Episode_Reward/lifting_object: 166.2765
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.56s
                      Time elapsed: 00:55:53
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 39113 steps/s (collection: 2.396s, learning 0.118s)
             Mean action noise std: 2.88
          Mean value_function loss: 175.2290
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.0072
                       Mean reward: 828.67
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 1.5828
     Episode_Reward/lifting_object: 165.0769
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.51s
                      Time elapsed: 00:55:55
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 38712 steps/s (collection: 2.390s, learning 0.149s)
             Mean action noise std: 2.89
          Mean value_function loss: 199.6956
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.0365
                       Mean reward: 854.01
               Mean episode length: 230.34
    Episode_Reward/reaching_object: 1.5999
     Episode_Reward/lifting_object: 167.2866
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.54s
                      Time elapsed: 00:55:58
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 37986 steps/s (collection: 2.441s, learning 0.147s)
             Mean action noise std: 2.89
          Mean value_function loss: 154.4759
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.0600
                       Mean reward: 868.71
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 1.6064
     Episode_Reward/lifting_object: 167.4667
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.59s
                      Time elapsed: 00:56:00
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 38948 steps/s (collection: 2.390s, learning 0.134s)
             Mean action noise std: 2.89
          Mean value_function loss: 139.1177
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.0776
                       Mean reward: 868.16
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 1.6115
     Episode_Reward/lifting_object: 167.0559
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.52s
                      Time elapsed: 00:56:03
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 37794 steps/s (collection: 2.450s, learning 0.151s)
             Mean action noise std: 2.89
          Mean value_function loss: 169.0338
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.0897
                       Mean reward: 866.95
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 1.6167
     Episode_Reward/lifting_object: 168.6793
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.60s
                      Time elapsed: 00:56:06
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 37083 steps/s (collection: 2.537s, learning 0.113s)
             Mean action noise std: 2.89
          Mean value_function loss: 110.2578
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.0974
                       Mean reward: 904.16
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 1.6601
     Episode_Reward/lifting_object: 173.5219
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.65s
                      Time elapsed: 00:56:08
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 39532 steps/s (collection: 2.361s, learning 0.126s)
             Mean action noise std: 2.89
          Mean value_function loss: 201.7761
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.1132
                       Mean reward: 814.14
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 1.5909
     Episode_Reward/lifting_object: 164.9745
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.49s
                      Time elapsed: 00:56:11
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 39048 steps/s (collection: 2.393s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 155.2765
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.1362
                       Mean reward: 845.66
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 1.6067
     Episode_Reward/lifting_object: 166.6329
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.52s
                      Time elapsed: 00:56:13
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 37783 steps/s (collection: 2.461s, learning 0.141s)
             Mean action noise std: 2.90
          Mean value_function loss: 166.9103
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.1603
                       Mean reward: 826.14
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 1.6024
     Episode_Reward/lifting_object: 166.6656
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.60s
                      Time elapsed: 00:56:16
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 37602 steps/s (collection: 2.490s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 142.8750
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.1789
                       Mean reward: 878.75
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.6183
     Episode_Reward/lifting_object: 167.5571
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.61s
                      Time elapsed: 00:56:18
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 37096 steps/s (collection: 2.506s, learning 0.143s)
             Mean action noise std: 2.90
          Mean value_function loss: 141.0161
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.1969
                       Mean reward: 866.90
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 1.6392
     Episode_Reward/lifting_object: 169.8868
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.65s
                      Time elapsed: 00:56:21
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 37308 steps/s (collection: 2.504s, learning 0.131s)
             Mean action noise std: 2.91
          Mean value_function loss: 132.6660
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.2149
                       Mean reward: 814.64
               Mean episode length: 222.01
    Episode_Reward/reaching_object: 1.6248
     Episode_Reward/lifting_object: 167.9621
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.63s
                      Time elapsed: 00:56:24
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 38898 steps/s (collection: 2.389s, learning 0.139s)
             Mean action noise std: 2.91
          Mean value_function loss: 134.9040
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.2342
                       Mean reward: 853.16
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.6303
     Episode_Reward/lifting_object: 169.2442
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.53s
                      Time elapsed: 00:56:26
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 39087 steps/s (collection: 2.375s, learning 0.140s)
             Mean action noise std: 2.91
          Mean value_function loss: 153.2667
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.2554
                       Mean reward: 844.10
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 1.6300
     Episode_Reward/lifting_object: 169.2751
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.51s
                      Time elapsed: 00:56:29
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 39627 steps/s (collection: 2.359s, learning 0.121s)
             Mean action noise std: 2.91
          Mean value_function loss: 150.8709
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.2656
                       Mean reward: 818.61
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.6154
     Episode_Reward/lifting_object: 167.4388
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.48s
                      Time elapsed: 00:56:31
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 39856 steps/s (collection: 2.351s, learning 0.115s)
             Mean action noise std: 2.91
          Mean value_function loss: 173.4452
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.2768
                       Mean reward: 868.25
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.6355
     Episode_Reward/lifting_object: 169.7830
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.47s
                      Time elapsed: 00:56:34
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 37152 steps/s (collection: 2.517s, learning 0.129s)
             Mean action noise std: 2.92
          Mean value_function loss: 191.1231
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.3002
                       Mean reward: 812.27
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 1.6062
     Episode_Reward/lifting_object: 166.8785
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.65s
                      Time elapsed: 00:56:36
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 39572 steps/s (collection: 2.371s, learning 0.114s)
             Mean action noise std: 2.92
          Mean value_function loss: 193.7297
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.3258
                       Mean reward: 775.61
               Mean episode length: 210.70
    Episode_Reward/reaching_object: 1.5498
     Episode_Reward/lifting_object: 160.6608
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.48s
                      Time elapsed: 00:56:39
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 37669 steps/s (collection: 2.481s, learning 0.129s)
             Mean action noise std: 2.92
          Mean value_function loss: 146.4523
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.3457
                       Mean reward: 840.68
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.5922
     Episode_Reward/lifting_object: 165.8817
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.61s
                      Time elapsed: 00:56:41
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 38485 steps/s (collection: 2.419s, learning 0.135s)
             Mean action noise std: 2.92
          Mean value_function loss: 206.7668
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.3658
                       Mean reward: 850.23
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 1.6267
     Episode_Reward/lifting_object: 169.3641
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.55s
                      Time elapsed: 00:56:44
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 37389 steps/s (collection: 2.487s, learning 0.142s)
             Mean action noise std: 2.93
          Mean value_function loss: 200.6752
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.3868
                       Mean reward: 854.75
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 1.5936
     Episode_Reward/lifting_object: 165.9134
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.63s
                      Time elapsed: 00:56:47
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 38495 steps/s (collection: 2.444s, learning 0.109s)
             Mean action noise std: 2.93
          Mean value_function loss: 178.9973
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.4024
                       Mean reward: 827.18
               Mean episode length: 223.41
    Episode_Reward/reaching_object: 1.5834
     Episode_Reward/lifting_object: 164.4444
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.55s
                      Time elapsed: 00:56:49
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 40084 steps/s (collection: 2.336s, learning 0.117s)
             Mean action noise std: 2.93
          Mean value_function loss: 155.8602
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.4200
                       Mean reward: 848.17
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 1.6026
     Episode_Reward/lifting_object: 167.3819
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.45s
                      Time elapsed: 00:56:52
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 38750 steps/s (collection: 2.411s, learning 0.126s)
             Mean action noise std: 2.93
          Mean value_function loss: 153.8851
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.4433
                       Mean reward: 855.74
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.6226
     Episode_Reward/lifting_object: 168.2099
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.54s
                      Time elapsed: 00:56:54
                               ETA: 00:36:48

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 37172 steps/s (collection: 2.499s, learning 0.145s)
             Mean action noise std: 2.94
          Mean value_function loss: 193.0661
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.4666
                       Mean reward: 814.58
               Mean episode length: 220.57
    Episode_Reward/reaching_object: 1.5821
     Episode_Reward/lifting_object: 164.5798
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.64s
                      Time elapsed: 00:56:57
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 37708 steps/s (collection: 2.456s, learning 0.151s)
             Mean action noise std: 2.94
          Mean value_function loss: 140.8241
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.4924
                       Mean reward: 871.83
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.5937
     Episode_Reward/lifting_object: 167.7601
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.61s
                      Time elapsed: 00:56:59
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 33937 steps/s (collection: 2.714s, learning 0.182s)
             Mean action noise std: 2.94
          Mean value_function loss: 194.2215
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.5077
                       Mean reward: 798.55
               Mean episode length: 215.74
    Episode_Reward/reaching_object: 1.6113
     Episode_Reward/lifting_object: 168.5742
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.90s
                      Time elapsed: 00:57:02
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 35794 steps/s (collection: 2.617s, learning 0.129s)
             Mean action noise std: 2.94
          Mean value_function loss: 171.8954
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.5325
                       Mean reward: 817.42
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 1.5722
     Episode_Reward/lifting_object: 164.4028
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.75s
                      Time elapsed: 00:57:05
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 38172 steps/s (collection: 2.450s, learning 0.125s)
             Mean action noise std: 2.95
          Mean value_function loss: 248.0945
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.5589
                       Mean reward: 781.11
               Mean episode length: 212.71
    Episode_Reward/reaching_object: 1.5161
     Episode_Reward/lifting_object: 158.1828
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.58s
                      Time elapsed: 00:57:08
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 38667 steps/s (collection: 2.403s, learning 0.139s)
             Mean action noise std: 2.95
          Mean value_function loss: 182.1717
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.5757
                       Mean reward: 861.31
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 1.5774
     Episode_Reward/lifting_object: 165.8291
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.54s
                      Time elapsed: 00:57:10
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 39468 steps/s (collection: 2.382s, learning 0.109s)
             Mean action noise std: 2.95
          Mean value_function loss: 147.9216
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.5917
                       Mean reward: 880.81
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.5973
     Episode_Reward/lifting_object: 168.3206
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.49s
                      Time elapsed: 00:57:13
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 39027 steps/s (collection: 2.393s, learning 0.126s)
             Mean action noise std: 2.95
          Mean value_function loss: 186.0464
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.6149
                       Mean reward: 759.45
               Mean episode length: 207.43
    Episode_Reward/reaching_object: 1.5309
     Episode_Reward/lifting_object: 160.5695
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.52s
                      Time elapsed: 00:57:15
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 39386 steps/s (collection: 2.367s, learning 0.129s)
             Mean action noise std: 2.95
          Mean value_function loss: 168.1702
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.6359
                       Mean reward: 831.50
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 1.5861
     Episode_Reward/lifting_object: 166.6915
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.50s
                      Time elapsed: 00:57:18
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 38864 steps/s (collection: 2.406s, learning 0.124s)
             Mean action noise std: 2.96
          Mean value_function loss: 166.3889
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.6508
                       Mean reward: 834.41
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 1.5812
     Episode_Reward/lifting_object: 165.4577
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.53s
                      Time elapsed: 00:57:20
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 38706 steps/s (collection: 2.420s, learning 0.119s)
             Mean action noise std: 2.96
          Mean value_function loss: 155.2268
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.6701
                       Mean reward: 814.60
               Mean episode length: 219.56
    Episode_Reward/reaching_object: 1.5713
     Episode_Reward/lifting_object: 165.2069
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.54s
                      Time elapsed: 00:57:23
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 38636 steps/s (collection: 2.395s, learning 0.149s)
             Mean action noise std: 2.96
          Mean value_function loss: 172.0929
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 64.6801
                       Mean reward: 866.67
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 1.6028
     Episode_Reward/lifting_object: 168.4383
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.54s
                      Time elapsed: 00:57:25
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 36703 steps/s (collection: 2.532s, learning 0.146s)
             Mean action noise std: 2.96
          Mean value_function loss: 190.5595
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 64.6847
                       Mean reward: 852.38
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 1.5679
     Episode_Reward/lifting_object: 164.6408
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.68s
                      Time elapsed: 00:57:28
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 37710 steps/s (collection: 2.483s, learning 0.124s)
             Mean action noise std: 2.96
          Mean value_function loss: 179.8055
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.6898
                       Mean reward: 857.99
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 1.5791
     Episode_Reward/lifting_object: 165.9698
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.61s
                      Time elapsed: 00:57:31
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 34404 steps/s (collection: 2.731s, learning 0.127s)
             Mean action noise std: 2.96
          Mean value_function loss: 189.7272
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.7026
                       Mean reward: 791.15
               Mean episode length: 215.39
    Episode_Reward/reaching_object: 1.5701
     Episode_Reward/lifting_object: 165.1334
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.86s
                      Time elapsed: 00:57:33
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 38251 steps/s (collection: 2.460s, learning 0.110s)
             Mean action noise std: 2.96
          Mean value_function loss: 185.8053
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.7183
                       Mean reward: 844.37
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 1.5609
     Episode_Reward/lifting_object: 163.9118
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.57s
                      Time elapsed: 00:57:36
                               ETA: 00:36:02

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 39223 steps/s (collection: 2.392s, learning 0.114s)
             Mean action noise std: 2.97
          Mean value_function loss: 195.8103
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.7337
                       Mean reward: 844.09
               Mean episode length: 225.65
    Episode_Reward/reaching_object: 1.5984
     Episode_Reward/lifting_object: 169.1142
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.51s
                      Time elapsed: 00:57:38
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 38643 steps/s (collection: 2.408s, learning 0.136s)
             Mean action noise std: 2.97
          Mean value_function loss: 197.6176
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.7542
                       Mean reward: 804.37
               Mean episode length: 221.27
    Episode_Reward/reaching_object: 1.5354
     Episode_Reward/lifting_object: 159.6025
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.54s
                      Time elapsed: 00:57:41
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 37823 steps/s (collection: 2.452s, learning 0.147s)
             Mean action noise std: 2.97
          Mean value_function loss: 174.9025
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.7724
                       Mean reward: 830.84
               Mean episode length: 224.88
    Episode_Reward/reaching_object: 1.5375
     Episode_Reward/lifting_object: 160.4915
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.60s
                      Time elapsed: 00:57:44
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 37941 steps/s (collection: 2.468s, learning 0.123s)
             Mean action noise std: 2.97
          Mean value_function loss: 191.1040
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.7906
                       Mean reward: 825.13
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 1.5527
     Episode_Reward/lifting_object: 162.4067
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.59s
                      Time elapsed: 00:57:46
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 37223 steps/s (collection: 2.522s, learning 0.118s)
             Mean action noise std: 2.97
          Mean value_function loss: 191.8329
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 64.8077
                       Mean reward: 787.18
               Mean episode length: 212.94
    Episode_Reward/reaching_object: 1.5414
     Episode_Reward/lifting_object: 161.4279
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.64s
                      Time elapsed: 00:57:49
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 37254 steps/s (collection: 2.517s, learning 0.122s)
             Mean action noise std: 2.98
          Mean value_function loss: 189.4521
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 64.8228
                       Mean reward: 805.88
               Mean episode length: 216.85
    Episode_Reward/reaching_object: 1.5831
     Episode_Reward/lifting_object: 166.7164
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.64s
                      Time elapsed: 00:57:51
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 38464 steps/s (collection: 2.438s, learning 0.118s)
             Mean action noise std: 2.98
          Mean value_function loss: 231.5227
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.8392
                       Mean reward: 792.84
               Mean episode length: 215.95
    Episode_Reward/reaching_object: 1.5067
     Episode_Reward/lifting_object: 157.8657
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.56s
                      Time elapsed: 00:57:54
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 38734 steps/s (collection: 2.414s, learning 0.124s)
             Mean action noise std: 2.98
          Mean value_function loss: 200.1650
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.8524
                       Mean reward: 822.01
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 1.5851
     Episode_Reward/lifting_object: 166.8480
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.54s
                      Time elapsed: 00:57:57
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 36867 steps/s (collection: 2.504s, learning 0.163s)
             Mean action noise std: 2.98
          Mean value_function loss: 220.8999
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.8603
                       Mean reward: 804.75
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 1.5387
     Episode_Reward/lifting_object: 161.0640
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.67s
                      Time elapsed: 00:57:59
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 38362 steps/s (collection: 2.439s, learning 0.124s)
             Mean action noise std: 2.98
          Mean value_function loss: 161.4674
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 64.8762
                       Mean reward: 824.88
               Mean episode length: 224.34
    Episode_Reward/reaching_object: 1.5644
     Episode_Reward/lifting_object: 163.8737
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.56s
                      Time elapsed: 00:58:02
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 38637 steps/s (collection: 2.414s, learning 0.130s)
             Mean action noise std: 2.99
          Mean value_function loss: 199.6382
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.8991
                       Mean reward: 816.68
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.5449
     Episode_Reward/lifting_object: 161.4352
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.54s
                      Time elapsed: 00:58:04
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 36219 steps/s (collection: 2.580s, learning 0.134s)
             Mean action noise std: 2.99
          Mean value_function loss: 178.8059
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.9224
                       Mean reward: 802.71
               Mean episode length: 219.18
    Episode_Reward/reaching_object: 1.5573
     Episode_Reward/lifting_object: 163.1431
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.71s
                      Time elapsed: 00:58:07
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 36765 steps/s (collection: 2.549s, learning 0.125s)
             Mean action noise std: 2.99
          Mean value_function loss: 172.5537
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.9407
                       Mean reward: 813.64
               Mean episode length: 219.89
    Episode_Reward/reaching_object: 1.5643
     Episode_Reward/lifting_object: 164.2002
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.67s
                      Time elapsed: 00:58:10
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 36938 steps/s (collection: 2.500s, learning 0.162s)
             Mean action noise std: 2.99
          Mean value_function loss: 221.9910
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 64.9534
                       Mean reward: 796.01
               Mean episode length: 214.94
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 164.4963
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.66s
                      Time elapsed: 00:58:12
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 37774 steps/s (collection: 2.468s, learning 0.134s)
             Mean action noise std: 3.00
          Mean value_function loss: 174.9412
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 64.9773
                       Mean reward: 829.28
               Mean episode length: 224.43
    Episode_Reward/reaching_object: 1.5497
     Episode_Reward/lifting_object: 161.5249
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.60s
                      Time elapsed: 00:58:15
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 38617 steps/s (collection: 2.374s, learning 0.171s)
             Mean action noise std: 3.00
          Mean value_function loss: 203.2042
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.0007
                       Mean reward: 799.97
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.5435
     Episode_Reward/lifting_object: 161.4299
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.55s
                      Time elapsed: 00:58:18
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 39296 steps/s (collection: 2.386s, learning 0.116s)
             Mean action noise std: 3.00
          Mean value_function loss: 182.2557
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.0120
                       Mean reward: 803.19
               Mean episode length: 215.91
    Episode_Reward/reaching_object: 1.5967
     Episode_Reward/lifting_object: 167.1335
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.50s
                      Time elapsed: 00:58:20
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 38488 steps/s (collection: 2.404s, learning 0.150s)
             Mean action noise std: 3.00
          Mean value_function loss: 184.1749
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.0277
                       Mean reward: 837.32
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 1.5882
     Episode_Reward/lifting_object: 164.6066
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.55s
                      Time elapsed: 00:58:23
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 39911 steps/s (collection: 2.352s, learning 0.111s)
             Mean action noise std: 3.00
          Mean value_function loss: 220.9281
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.0498
                       Mean reward: 808.18
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 1.5652
     Episode_Reward/lifting_object: 163.1413
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.46s
                      Time elapsed: 00:58:25
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 38958 steps/s (collection: 2.362s, learning 0.161s)
             Mean action noise std: 3.01
          Mean value_function loss: 177.1593
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.0734
                       Mean reward: 843.47
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.5910
     Episode_Reward/lifting_object: 166.2655
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.52s
                      Time elapsed: 00:58:28
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 38140 steps/s (collection: 2.452s, learning 0.125s)
             Mean action noise std: 3.01
          Mean value_function loss: 187.0973
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 65.0844
                       Mean reward: 856.39
               Mean episode length: 229.11
    Episode_Reward/reaching_object: 1.5870
     Episode_Reward/lifting_object: 165.8187
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.58s
                      Time elapsed: 00:58:30
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 37862 steps/s (collection: 2.449s, learning 0.147s)
             Mean action noise std: 3.01
          Mean value_function loss: 210.8584
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 65.0894
                       Mean reward: 833.89
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.5610
     Episode_Reward/lifting_object: 163.2802
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.60s
                      Time elapsed: 00:58:33
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 38442 steps/s (collection: 2.426s, learning 0.131s)
             Mean action noise std: 3.01
          Mean value_function loss: 161.2738
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 65.0935
                       Mean reward: 805.98
               Mean episode length: 216.99
    Episode_Reward/reaching_object: 1.6091
     Episode_Reward/lifting_object: 168.3350
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.56s
                      Time elapsed: 00:58:35
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 39165 steps/s (collection: 2.393s, learning 0.117s)
             Mean action noise std: 3.01
          Mean value_function loss: 189.9983
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.1022
                       Mean reward: 837.46
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.5517
     Episode_Reward/lifting_object: 162.2462
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.51s
                      Time elapsed: 00:58:38
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 36819 steps/s (collection: 2.521s, learning 0.149s)
             Mean action noise std: 3.01
          Mean value_function loss: 227.8271
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.1223
                       Mean reward: 791.73
               Mean episode length: 215.32
    Episode_Reward/reaching_object: 1.5595
     Episode_Reward/lifting_object: 162.3986
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.67s
                      Time elapsed: 00:58:41
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 35796 steps/s (collection: 2.614s, learning 0.133s)
             Mean action noise std: 3.01
          Mean value_function loss: 186.3712
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.1458
                       Mean reward: 826.37
               Mean episode length: 222.55
    Episode_Reward/reaching_object: 1.6062
     Episode_Reward/lifting_object: 168.3465
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.75s
                      Time elapsed: 00:58:43
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 37356 steps/s (collection: 2.506s, learning 0.125s)
             Mean action noise std: 3.02
          Mean value_function loss: 169.4230
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.1650
                       Mean reward: 838.85
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 1.6039
     Episode_Reward/lifting_object: 168.0184
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.63s
                      Time elapsed: 00:58:46
                               ETA: 00:34:42

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 37198 steps/s (collection: 2.513s, learning 0.130s)
             Mean action noise std: 3.02
          Mean value_function loss: 196.0811
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.1830
                       Mean reward: 815.85
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 1.5307
     Episode_Reward/lifting_object: 159.6917
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.64s
                      Time elapsed: 00:58:49
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 36183 steps/s (collection: 2.560s, learning 0.157s)
             Mean action noise std: 3.02
          Mean value_function loss: 235.0254
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.1992
                       Mean reward: 884.09
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.5836
     Episode_Reward/lifting_object: 165.7769
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.72s
                      Time elapsed: 00:58:51
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 35502 steps/s (collection: 2.626s, learning 0.142s)
             Mean action noise std: 3.02
          Mean value_function loss: 191.7084
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.2179
                       Mean reward: 849.75
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 1.5739
     Episode_Reward/lifting_object: 164.8668
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.77s
                      Time elapsed: 00:58:54
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 38102 steps/s (collection: 2.436s, learning 0.144s)
             Mean action noise std: 3.02
          Mean value_function loss: 193.5116
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.2332
                       Mean reward: 843.27
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 1.5230
     Episode_Reward/lifting_object: 158.6941
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.58s
                      Time elapsed: 00:58:57
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 39042 steps/s (collection: 2.368s, learning 0.150s)
             Mean action noise std: 3.03
          Mean value_function loss: 176.1466
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.2457
                       Mean reward: 837.40
               Mean episode length: 226.66
    Episode_Reward/reaching_object: 1.5953
     Episode_Reward/lifting_object: 167.0504
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.52s
                      Time elapsed: 00:58:59
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 38945 steps/s (collection: 2.380s, learning 0.144s)
             Mean action noise std: 3.03
          Mean value_function loss: 178.0423
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.2610
                       Mean reward: 785.96
               Mean episode length: 212.54
    Episode_Reward/reaching_object: 1.5542
     Episode_Reward/lifting_object: 161.8098
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.52s
                      Time elapsed: 00:59:02
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 38189 steps/s (collection: 2.452s, learning 0.122s)
             Mean action noise std: 3.03
          Mean value_function loss: 165.1373
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.2773
                       Mean reward: 864.00
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 1.5888
     Episode_Reward/lifting_object: 166.1588
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.57s
                      Time elapsed: 00:59:04
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 39169 steps/s (collection: 2.369s, learning 0.141s)
             Mean action noise std: 3.03
          Mean value_function loss: 176.7604
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.2993
                       Mean reward: 831.34
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 1.5843
     Episode_Reward/lifting_object: 165.1607
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.51s
                      Time elapsed: 00:59:07
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 39047 steps/s (collection: 2.397s, learning 0.120s)
             Mean action noise std: 3.03
          Mean value_function loss: 205.9174
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 65.3126
                       Mean reward: 848.75
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.5405
     Episode_Reward/lifting_object: 161.0016
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.52s
                      Time elapsed: 00:59:09
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 39167 steps/s (collection: 2.375s, learning 0.135s)
             Mean action noise std: 3.04
          Mean value_function loss: 213.0886
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.3212
                       Mean reward: 812.84
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 1.5491
     Episode_Reward/lifting_object: 160.6785
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.51s
                      Time elapsed: 00:59:12
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 40020 steps/s (collection: 2.323s, learning 0.134s)
             Mean action noise std: 3.04
          Mean value_function loss: 156.9892
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.3404
                       Mean reward: 821.63
               Mean episode length: 222.16
    Episode_Reward/reaching_object: 1.5737
     Episode_Reward/lifting_object: 164.5206
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.46s
                      Time elapsed: 00:59:14
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 38771 steps/s (collection: 2.404s, learning 0.131s)
             Mean action noise std: 3.04
          Mean value_function loss: 191.0153
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.3686
                       Mean reward: 820.54
               Mean episode length: 223.04
    Episode_Reward/reaching_object: 1.5645
     Episode_Reward/lifting_object: 162.3723
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.54s
                      Time elapsed: 00:59:17
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 38419 steps/s (collection: 2.426s, learning 0.133s)
             Mean action noise std: 3.04
          Mean value_function loss: 190.3494
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.3979
                       Mean reward: 858.72
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 1.6090
     Episode_Reward/lifting_object: 167.5345
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.56s
                      Time elapsed: 00:59:19
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 37145 steps/s (collection: 2.473s, learning 0.173s)
             Mean action noise std: 3.05
          Mean value_function loss: 220.5032
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.4200
                       Mean reward: 818.45
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 1.5832
     Episode_Reward/lifting_object: 165.1646
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.65s
                      Time elapsed: 00:59:22
                               ETA: 00:34:01

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 38419 steps/s (collection: 2.443s, learning 0.116s)
             Mean action noise std: 3.05
          Mean value_function loss: 254.8502
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.4381
                       Mean reward: 767.47
               Mean episode length: 209.60
    Episode_Reward/reaching_object: 1.5404
     Episode_Reward/lifting_object: 159.4832
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.56s
                      Time elapsed: 00:59:24
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 38097 steps/s (collection: 2.424s, learning 0.157s)
             Mean action noise std: 3.05
          Mean value_function loss: 201.8036
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.4532
                       Mean reward: 826.29
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 1.5538
     Episode_Reward/lifting_object: 161.1560
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.58s
                      Time elapsed: 00:59:27
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 38106 steps/s (collection: 2.430s, learning 0.149s)
             Mean action noise std: 3.05
          Mean value_function loss: 170.9540
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.4708
                       Mean reward: 800.44
               Mean episode length: 217.10
    Episode_Reward/reaching_object: 1.5549
     Episode_Reward/lifting_object: 161.6818
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.58s
                      Time elapsed: 00:59:30
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 38740 steps/s (collection: 2.390s, learning 0.147s)
             Mean action noise std: 3.05
          Mean value_function loss: 200.0624
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.4800
                       Mean reward: 876.24
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.5639
     Episode_Reward/lifting_object: 162.7877
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.54s
                      Time elapsed: 00:59:32
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 39080 steps/s (collection: 2.401s, learning 0.115s)
             Mean action noise std: 3.06
          Mean value_function loss: 195.5085
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.4933
                       Mean reward: 866.54
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.5834
     Episode_Reward/lifting_object: 165.0419
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.52s
                      Time elapsed: 00:59:35
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 36539 steps/s (collection: 2.545s, learning 0.145s)
             Mean action noise std: 3.06
          Mean value_function loss: 202.6340
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.5132
                       Mean reward: 772.89
               Mean episode length: 211.53
    Episode_Reward/reaching_object: 1.5298
     Episode_Reward/lifting_object: 159.6451
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.69s
                      Time elapsed: 00:59:37
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 36219 steps/s (collection: 2.538s, learning 0.177s)
             Mean action noise std: 3.06
          Mean value_function loss: 178.4648
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.5306
                       Mean reward: 816.19
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.5494
     Episode_Reward/lifting_object: 161.8649
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.71s
                      Time elapsed: 00:59:40
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 35828 steps/s (collection: 2.573s, learning 0.171s)
             Mean action noise std: 3.06
          Mean value_function loss: 167.2561
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.5432
                       Mean reward: 866.59
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 1.5937
     Episode_Reward/lifting_object: 167.4441
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.74s
                      Time elapsed: 00:59:43
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 34219 steps/s (collection: 2.731s, learning 0.142s)
             Mean action noise std: 3.06
          Mean value_function loss: 180.4975
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.5611
                       Mean reward: 875.15
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.5749
     Episode_Reward/lifting_object: 165.1331
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.87s
                      Time elapsed: 00:59:46
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 34703 steps/s (collection: 2.684s, learning 0.148s)
             Mean action noise std: 3.07
          Mean value_function loss: 178.8561
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.5757
                       Mean reward: 894.88
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 1.6057
     Episode_Reward/lifting_object: 167.7497
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.83s
                      Time elapsed: 00:59:49
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 36033 steps/s (collection: 2.563s, learning 0.166s)
             Mean action noise std: 3.07
          Mean value_function loss: 168.7560
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.5892
                       Mean reward: 849.98
               Mean episode length: 227.82
    Episode_Reward/reaching_object: 1.5991
     Episode_Reward/lifting_object: 167.5567
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.73s
                      Time elapsed: 00:59:51
                               ETA: 00:33:30

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 35418 steps/s (collection: 2.623s, learning 0.152s)
             Mean action noise std: 3.07
          Mean value_function loss: 174.3376
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.6101
                       Mean reward: 815.63
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 1.5785
     Episode_Reward/lifting_object: 165.0759
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.78s
                      Time elapsed: 00:59:54
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 35238 steps/s (collection: 2.604s, learning 0.186s)
             Mean action noise std: 3.07
          Mean value_function loss: 186.9378
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.6287
                       Mean reward: 850.24
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 1.5934
     Episode_Reward/lifting_object: 165.7982
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.79s
                      Time elapsed: 00:59:57
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 35157 steps/s (collection: 2.653s, learning 0.143s)
             Mean action noise std: 3.07
          Mean value_function loss: 170.4314
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.6430
                       Mean reward: 832.29
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 1.5806
     Episode_Reward/lifting_object: 164.7608
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.80s
                      Time elapsed: 01:00:00
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 36409 steps/s (collection: 2.540s, learning 0.160s)
             Mean action noise std: 3.08
          Mean value_function loss: 191.2737
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 65.6634
                       Mean reward: 792.91
               Mean episode length: 216.05
    Episode_Reward/reaching_object: 1.5780
     Episode_Reward/lifting_object: 164.3747
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.70s
                      Time elapsed: 01:00:02
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 36222 steps/s (collection: 2.574s, learning 0.140s)
             Mean action noise std: 3.08
          Mean value_function loss: 203.8863
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.6871
                       Mean reward: 824.18
               Mean episode length: 224.90
    Episode_Reward/reaching_object: 1.5749
     Episode_Reward/lifting_object: 163.9381
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.71s
                      Time elapsed: 01:00:05
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 36098 steps/s (collection: 2.581s, learning 0.142s)
             Mean action noise std: 3.08
          Mean value_function loss: 222.3167
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.7005
                       Mean reward: 802.45
               Mean episode length: 217.30
    Episode_Reward/reaching_object: 1.5718
     Episode_Reward/lifting_object: 163.2884
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.72s
                      Time elapsed: 01:00:08
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 36502 steps/s (collection: 2.539s, learning 0.155s)
             Mean action noise std: 3.08
          Mean value_function loss: 173.2215
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.7187
                       Mean reward: 839.81
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 1.5925
     Episode_Reward/lifting_object: 165.8171
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.69s
                      Time elapsed: 01:00:10
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 35571 steps/s (collection: 2.619s, learning 0.145s)
             Mean action noise std: 3.09
          Mean value_function loss: 185.0248
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.7427
                       Mean reward: 824.90
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.5913
     Episode_Reward/lifting_object: 164.8394
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.76s
                      Time elapsed: 01:00:13
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 36177 steps/s (collection: 2.589s, learning 0.128s)
             Mean action noise std: 3.09
          Mean value_function loss: 213.0114
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.7562
                       Mean reward: 781.49
               Mean episode length: 211.82
    Episode_Reward/reaching_object: 1.5505
     Episode_Reward/lifting_object: 160.6780
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.72s
                      Time elapsed: 01:00:16
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 36882 steps/s (collection: 2.516s, learning 0.149s)
             Mean action noise std: 3.09
          Mean value_function loss: 211.1082
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.7757
                       Mean reward: 818.20
               Mean episode length: 220.04
    Episode_Reward/reaching_object: 1.5549
     Episode_Reward/lifting_object: 162.3803
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.67s
                      Time elapsed: 01:00:19
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 35569 steps/s (collection: 2.607s, learning 0.157s)
             Mean action noise std: 3.09
          Mean value_function loss: 227.2705
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.7905
                       Mean reward: 856.32
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 1.5783
     Episode_Reward/lifting_object: 165.2055
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.76s
                      Time elapsed: 01:00:21
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 36757 steps/s (collection: 2.539s, learning 0.135s)
             Mean action noise std: 3.09
          Mean value_function loss: 231.9397
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.8093
                       Mean reward: 867.99
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.5625
     Episode_Reward/lifting_object: 162.5156
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.67s
                      Time elapsed: 01:00:24
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 35649 steps/s (collection: 2.589s, learning 0.169s)
             Mean action noise std: 3.10
          Mean value_function loss: 237.4538
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.8292
                       Mean reward: 802.96
               Mean episode length: 218.20
    Episode_Reward/reaching_object: 1.4981
     Episode_Reward/lifting_object: 155.7266
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.76s
                      Time elapsed: 01:00:27
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 36233 steps/s (collection: 2.555s, learning 0.158s)
             Mean action noise std: 3.10
          Mean value_function loss: 226.6033
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.8405
                       Mean reward: 790.90
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.5448
     Episode_Reward/lifting_object: 161.4663
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.71s
                      Time elapsed: 01:00:30
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 35911 steps/s (collection: 2.592s, learning 0.146s)
             Mean action noise std: 3.10
          Mean value_function loss: 183.8167
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 65.8470
                       Mean reward: 848.33
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 1.5595
     Episode_Reward/lifting_object: 163.5715
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.74s
                      Time elapsed: 01:00:32
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 34605 steps/s (collection: 2.704s, learning 0.137s)
             Mean action noise std: 3.10
          Mean value_function loss: 210.7776
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 65.8498
                       Mean reward: 833.99
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.5403
     Episode_Reward/lifting_object: 161.6160
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.84s
                      Time elapsed: 01:00:35
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 35728 steps/s (collection: 2.609s, learning 0.142s)
             Mean action noise std: 3.10
          Mean value_function loss: 230.8028
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 65.8519
                       Mean reward: 764.83
               Mean episode length: 215.33
    Episode_Reward/reaching_object: 1.5509
     Episode_Reward/lifting_object: 162.2095
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.75s
                      Time elapsed: 01:00:38
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 35925 steps/s (collection: 2.570s, learning 0.166s)
             Mean action noise std: 3.10
          Mean value_function loss: 179.2441
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 65.8535
                       Mean reward: 851.27
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.5738
     Episode_Reward/lifting_object: 165.5104
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.74s
                      Time elapsed: 01:00:41
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 35741 steps/s (collection: 2.581s, learning 0.170s)
             Mean action noise std: 3.10
          Mean value_function loss: 175.4124
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.8561
                       Mean reward: 855.06
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: 167.0682
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.75s
                      Time elapsed: 01:00:43
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 36650 steps/s (collection: 2.529s, learning 0.154s)
             Mean action noise std: 3.10
          Mean value_function loss: 183.9068
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 65.8590
                       Mean reward: 829.71
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 1.5950
     Episode_Reward/lifting_object: 168.1316
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.68s
                      Time elapsed: 01:00:46
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 35332 steps/s (collection: 2.648s, learning 0.134s)
             Mean action noise std: 3.10
          Mean value_function loss: 197.1993
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.8644
                       Mean reward: 859.91
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.5372
     Episode_Reward/lifting_object: 161.3285
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.78s
                      Time elapsed: 01:00:49
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 36241 steps/s (collection: 2.560s, learning 0.152s)
             Mean action noise std: 3.10
          Mean value_function loss: 187.3488
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.8738
                       Mean reward: 834.05
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.5794
     Episode_Reward/lifting_object: 166.0849
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.71s
                      Time elapsed: 01:00:52
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 37540 steps/s (collection: 2.490s, learning 0.129s)
             Mean action noise std: 3.10
          Mean value_function loss: 221.4987
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.8893
                       Mean reward: 774.81
               Mean episode length: 212.53
    Episode_Reward/reaching_object: 1.5271
     Episode_Reward/lifting_object: 159.5835
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.62s
                      Time elapsed: 01:00:54
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 36577 steps/s (collection: 2.549s, learning 0.139s)
             Mean action noise std: 3.11
          Mean value_function loss: 153.8604
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.9098
                       Mean reward: 819.34
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 1.5811
     Episode_Reward/lifting_object: 165.8202
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.69s
                      Time elapsed: 01:00:57
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 36125 steps/s (collection: 2.569s, learning 0.152s)
             Mean action noise std: 3.11
          Mean value_function loss: 175.8657
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.9294
                       Mean reward: 839.75
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.6046
     Episode_Reward/lifting_object: 167.9797
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.72s
                      Time elapsed: 01:01:00
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 36695 steps/s (collection: 2.520s, learning 0.159s)
             Mean action noise std: 3.11
          Mean value_function loss: 173.2737
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.9415
                       Mean reward: 862.86
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.5842
     Episode_Reward/lifting_object: 165.8086
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.68s
                      Time elapsed: 01:01:02
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 35583 steps/s (collection: 2.610s, learning 0.153s)
             Mean action noise std: 3.11
          Mean value_function loss: 167.9395
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.9500
                       Mean reward: 843.48
               Mean episode length: 227.30
    Episode_Reward/reaching_object: 1.6008
     Episode_Reward/lifting_object: 167.1907
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.76s
                      Time elapsed: 01:01:05
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 34520 steps/s (collection: 2.685s, learning 0.163s)
             Mean action noise std: 3.11
          Mean value_function loss: 190.2341
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.9678
                       Mean reward: 872.59
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.5395
     Episode_Reward/lifting_object: 160.5668
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.85s
                      Time elapsed: 01:01:08
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 32329 steps/s (collection: 2.879s, learning 0.162s)
             Mean action noise std: 3.12
          Mean value_function loss: 207.0248
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.9897
                       Mean reward: 795.97
               Mean episode length: 217.65
    Episode_Reward/reaching_object: 1.5629
     Episode_Reward/lifting_object: 162.6488
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 3.04s
                      Time elapsed: 01:01:11
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 36267 steps/s (collection: 2.565s, learning 0.146s)
             Mean action noise std: 3.12
          Mean value_function loss: 164.5246
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 66.0084
                       Mean reward: 830.45
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.6160
     Episode_Reward/lifting_object: 168.3629
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.71s
                      Time elapsed: 01:01:14
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 36470 steps/s (collection: 2.558s, learning 0.138s)
             Mean action noise std: 3.12
          Mean value_function loss: 171.9475
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.0259
                       Mean reward: 830.87
               Mean episode length: 225.46
    Episode_Reward/reaching_object: 1.5662
     Episode_Reward/lifting_object: 162.3794
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.70s
                      Time elapsed: 01:01:16
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 36057 steps/s (collection: 2.595s, learning 0.131s)
             Mean action noise std: 3.12
          Mean value_function loss: 189.6133
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.0430
                       Mean reward: 848.54
               Mean episode length: 229.50
    Episode_Reward/reaching_object: 1.6079
     Episode_Reward/lifting_object: 168.2243
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.73s
                      Time elapsed: 01:01:19
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 35042 steps/s (collection: 2.631s, learning 0.174s)
             Mean action noise std: 3.13
          Mean value_function loss: 203.8435
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.0627
                       Mean reward: 816.57
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 1.5765
     Episode_Reward/lifting_object: 163.9705
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.81s
                      Time elapsed: 01:01:22
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 36117 steps/s (collection: 2.585s, learning 0.136s)
             Mean action noise std: 3.13
          Mean value_function loss: 214.5109
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.0963
                       Mean reward: 775.64
               Mean episode length: 212.29
    Episode_Reward/reaching_object: 1.5612
     Episode_Reward/lifting_object: 162.3320
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.72s
                      Time elapsed: 01:01:25
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 39944 steps/s (collection: 2.327s, learning 0.134s)
             Mean action noise std: 3.13
          Mean value_function loss: 183.1847
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 66.1270
                       Mean reward: 874.67
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.5796
     Episode_Reward/lifting_object: 164.3543
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.46s
                      Time elapsed: 01:01:27
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 39060 steps/s (collection: 2.363s, learning 0.154s)
             Mean action noise std: 3.14
          Mean value_function loss: 176.5121
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.1402
                       Mean reward: 782.81
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.5521
     Episode_Reward/lifting_object: 160.8771
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.52s
                      Time elapsed: 01:01:30
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 38186 steps/s (collection: 2.454s, learning 0.120s)
             Mean action noise std: 3.14
          Mean value_function loss: 160.7562
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.1548
                       Mean reward: 813.99
               Mean episode length: 220.39
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 163.7660
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.57s
                      Time elapsed: 01:01:32
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 38442 steps/s (collection: 2.416s, learning 0.142s)
             Mean action noise std: 3.14
          Mean value_function loss: 193.5299
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.1753
                       Mean reward: 814.76
               Mean episode length: 220.28
    Episode_Reward/reaching_object: 1.5781
     Episode_Reward/lifting_object: 164.5394
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.56s
                      Time elapsed: 01:01:35
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 37336 steps/s (collection: 2.483s, learning 0.150s)
             Mean action noise std: 3.14
          Mean value_function loss: 133.6804
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.1958
                       Mean reward: 853.43
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 1.6523
     Episode_Reward/lifting_object: 172.3695
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.63s
                      Time elapsed: 01:01:37
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 37844 steps/s (collection: 2.442s, learning 0.156s)
             Mean action noise std: 3.15
          Mean value_function loss: 178.3936
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.2207
                       Mean reward: 867.63
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.6248
     Episode_Reward/lifting_object: 169.9679
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.60s
                      Time elapsed: 01:01:40
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 36540 steps/s (collection: 2.546s, learning 0.144s)
             Mean action noise std: 3.15
          Mean value_function loss: 164.6077
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.2368
                       Mean reward: 848.78
               Mean episode length: 231.45
    Episode_Reward/reaching_object: 1.5941
     Episode_Reward/lifting_object: 165.6913
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.69s
                      Time elapsed: 01:01:43
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 38076 steps/s (collection: 2.456s, learning 0.125s)
             Mean action noise std: 3.15
          Mean value_function loss: 176.3255
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.2468
                       Mean reward: 834.02
               Mean episode length: 226.74
    Episode_Reward/reaching_object: 1.6038
     Episode_Reward/lifting_object: 166.6776
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.58s
                      Time elapsed: 01:01:45
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 35709 steps/s (collection: 2.592s, learning 0.161s)
             Mean action noise std: 3.15
          Mean value_function loss: 165.6322
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.2666
                       Mean reward: 871.37
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 1.5979
     Episode_Reward/lifting_object: 166.0753
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.75s
                      Time elapsed: 01:01:48
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 37434 steps/s (collection: 2.470s, learning 0.156s)
             Mean action noise std: 3.15
          Mean value_function loss: 188.4769
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.2793
                       Mean reward: 830.61
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.5834
     Episode_Reward/lifting_object: 164.7036
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.63s
                      Time elapsed: 01:01:51
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 38344 steps/s (collection: 2.427s, learning 0.137s)
             Mean action noise std: 3.16
          Mean value_function loss: 197.6890
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.2959
                       Mean reward: 849.28
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 1.5895
     Episode_Reward/lifting_object: 164.8719
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.56s
                      Time elapsed: 01:01:53
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 35134 steps/s (collection: 2.641s, learning 0.157s)
             Mean action noise std: 3.16
          Mean value_function loss: 197.6886
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.3151
                       Mean reward: 822.75
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 1.5747
     Episode_Reward/lifting_object: 163.2695
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.80s
                      Time elapsed: 01:01:56
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 37255 steps/s (collection: 2.505s, learning 0.133s)
             Mean action noise std: 3.16
          Mean value_function loss: 235.7948
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.3364
                       Mean reward: 758.94
               Mean episode length: 207.91
    Episode_Reward/reaching_object: 1.5100
     Episode_Reward/lifting_object: 156.0839
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.64s
                      Time elapsed: 01:01:59
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 37643 steps/s (collection: 2.484s, learning 0.128s)
             Mean action noise std: 3.16
          Mean value_function loss: 245.0459
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.3543
                       Mean reward: 812.20
               Mean episode length: 219.61
    Episode_Reward/reaching_object: 1.5437
     Episode_Reward/lifting_object: 159.5317
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.61s
                      Time elapsed: 01:02:01
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 38402 steps/s (collection: 2.405s, learning 0.154s)
             Mean action noise std: 3.17
          Mean value_function loss: 188.8168
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.3811
                       Mean reward: 833.84
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.5781
     Episode_Reward/lifting_object: 164.0307
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.56s
                      Time elapsed: 01:02:04
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 38310 steps/s (collection: 2.430s, learning 0.136s)
             Mean action noise std: 3.17
          Mean value_function loss: 205.1894
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.4030
                       Mean reward: 812.56
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.5954
     Episode_Reward/lifting_object: 165.6015
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.57s
                      Time elapsed: 01:02:06
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 16517 steps/s (collection: 5.831s, learning 0.121s)
             Mean action noise std: 3.17
          Mean value_function loss: 214.0542
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.4205
                       Mean reward: 858.44
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 1.5707
     Episode_Reward/lifting_object: 162.4329
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.95s
                      Time elapsed: 01:02:12
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 12915 steps/s (collection: 7.480s, learning 0.131s)
             Mean action noise std: 3.17
          Mean value_function loss: 181.8287
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.4425
                       Mean reward: 883.13
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 1.5865
     Episode_Reward/lifting_object: 164.3812
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.61s
                      Time elapsed: 01:02:20
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 12890 steps/s (collection: 7.467s, learning 0.159s)
             Mean action noise std: 3.18
          Mean value_function loss: 185.8123
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.4626
                       Mean reward: 818.12
               Mean episode length: 221.36
    Episode_Reward/reaching_object: 1.5450
     Episode_Reward/lifting_object: 158.8783
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.63s
                      Time elapsed: 01:02:27
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 13049 steps/s (collection: 7.410s, learning 0.123s)
             Mean action noise std: 3.18
          Mean value_function loss: 184.3908
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.4857
                       Mean reward: 807.54
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.6054
     Episode_Reward/lifting_object: 167.0994
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.53s
                      Time elapsed: 01:02:35
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13062 steps/s (collection: 7.378s, learning 0.148s)
             Mean action noise std: 3.18
          Mean value_function loss: 198.2353
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.5002
                       Mean reward: 799.94
               Mean episode length: 217.47
    Episode_Reward/reaching_object: 1.5867
     Episode_Reward/lifting_object: 164.7256
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.53s
                      Time elapsed: 01:02:43
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 12963 steps/s (collection: 7.455s, learning 0.129s)
             Mean action noise std: 3.18
          Mean value_function loss: 172.5121
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.5108
                       Mean reward: 864.63
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.5972
     Episode_Reward/lifting_object: 166.0586
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.58s
                      Time elapsed: 01:02:50
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 12853 steps/s (collection: 7.489s, learning 0.159s)
             Mean action noise std: 3.19
          Mean value_function loss: 198.6925
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.5291
                       Mean reward: 855.68
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.5667
     Episode_Reward/lifting_object: 162.7366
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.65s
                      Time elapsed: 01:02:58
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 12068 steps/s (collection: 7.981s, learning 0.165s)
             Mean action noise std: 3.19
          Mean value_function loss: 156.7203
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.5495
                       Mean reward: 872.72
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 1.6171
     Episode_Reward/lifting_object: 167.2588
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 8.15s
                      Time elapsed: 01:03:06
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 11153 steps/s (collection: 8.681s, learning 0.133s)
             Mean action noise std: 3.19
          Mean value_function loss: 152.5990
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.5688
                       Mean reward: 819.00
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 1.6241
     Episode_Reward/lifting_object: 168.9521
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 8.81s
                      Time elapsed: 01:03:15
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 38672 steps/s (collection: 2.429s, learning 0.113s)
             Mean action noise std: 3.19
          Mean value_function loss: 178.7525
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.5911
                       Mean reward: 802.27
               Mean episode length: 217.98
    Episode_Reward/reaching_object: 1.5897
     Episode_Reward/lifting_object: 165.3022
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.54s
                      Time elapsed: 01:03:17
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 39030 steps/s (collection: 2.384s, learning 0.134s)
             Mean action noise std: 3.20
          Mean value_function loss: 204.1045
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.6130
                       Mean reward: 831.58
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 1.5909
     Episode_Reward/lifting_object: 164.9178
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.52s
                      Time elapsed: 01:03:20
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 39703 steps/s (collection: 2.339s, learning 0.137s)
             Mean action noise std: 3.20
          Mean value_function loss: 183.3790
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.6279
                       Mean reward: 807.20
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 1.5451
     Episode_Reward/lifting_object: 160.6531
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.48s
                      Time elapsed: 01:03:22
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 39851 steps/s (collection: 2.347s, learning 0.120s)
             Mean action noise std: 3.20
          Mean value_function loss: 174.8011
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.6438
                       Mean reward: 826.95
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 1.5997
     Episode_Reward/lifting_object: 166.0986
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.47s
                      Time elapsed: 01:03:25
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 38667 steps/s (collection: 2.408s, learning 0.134s)
             Mean action noise std: 3.20
          Mean value_function loss: 209.3587
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.6640
                       Mean reward: 844.01
               Mean episode length: 228.71
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 164.7505
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.54s
                      Time elapsed: 01:03:27
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 39374 steps/s (collection: 2.365s, learning 0.132s)
             Mean action noise std: 3.20
          Mean value_function loss: 199.4884
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.6822
                       Mean reward: 830.86
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 160.7210
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.50s
                      Time elapsed: 01:03:30
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 40350 steps/s (collection: 2.314s, learning 0.122s)
             Mean action noise std: 3.21
          Mean value_function loss: 221.7070
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.6942
                       Mean reward: 811.57
               Mean episode length: 220.69
    Episode_Reward/reaching_object: 1.5877
     Episode_Reward/lifting_object: 165.0824
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.44s
                      Time elapsed: 01:03:32
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 39414 steps/s (collection: 2.388s, learning 0.106s)
             Mean action noise std: 3.21
          Mean value_function loss: 204.5497
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.7052
                       Mean reward: 825.33
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 1.5986
     Episode_Reward/lifting_object: 166.6903
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.49s
                      Time elapsed: 01:03:35
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 39632 steps/s (collection: 2.361s, learning 0.119s)
             Mean action noise std: 3.21
          Mean value_function loss: 185.8112
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.7163
                       Mean reward: 822.19
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.5891
     Episode_Reward/lifting_object: 164.9403
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.48s
                      Time elapsed: 01:03:37
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 35422 steps/s (collection: 2.620s, learning 0.156s)
             Mean action noise std: 3.21
          Mean value_function loss: 177.8560
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.7259
                       Mean reward: 829.64
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 1.6201
     Episode_Reward/lifting_object: 169.3603
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.78s
                      Time elapsed: 01:03:40
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 35035 steps/s (collection: 2.642s, learning 0.163s)
             Mean action noise std: 3.21
          Mean value_function loss: 236.9865
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.7358
                       Mean reward: 871.77
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.5812
     Episode_Reward/lifting_object: 164.2487
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.81s
                      Time elapsed: 01:03:43
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 35181 steps/s (collection: 2.665s, learning 0.129s)
             Mean action noise std: 3.21
          Mean value_function loss: 157.0412
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.7508
                       Mean reward: 836.91
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 1.6157
     Episode_Reward/lifting_object: 169.4572
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.79s
                      Time elapsed: 01:03:46
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 36129 steps/s (collection: 2.593s, learning 0.128s)
             Mean action noise std: 3.21
          Mean value_function loss: 169.6642
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.7654
                       Mean reward: 828.18
               Mean episode length: 224.40
    Episode_Reward/reaching_object: 1.5765
     Episode_Reward/lifting_object: 165.3629
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.72s
                      Time elapsed: 01:03:48
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 36513 steps/s (collection: 2.542s, learning 0.150s)
             Mean action noise std: 3.22
          Mean value_function loss: 190.1850
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.7777
                       Mean reward: 761.90
               Mean episode length: 212.70
    Episode_Reward/reaching_object: 1.5612
     Episode_Reward/lifting_object: 162.7894
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.69s
                      Time elapsed: 01:03:51
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 37766 steps/s (collection: 2.460s, learning 0.143s)
             Mean action noise std: 3.22
          Mean value_function loss: 177.0815
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.7964
                       Mean reward: 834.63
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 1.5601
     Episode_Reward/lifting_object: 163.7344
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.60s
                      Time elapsed: 01:03:54
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 38145 steps/s (collection: 2.450s, learning 0.128s)
             Mean action noise std: 3.22
          Mean value_function loss: 190.5724
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.8150
                       Mean reward: 831.08
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 1.5545
     Episode_Reward/lifting_object: 163.4764
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.58s
                      Time elapsed: 01:03:56
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 38760 steps/s (collection: 2.418s, learning 0.118s)
             Mean action noise std: 3.22
          Mean value_function loss: 165.1671
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.8292
                       Mean reward: 868.15
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.6105
     Episode_Reward/lifting_object: 169.9724
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.54s
                      Time elapsed: 01:03:59
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 38200 steps/s (collection: 2.446s, learning 0.128s)
             Mean action noise std: 3.23
          Mean value_function loss: 197.9826
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.8515
                       Mean reward: 834.01
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.5823
     Episode_Reward/lifting_object: 165.8524
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.57s
                      Time elapsed: 01:04:01
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 39288 steps/s (collection: 2.367s, learning 0.135s)
             Mean action noise std: 3.23
          Mean value_function loss: 211.8259
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.8740
                       Mean reward: 793.60
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 1.5089
     Episode_Reward/lifting_object: 157.1584
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.50s
                      Time elapsed: 01:04:04
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 38118 steps/s (collection: 2.446s, learning 0.133s)
             Mean action noise std: 3.23
          Mean value_function loss: 216.2696
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.8927
                       Mean reward: 866.72
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.5314
     Episode_Reward/lifting_object: 160.8873
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.58s
                      Time elapsed: 01:04:06
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 38412 steps/s (collection: 2.420s, learning 0.139s)
             Mean action noise std: 3.23
          Mean value_function loss: 222.0694
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 66.9084
                       Mean reward: 789.93
               Mean episode length: 214.67
    Episode_Reward/reaching_object: 1.5032
     Episode_Reward/lifting_object: 157.6323
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.56s
                      Time elapsed: 01:04:09
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 39084 steps/s (collection: 2.389s, learning 0.127s)
             Mean action noise std: 3.24
          Mean value_function loss: 211.3312
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.9172
                       Mean reward: 795.13
               Mean episode length: 217.61
    Episode_Reward/reaching_object: 1.5461
     Episode_Reward/lifting_object: 162.3942
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.52s
                      Time elapsed: 01:04:11
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 39700 steps/s (collection: 2.342s, learning 0.134s)
             Mean action noise std: 3.24
          Mean value_function loss: 182.5090
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.9346
                       Mean reward: 855.49
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 1.6063
     Episode_Reward/lifting_object: 169.2464
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.48s
                      Time elapsed: 01:04:14
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 39438 steps/s (collection: 2.361s, learning 0.132s)
             Mean action noise std: 3.24
          Mean value_function loss: 218.6144
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.9448
                       Mean reward: 815.90
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.5366
     Episode_Reward/lifting_object: 161.8625
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.49s
                      Time elapsed: 01:04:16
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 39857 steps/s (collection: 2.326s, learning 0.140s)
             Mean action noise std: 3.24
          Mean value_function loss: 232.5592
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.9600
                       Mean reward: 764.36
               Mean episode length: 207.69
    Episode_Reward/reaching_object: 1.5225
     Episode_Reward/lifting_object: 160.6087
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.47s
                      Time elapsed: 01:04:19
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 38817 steps/s (collection: 2.403s, learning 0.130s)
             Mean action noise std: 3.24
          Mean value_function loss: 204.3661
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.9810
                       Mean reward: 785.67
               Mean episode length: 214.60
    Episode_Reward/reaching_object: 1.5209
     Episode_Reward/lifting_object: 160.4930
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.53s
                      Time elapsed: 01:04:21
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 38515 steps/s (collection: 2.410s, learning 0.142s)
             Mean action noise std: 3.24
          Mean value_function loss: 181.8339
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.9923
                       Mean reward: 800.93
               Mean episode length: 221.19
    Episode_Reward/reaching_object: 1.5443
     Episode_Reward/lifting_object: 162.8976
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.55s
                      Time elapsed: 01:04:24
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 39136 steps/s (collection: 2.370s, learning 0.142s)
             Mean action noise std: 3.25
          Mean value_function loss: 183.1207
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.0055
                       Mean reward: 847.24
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.5304
     Episode_Reward/lifting_object: 162.3618
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.51s
                      Time elapsed: 01:04:26
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 39222 steps/s (collection: 2.386s, learning 0.121s)
             Mean action noise std: 3.25
          Mean value_function loss: 182.5321
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.0214
                       Mean reward: 788.67
               Mean episode length: 214.31
    Episode_Reward/reaching_object: 1.5694
     Episode_Reward/lifting_object: 166.5811
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.51s
                      Time elapsed: 01:04:29
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 36350 steps/s (collection: 2.560s, learning 0.145s)
             Mean action noise std: 3.25
          Mean value_function loss: 166.8323
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.0376
                       Mean reward: 837.18
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.5751
     Episode_Reward/lifting_object: 167.1715
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.70s
                      Time elapsed: 01:04:32
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 36927 steps/s (collection: 2.523s, learning 0.140s)
             Mean action noise std: 3.25
          Mean value_function loss: 234.1708
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.0562
                       Mean reward: 818.00
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.5250
     Episode_Reward/lifting_object: 161.7959
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.66s
                      Time elapsed: 01:04:34
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 37576 steps/s (collection: 2.458s, learning 0.158s)
             Mean action noise std: 3.26
          Mean value_function loss: 218.6408
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.0731
                       Mean reward: 778.65
               Mean episode length: 214.19
    Episode_Reward/reaching_object: 1.5399
     Episode_Reward/lifting_object: 162.8028
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.62s
                      Time elapsed: 01:04:37
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 37293 steps/s (collection: 2.502s, learning 0.134s)
             Mean action noise std: 3.26
          Mean value_function loss: 177.1792
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.0834
                       Mean reward: 831.77
               Mean episode length: 223.25
    Episode_Reward/reaching_object: 1.5405
     Episode_Reward/lifting_object: 163.8257
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.64s
                      Time elapsed: 01:04:40
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 39246 steps/s (collection: 2.384s, learning 0.120s)
             Mean action noise std: 3.26
          Mean value_function loss: 165.5090
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.0954
                       Mean reward: 852.09
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.5607
     Episode_Reward/lifting_object: 165.4081
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.50s
                      Time elapsed: 01:04:42
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 38170 steps/s (collection: 2.438s, learning 0.137s)
             Mean action noise std: 3.26
          Mean value_function loss: 172.3100
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.1116
                       Mean reward: 888.58
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.5881
     Episode_Reward/lifting_object: 169.0323
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.58s
                      Time elapsed: 01:04:45
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 37980 steps/s (collection: 2.462s, learning 0.127s)
             Mean action noise std: 3.26
          Mean value_function loss: 145.5168
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.1291
                       Mean reward: 837.45
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 1.5470
     Episode_Reward/lifting_object: 163.9598
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.59s
                      Time elapsed: 01:04:47
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 39181 steps/s (collection: 2.353s, learning 0.156s)
             Mean action noise std: 3.27
          Mean value_function loss: 183.1106
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.1447
                       Mean reward: 843.20
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.5344
     Episode_Reward/lifting_object: 162.2514
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.51s
                      Time elapsed: 01:04:50
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 38799 steps/s (collection: 2.386s, learning 0.147s)
             Mean action noise std: 3.27
          Mean value_function loss: 165.2084
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.1628
                       Mean reward: 851.74
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 1.5671
     Episode_Reward/lifting_object: 165.6931
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.53s
                      Time elapsed: 01:04:52
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 37759 steps/s (collection: 2.473s, learning 0.130s)
             Mean action noise std: 3.27
          Mean value_function loss: 202.8834
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.1774
                       Mean reward: 816.70
               Mean episode length: 220.18
    Episode_Reward/reaching_object: 1.5610
     Episode_Reward/lifting_object: 165.4192
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.60s
                      Time elapsed: 01:04:55
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 39040 steps/s (collection: 2.383s, learning 0.135s)
             Mean action noise std: 3.27
          Mean value_function loss: 170.3617
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 67.1849
                       Mean reward: 813.17
               Mean episode length: 219.92
    Episode_Reward/reaching_object: 1.5378
     Episode_Reward/lifting_object: 162.3095
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.52s
                      Time elapsed: 01:04:57
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 38846 steps/s (collection: 2.401s, learning 0.130s)
             Mean action noise std: 3.27
          Mean value_function loss: 177.8644
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 67.1888
                       Mean reward: 823.68
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.5630
     Episode_Reward/lifting_object: 165.1521
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.53s
                      Time elapsed: 01:05:00
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 39059 steps/s (collection: 2.375s, learning 0.142s)
             Mean action noise std: 3.27
          Mean value_function loss: 199.8783
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.1936
                       Mean reward: 831.60
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.5584
     Episode_Reward/lifting_object: 163.7245
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.52s
                      Time elapsed: 01:05:02
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 38977 steps/s (collection: 2.408s, learning 0.114s)
             Mean action noise std: 3.27
          Mean value_function loss: 176.6404
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.2048
                       Mean reward: 816.27
               Mean episode length: 222.03
    Episode_Reward/reaching_object: 1.5398
     Episode_Reward/lifting_object: 162.7354
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.52s
                      Time elapsed: 01:05:05
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 38650 steps/s (collection: 2.392s, learning 0.151s)
             Mean action noise std: 3.27
          Mean value_function loss: 173.2348
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.2188
                       Mean reward: 862.03
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.6025
     Episode_Reward/lifting_object: 169.2275
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.54s
                      Time elapsed: 01:05:08
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 37892 steps/s (collection: 2.473s, learning 0.121s)
             Mean action noise std: 3.28
          Mean value_function loss: 202.4879
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.2316
                       Mean reward: 840.95
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 1.5423
     Episode_Reward/lifting_object: 163.5738
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.59s
                      Time elapsed: 01:05:10
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 37932 steps/s (collection: 2.455s, learning 0.136s)
             Mean action noise std: 3.28
          Mean value_function loss: 181.5060
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.2438
                       Mean reward: 831.57
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.5261
     Episode_Reward/lifting_object: 161.4617
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.59s
                      Time elapsed: 01:05:13
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 38958 steps/s (collection: 2.405s, learning 0.118s)
             Mean action noise std: 3.28
          Mean value_function loss: 194.4358
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.2619
                       Mean reward: 817.17
               Mean episode length: 222.09
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 161.2730
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.52s
                      Time elapsed: 01:05:15
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 38398 steps/s (collection: 2.424s, learning 0.136s)
             Mean action noise std: 3.28
          Mean value_function loss: 248.6180
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.2796
                       Mean reward: 814.39
               Mean episode length: 220.12
    Episode_Reward/reaching_object: 1.5468
     Episode_Reward/lifting_object: 163.8180
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.56s
                      Time elapsed: 01:05:18
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 37143 steps/s (collection: 2.510s, learning 0.136s)
             Mean action noise std: 3.29
          Mean value_function loss: 185.2533
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.2940
                       Mean reward: 824.25
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 1.5488
     Episode_Reward/lifting_object: 164.0965
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.65s
                      Time elapsed: 01:05:20
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 39108 steps/s (collection: 2.378s, learning 0.135s)
             Mean action noise std: 3.29
          Mean value_function loss: 155.0182
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.3161
                       Mean reward: 820.75
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 1.5464
     Episode_Reward/lifting_object: 163.5047
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.51s
                      Time elapsed: 01:05:23
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 39528 steps/s (collection: 2.348s, learning 0.139s)
             Mean action noise std: 3.29
          Mean value_function loss: 169.6590
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.3336
                       Mean reward: 838.56
               Mean episode length: 225.08
    Episode_Reward/reaching_object: 1.5984
     Episode_Reward/lifting_object: 169.7634
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.49s
                      Time elapsed: 01:05:25
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 38626 steps/s (collection: 2.413s, learning 0.132s)
             Mean action noise std: 3.29
          Mean value_function loss: 186.2143
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.3496
                       Mean reward: 816.03
               Mean episode length: 220.84
    Episode_Reward/reaching_object: 1.5566
     Episode_Reward/lifting_object: 165.1141
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.55s
                      Time elapsed: 01:05:28
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 38620 steps/s (collection: 2.428s, learning 0.117s)
             Mean action noise std: 3.29
          Mean value_function loss: 180.2306
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.3666
                       Mean reward: 833.25
               Mean episode length: 223.69
    Episode_Reward/reaching_object: 1.5510
     Episode_Reward/lifting_object: 165.3068
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.55s
                      Time elapsed: 01:05:31
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 38992 steps/s (collection: 2.395s, learning 0.126s)
             Mean action noise std: 3.30
          Mean value_function loss: 160.8887
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.3859
                       Mean reward: 842.97
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 1.5554
     Episode_Reward/lifting_object: 165.9012
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.52s
                      Time elapsed: 01:05:33
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 39793 steps/s (collection: 2.349s, learning 0.121s)
             Mean action noise std: 3.30
          Mean value_function loss: 168.5812
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.3992
                       Mean reward: 830.62
               Mean episode length: 224.43
    Episode_Reward/reaching_object: 1.5224
     Episode_Reward/lifting_object: 161.4920
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.47s
                      Time elapsed: 01:05:36
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 39014 steps/s (collection: 2.404s, learning 0.116s)
             Mean action noise std: 3.30
          Mean value_function loss: 209.3701
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.4100
                       Mean reward: 812.74
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 1.5299
     Episode_Reward/lifting_object: 162.2394
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.52s
                      Time elapsed: 01:05:38
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 38520 steps/s (collection: 2.428s, learning 0.124s)
             Mean action noise std: 3.30
          Mean value_function loss: 169.4052
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.4227
                       Mean reward: 795.24
               Mean episode length: 214.53
    Episode_Reward/reaching_object: 1.5393
     Episode_Reward/lifting_object: 163.9703
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.55s
                      Time elapsed: 01:05:41
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 39796 steps/s (collection: 2.345s, learning 0.125s)
             Mean action noise std: 3.31
          Mean value_function loss: 180.2127
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4417
                       Mean reward: 802.93
               Mean episode length: 217.42
    Episode_Reward/reaching_object: 1.5426
     Episode_Reward/lifting_object: 163.7601
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.47s
                      Time elapsed: 01:05:43
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 39726 steps/s (collection: 2.365s, learning 0.109s)
             Mean action noise std: 3.31
          Mean value_function loss: 193.9097
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.4694
                       Mean reward: 872.99
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 167.8290
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.47s
                      Time elapsed: 01:05:46
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 39927 steps/s (collection: 2.329s, learning 0.133s)
             Mean action noise std: 3.31
          Mean value_function loss: 159.0076
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4919
                       Mean reward: 833.47
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 169.8820
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.46s
                      Time elapsed: 01:05:48
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 39326 steps/s (collection: 2.352s, learning 0.147s)
             Mean action noise std: 3.31
          Mean value_function loss: 176.2689
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.5121
                       Mean reward: 812.72
               Mean episode length: 220.05
    Episode_Reward/reaching_object: 1.5596
     Episode_Reward/lifting_object: 166.1953
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.50s
                      Time elapsed: 01:05:50
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 40280 steps/s (collection: 2.311s, learning 0.130s)
             Mean action noise std: 3.32
          Mean value_function loss: 194.7522
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.5394
                       Mean reward: 832.17
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 1.5712
     Episode_Reward/lifting_object: 166.9957
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.44s
                      Time elapsed: 01:05:53
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 39139 steps/s (collection: 2.370s, learning 0.142s)
             Mean action noise std: 3.32
          Mean value_function loss: 190.3483
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.5590
                       Mean reward: 775.39
               Mean episode length: 211.70
    Episode_Reward/reaching_object: 1.5282
     Episode_Reward/lifting_object: 162.7476
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.51s
                      Time elapsed: 01:05:55
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 39036 steps/s (collection: 2.394s, learning 0.125s)
             Mean action noise std: 3.32
          Mean value_function loss: 226.4426
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.5702
                       Mean reward: 847.59
               Mean episode length: 225.84
    Episode_Reward/reaching_object: 1.5200
     Episode_Reward/lifting_object: 161.8936
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.52s
                      Time elapsed: 01:05:58
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 38667 steps/s (collection: 2.427s, learning 0.115s)
             Mean action noise std: 3.32
          Mean value_function loss: 180.7040
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 67.5832
                       Mean reward: 842.67
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.5200
     Episode_Reward/lifting_object: 161.1094
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.54s
                      Time elapsed: 01:06:00
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 39896 steps/s (collection: 2.340s, learning 0.124s)
             Mean action noise std: 3.32
          Mean value_function loss: 193.1553
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.5906
                       Mean reward: 821.82
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 1.5509
     Episode_Reward/lifting_object: 165.2259
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.46s
                      Time elapsed: 01:06:03
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 38910 steps/s (collection: 2.418s, learning 0.108s)
             Mean action noise std: 3.33
          Mean value_function loss: 172.4906
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.6065
                       Mean reward: 848.82
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.5420
     Episode_Reward/lifting_object: 164.1649
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.53s
                      Time elapsed: 01:06:05
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 36918 steps/s (collection: 2.512s, learning 0.151s)
             Mean action noise std: 3.33
          Mean value_function loss: 170.8594
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.6306
                       Mean reward: 771.40
               Mean episode length: 208.94
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 159.3923
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.66s
                      Time elapsed: 01:06:08
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 39646 steps/s (collection: 2.366s, learning 0.113s)
             Mean action noise std: 3.33
          Mean value_function loss: 222.9542
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.6527
                       Mean reward: 850.92
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.5725
     Episode_Reward/lifting_object: 167.9897
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.48s
                      Time elapsed: 01:06:11
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 39457 steps/s (collection: 2.369s, learning 0.123s)
             Mean action noise std: 3.33
          Mean value_function loss: 237.2465
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.6679
                       Mean reward: 887.28
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.5591
     Episode_Reward/lifting_object: 166.4482
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.49s
                      Time elapsed: 01:06:13
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 38816 steps/s (collection: 2.417s, learning 0.115s)
             Mean action noise std: 3.34
          Mean value_function loss: 173.0199
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.6774
                       Mean reward: 837.49
               Mean episode length: 226.79
    Episode_Reward/reaching_object: 1.5707
     Episode_Reward/lifting_object: 167.1585
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.53s
                      Time elapsed: 01:06:16
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 39468 steps/s (collection: 2.332s, learning 0.159s)
             Mean action noise std: 3.34
          Mean value_function loss: 151.9027
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.6876
                       Mean reward: 885.40
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 1.5275
     Episode_Reward/lifting_object: 163.2062
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.49s
                      Time elapsed: 01:06:18
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 38310 steps/s (collection: 2.454s, learning 0.112s)
             Mean action noise std: 3.34
          Mean value_function loss: 170.7093
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.6968
                       Mean reward: 863.31
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.5697
     Episode_Reward/lifting_object: 166.8108
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.57s
                      Time elapsed: 01:06:21
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 39482 steps/s (collection: 2.349s, learning 0.141s)
             Mean action noise std: 3.34
          Mean value_function loss: 174.6596
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.7052
                       Mean reward: 852.06
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.5713
     Episode_Reward/lifting_object: 167.6454
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.49s
                      Time elapsed: 01:06:23
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 40192 steps/s (collection: 2.316s, learning 0.129s)
             Mean action noise std: 3.34
          Mean value_function loss: 198.8996
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.7197
                       Mean reward: 845.94
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 1.5374
     Episode_Reward/lifting_object: 163.8294
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.45s
                      Time elapsed: 01:06:26
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 38981 steps/s (collection: 2.390s, learning 0.132s)
             Mean action noise std: 3.34
          Mean value_function loss: 189.7416
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.7407
                       Mean reward: 837.34
               Mean episode length: 226.86
    Episode_Reward/reaching_object: 1.5108
     Episode_Reward/lifting_object: 160.3257
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.52s
                      Time elapsed: 01:06:28
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 33562 steps/s (collection: 2.799s, learning 0.130s)
             Mean action noise std: 3.35
          Mean value_function loss: 180.4673
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 67.7548
                       Mean reward: 835.59
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 1.5542
     Episode_Reward/lifting_object: 165.6989
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.93s
                      Time elapsed: 01:06:31
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 39541 steps/s (collection: 2.361s, learning 0.126s)
             Mean action noise std: 3.35
          Mean value_function loss: 162.6224
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.7764
                       Mean reward: 828.31
               Mean episode length: 223.80
    Episode_Reward/reaching_object: 1.5864
     Episode_Reward/lifting_object: 169.3917
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.49s
                      Time elapsed: 01:06:34
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 36280 steps/s (collection: 2.587s, learning 0.122s)
             Mean action noise std: 3.35
          Mean value_function loss: 129.0024
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.8001
                       Mean reward: 861.72
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.5946
     Episode_Reward/lifting_object: 169.9781
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.71s
                      Time elapsed: 01:06:36
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 37877 steps/s (collection: 2.468s, learning 0.127s)
             Mean action noise std: 3.35
          Mean value_function loss: 143.1951
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.8172
                       Mean reward: 878.87
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 1.5719
     Episode_Reward/lifting_object: 167.5162
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.60s
                      Time elapsed: 01:06:39
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 39050 steps/s (collection: 2.385s, learning 0.133s)
             Mean action noise std: 3.36
          Mean value_function loss: 132.6866
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.8298
                       Mean reward: 836.11
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.6019
     Episode_Reward/lifting_object: 171.0497
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.52s
                      Time elapsed: 01:06:41
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 37572 steps/s (collection: 2.478s, learning 0.139s)
             Mean action noise std: 3.36
          Mean value_function loss: 158.2344
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.8429
                       Mean reward: 781.89
               Mean episode length: 214.12
    Episode_Reward/reaching_object: 1.5496
     Episode_Reward/lifting_object: 164.5273
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.62s
                      Time elapsed: 01:06:44
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 34599 steps/s (collection: 2.703s, learning 0.138s)
             Mean action noise std: 3.36
          Mean value_function loss: 195.1851
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.8548
                       Mean reward: 807.49
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 1.5461
     Episode_Reward/lifting_object: 164.4360
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.84s
                      Time elapsed: 01:06:47
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 37898 steps/s (collection: 2.448s, learning 0.146s)
             Mean action noise std: 3.36
          Mean value_function loss: 170.4997
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.8709
                       Mean reward: 781.05
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 1.5674
     Episode_Reward/lifting_object: 165.6880
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.59s
                      Time elapsed: 01:06:49
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 35145 steps/s (collection: 2.675s, learning 0.122s)
             Mean action noise std: 3.36
          Mean value_function loss: 196.1658
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.8854
                       Mean reward: 792.03
               Mean episode length: 215.10
    Episode_Reward/reaching_object: 1.5384
     Episode_Reward/lifting_object: 162.8080
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.80s
                      Time elapsed: 01:06:52
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 37781 steps/s (collection: 2.470s, learning 0.132s)
             Mean action noise std: 3.36
          Mean value_function loss: 177.0344
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.8963
                       Mean reward: 870.43
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.5724
     Episode_Reward/lifting_object: 166.2928
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.60s
                      Time elapsed: 01:06:55
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 34726 steps/s (collection: 2.709s, learning 0.122s)
             Mean action noise std: 3.37
          Mean value_function loss: 216.8252
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.9145
                       Mean reward: 831.64
               Mean episode length: 224.51
    Episode_Reward/reaching_object: 1.5534
     Episode_Reward/lifting_object: 164.2661
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.83s
                      Time elapsed: 01:06:58
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 35669 steps/s (collection: 2.608s, learning 0.148s)
             Mean action noise std: 3.37
          Mean value_function loss: 187.6695
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.9333
                       Mean reward: 823.17
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 1.5730
     Episode_Reward/lifting_object: 165.5737
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.76s
                      Time elapsed: 01:07:00
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 35313 steps/s (collection: 2.654s, learning 0.130s)
             Mean action noise std: 3.37
          Mean value_function loss: 192.9556
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.9454
                       Mean reward: 807.84
               Mean episode length: 218.98
    Episode_Reward/reaching_object: 1.5108
     Episode_Reward/lifting_object: 159.2746
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.78s
                      Time elapsed: 01:07:03
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 37145 steps/s (collection: 2.503s, learning 0.143s)
             Mean action noise std: 3.37
          Mean value_function loss: 155.9883
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.9571
                       Mean reward: 850.00
               Mean episode length: 228.12
    Episode_Reward/reaching_object: 1.5922
     Episode_Reward/lifting_object: 167.8391
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.65s
                      Time elapsed: 01:07:06
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 36032 steps/s (collection: 2.582s, learning 0.146s)
             Mean action noise std: 3.37
          Mean value_function loss: 166.6835
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 67.9675
                       Mean reward: 857.66
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.5723
     Episode_Reward/lifting_object: 166.2802
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.73s
                      Time elapsed: 01:07:09
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 37459 steps/s (collection: 2.500s, learning 0.125s)
             Mean action noise std: 3.38
          Mean value_function loss: 249.0637
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.9861
                       Mean reward: 759.10
               Mean episode length: 206.40
    Episode_Reward/reaching_object: 1.4784
     Episode_Reward/lifting_object: 155.1338
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.62s
                      Time elapsed: 01:07:11
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 37170 steps/s (collection: 2.528s, learning 0.117s)
             Mean action noise std: 3.38
          Mean value_function loss: 160.8956
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.0019
                       Mean reward: 849.69
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 1.5643
     Episode_Reward/lifting_object: 165.3734
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.64s
                      Time elapsed: 01:07:14
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 35826 steps/s (collection: 2.607s, learning 0.136s)
             Mean action noise std: 3.38
          Mean value_function loss: 172.9047
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.0200
                       Mean reward: 827.09
               Mean episode length: 223.24
    Episode_Reward/reaching_object: 1.6018
     Episode_Reward/lifting_object: 169.7094
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.74s
                      Time elapsed: 01:07:17
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 36471 steps/s (collection: 2.569s, learning 0.126s)
             Mean action noise std: 3.38
          Mean value_function loss: 191.3391
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.0402
                       Mean reward: 806.26
               Mean episode length: 220.24
    Episode_Reward/reaching_object: 1.5197
     Episode_Reward/lifting_object: 159.6248
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.70s
                      Time elapsed: 01:07:19
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 36573 steps/s (collection: 2.555s, learning 0.133s)
             Mean action noise std: 3.39
          Mean value_function loss: 165.4072
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.0607
                       Mean reward: 833.09
               Mean episode length: 224.29
    Episode_Reward/reaching_object: 1.5883
     Episode_Reward/lifting_object: 167.6175
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.69s
                      Time elapsed: 01:07:22
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 36572 steps/s (collection: 2.530s, learning 0.158s)
             Mean action noise std: 3.39
          Mean value_function loss: 163.2853
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 68.0717
                       Mean reward: 867.84
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 1.5855
     Episode_Reward/lifting_object: 167.1656
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.69s
                      Time elapsed: 01:07:25
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 36184 steps/s (collection: 2.554s, learning 0.162s)
             Mean action noise std: 3.39
          Mean value_function loss: 182.6082
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.0802
                       Mean reward: 791.65
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 1.5325
     Episode_Reward/lifting_object: 160.7150
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.72s
                      Time elapsed: 01:07:27
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 37093 steps/s (collection: 2.480s, learning 0.170s)
             Mean action noise std: 3.39
          Mean value_function loss: 175.9599
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.0945
                       Mean reward: 845.91
               Mean episode length: 228.69
    Episode_Reward/reaching_object: 1.5625
     Episode_Reward/lifting_object: 165.4828
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.65s
                      Time elapsed: 01:07:30
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 36065 steps/s (collection: 2.599s, learning 0.127s)
             Mean action noise std: 3.39
          Mean value_function loss: 148.3726
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.1145
                       Mean reward: 878.56
               Mean episode length: 234.24
    Episode_Reward/reaching_object: 1.5988
     Episode_Reward/lifting_object: 168.9725
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.73s
                      Time elapsed: 01:07:33
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 37329 steps/s (collection: 2.476s, learning 0.158s)
             Mean action noise std: 3.40
          Mean value_function loss: 184.7056
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.1256
                       Mean reward: 796.96
               Mean episode length: 216.86
    Episode_Reward/reaching_object: 1.5627
     Episode_Reward/lifting_object: 164.4732
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.63s
                      Time elapsed: 01:07:35
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 38508 steps/s (collection: 2.422s, learning 0.131s)
             Mean action noise std: 3.40
          Mean value_function loss: 176.5436
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.1412
                       Mean reward: 875.65
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.5616
     Episode_Reward/lifting_object: 164.7653
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.55s
                      Time elapsed: 01:07:38
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 36759 steps/s (collection: 2.526s, learning 0.149s)
             Mean action noise std: 3.40
          Mean value_function loss: 175.6592
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 68.1594
                       Mean reward: 850.47
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 1.5550
     Episode_Reward/lifting_object: 163.8346
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.67s
                      Time elapsed: 01:07:41
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 36648 steps/s (collection: 2.517s, learning 0.166s)
             Mean action noise std: 3.40
          Mean value_function loss: 164.1551
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.1658
                       Mean reward: 871.55
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 1.5434
     Episode_Reward/lifting_object: 162.3221
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.68s
                      Time elapsed: 01:07:43
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 37169 steps/s (collection: 2.505s, learning 0.140s)
             Mean action noise std: 3.40
          Mean value_function loss: 198.7096
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.1727
                       Mean reward: 825.51
               Mean episode length: 222.58
    Episode_Reward/reaching_object: 1.5515
     Episode_Reward/lifting_object: 162.8468
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.64s
                      Time elapsed: 01:07:46
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 37670 steps/s (collection: 2.479s, learning 0.130s)
             Mean action noise std: 3.40
          Mean value_function loss: 155.8643
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.1864
                       Mean reward: 843.49
               Mean episode length: 227.43
    Episode_Reward/reaching_object: 1.5631
     Episode_Reward/lifting_object: 163.4483
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.61s
                      Time elapsed: 01:07:49
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 38571 steps/s (collection: 2.416s, learning 0.132s)
             Mean action noise std: 3.41
          Mean value_function loss: 160.9815
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.2038
                       Mean reward: 862.78
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 169.4849
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.55s
                      Time elapsed: 01:07:51
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 37826 steps/s (collection: 2.451s, learning 0.148s)
             Mean action noise std: 3.41
          Mean value_function loss: 195.2398
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.2227
                       Mean reward: 835.52
               Mean episode length: 223.53
    Episode_Reward/reaching_object: 1.5428
     Episode_Reward/lifting_object: 162.5185
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.60s
                      Time elapsed: 01:07:54
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 36689 steps/s (collection: 2.541s, learning 0.138s)
             Mean action noise std: 3.41
          Mean value_function loss: 201.7025
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.2482
                       Mean reward: 824.49
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.5571
     Episode_Reward/lifting_object: 163.9029
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.68s
                      Time elapsed: 01:07:56
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 37483 steps/s (collection: 2.471s, learning 0.151s)
             Mean action noise std: 3.42
          Mean value_function loss: 173.2328
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.2750
                       Mean reward: 827.97
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 1.5764
     Episode_Reward/lifting_object: 166.2881
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.62s
                      Time elapsed: 01:07:59
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 39201 steps/s (collection: 2.362s, learning 0.146s)
             Mean action noise std: 3.42
          Mean value_function loss: 150.0830
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.2936
                       Mean reward: 857.89
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 1.6140
     Episode_Reward/lifting_object: 170.2094
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.51s
                      Time elapsed: 01:08:02
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 39773 steps/s (collection: 2.313s, learning 0.159s)
             Mean action noise std: 3.42
          Mean value_function loss: 193.7627
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.3107
                       Mean reward: 832.12
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.5689
     Episode_Reward/lifting_object: 165.2583
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.47s
                      Time elapsed: 01:08:04
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 38189 steps/s (collection: 2.434s, learning 0.140s)
             Mean action noise std: 3.42
          Mean value_function loss: 220.3896
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.3215
                       Mean reward: 807.19
               Mean episode length: 218.47
    Episode_Reward/reaching_object: 1.5406
     Episode_Reward/lifting_object: 162.1189
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.57s
                      Time elapsed: 01:08:07
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 39229 steps/s (collection: 2.371s, learning 0.135s)
             Mean action noise std: 3.42
          Mean value_function loss: 203.1652
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.3371
                       Mean reward: 871.79
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.5707
     Episode_Reward/lifting_object: 165.0700
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.51s
                      Time elapsed: 01:08:09
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 36824 steps/s (collection: 2.534s, learning 0.135s)
             Mean action noise std: 3.43
          Mean value_function loss: 178.1284
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.3563
                       Mean reward: 819.52
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 1.5218
     Episode_Reward/lifting_object: 160.1100
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.67s
                      Time elapsed: 01:08:12
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 37397 steps/s (collection: 2.495s, learning 0.134s)
             Mean action noise std: 3.43
          Mean value_function loss: 176.7507
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 68.3757
                       Mean reward: 829.53
               Mean episode length: 225.26
    Episode_Reward/reaching_object: 1.5542
     Episode_Reward/lifting_object: 163.4734
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.63s
                      Time elapsed: 01:08:14
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 36955 steps/s (collection: 2.520s, learning 0.140s)
             Mean action noise std: 3.43
          Mean value_function loss: 184.0981
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.3887
                       Mean reward: 821.41
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.5800
     Episode_Reward/lifting_object: 166.8638
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.66s
                      Time elapsed: 01:08:17
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 39060 steps/s (collection: 2.391s, learning 0.126s)
             Mean action noise std: 3.43
          Mean value_function loss: 159.8106
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.4028
                       Mean reward: 838.48
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.5586
     Episode_Reward/lifting_object: 163.0678
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.52s
                      Time elapsed: 01:08:20
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 39056 steps/s (collection: 2.401s, learning 0.116s)
             Mean action noise std: 3.43
          Mean value_function loss: 224.8738
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.4149
                       Mean reward: 836.17
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.5561
     Episode_Reward/lifting_object: 164.3958
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.52s
                      Time elapsed: 01:08:22
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 38774 steps/s (collection: 2.417s, learning 0.118s)
             Mean action noise std: 3.44
          Mean value_function loss: 185.9648
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.4322
                       Mean reward: 837.12
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 1.5411
     Episode_Reward/lifting_object: 162.3509
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.54s
                      Time elapsed: 01:08:25
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 38140 steps/s (collection: 2.465s, learning 0.112s)
             Mean action noise std: 3.44
          Mean value_function loss: 182.9954
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.4429
                       Mean reward: 855.65
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.5436
     Episode_Reward/lifting_object: 161.7886
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.58s
                      Time elapsed: 01:08:27
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 37664 steps/s (collection: 2.481s, learning 0.129s)
             Mean action noise std: 3.44
          Mean value_function loss: 186.0457
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.4557
                       Mean reward: 803.22
               Mean episode length: 219.77
    Episode_Reward/reaching_object: 1.5417
     Episode_Reward/lifting_object: 162.5772
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.61s
                      Time elapsed: 01:08:30
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 39501 steps/s (collection: 2.364s, learning 0.124s)
             Mean action noise std: 3.44
          Mean value_function loss: 176.0300
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.4703
                       Mean reward: 805.83
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 1.5859
     Episode_Reward/lifting_object: 167.7657
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.49s
                      Time elapsed: 01:08:32
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 39783 steps/s (collection: 2.365s, learning 0.106s)
             Mean action noise std: 3.44
          Mean value_function loss: 180.7498
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.4859
                       Mean reward: 846.53
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.5492
     Episode_Reward/lifting_object: 163.9912
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.47s
                      Time elapsed: 01:08:35
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 38190 steps/s (collection: 2.453s, learning 0.121s)
             Mean action noise std: 3.45
          Mean value_function loss: 210.5088
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.5023
                       Mean reward: 870.29
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.5396
     Episode_Reward/lifting_object: 162.6510
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.57s
                      Time elapsed: 01:08:37
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 37899 steps/s (collection: 2.463s, learning 0.131s)
             Mean action noise std: 3.45
          Mean value_function loss: 224.7498
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.5151
                       Mean reward: 794.44
               Mean episode length: 217.09
    Episode_Reward/reaching_object: 1.5349
     Episode_Reward/lifting_object: 162.7696
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.59s
                      Time elapsed: 01:08:40
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 38533 steps/s (collection: 2.422s, learning 0.130s)
             Mean action noise std: 3.45
          Mean value_function loss: 228.1661
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.5334
                       Mean reward: 779.91
               Mean episode length: 216.30
    Episode_Reward/reaching_object: 1.5406
     Episode_Reward/lifting_object: 163.1962
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.55s
                      Time elapsed: 01:08:42
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 37976 steps/s (collection: 2.467s, learning 0.122s)
             Mean action noise std: 3.45
          Mean value_function loss: 171.4400
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.5490
                       Mean reward: 751.03
               Mean episode length: 206.77
    Episode_Reward/reaching_object: 1.5196
     Episode_Reward/lifting_object: 161.1154
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.59s
                      Time elapsed: 01:08:45
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 38805 steps/s (collection: 2.382s, learning 0.152s)
             Mean action noise std: 3.45
          Mean value_function loss: 201.5953
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.5628
                       Mean reward: 775.19
               Mean episode length: 210.35
    Episode_Reward/reaching_object: 1.5593
     Episode_Reward/lifting_object: 165.9449
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.53s
                      Time elapsed: 01:08:48
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 37104 steps/s (collection: 2.551s, learning 0.098s)
             Mean action noise std: 3.46
          Mean value_function loss: 177.0622
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.5728
                       Mean reward: 785.05
               Mean episode length: 213.69
    Episode_Reward/reaching_object: 1.5163
     Episode_Reward/lifting_object: 160.9076
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.65s
                      Time elapsed: 01:08:50
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 39545 steps/s (collection: 2.373s, learning 0.113s)
             Mean action noise std: 3.46
          Mean value_function loss: 153.0675
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.5912
                       Mean reward: 863.93
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.5770
     Episode_Reward/lifting_object: 167.9265
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.49s
                      Time elapsed: 01:08:53
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 39640 steps/s (collection: 2.365s, learning 0.115s)
             Mean action noise std: 3.46
          Mean value_function loss: 161.2951
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.6159
                       Mean reward: 872.32
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.5464
     Episode_Reward/lifting_object: 163.9649
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.48s
                      Time elapsed: 01:08:55
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 38038 steps/s (collection: 2.445s, learning 0.139s)
             Mean action noise std: 3.46
          Mean value_function loss: 169.3629
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 68.6319
                       Mean reward: 789.89
               Mean episode length: 213.96
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 165.1367
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.58s
                      Time elapsed: 01:08:58
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 39132 steps/s (collection: 2.383s, learning 0.129s)
             Mean action noise std: 3.47
          Mean value_function loss: 182.6673
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 68.6422
                       Mean reward: 828.16
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 1.5622
     Episode_Reward/lifting_object: 165.7141
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.51s
                      Time elapsed: 01:09:00
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 37492 steps/s (collection: 2.480s, learning 0.142s)
             Mean action noise std: 3.47
          Mean value_function loss: 169.9934
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.6534
                       Mean reward: 796.48
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 1.5429
     Episode_Reward/lifting_object: 163.3926
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.62s
                      Time elapsed: 01:09:03
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 37664 steps/s (collection: 2.501s, learning 0.109s)
             Mean action noise std: 3.47
          Mean value_function loss: 199.5570
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.6662
                       Mean reward: 811.23
               Mean episode length: 219.18
    Episode_Reward/reaching_object: 1.5322
     Episode_Reward/lifting_object: 162.7028
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.61s
                      Time elapsed: 01:09:06
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 39844 steps/s (collection: 2.324s, learning 0.143s)
             Mean action noise std: 3.47
          Mean value_function loss: 246.8191
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.6849
                       Mean reward: 793.63
               Mean episode length: 216.46
    Episode_Reward/reaching_object: 1.4973
     Episode_Reward/lifting_object: 158.8295
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.47s
                      Time elapsed: 01:09:08
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 40022 steps/s (collection: 2.331s, learning 0.126s)
             Mean action noise std: 3.47
          Mean value_function loss: 172.8233
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 68.6988
                       Mean reward: 821.87
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 1.5475
     Episode_Reward/lifting_object: 164.8330
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.46s
                      Time elapsed: 01:09:10
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 40091 steps/s (collection: 2.335s, learning 0.117s)
             Mean action noise std: 3.48
          Mean value_function loss: 181.2641
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.7064
                       Mean reward: 879.61
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 1.5425
     Episode_Reward/lifting_object: 164.2390
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.45s
                      Time elapsed: 01:09:13
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 37334 steps/s (collection: 2.519s, learning 0.114s)
             Mean action noise std: 3.48
          Mean value_function loss: 234.4544
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.7184
                       Mean reward: 820.25
               Mean episode length: 220.59
    Episode_Reward/reaching_object: 1.5076
     Episode_Reward/lifting_object: 160.2228
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.63s
                      Time elapsed: 01:09:16
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 40000 steps/s (collection: 2.333s, learning 0.125s)
             Mean action noise std: 3.48
          Mean value_function loss: 200.8068
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.7311
                       Mean reward: 832.67
               Mean episode length: 223.14
    Episode_Reward/reaching_object: 1.5361
     Episode_Reward/lifting_object: 163.6661
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.46s
                      Time elapsed: 01:09:18
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 38027 steps/s (collection: 2.478s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 182.4019
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.7436
                       Mean reward: 804.21
               Mean episode length: 217.52
    Episode_Reward/reaching_object: 1.5221
     Episode_Reward/lifting_object: 161.2638
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.59s
                      Time elapsed: 01:09:21
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 39729 steps/s (collection: 2.342s, learning 0.132s)
             Mean action noise std: 3.48
          Mean value_function loss: 176.5094
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.7555
                       Mean reward: 815.90
               Mean episode length: 220.38
    Episode_Reward/reaching_object: 1.5687
     Episode_Reward/lifting_object: 166.8025
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.47s
                      Time elapsed: 01:09:23
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 39111 steps/s (collection: 2.391s, learning 0.122s)
             Mean action noise std: 3.48
          Mean value_function loss: 189.7315
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.7653
                       Mean reward: 803.73
               Mean episode length: 217.50
    Episode_Reward/reaching_object: 1.5340
     Episode_Reward/lifting_object: 162.6806
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.51s
                      Time elapsed: 01:09:26
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 38768 steps/s (collection: 2.405s, learning 0.131s)
             Mean action noise std: 3.49
          Mean value_function loss: 223.3560
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 68.7796
                       Mean reward: 831.55
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.5698
     Episode_Reward/lifting_object: 165.9793
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.54s
                      Time elapsed: 01:09:28
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 39510 steps/s (collection: 2.374s, learning 0.114s)
             Mean action noise std: 3.49
          Mean value_function loss: 144.0010
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.7947
                       Mean reward: 890.36
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.6118
     Episode_Reward/lifting_object: 170.9673
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.49s
                      Time elapsed: 01:09:31
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 39766 steps/s (collection: 2.334s, learning 0.138s)
             Mean action noise std: 3.49
          Mean value_function loss: 180.8528
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 68.8092
                       Mean reward: 790.34
               Mean episode length: 215.28
    Episode_Reward/reaching_object: 1.5806
     Episode_Reward/lifting_object: 167.1163
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.47s
                      Time elapsed: 01:09:33
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 37763 steps/s (collection: 2.491s, learning 0.112s)
             Mean action noise std: 3.49
          Mean value_function loss: 200.9987
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.8209
                       Mean reward: 860.53
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 163.9995
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.60s
                      Time elapsed: 01:09:36
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 39335 steps/s (collection: 2.385s, learning 0.114s)
             Mean action noise std: 3.49
          Mean value_function loss: 169.8696
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.8375
                       Mean reward: 837.01
               Mean episode length: 227.61
    Episode_Reward/reaching_object: 1.5693
     Episode_Reward/lifting_object: 164.9211
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.50s
                      Time elapsed: 01:09:38
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 37762 steps/s (collection: 2.481s, learning 0.123s)
             Mean action noise std: 3.50
          Mean value_function loss: 144.8896
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.8526
                       Mean reward: 819.10
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.5975
     Episode_Reward/lifting_object: 167.9173
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.60s
                      Time elapsed: 01:09:41
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 36659 steps/s (collection: 2.524s, learning 0.157s)
             Mean action noise std: 3.50
          Mean value_function loss: 168.4922
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 68.8651
                       Mean reward: 840.41
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.5642
     Episode_Reward/lifting_object: 164.4364
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.68s
                      Time elapsed: 01:09:43
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 36605 steps/s (collection: 2.560s, learning 0.125s)
             Mean action noise std: 3.50
          Mean value_function loss: 149.3277
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.8815
                       Mean reward: 824.94
               Mean episode length: 222.02
    Episode_Reward/reaching_object: 1.5917
     Episode_Reward/lifting_object: 168.1312
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.69s
                      Time elapsed: 01:09:46
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 38031 steps/s (collection: 2.467s, learning 0.118s)
             Mean action noise std: 3.50
          Mean value_function loss: 168.0099
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.9022
                       Mean reward: 775.38
               Mean episode length: 210.11
    Episode_Reward/reaching_object: 1.5341
     Episode_Reward/lifting_object: 161.4348
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.58s
                      Time elapsed: 01:09:49
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 36734 steps/s (collection: 2.556s, learning 0.121s)
             Mean action noise std: 3.51
          Mean value_function loss: 168.0379
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.9193
                       Mean reward: 826.81
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 1.6107
     Episode_Reward/lifting_object: 170.3206
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.68s
                      Time elapsed: 01:09:51
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 37769 steps/s (collection: 2.479s, learning 0.124s)
             Mean action noise std: 3.51
          Mean value_function loss: 181.7944
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.9465
                       Mean reward: 837.94
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 1.5702
     Episode_Reward/lifting_object: 165.4881
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.60s
                      Time elapsed: 01:09:54
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 37412 steps/s (collection: 2.505s, learning 0.122s)
             Mean action noise std: 3.51
          Mean value_function loss: 201.6237
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.9749
                       Mean reward: 781.60
               Mean episode length: 212.82
    Episode_Reward/reaching_object: 1.5296
     Episode_Reward/lifting_object: 161.1287
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.63s
                      Time elapsed: 01:09:57
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 37048 steps/s (collection: 2.506s, learning 0.147s)
             Mean action noise std: 3.52
          Mean value_function loss: 211.8968
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.9994
                       Mean reward: 836.45
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 1.5482
     Episode_Reward/lifting_object: 162.0992
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.65s
                      Time elapsed: 01:09:59
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 36160 steps/s (collection: 2.575s, learning 0.144s)
             Mean action noise std: 3.52
          Mean value_function loss: 281.3085
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.0158
                       Mean reward: 854.75
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 1.5336
     Episode_Reward/lifting_object: 161.2788
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.72s
                      Time elapsed: 01:10:02
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 34939 steps/s (collection: 2.685s, learning 0.128s)
             Mean action noise std: 3.52
          Mean value_function loss: 172.4772
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.0342
                       Mean reward: 855.58
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.5460
     Episode_Reward/lifting_object: 163.3685
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.81s
                      Time elapsed: 01:10:05
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 36280 steps/s (collection: 2.581s, learning 0.128s)
             Mean action noise std: 3.52
          Mean value_function loss: 173.2350
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.0481
                       Mean reward: 834.31
               Mean episode length: 224.68
    Episode_Reward/reaching_object: 1.5248
     Episode_Reward/lifting_object: 161.7034
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.71s
                      Time elapsed: 01:10:08
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 39819 steps/s (collection: 2.335s, learning 0.134s)
             Mean action noise std: 3.53
          Mean value_function loss: 256.5190
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.0644
                       Mean reward: 818.71
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 1.5449
     Episode_Reward/lifting_object: 163.7872
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.47s
                      Time elapsed: 01:10:10
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 39628 steps/s (collection: 2.350s, learning 0.130s)
             Mean action noise std: 3.53
          Mean value_function loss: 156.6867
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.0863
                       Mean reward: 882.84
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 1.5871
     Episode_Reward/lifting_object: 168.4306
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.48s
                      Time elapsed: 01:10:12
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 37462 steps/s (collection: 2.508s, learning 0.116s)
             Mean action noise std: 3.53
          Mean value_function loss: 199.3379
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.1049
                       Mean reward: 806.35
               Mean episode length: 218.72
    Episode_Reward/reaching_object: 1.5045
     Episode_Reward/lifting_object: 159.5678
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.62s
                      Time elapsed: 01:10:15
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 38367 steps/s (collection: 2.436s, learning 0.126s)
             Mean action noise std: 3.53
          Mean value_function loss: 169.0309
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.1223
                       Mean reward: 806.77
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 1.5402
     Episode_Reward/lifting_object: 163.7527
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.56s
                      Time elapsed: 01:10:18
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 38166 steps/s (collection: 2.463s, learning 0.113s)
             Mean action noise std: 3.54
          Mean value_function loss: 198.1612
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.1353
                       Mean reward: 784.85
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 1.5548
     Episode_Reward/lifting_object: 165.2952
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.58s
                      Time elapsed: 01:10:20
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 37659 steps/s (collection: 2.471s, learning 0.139s)
             Mean action noise std: 3.54
          Mean value_function loss: 171.0312
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.1498
                       Mean reward: 865.03
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.5488
     Episode_Reward/lifting_object: 164.4048
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.61s
                      Time elapsed: 01:10:23
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 37115 steps/s (collection: 2.512s, learning 0.136s)
             Mean action noise std: 3.54
          Mean value_function loss: 199.6976
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.1610
                       Mean reward: 782.76
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 1.5226
     Episode_Reward/lifting_object: 161.3230
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.65s
                      Time elapsed: 01:10:25
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 38935 steps/s (collection: 2.390s, learning 0.135s)
             Mean action noise std: 3.54
          Mean value_function loss: 174.8059
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.1777
                       Mean reward: 865.23
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.5441
     Episode_Reward/lifting_object: 164.2405
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.52s
                      Time elapsed: 01:10:28
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 39248 steps/s (collection: 2.363s, learning 0.141s)
             Mean action noise std: 3.54
          Mean value_function loss: 163.3095
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.1906
                       Mean reward: 819.17
               Mean episode length: 223.41
    Episode_Reward/reaching_object: 1.5259
     Episode_Reward/lifting_object: 162.2174
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.50s
                      Time elapsed: 01:10:31
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 38136 steps/s (collection: 2.430s, learning 0.148s)
             Mean action noise std: 3.55
          Mean value_function loss: 155.9720
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 69.2013
                       Mean reward: 872.79
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.5595
     Episode_Reward/lifting_object: 165.3860
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.58s
                      Time elapsed: 01:10:33
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 36938 steps/s (collection: 2.541s, learning 0.120s)
             Mean action noise std: 3.55
          Mean value_function loss: 124.3977
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.2107
                       Mean reward: 882.01
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.5883
     Episode_Reward/lifting_object: 169.8409
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.66s
                      Time elapsed: 01:10:36
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 38016 steps/s (collection: 2.441s, learning 0.145s)
             Mean action noise std: 3.55
          Mean value_function loss: 163.7287
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.2272
                       Mean reward: 875.38
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 1.6118
     Episode_Reward/lifting_object: 172.3896
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.59s
                      Time elapsed: 01:10:38
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 38448 steps/s (collection: 2.410s, learning 0.147s)
             Mean action noise std: 3.55
          Mean value_function loss: 179.9760
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.2458
                       Mean reward: 821.33
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 1.5258
     Episode_Reward/lifting_object: 162.3236
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.56s
                      Time elapsed: 01:10:41
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 40112 steps/s (collection: 2.335s, learning 0.116s)
             Mean action noise std: 3.55
          Mean value_function loss: 229.1564
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 69.2620
                       Mean reward: 795.23
               Mean episode length: 215.98
    Episode_Reward/reaching_object: 1.5256
     Episode_Reward/lifting_object: 161.8632
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.45s
                      Time elapsed: 01:10:43
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 39688 steps/s (collection: 2.348s, learning 0.129s)
             Mean action noise std: 3.56
          Mean value_function loss: 184.8676
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 69.2758
                       Mean reward: 844.85
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.5624
     Episode_Reward/lifting_object: 166.6987
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.48s
                      Time elapsed: 01:10:46
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 39470 steps/s (collection: 2.371s, learning 0.120s)
             Mean action noise std: 3.56
          Mean value_function loss: 171.4859
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.2783
                       Mean reward: 816.20
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.5633
     Episode_Reward/lifting_object: 166.5678
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.49s
                      Time elapsed: 01:10:48
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 37846 steps/s (collection: 2.460s, learning 0.138s)
             Mean action noise std: 3.56
          Mean value_function loss: 228.0991
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.2875
                       Mean reward: 781.27
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.5242
     Episode_Reward/lifting_object: 161.1671
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.60s
                      Time elapsed: 01:10:51
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 37397 steps/s (collection: 2.469s, learning 0.159s)
             Mean action noise std: 3.56
          Mean value_function loss: 210.9413
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.3039
                       Mean reward: 818.01
               Mean episode length: 219.66
    Episode_Reward/reaching_object: 1.5280
     Episode_Reward/lifting_object: 162.6598
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.63s
                      Time elapsed: 01:10:54
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 38978 steps/s (collection: 2.380s, learning 0.142s)
             Mean action noise std: 3.56
          Mean value_function loss: 187.5684
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.3153
                       Mean reward: 816.07
               Mean episode length: 220.84
    Episode_Reward/reaching_object: 1.5510
     Episode_Reward/lifting_object: 164.5844
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.52s
                      Time elapsed: 01:10:56
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 40308 steps/s (collection: 2.317s, learning 0.122s)
             Mean action noise std: 3.56
          Mean value_function loss: 191.8780
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.3311
                       Mean reward: 791.51
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 1.5331
     Episode_Reward/lifting_object: 162.4332
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.44s
                      Time elapsed: 01:10:59
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 38842 steps/s (collection: 2.411s, learning 0.120s)
             Mean action noise std: 3.57
          Mean value_function loss: 160.0516
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.3415
                       Mean reward: 862.06
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 1.5516
     Episode_Reward/lifting_object: 165.2584
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.53s
                      Time elapsed: 01:11:01
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 38927 steps/s (collection: 2.389s, learning 0.136s)
             Mean action noise std: 3.57
          Mean value_function loss: 131.7310
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.3543
                       Mean reward: 870.37
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.5661
     Episode_Reward/lifting_object: 166.5739
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.53s
                      Time elapsed: 01:11:04
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 38978 steps/s (collection: 2.393s, learning 0.129s)
             Mean action noise std: 3.57
          Mean value_function loss: 195.2163
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.3704
                       Mean reward: 849.85
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 1.5468
     Episode_Reward/lifting_object: 164.2725
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.52s
                      Time elapsed: 01:11:06
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 39656 steps/s (collection: 2.361s, learning 0.118s)
             Mean action noise std: 3.57
          Mean value_function loss: 161.3646
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.3823
                       Mean reward: 843.11
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.5600
     Episode_Reward/lifting_object: 166.5517
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.48s
                      Time elapsed: 01:11:09
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 39393 steps/s (collection: 2.355s, learning 0.140s)
             Mean action noise std: 3.57
          Mean value_function loss: 175.5611
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 69.3913
                       Mean reward: 795.02
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 1.5589
     Episode_Reward/lifting_object: 165.4765
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.50s
                      Time elapsed: 01:11:11
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 39597 steps/s (collection: 2.370s, learning 0.112s)
             Mean action noise std: 3.57
          Mean value_function loss: 175.9073
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.3971
                       Mean reward: 815.59
               Mean episode length: 220.55
    Episode_Reward/reaching_object: 1.5610
     Episode_Reward/lifting_object: 165.2439
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.48s
                      Time elapsed: 01:11:14
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 39143 steps/s (collection: 2.373s, learning 0.139s)
             Mean action noise std: 3.58
          Mean value_function loss: 152.7724
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.4118
                       Mean reward: 827.18
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 1.5874
     Episode_Reward/lifting_object: 168.6035
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.51s
                      Time elapsed: 01:11:16
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 39777 steps/s (collection: 2.364s, learning 0.107s)
             Mean action noise std: 3.58
          Mean value_function loss: 214.9384
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.4241
                       Mean reward: 831.31
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 1.5345
     Episode_Reward/lifting_object: 162.3784
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.47s
                      Time elapsed: 01:11:19
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 38955 steps/s (collection: 2.361s, learning 0.163s)
             Mean action noise std: 3.58
          Mean value_function loss: 194.6162
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.4410
                       Mean reward: 790.07
               Mean episode length: 215.44
    Episode_Reward/reaching_object: 1.5379
     Episode_Reward/lifting_object: 161.9359
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.52s
                      Time elapsed: 01:11:21
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 34868 steps/s (collection: 2.694s, learning 0.125s)
             Mean action noise std: 3.58
          Mean value_function loss: 169.0696
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.4593
                       Mean reward: 865.70
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.5728
     Episode_Reward/lifting_object: 167.1319
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.82s
                      Time elapsed: 01:11:24
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 37364 steps/s (collection: 2.495s, learning 0.136s)
             Mean action noise std: 3.59
          Mean value_function loss: 150.7302
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.4826
                       Mean reward: 798.32
               Mean episode length: 215.36
    Episode_Reward/reaching_object: 1.5597
     Episode_Reward/lifting_object: 165.3960
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.63s
                      Time elapsed: 01:11:27
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 38106 steps/s (collection: 2.459s, learning 0.120s)
             Mean action noise std: 3.59
          Mean value_function loss: 159.6455
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.4990
                       Mean reward: 831.69
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 1.5599
     Episode_Reward/lifting_object: 165.0414
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.58s
                      Time elapsed: 01:11:29
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 33057 steps/s (collection: 2.833s, learning 0.141s)
             Mean action noise std: 3.59
          Mean value_function loss: 172.2121
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.5110
                       Mean reward: 825.19
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 1.5583
     Episode_Reward/lifting_object: 164.4116
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.97s
                      Time elapsed: 01:11:32
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 38834 steps/s (collection: 2.394s, learning 0.137s)
             Mean action noise std: 3.59
          Mean value_function loss: 183.0150
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.5243
                       Mean reward: 832.52
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.5516
     Episode_Reward/lifting_object: 163.5716
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.53s
                      Time elapsed: 01:11:35
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 37510 steps/s (collection: 2.476s, learning 0.144s)
             Mean action noise std: 3.59
          Mean value_function loss: 175.3123
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.5379
                       Mean reward: 850.50
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 1.5387
     Episode_Reward/lifting_object: 162.7898
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.62s
                      Time elapsed: 01:11:37
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 37105 steps/s (collection: 2.501s, learning 0.148s)
             Mean action noise std: 3.60
          Mean value_function loss: 196.2861
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.5517
                       Mean reward: 860.16
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.5798
     Episode_Reward/lifting_object: 168.1889
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.65s
                      Time elapsed: 01:11:40
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 38100 steps/s (collection: 2.451s, learning 0.129s)
             Mean action noise std: 3.60
          Mean value_function loss: 190.3587
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.5695
                       Mean reward: 769.50
               Mean episode length: 210.74
    Episode_Reward/reaching_object: 1.5329
     Episode_Reward/lifting_object: 162.1626
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.58s
                      Time elapsed: 01:11:42
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 34463 steps/s (collection: 2.694s, learning 0.158s)
             Mean action noise std: 3.60
          Mean value_function loss: 169.6510
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.5873
                       Mean reward: 833.17
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 1.5756
     Episode_Reward/lifting_object: 167.3876
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.85s
                      Time elapsed: 01:11:45
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 37136 steps/s (collection: 2.520s, learning 0.127s)
             Mean action noise std: 3.60
          Mean value_function loss: 192.8554
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 69.6034
                       Mean reward: 865.49
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.5394
     Episode_Reward/lifting_object: 163.4568
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.65s
                      Time elapsed: 01:11:48
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 38050 steps/s (collection: 2.471s, learning 0.113s)
             Mean action noise std: 3.61
          Mean value_function loss: 201.2007
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.6139
                       Mean reward: 759.15
               Mean episode length: 208.33
    Episode_Reward/reaching_object: 1.4978
     Episode_Reward/lifting_object: 159.0586
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.58s
                      Time elapsed: 01:11:51
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 38774 steps/s (collection: 2.397s, learning 0.138s)
             Mean action noise std: 3.61
          Mean value_function loss: 246.0958
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.6360
                       Mean reward: 828.35
               Mean episode length: 223.76
    Episode_Reward/reaching_object: 1.5654
     Episode_Reward/lifting_object: 166.9399
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.54s
                      Time elapsed: 01:11:53
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 39463 steps/s (collection: 2.348s, learning 0.143s)
             Mean action noise std: 3.61
          Mean value_function loss: 188.7484
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.6526
                       Mean reward: 843.82
               Mean episode length: 226.60
    Episode_Reward/reaching_object: 1.5313
     Episode_Reward/lifting_object: 162.8327
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.49s
                      Time elapsed: 01:11:56
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 39215 steps/s (collection: 2.368s, learning 0.139s)
             Mean action noise std: 3.61
          Mean value_function loss: 191.3999
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.6698
                       Mean reward: 845.36
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 1.5470
     Episode_Reward/lifting_object: 164.1469
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.51s
                      Time elapsed: 01:11:58
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 39749 steps/s (collection: 2.338s, learning 0.135s)
             Mean action noise std: 3.61
          Mean value_function loss: 187.6902
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.6810
                       Mean reward: 822.21
               Mean episode length: 221.87
    Episode_Reward/reaching_object: 1.5607
     Episode_Reward/lifting_object: 166.6218
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.47s
                      Time elapsed: 01:12:01
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 38447 steps/s (collection: 2.415s, learning 0.141s)
             Mean action noise std: 3.62
          Mean value_function loss: 168.3266
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.6939
                       Mean reward: 831.24
               Mean episode length: 222.35
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 165.8467
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.56s
                      Time elapsed: 01:12:03
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 39056 steps/s (collection: 2.375s, learning 0.142s)
             Mean action noise std: 3.62
          Mean value_function loss: 210.2362
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 69.7029
                       Mean reward: 837.47
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 1.5101
     Episode_Reward/lifting_object: 160.6044
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.52s
                      Time elapsed: 01:12:06
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 40433 steps/s (collection: 2.308s, learning 0.123s)
             Mean action noise std: 3.62
          Mean value_function loss: 233.6701
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.7083
                       Mean reward: 807.29
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 1.5392
     Episode_Reward/lifting_object: 164.1070
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.43s
                      Time elapsed: 01:12:08
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 40299 steps/s (collection: 2.331s, learning 0.109s)
             Mean action noise std: 3.62
          Mean value_function loss: 198.7112
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.7222
                       Mean reward: 861.25
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 1.5281
     Episode_Reward/lifting_object: 162.9076
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.44s
                      Time elapsed: 01:12:10
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 40199 steps/s (collection: 2.321s, learning 0.124s)
             Mean action noise std: 3.62
          Mean value_function loss: 161.5692
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.7374
                       Mean reward: 816.45
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 1.5457
     Episode_Reward/lifting_object: 164.7995
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.45s
                      Time elapsed: 01:12:13
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 37489 steps/s (collection: 2.471s, learning 0.151s)
             Mean action noise std: 3.63
          Mean value_function loss: 136.5546
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.7581
                       Mean reward: 874.32
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 1.5955
     Episode_Reward/lifting_object: 167.9432
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.62s
                      Time elapsed: 01:12:16
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 39252 steps/s (collection: 2.379s, learning 0.125s)
             Mean action noise std: 3.63
          Mean value_function loss: 209.5554
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.7797
                       Mean reward: 871.66
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.5165
     Episode_Reward/lifting_object: 161.5597
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.50s
                      Time elapsed: 01:12:18
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 38105 steps/s (collection: 2.447s, learning 0.132s)
             Mean action noise std: 3.63
          Mean value_function loss: 179.4363
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.7935
                       Mean reward: 809.26
               Mean episode length: 218.74
    Episode_Reward/reaching_object: 1.5260
     Episode_Reward/lifting_object: 161.8890
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.58s
                      Time elapsed: 01:12:21
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 38295 steps/s (collection: 2.447s, learning 0.119s)
             Mean action noise std: 3.63
          Mean value_function loss: 150.8678
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.8066
                       Mean reward: 821.38
               Mean episode length: 220.97
    Episode_Reward/reaching_object: 1.5488
     Episode_Reward/lifting_object: 165.5402
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.57s
                      Time elapsed: 01:12:23
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 38182 steps/s (collection: 2.422s, learning 0.153s)
             Mean action noise std: 3.63
          Mean value_function loss: 150.5301
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 69.8214
                       Mean reward: 865.23
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.5631
     Episode_Reward/lifting_object: 166.8495
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.57s
                      Time elapsed: 01:12:26
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 38966 steps/s (collection: 2.387s, learning 0.135s)
             Mean action noise std: 3.64
          Mean value_function loss: 170.5511
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.8357
                       Mean reward: 846.15
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 1.5268
     Episode_Reward/lifting_object: 162.6280
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.52s
                      Time elapsed: 01:12:28
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 38820 steps/s (collection: 2.404s, learning 0.128s)
             Mean action noise std: 3.64
          Mean value_function loss: 155.6870
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.8561
                       Mean reward: 828.57
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 168.0593
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.53s
                      Time elapsed: 01:12:31
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 39485 steps/s (collection: 2.360s, learning 0.129s)
             Mean action noise std: 3.64
          Mean value_function loss: 200.1993
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.8783
                       Mean reward: 805.06
               Mean episode length: 217.24
    Episode_Reward/reaching_object: 1.5483
     Episode_Reward/lifting_object: 164.6180
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.49s
                      Time elapsed: 01:12:33
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 38761 steps/s (collection: 2.389s, learning 0.147s)
             Mean action noise std: 3.64
          Mean value_function loss: 190.5847
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.8893
                       Mean reward: 756.77
               Mean episode length: 207.24
    Episode_Reward/reaching_object: 1.5000
     Episode_Reward/lifting_object: 159.1915
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.54s
                      Time elapsed: 01:12:36
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 37251 steps/s (collection: 2.519s, learning 0.120s)
             Mean action noise std: 3.64
          Mean value_function loss: 185.2065
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.8995
                       Mean reward: 810.08
               Mean episode length: 219.20
    Episode_Reward/reaching_object: 1.5360
     Episode_Reward/lifting_object: 163.1506
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.64s
                      Time elapsed: 01:12:38
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 39337 steps/s (collection: 2.366s, learning 0.133s)
             Mean action noise std: 3.65
          Mean value_function loss: 179.4662
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.9064
                       Mean reward: 777.57
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 1.5494
     Episode_Reward/lifting_object: 164.9149
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.50s
                      Time elapsed: 01:12:41
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 38974 steps/s (collection: 2.409s, learning 0.113s)
             Mean action noise std: 3.65
          Mean value_function loss: 189.6477
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.9151
                       Mean reward: 795.34
               Mean episode length: 214.12
    Episode_Reward/reaching_object: 1.5248
     Episode_Reward/lifting_object: 161.6071
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.52s
                      Time elapsed: 01:12:44
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 38655 steps/s (collection: 2.400s, learning 0.144s)
             Mean action noise std: 3.65
          Mean value_function loss: 147.2862
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.9231
                       Mean reward: 811.70
               Mean episode length: 220.07
    Episode_Reward/reaching_object: 1.5169
     Episode_Reward/lifting_object: 160.8008
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.54s
                      Time elapsed: 01:12:46
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 39134 steps/s (collection: 2.390s, learning 0.122s)
             Mean action noise std: 3.65
          Mean value_function loss: 147.9175
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.9389
                       Mean reward: 800.99
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 1.5563
     Episode_Reward/lifting_object: 165.4247
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.51s
                      Time elapsed: 01:12:49
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 38523 steps/s (collection: 2.424s, learning 0.128s)
             Mean action noise std: 3.65
          Mean value_function loss: 155.2984
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 69.9571
                       Mean reward: 830.78
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.5769
     Episode_Reward/lifting_object: 167.6689
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.55s
                      Time elapsed: 01:12:51
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 37238 steps/s (collection: 2.485s, learning 0.155s)
             Mean action noise std: 3.65
          Mean value_function loss: 190.3675
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.9633
                       Mean reward: 824.58
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 1.5261
     Episode_Reward/lifting_object: 162.1848
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.64s
                      Time elapsed: 01:12:54
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 35809 steps/s (collection: 2.588s, learning 0.157s)
             Mean action noise std: 3.66
          Mean value_function loss: 178.4255
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.9734
                       Mean reward: 816.96
               Mean episode length: 220.55
    Episode_Reward/reaching_object: 1.5215
     Episode_Reward/lifting_object: 161.7830
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.75s
                      Time elapsed: 01:12:57
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 37543 steps/s (collection: 2.500s, learning 0.118s)
             Mean action noise std: 3.66
          Mean value_function loss: 168.1581
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.9911
                       Mean reward: 835.89
               Mean episode length: 226.29
    Episode_Reward/reaching_object: 1.5822
     Episode_Reward/lifting_object: 168.2105
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.62s
                      Time elapsed: 01:12:59
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 38136 steps/s (collection: 2.442s, learning 0.136s)
             Mean action noise std: 3.66
          Mean value_function loss: 182.2883
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.0066
                       Mean reward: 835.27
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 1.5309
     Episode_Reward/lifting_object: 162.5192
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.58s
                      Time elapsed: 01:13:02
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 38486 steps/s (collection: 2.413s, learning 0.141s)
             Mean action noise std: 3.66
          Mean value_function loss: 236.3394
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 70.0246
                       Mean reward: 833.68
               Mean episode length: 224.23
    Episode_Reward/reaching_object: 1.5069
     Episode_Reward/lifting_object: 160.5995
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.55s
                      Time elapsed: 01:13:04
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 38805 steps/s (collection: 2.395s, learning 0.138s)
             Mean action noise std: 3.67
          Mean value_function loss: 177.1043
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.0398
                       Mean reward: 821.37
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 1.5252
     Episode_Reward/lifting_object: 162.7013
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.53s
                      Time elapsed: 01:13:07
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 39827 steps/s (collection: 2.348s, learning 0.120s)
             Mean action noise std: 3.67
          Mean value_function loss: 174.4827
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.0529
                       Mean reward: 824.87
               Mean episode length: 223.73
    Episode_Reward/reaching_object: 1.5190
     Episode_Reward/lifting_object: 161.7287
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.47s
                      Time elapsed: 01:13:09
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 39298 steps/s (collection: 2.360s, learning 0.142s)
             Mean action noise std: 3.67
          Mean value_function loss: 184.0310
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.0662
                       Mean reward: 800.34
               Mean episode length: 215.25
    Episode_Reward/reaching_object: 1.4984
     Episode_Reward/lifting_object: 161.2209
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.50s
                      Time elapsed: 01:13:12
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 38945 steps/s (collection: 2.396s, learning 0.128s)
             Mean action noise std: 3.67
          Mean value_function loss: 173.8026
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 70.0780
                       Mean reward: 856.07
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.5538
     Episode_Reward/lifting_object: 167.2749
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.52s
                      Time elapsed: 01:13:14
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 37675 steps/s (collection: 2.457s, learning 0.153s)
             Mean action noise std: 3.67
          Mean value_function loss: 174.1665
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 70.0944
                       Mean reward: 848.77
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 1.5041
     Episode_Reward/lifting_object: 161.3248
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.61s
                      Time elapsed: 01:13:17
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 38278 steps/s (collection: 2.434s, learning 0.134s)
             Mean action noise std: 3.68
          Mean value_function loss: 203.6283
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 70.1096
                       Mean reward: 827.38
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 1.5209
     Episode_Reward/lifting_object: 163.6087
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.57s
                      Time elapsed: 01:13:19
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 37492 steps/s (collection: 2.471s, learning 0.151s)
             Mean action noise std: 3.68
          Mean value_function loss: 211.1721
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.1185
                       Mean reward: 778.32
               Mean episode length: 211.08
    Episode_Reward/reaching_object: 1.5443
     Episode_Reward/lifting_object: 166.1159
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.62s
                      Time elapsed: 01:13:22
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 37930 steps/s (collection: 2.460s, learning 0.131s)
             Mean action noise std: 3.68
          Mean value_function loss: 218.0175
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.1302
                       Mean reward: 837.94
               Mean episode length: 224.13
    Episode_Reward/reaching_object: 1.4702
     Episode_Reward/lifting_object: 158.1822
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.59s
                      Time elapsed: 01:13:25
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 37130 steps/s (collection: 2.502s, learning 0.146s)
             Mean action noise std: 3.68
          Mean value_function loss: 194.5623
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 70.1420
                       Mean reward: 827.55
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.5054
     Episode_Reward/lifting_object: 161.8732
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.65s
                      Time elapsed: 01:13:27
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 37511 steps/s (collection: 2.479s, learning 0.142s)
             Mean action noise std: 3.68
          Mean value_function loss: 162.7360
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.1544
                       Mean reward: 855.09
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 1.5245
     Episode_Reward/lifting_object: 164.3577
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.62s
                      Time elapsed: 01:13:30
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 37750 steps/s (collection: 2.455s, learning 0.149s)
             Mean action noise std: 3.68
          Mean value_function loss: 209.3959
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 70.1676
                       Mean reward: 799.21
               Mean episode length: 217.40
    Episode_Reward/reaching_object: 1.4682
     Episode_Reward/lifting_object: 157.9797
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.60s
                      Time elapsed: 01:13:33
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 38111 steps/s (collection: 2.455s, learning 0.125s)
             Mean action noise std: 3.69
          Mean value_function loss: 187.2585
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.1755
                       Mean reward: 833.74
               Mean episode length: 224.03
    Episode_Reward/reaching_object: 1.4740
     Episode_Reward/lifting_object: 159.6467
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.58s
                      Time elapsed: 01:13:35
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 36465 steps/s (collection: 2.542s, learning 0.154s)
             Mean action noise std: 3.69
          Mean value_function loss: 212.2375
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.1829
                       Mean reward: 821.66
               Mean episode length: 221.56
    Episode_Reward/reaching_object: 1.5194
     Episode_Reward/lifting_object: 164.5075
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.70s
                      Time elapsed: 01:13:38
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 36183 steps/s (collection: 2.546s, learning 0.171s)
             Mean action noise std: 3.69
          Mean value_function loss: 167.2795
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.1886
                       Mean reward: 849.61
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 1.5215
     Episode_Reward/lifting_object: 164.5744
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.72s
                      Time elapsed: 01:13:41
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 37981 steps/s (collection: 2.446s, learning 0.142s)
             Mean action noise std: 3.69
          Mean value_function loss: 192.7083
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.1979
                       Mean reward: 798.97
               Mean episode length: 215.89
    Episode_Reward/reaching_object: 1.4872
     Episode_Reward/lifting_object: 160.7681
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.59s
                      Time elapsed: 01:13:43
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 37674 steps/s (collection: 2.460s, learning 0.149s)
             Mean action noise std: 3.69
          Mean value_function loss: 228.2728
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.2135
                       Mean reward: 846.84
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.4567
     Episode_Reward/lifting_object: 156.4497
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.61s
                      Time elapsed: 01:13:46
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 39846 steps/s (collection: 2.351s, learning 0.116s)
             Mean action noise std: 3.69
          Mean value_function loss: 205.3442
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.2289
                       Mean reward: 855.74
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 1.4810
     Episode_Reward/lifting_object: 159.0257
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.47s
                      Time elapsed: 01:13:48
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 38024 steps/s (collection: 2.426s, learning 0.159s)
             Mean action noise std: 3.70
          Mean value_function loss: 181.5570
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.2411
                       Mean reward: 798.52
               Mean episode length: 217.41
    Episode_Reward/reaching_object: 1.5316
     Episode_Reward/lifting_object: 165.2272
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.59s
                      Time elapsed: 01:13:51
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 38249 steps/s (collection: 2.425s, learning 0.145s)
             Mean action noise std: 3.70
          Mean value_function loss: 202.7615
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 70.2500
                       Mean reward: 854.14
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 1.5358
     Episode_Reward/lifting_object: 165.3123
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.57s
                      Time elapsed: 01:13:53
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 36896 steps/s (collection: 2.523s, learning 0.141s)
             Mean action noise std: 3.70
          Mean value_function loss: 217.5881
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.2601
                       Mean reward: 813.41
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 1.4731
     Episode_Reward/lifting_object: 156.8254
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.66s
                      Time elapsed: 01:13:56
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 39078 steps/s (collection: 2.410s, learning 0.106s)
             Mean action noise std: 3.70
          Mean value_function loss: 232.4024
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.2704
                       Mean reward: 803.94
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 1.4963
     Episode_Reward/lifting_object: 160.5666
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.52s
                      Time elapsed: 01:13:59
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 39502 steps/s (collection: 2.369s, learning 0.119s)
             Mean action noise std: 3.70
          Mean value_function loss: 200.3237
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.2776
                       Mean reward: 844.48
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 1.4997
     Episode_Reward/lifting_object: 160.8931
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.49s
                      Time elapsed: 01:14:01
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 37543 steps/s (collection: 2.483s, learning 0.135s)
             Mean action noise std: 3.70
          Mean value_function loss: 247.0140
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.2856
                       Mean reward: 857.71
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 1.5120
     Episode_Reward/lifting_object: 161.1197
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.62s
                      Time elapsed: 01:14:04
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 37612 steps/s (collection: 2.475s, learning 0.139s)
             Mean action noise std: 3.71
          Mean value_function loss: 212.1164
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.3018
                       Mean reward: 824.58
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.5162
     Episode_Reward/lifting_object: 162.4153
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.61s
                      Time elapsed: 01:14:06
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 38121 steps/s (collection: 2.424s, learning 0.155s)
             Mean action noise std: 3.71
          Mean value_function loss: 209.5887
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.3231
                       Mean reward: 825.28
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.5002
     Episode_Reward/lifting_object: 159.7846
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.58s
                      Time elapsed: 01:14:09
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 37987 steps/s (collection: 2.469s, learning 0.119s)
             Mean action noise std: 3.71
          Mean value_function loss: 181.0286
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.3384
                       Mean reward: 812.45
               Mean episode length: 219.01
    Episode_Reward/reaching_object: 1.4876
     Episode_Reward/lifting_object: 157.6651
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.59s
                      Time elapsed: 01:14:11
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 37991 steps/s (collection: 2.443s, learning 0.145s)
             Mean action noise std: 3.71
          Mean value_function loss: 198.5517
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.3497
                       Mean reward: 774.98
               Mean episode length: 212.46
    Episode_Reward/reaching_object: 1.5134
     Episode_Reward/lifting_object: 161.2482
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.59s
                      Time elapsed: 01:14:14
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 37620 steps/s (collection: 2.459s, learning 0.154s)
             Mean action noise std: 3.71
          Mean value_function loss: 200.1492
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.3619
                       Mean reward: 796.20
               Mean episode length: 214.63
    Episode_Reward/reaching_object: 1.5369
     Episode_Reward/lifting_object: 164.5358
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.61s
                      Time elapsed: 01:14:17
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 38180 steps/s (collection: 2.431s, learning 0.144s)
             Mean action noise std: 3.72
          Mean value_function loss: 182.3211
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.3810
                       Mean reward: 856.39
               Mean episode length: 229.10
    Episode_Reward/reaching_object: 1.5030
     Episode_Reward/lifting_object: 160.0646
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.57s
                      Time elapsed: 01:14:19
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 38825 steps/s (collection: 2.415s, learning 0.117s)
             Mean action noise std: 3.72
          Mean value_function loss: 189.8413
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.3988
                       Mean reward: 834.68
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 1.5260
     Episode_Reward/lifting_object: 162.4561
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.53s
                      Time elapsed: 01:14:22
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 37749 steps/s (collection: 2.467s, learning 0.138s)
             Mean action noise std: 3.72
          Mean value_function loss: 204.4010
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.4135
                       Mean reward: 814.98
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 159.1025
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.60s
                      Time elapsed: 01:14:24
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 38203 steps/s (collection: 2.441s, learning 0.132s)
             Mean action noise std: 3.72
          Mean value_function loss: 218.3608
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.4253
                       Mean reward: 845.23
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.5159
     Episode_Reward/lifting_object: 160.6311
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.57s
                      Time elapsed: 01:14:27
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 38625 steps/s (collection: 2.407s, learning 0.138s)
             Mean action noise std: 3.73
          Mean value_function loss: 240.6535
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.4364
                       Mean reward: 808.07
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 1.4678
     Episode_Reward/lifting_object: 155.7547
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.55s
                      Time elapsed: 01:14:29
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 38434 steps/s (collection: 2.423s, learning 0.134s)
             Mean action noise std: 3.73
          Mean value_function loss: 197.1210
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 70.4504
                       Mean reward: 795.81
               Mean episode length: 215.61
    Episode_Reward/reaching_object: 1.5324
     Episode_Reward/lifting_object: 163.2329
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.56s
                      Time elapsed: 01:14:32
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 38932 steps/s (collection: 2.403s, learning 0.122s)
             Mean action noise std: 3.73
          Mean value_function loss: 214.8188
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.4551
                       Mean reward: 812.52
               Mean episode length: 220.46
    Episode_Reward/reaching_object: 1.4984
     Episode_Reward/lifting_object: 159.4491
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.52s
                      Time elapsed: 01:14:35
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 39336 steps/s (collection: 2.351s, learning 0.148s)
             Mean action noise std: 3.73
          Mean value_function loss: 185.3494
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.4651
                       Mean reward: 808.74
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 164.6392
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.50s
                      Time elapsed: 01:14:37
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 40026 steps/s (collection: 2.342s, learning 0.114s)
             Mean action noise std: 3.73
          Mean value_function loss: 165.3464
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.4802
                       Mean reward: 846.30
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 1.5231
     Episode_Reward/lifting_object: 161.1930
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.46s
                      Time elapsed: 01:14:39
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 39827 steps/s (collection: 2.343s, learning 0.125s)
             Mean action noise std: 3.73
          Mean value_function loss: 181.0391
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.4896
                       Mean reward: 838.25
               Mean episode length: 226.39
    Episode_Reward/reaching_object: 1.5191
     Episode_Reward/lifting_object: 161.3681
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.47s
                      Time elapsed: 01:14:42
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 38019 steps/s (collection: 2.451s, learning 0.135s)
             Mean action noise std: 3.74
          Mean value_function loss: 177.1677
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.5005
                       Mean reward: 817.96
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 1.5641
     Episode_Reward/lifting_object: 165.9187
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.59s
                      Time elapsed: 01:14:45
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 38393 steps/s (collection: 2.430s, learning 0.130s)
             Mean action noise std: 3.74
          Mean value_function loss: 177.8464
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 70.5114
                       Mean reward: 785.83
               Mean episode length: 212.48
    Episode_Reward/reaching_object: 1.5151
     Episode_Reward/lifting_object: 160.8173
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.56s
                      Time elapsed: 01:14:47
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 36462 steps/s (collection: 2.583s, learning 0.113s)
             Mean action noise std: 3.74
          Mean value_function loss: 160.9808
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 70.5212
                       Mean reward: 822.97
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.5750
     Episode_Reward/lifting_object: 167.1128
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.70s
                      Time elapsed: 01:14:50
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 37409 steps/s (collection: 2.501s, learning 0.127s)
             Mean action noise std: 3.74
          Mean value_function loss: 134.8709
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.5351
                       Mean reward: 850.90
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 1.5717
     Episode_Reward/lifting_object: 167.5418
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.63s
                      Time elapsed: 01:14:52
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 37334 steps/s (collection: 2.473s, learning 0.160s)
             Mean action noise std: 3.74
          Mean value_function loss: 153.4924
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.5472
                       Mean reward: 867.66
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.5708
     Episode_Reward/lifting_object: 167.4073
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.63s
                      Time elapsed: 01:14:55
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 38283 steps/s (collection: 2.434s, learning 0.134s)
             Mean action noise std: 3.74
          Mean value_function loss: 166.1874
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 70.5622
                       Mean reward: 832.49
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 1.5418
     Episode_Reward/lifting_object: 163.7935
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.57s
                      Time elapsed: 01:14:58
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 36672 steps/s (collection: 2.507s, learning 0.174s)
             Mean action noise std: 3.75
          Mean value_function loss: 176.7445
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.5747
                       Mean reward: 793.21
               Mean episode length: 215.82
    Episode_Reward/reaching_object: 1.5176
     Episode_Reward/lifting_object: 161.6686
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.68s
                      Time elapsed: 01:15:00
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 37527 steps/s (collection: 2.493s, learning 0.127s)
             Mean action noise std: 3.75
          Mean value_function loss: 151.0912
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.5871
                       Mean reward: 828.72
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 1.5563
     Episode_Reward/lifting_object: 166.2086
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.62s
                      Time elapsed: 01:15:03
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 38533 steps/s (collection: 2.415s, learning 0.136s)
             Mean action noise std: 3.75
          Mean value_function loss: 228.4433
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.6059
                       Mean reward: 804.01
               Mean episode length: 215.81
    Episode_Reward/reaching_object: 1.5014
     Episode_Reward/lifting_object: 160.0931
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.55s
                      Time elapsed: 01:15:05
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 38212 steps/s (collection: 2.442s, learning 0.131s)
             Mean action noise std: 3.75
          Mean value_function loss: 168.7723
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 70.6225
                       Mean reward: 852.53
               Mean episode length: 228.71
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 162.4943
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.57s
                      Time elapsed: 01:15:08
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 39399 steps/s (collection: 2.361s, learning 0.134s)
             Mean action noise std: 3.75
          Mean value_function loss: 177.0993
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.6305
                       Mean reward: 814.78
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 1.5243
     Episode_Reward/lifting_object: 161.9798
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.50s
                      Time elapsed: 01:15:11
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 40742 steps/s (collection: 2.297s, learning 0.115s)
             Mean action noise std: 3.76
          Mean value_function loss: 188.6379
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.6369
                       Mean reward: 825.07
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 1.5298
     Episode_Reward/lifting_object: 163.5849
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.41s
                      Time elapsed: 01:15:13
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 36737 steps/s (collection: 2.515s, learning 0.161s)
             Mean action noise std: 3.76
          Mean value_function loss: 167.8736
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.6501
                       Mean reward: 769.34
               Mean episode length: 209.53
    Episode_Reward/reaching_object: 1.5269
     Episode_Reward/lifting_object: 163.6723
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.68s
                      Time elapsed: 01:15:16
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 38784 steps/s (collection: 2.389s, learning 0.145s)
             Mean action noise std: 3.76
          Mean value_function loss: 190.7782
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 70.6638
                       Mean reward: 779.51
               Mean episode length: 213.70
    Episode_Reward/reaching_object: 1.5261
     Episode_Reward/lifting_object: 163.0457
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.53s
                      Time elapsed: 01:15:18
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 38332 steps/s (collection: 2.397s, learning 0.167s)
             Mean action noise std: 3.76
          Mean value_function loss: 153.0775
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.6756
                       Mean reward: 831.96
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.5388
     Episode_Reward/lifting_object: 165.0095
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.56s
                      Time elapsed: 01:15:21
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 36956 steps/s (collection: 2.530s, learning 0.130s)
             Mean action noise std: 3.76
          Mean value_function loss: 162.5602
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 70.6901
                       Mean reward: 822.85
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 1.5393
     Episode_Reward/lifting_object: 165.0120
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.66s
                      Time elapsed: 01:15:23
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 39649 steps/s (collection: 2.364s, learning 0.116s)
             Mean action noise std: 3.77
          Mean value_function loss: 155.9267
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.6992
                       Mean reward: 850.16
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.5175
     Episode_Reward/lifting_object: 162.5243
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.48s
                      Time elapsed: 01:15:26
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 39112 steps/s (collection: 2.397s, learning 0.116s)
             Mean action noise std: 3.77
          Mean value_function loss: 174.6377
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.7152
                       Mean reward: 811.95
               Mean episode length: 220.03
    Episode_Reward/reaching_object: 1.5430
     Episode_Reward/lifting_object: 165.6438
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.51s
                      Time elapsed: 01:15:28
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 39061 steps/s (collection: 2.398s, learning 0.119s)
             Mean action noise std: 3.77
          Mean value_function loss: 185.3549
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.7334
                       Mean reward: 808.50
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 164.7128
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.52s
                      Time elapsed: 01:15:31
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 39743 steps/s (collection: 2.359s, learning 0.114s)
             Mean action noise std: 3.77
          Mean value_function loss: 182.6541
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.7425
                       Mean reward: 825.83
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 1.5240
     Episode_Reward/lifting_object: 164.4190
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.47s
                      Time elapsed: 01:15:33
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 39757 steps/s (collection: 2.352s, learning 0.120s)
             Mean action noise std: 3.77
          Mean value_function loss: 194.2582
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.7551
                       Mean reward: 817.18
               Mean episode length: 220.81
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 168.2489
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.47s
                      Time elapsed: 01:15:36
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 39732 steps/s (collection: 2.357s, learning 0.117s)
             Mean action noise std: 3.78
          Mean value_function loss: 174.7751
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.7696
                       Mean reward: 791.25
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.4925
     Episode_Reward/lifting_object: 160.8204
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.47s
                      Time elapsed: 01:15:38
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 39621 steps/s (collection: 2.365s, learning 0.117s)
             Mean action noise std: 3.78
          Mean value_function loss: 208.4818
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.7838
                       Mean reward: 847.04
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.5642
     Episode_Reward/lifting_object: 169.6361
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.48s
                      Time elapsed: 01:15:41
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 37333 steps/s (collection: 2.490s, learning 0.144s)
             Mean action noise std: 3.78
          Mean value_function loss: 180.8746
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.7983
                       Mean reward: 843.58
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 1.5060
     Episode_Reward/lifting_object: 163.3304
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.63s
                      Time elapsed: 01:15:43
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 39250 steps/s (collection: 2.389s, learning 0.115s)
             Mean action noise std: 3.78
          Mean value_function loss: 178.8336
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.8126
                       Mean reward: 871.56
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.5331
     Episode_Reward/lifting_object: 166.3393
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.50s
                      Time elapsed: 01:15:46
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 38905 steps/s (collection: 2.404s, learning 0.123s)
             Mean action noise std: 3.78
          Mean value_function loss: 187.8061
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 70.8196
                       Mean reward: 858.73
               Mean episode length: 229.98
    Episode_Reward/reaching_object: 1.5031
     Episode_Reward/lifting_object: 163.1515
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.53s
                      Time elapsed: 01:15:48
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 39566 steps/s (collection: 2.369s, learning 0.115s)
             Mean action noise std: 3.78
          Mean value_function loss: 157.8426
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.8254
                       Mean reward: 800.07
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 1.5293
     Episode_Reward/lifting_object: 167.0326
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.48s
                      Time elapsed: 01:15:51
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 37175 steps/s (collection: 2.539s, learning 0.106s)
             Mean action noise std: 3.79
          Mean value_function loss: 187.1947
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 70.8397
                       Mean reward: 827.22
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 1.5237
     Episode_Reward/lifting_object: 166.1154
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.64s
                      Time elapsed: 01:15:54
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 39195 steps/s (collection: 2.380s, learning 0.128s)
             Mean action noise std: 3.79
          Mean value_function loss: 189.2914
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.8519
                       Mean reward: 867.34
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.5284
     Episode_Reward/lifting_object: 166.0515
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.51s
                      Time elapsed: 01:15:56
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 37684 steps/s (collection: 2.495s, learning 0.113s)
             Mean action noise std: 3.79
          Mean value_function loss: 168.2433
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.8606
                       Mean reward: 832.54
               Mean episode length: 223.32
    Episode_Reward/reaching_object: 1.5225
     Episode_Reward/lifting_object: 165.7589
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.61s
                      Time elapsed: 01:15:59
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 38614 steps/s (collection: 2.423s, learning 0.123s)
             Mean action noise std: 3.79
          Mean value_function loss: 131.1055
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.8699
                       Mean reward: 860.63
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.5967
     Episode_Reward/lifting_object: 173.8524
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.55s
                      Time elapsed: 01:16:01
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 37541 steps/s (collection: 2.502s, learning 0.116s)
             Mean action noise std: 3.79
          Mean value_function loss: 142.6579
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.8792
                       Mean reward: 846.95
               Mean episode length: 227.12
    Episode_Reward/reaching_object: 1.5316
     Episode_Reward/lifting_object: 166.4465
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.62s
                      Time elapsed: 01:16:04
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 39128 steps/s (collection: 2.393s, learning 0.120s)
             Mean action noise std: 3.80
          Mean value_function loss: 159.6546
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.8922
                       Mean reward: 824.53
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 1.5562
     Episode_Reward/lifting_object: 169.2741
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.51s
                      Time elapsed: 01:16:06
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 38650 steps/s (collection: 2.394s, learning 0.149s)
             Mean action noise std: 3.80
          Mean value_function loss: 172.6711
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.9046
                       Mean reward: 796.21
               Mean episode length: 215.14
    Episode_Reward/reaching_object: 1.5086
     Episode_Reward/lifting_object: 161.4720
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.54s
                      Time elapsed: 01:16:09
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 39663 steps/s (collection: 2.348s, learning 0.130s)
             Mean action noise std: 3.80
          Mean value_function loss: 153.1868
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.9146
                       Mean reward: 823.91
               Mean episode length: 222.24
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 165.5681
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.48s
                      Time elapsed: 01:16:11
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 39254 steps/s (collection: 2.359s, learning 0.145s)
             Mean action noise std: 3.80
          Mean value_function loss: 209.1245
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.9267
                       Mean reward: 826.43
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 1.5175
     Episode_Reward/lifting_object: 164.1590
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.50s
                      Time elapsed: 01:16:14
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 39716 steps/s (collection: 2.374s, learning 0.102s)
             Mean action noise std: 3.80
          Mean value_function loss: 169.8185
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.9386
                       Mean reward: 794.61
               Mean episode length: 215.74
    Episode_Reward/reaching_object: 1.5233
     Episode_Reward/lifting_object: 164.8408
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.48s
                      Time elapsed: 01:16:16
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 38907 steps/s (collection: 2.375s, learning 0.151s)
             Mean action noise std: 3.80
          Mean value_function loss: 210.4387
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.9505
                       Mean reward: 812.29
               Mean episode length: 219.25
    Episode_Reward/reaching_object: 1.5226
     Episode_Reward/lifting_object: 164.1496
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.53s
                      Time elapsed: 01:16:19
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 39460 steps/s (collection: 2.376s, learning 0.115s)
             Mean action noise std: 3.81
          Mean value_function loss: 211.1005
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 70.9664
                       Mean reward: 817.04
               Mean episode length: 219.76
    Episode_Reward/reaching_object: 1.5154
     Episode_Reward/lifting_object: 163.2061
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.49s
                      Time elapsed: 01:16:21
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 39803 steps/s (collection: 2.349s, learning 0.121s)
             Mean action noise std: 3.81
          Mean value_function loss: 204.6960
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.9792
                       Mean reward: 811.40
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 1.5291
     Episode_Reward/lifting_object: 164.8951
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.47s
                      Time elapsed: 01:16:24
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 39822 steps/s (collection: 2.352s, learning 0.117s)
             Mean action noise std: 3.81
          Mean value_function loss: 179.2971
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.9959
                       Mean reward: 840.01
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 1.5301
     Episode_Reward/lifting_object: 165.9958
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.47s
                      Time elapsed: 01:16:26
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 37333 steps/s (collection: 2.499s, learning 0.134s)
             Mean action noise std: 3.81
          Mean value_function loss: 193.6159
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 71.0051
                       Mean reward: 837.98
               Mean episode length: 226.18
    Episode_Reward/reaching_object: 1.5117
     Episode_Reward/lifting_object: 163.3675
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.63s
                      Time elapsed: 01:16:29
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 39697 steps/s (collection: 2.358s, learning 0.118s)
             Mean action noise std: 3.81
          Mean value_function loss: 189.3807
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.0111
                       Mean reward: 881.89
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 1.5392
     Episode_Reward/lifting_object: 164.7266
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.48s
                      Time elapsed: 01:16:31
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 37845 steps/s (collection: 2.454s, learning 0.144s)
             Mean action noise std: 3.81
          Mean value_function loss: 188.3659
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.0219
                       Mean reward: 846.08
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 1.5341
     Episode_Reward/lifting_object: 165.0991
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.60s
                      Time elapsed: 01:16:34
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 39188 steps/s (collection: 2.383s, learning 0.126s)
             Mean action noise std: 3.82
          Mean value_function loss: 172.2594
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.0360
                       Mean reward: 854.03
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 1.5796
     Episode_Reward/lifting_object: 170.5797
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.51s
                      Time elapsed: 01:16:37
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 35961 steps/s (collection: 2.574s, learning 0.160s)
             Mean action noise std: 3.82
          Mean value_function loss: 181.8134
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.0484
                       Mean reward: 839.02
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.5274
     Episode_Reward/lifting_object: 164.6883
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.73s
                      Time elapsed: 01:16:39
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 37044 steps/s (collection: 2.507s, learning 0.147s)
             Mean action noise std: 3.82
          Mean value_function loss: 174.9836
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.0546
                       Mean reward: 830.19
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.5797
     Episode_Reward/lifting_object: 170.5747
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.65s
                      Time elapsed: 01:16:42
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 38218 steps/s (collection: 2.441s, learning 0.131s)
             Mean action noise std: 3.82
          Mean value_function loss: 172.2298
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.0616
                       Mean reward: 820.79
               Mean episode length: 222.28
    Episode_Reward/reaching_object: 1.5397
     Episode_Reward/lifting_object: 164.1541
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.57s
                      Time elapsed: 01:16:45
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 39789 steps/s (collection: 2.344s, learning 0.126s)
             Mean action noise std: 3.82
          Mean value_function loss: 169.1544
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.0682
                       Mean reward: 860.44
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.5412
     Episode_Reward/lifting_object: 165.9633
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.47s
                      Time elapsed: 01:16:47
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 38173 steps/s (collection: 2.438s, learning 0.137s)
             Mean action noise std: 3.82
          Mean value_function loss: 164.1956
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.0780
                       Mean reward: 838.12
               Mean episode length: 225.48
    Episode_Reward/reaching_object: 1.5648
     Episode_Reward/lifting_object: 168.4942
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.58s
                      Time elapsed: 01:16:50
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 39937 steps/s (collection: 2.358s, learning 0.103s)
             Mean action noise std: 3.83
          Mean value_function loss: 166.6138
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.0964
                       Mean reward: 787.87
               Mean episode length: 212.02
    Episode_Reward/reaching_object: 1.5403
     Episode_Reward/lifting_object: 165.5293
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.46s
                      Time elapsed: 01:16:52
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 41563 steps/s (collection: 2.254s, learning 0.111s)
             Mean action noise std: 3.83
          Mean value_function loss: 163.9013
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.1159
                       Mean reward: 827.30
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.4979
     Episode_Reward/lifting_object: 161.2695
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.37s
                      Time elapsed: 01:16:54
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 40628 steps/s (collection: 2.282s, learning 0.137s)
             Mean action noise std: 3.83
          Mean value_function loss: 213.1714
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.1303
                       Mean reward: 838.65
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.5244
     Episode_Reward/lifting_object: 161.2792
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.42s
                      Time elapsed: 01:16:57
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 40710 steps/s (collection: 2.293s, learning 0.122s)
             Mean action noise std: 3.84
          Mean value_function loss: 190.2387
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.1499
                       Mean reward: 790.72
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 1.5189
     Episode_Reward/lifting_object: 162.8713
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.41s
                      Time elapsed: 01:16:59
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 41607 steps/s (collection: 2.264s, learning 0.099s)
             Mean action noise std: 3.84
          Mean value_function loss: 234.7437
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 71.1710
                       Mean reward: 783.03
               Mean episode length: 213.33
    Episode_Reward/reaching_object: 1.4825
     Episode_Reward/lifting_object: 158.3920
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.36s
                      Time elapsed: 01:17:02
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 40486 steps/s (collection: 2.295s, learning 0.134s)
             Mean action noise std: 3.84
          Mean value_function loss: 237.8756
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.1811
                       Mean reward: 768.67
               Mean episode length: 209.83
    Episode_Reward/reaching_object: 1.4658
     Episode_Reward/lifting_object: 156.3163
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.43s
                      Time elapsed: 01:17:04
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 40118 steps/s (collection: 2.330s, learning 0.121s)
             Mean action noise std: 3.84
          Mean value_function loss: 173.5259
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.1935
                       Mean reward: 835.92
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.5435
     Episode_Reward/lifting_object: 163.9505
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.45s
                      Time elapsed: 01:17:06
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 24513 steps/s (collection: 3.881s, learning 0.129s)
             Mean action noise std: 3.84
          Mean value_function loss: 243.9484
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 71.2095
                       Mean reward: 812.08
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 1.4898
     Episode_Reward/lifting_object: 160.0085
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 4.01s
                      Time elapsed: 01:17:10
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13288 steps/s (collection: 7.270s, learning 0.127s)
             Mean action noise std: 3.84
          Mean value_function loss: 212.8675
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 71.2202
                       Mean reward: 799.00
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 1.4857
     Episode_Reward/lifting_object: 158.9438
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.40s
                      Time elapsed: 01:17:18
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13277 steps/s (collection: 7.284s, learning 0.120s)
             Mean action noise std: 3.85
          Mean value_function loss: 177.1331
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.2303
                       Mean reward: 765.94
               Mean episode length: 209.13
    Episode_Reward/reaching_object: 1.4847
     Episode_Reward/lifting_object: 159.2437
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.40s
                      Time elapsed: 01:17:25
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13033 steps/s (collection: 7.418s, learning 0.124s)
             Mean action noise std: 3.85
          Mean value_function loss: 177.0091
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.2456
                       Mean reward: 809.18
               Mean episode length: 217.75
    Episode_Reward/reaching_object: 1.5476
     Episode_Reward/lifting_object: 167.1450
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.54s
                      Time elapsed: 01:17:33
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 12979 steps/s (collection: 7.459s, learning 0.114s)
             Mean action noise std: 3.85
          Mean value_function loss: 179.0626
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 71.2551
                       Mean reward: 815.57
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 1.4880
     Episode_Reward/lifting_object: 159.5053
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.57s
                      Time elapsed: 01:17:40
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13192 steps/s (collection: 7.314s, learning 0.137s)
             Mean action noise std: 3.85
          Mean value_function loss: 205.6931
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.2606
                       Mean reward: 834.00
               Mean episode length: 225.53
    Episode_Reward/reaching_object: 1.5120
     Episode_Reward/lifting_object: 162.3517
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.45s
                      Time elapsed: 01:17:48
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13267 steps/s (collection: 7.281s, learning 0.128s)
             Mean action noise std: 3.85
          Mean value_function loss: 173.6243
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.2741
                       Mean reward: 840.56
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 1.5212
     Episode_Reward/lifting_object: 164.4845
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.41s
                      Time elapsed: 01:17:55
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13771 steps/s (collection: 7.017s, learning 0.121s)
             Mean action noise std: 3.86
          Mean value_function loss: 166.7205
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.2854
                       Mean reward: 745.53
               Mean episode length: 204.41
    Episode_Reward/reaching_object: 1.5190
     Episode_Reward/lifting_object: 163.8691
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.14s
                      Time elapsed: 01:18:02
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13461 steps/s (collection: 7.182s, learning 0.121s)
             Mean action noise std: 3.86
          Mean value_function loss: 260.2489
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.3001
                       Mean reward: 797.86
               Mean episode length: 216.48
    Episode_Reward/reaching_object: 1.4716
     Episode_Reward/lifting_object: 158.2389
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.30s
                      Time elapsed: 01:18:10
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 20455 steps/s (collection: 4.683s, learning 0.123s)
             Mean action noise std: 3.86
          Mean value_function loss: 175.7048
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.3162
                       Mean reward: 813.90
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 1.5006
     Episode_Reward/lifting_object: 161.4805
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.81s
                      Time elapsed: 01:18:15
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 41859 steps/s (collection: 2.241s, learning 0.108s)
             Mean action noise std: 3.86
          Mean value_function loss: 171.8528
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.3267
                       Mean reward: 823.58
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 1.4929
     Episode_Reward/lifting_object: 161.0448
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.35s
                      Time elapsed: 01:18:17
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 42317 steps/s (collection: 2.223s, learning 0.100s)
             Mean action noise std: 3.86
          Mean value_function loss: 211.8128
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.3453
                       Mean reward: 803.70
               Mean episode length: 217.36
    Episode_Reward/reaching_object: 1.5193
     Episode_Reward/lifting_object: 163.1709
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.32s
                      Time elapsed: 01:18:19
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 39500 steps/s (collection: 2.358s, learning 0.131s)
             Mean action noise std: 3.87
          Mean value_function loss: 210.0001
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.3604
                       Mean reward: 843.10
               Mean episode length: 226.44
    Episode_Reward/reaching_object: 1.4759
     Episode_Reward/lifting_object: 159.4251
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.49s
                      Time elapsed: 01:18:22
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 40848 steps/s (collection: 2.274s, learning 0.133s)
             Mean action noise std: 3.87
          Mean value_function loss: 161.8888
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.3724
                       Mean reward: 822.33
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 1.5192
     Episode_Reward/lifting_object: 164.5045
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.41s
                      Time elapsed: 01:18:24
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 40942 steps/s (collection: 2.250s, learning 0.151s)
             Mean action noise std: 3.87
          Mean value_function loss: 176.0909
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.3845
                       Mean reward: 789.95
               Mean episode length: 213.72
    Episode_Reward/reaching_object: 1.5007
     Episode_Reward/lifting_object: 162.2867
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.40s
                      Time elapsed: 01:18:26
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 41196 steps/s (collection: 2.262s, learning 0.125s)
             Mean action noise std: 3.87
          Mean value_function loss: 229.0763
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.3956
                       Mean reward: 789.02
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 1.4160
     Episode_Reward/lifting_object: 152.2607
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.39s
                      Time elapsed: 01:18:29
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 42389 steps/s (collection: 2.207s, learning 0.113s)
             Mean action noise std: 3.87
          Mean value_function loss: 179.5298
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.4075
                       Mean reward: 842.97
               Mean episode length: 226.86
    Episode_Reward/reaching_object: 1.5246
     Episode_Reward/lifting_object: 165.1248
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.32s
                      Time elapsed: 01:18:31
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 42060 steps/s (collection: 2.236s, learning 0.102s)
             Mean action noise std: 3.88
          Mean value_function loss: 214.4329
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 71.4193
                       Mean reward: 858.70
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 1.5094
     Episode_Reward/lifting_object: 163.2131
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.34s
                      Time elapsed: 01:18:34
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 40424 steps/s (collection: 2.311s, learning 0.121s)
             Mean action noise std: 3.88
          Mean value_function loss: 166.7808
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.4322
                       Mean reward: 801.19
               Mean episode length: 216.98
    Episode_Reward/reaching_object: 1.5089
     Episode_Reward/lifting_object: 163.5607
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.43s
                      Time elapsed: 01:18:36
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 41295 steps/s (collection: 2.280s, learning 0.100s)
             Mean action noise std: 3.88
          Mean value_function loss: 151.9993
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.4546
                       Mean reward: 871.44
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 1.5790
     Episode_Reward/lifting_object: 171.9948
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.38s
                      Time elapsed: 01:18:38
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 42046 steps/s (collection: 2.225s, learning 0.113s)
             Mean action noise std: 3.89
          Mean value_function loss: 204.7878
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 71.4774
                       Mean reward: 839.04
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.4951
     Episode_Reward/lifting_object: 161.3792
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.34s
                      Time elapsed: 01:18:41
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 42235 steps/s (collection: 2.212s, learning 0.115s)
             Mean action noise std: 3.89
          Mean value_function loss: 164.2944
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 71.4859
                       Mean reward: 846.22
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.5311
     Episode_Reward/lifting_object: 165.7999
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.33s
                      Time elapsed: 01:18:43
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 40568 steps/s (collection: 2.304s, learning 0.119s)
             Mean action noise std: 3.89
          Mean value_function loss: 178.0107
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.4965
                       Mean reward: 788.18
               Mean episode length: 213.12
    Episode_Reward/reaching_object: 1.5215
     Episode_Reward/lifting_object: 165.7432
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.42s
                      Time elapsed: 01:18:45
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 40466 steps/s (collection: 2.318s, learning 0.111s)
             Mean action noise std: 3.89
          Mean value_function loss: 180.2691
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.5130
                       Mean reward: 818.90
               Mean episode length: 220.00
    Episode_Reward/reaching_object: 1.5166
     Episode_Reward/lifting_object: 165.3024
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.43s
                      Time elapsed: 01:18:48
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 39143 steps/s (collection: 2.390s, learning 0.121s)
             Mean action noise std: 3.89
          Mean value_function loss: 189.7912
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.5273
                       Mean reward: 832.61
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 167.3255
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.51s
                      Time elapsed: 01:18:50
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 41076 steps/s (collection: 2.269s, learning 0.125s)
             Mean action noise std: 3.90
          Mean value_function loss: 193.9388
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.5403
                       Mean reward: 838.27
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 1.5245
     Episode_Reward/lifting_object: 165.9306
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.39s
                      Time elapsed: 01:18:53
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 41219 steps/s (collection: 2.252s, learning 0.133s)
             Mean action noise std: 3.90
          Mean value_function loss: 236.7494
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.5548
                       Mean reward: 836.87
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 1.5172
     Episode_Reward/lifting_object: 164.0365
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.38s
                      Time elapsed: 01:18:55
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 40539 steps/s (collection: 2.293s, learning 0.132s)
             Mean action noise std: 3.90
          Mean value_function loss: 155.9092
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.5712
                       Mean reward: 809.48
               Mean episode length: 219.62
    Episode_Reward/reaching_object: 1.4932
     Episode_Reward/lifting_object: 162.0006
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.42s
                      Time elapsed: 01:18:58
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 42057 steps/s (collection: 2.235s, learning 0.102s)
             Mean action noise std: 3.90
          Mean value_function loss: 160.5297
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.5816
                       Mean reward: 799.20
               Mean episode length: 216.29
    Episode_Reward/reaching_object: 1.4856
     Episode_Reward/lifting_object: 161.2452
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.34s
                      Time elapsed: 01:19:00
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 41700 steps/s (collection: 2.252s, learning 0.105s)
             Mean action noise std: 3.91
          Mean value_function loss: 142.0686
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.5990
                       Mean reward: 830.77
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 1.5265
     Episode_Reward/lifting_object: 166.4274
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.36s
                      Time elapsed: 01:19:02
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 40920 steps/s (collection: 2.288s, learning 0.114s)
             Mean action noise std: 3.91
          Mean value_function loss: 172.2790
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.6176
                       Mean reward: 842.59
               Mean episode length: 226.44
    Episode_Reward/reaching_object: 1.5258
     Episode_Reward/lifting_object: 165.8654
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.40s
                      Time elapsed: 01:19:05
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 40968 steps/s (collection: 2.278s, learning 0.121s)
             Mean action noise std: 3.91
          Mean value_function loss: 196.5585
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.6336
                       Mean reward: 736.75
               Mean episode length: 200.20
    Episode_Reward/reaching_object: 1.4525
     Episode_Reward/lifting_object: 157.0066
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.40s
                      Time elapsed: 01:19:07
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 41032 steps/s (collection: 2.291s, learning 0.105s)
             Mean action noise std: 3.91
          Mean value_function loss: 200.0958
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.6468
                       Mean reward: 843.31
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 1.4894
     Episode_Reward/lifting_object: 162.4233
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.40s
                      Time elapsed: 01:19:09
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 40849 steps/s (collection: 2.278s, learning 0.128s)
             Mean action noise std: 3.91
          Mean value_function loss: 179.8732
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.6573
                       Mean reward: 867.08
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 1.4882
     Episode_Reward/lifting_object: 161.5475
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.41s
                      Time elapsed: 01:19:12
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 40846 steps/s (collection: 2.303s, learning 0.104s)
             Mean action noise std: 3.92
          Mean value_function loss: 218.3083
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.6723
                       Mean reward: 796.91
               Mean episode length: 214.77
    Episode_Reward/reaching_object: 1.5050
     Episode_Reward/lifting_object: 163.5723
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.41s
                      Time elapsed: 01:19:14
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 40827 steps/s (collection: 2.287s, learning 0.121s)
             Mean action noise std: 3.92
          Mean value_function loss: 202.9362
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.6899
                       Mean reward: 842.37
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 1.5117
     Episode_Reward/lifting_object: 163.6102
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.41s
                      Time elapsed: 01:19:17
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 41917 steps/s (collection: 2.236s, learning 0.109s)
             Mean action noise std: 3.92
          Mean value_function loss: 177.2791
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 71.7062
                       Mean reward: 852.11
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 1.5379
     Episode_Reward/lifting_object: 165.7103
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.35s
                      Time elapsed: 01:19:19
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 40464 steps/s (collection: 2.294s, learning 0.135s)
             Mean action noise std: 3.92
          Mean value_function loss: 150.4965
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.7129
                       Mean reward: 852.83
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 1.5267
     Episode_Reward/lifting_object: 164.6303
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.43s
                      Time elapsed: 01:19:21
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 39174 steps/s (collection: 2.369s, learning 0.140s)
             Mean action noise std: 3.92
          Mean value_function loss: 220.0728
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.7236
                       Mean reward: 769.17
               Mean episode length: 208.13
    Episode_Reward/reaching_object: 1.4881
     Episode_Reward/lifting_object: 160.4354
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.51s
                      Time elapsed: 01:19:24
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 39648 steps/s (collection: 2.364s, learning 0.115s)
             Mean action noise std: 3.93
          Mean value_function loss: 187.9967
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.7362
                       Mean reward: 825.48
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 1.4897
     Episode_Reward/lifting_object: 159.6699
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.48s
                      Time elapsed: 01:19:26
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 41275 steps/s (collection: 2.237s, learning 0.145s)
             Mean action noise std: 3.93
          Mean value_function loss: 258.7775
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 71.7556
                       Mean reward: 837.86
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 1.5253
     Episode_Reward/lifting_object: 164.1524
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.38s
                      Time elapsed: 01:19:29
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 39995 steps/s (collection: 2.326s, learning 0.132s)
             Mean action noise std: 3.93
          Mean value_function loss: 225.8132
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.7687
                       Mean reward: 786.08
               Mean episode length: 214.18
    Episode_Reward/reaching_object: 1.4925
     Episode_Reward/lifting_object: 158.6190
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.46s
                      Time elapsed: 01:19:31
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 39802 steps/s (collection: 2.334s, learning 0.135s)
             Mean action noise std: 3.93
          Mean value_function loss: 249.5663
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.7838
                       Mean reward: 794.40
               Mean episode length: 216.23
    Episode_Reward/reaching_object: 1.4660
     Episode_Reward/lifting_object: 156.5233
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.47s
                      Time elapsed: 01:19:34
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 39385 steps/s (collection: 2.390s, learning 0.106s)
             Mean action noise std: 3.94
          Mean value_function loss: 222.8750
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.8017
                       Mean reward: 835.71
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 1.5411
     Episode_Reward/lifting_object: 165.2937
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.50s
                      Time elapsed: 01:19:36
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 37293 steps/s (collection: 2.487s, learning 0.149s)
             Mean action noise std: 3.94
          Mean value_function loss: 201.2079
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.8179
                       Mean reward: 837.60
               Mean episode length: 224.01
    Episode_Reward/reaching_object: 1.5249
     Episode_Reward/lifting_object: 164.1738
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0996
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.64s
                      Time elapsed: 01:19:39
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 37016 steps/s (collection: 2.495s, learning 0.161s)
             Mean action noise std: 3.94
          Mean value_function loss: 179.5257
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.8351
                       Mean reward: 796.73
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.5336
     Episode_Reward/lifting_object: 164.2664
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0996
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.66s
                      Time elapsed: 01:19:42
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 39142 steps/s (collection: 2.373s, learning 0.139s)
             Mean action noise std: 3.95
          Mean value_function loss: 190.9176
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.8572
                       Mean reward: 845.34
               Mean episode length: 231.50
    Episode_Reward/reaching_object: 1.5372
     Episode_Reward/lifting_object: 162.2132
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.1015
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.51s
                      Time elapsed: 01:19:44
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 40189 steps/s (collection: 2.309s, learning 0.137s)
             Mean action noise std: 3.95
          Mean value_function loss: 222.1942
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.8697
                       Mean reward: 816.52
               Mean episode length: 219.41
    Episode_Reward/reaching_object: 1.5252
     Episode_Reward/lifting_object: 163.8407
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.45s
                      Time elapsed: 01:19:46
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 40374 steps/s (collection: 2.301s, learning 0.134s)
             Mean action noise std: 3.95
          Mean value_function loss: 161.3929
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.8800
                       Mean reward: 875.02
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 162.6697
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.43s
                      Time elapsed: 01:19:49
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 40580 steps/s (collection: 2.294s, learning 0.128s)
             Mean action noise std: 3.95
          Mean value_function loss: 154.5658
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.8890
                       Mean reward: 855.67
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 1.5454
     Episode_Reward/lifting_object: 163.3152
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.42s
                      Time elapsed: 01:19:51
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 40837 steps/s (collection: 2.268s, learning 0.139s)
             Mean action noise std: 3.95
          Mean value_function loss: 152.7746
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.8973
                       Mean reward: 837.31
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 167.6419
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.41s
                      Time elapsed: 01:19:54
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 42302 steps/s (collection: 2.211s, learning 0.113s)
             Mean action noise std: 3.95
          Mean value_function loss: 175.7186
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9041
                       Mean reward: 854.28
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 1.5433
     Episode_Reward/lifting_object: 166.4809
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.32s
                      Time elapsed: 01:19:56
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 40894 steps/s (collection: 2.276s, learning 0.128s)
             Mean action noise std: 3.95
          Mean value_function loss: 155.4768
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.9116
                       Mean reward: 849.97
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.5299
     Episode_Reward/lifting_object: 164.7891
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.40s
                      Time elapsed: 01:19:58
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 40238 steps/s (collection: 2.335s, learning 0.108s)
             Mean action noise std: 3.96
          Mean value_function loss: 163.4124
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.9193
                       Mean reward: 840.22
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 1.5349
     Episode_Reward/lifting_object: 165.1440
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.44s
                      Time elapsed: 01:20:01
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 39179 steps/s (collection: 2.365s, learning 0.144s)
             Mean action noise std: 3.96
          Mean value_function loss: 192.3772
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.9342
                       Mean reward: 750.88
               Mean episode length: 207.61
    Episode_Reward/reaching_object: 1.4940
     Episode_Reward/lifting_object: 159.6744
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.51s
                      Time elapsed: 01:20:03
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 41567 steps/s (collection: 2.249s, learning 0.116s)
             Mean action noise std: 3.96
          Mean value_function loss: 146.7851
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.9463
                       Mean reward: 764.36
               Mean episode length: 207.84
    Episode_Reward/reaching_object: 1.5255
     Episode_Reward/lifting_object: 164.1645
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.36s
                      Time elapsed: 01:20:06
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 40184 steps/s (collection: 2.322s, learning 0.125s)
             Mean action noise std: 3.96
          Mean value_function loss: 173.8805
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.9628
                       Mean reward: 849.19
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 1.5146
     Episode_Reward/lifting_object: 163.2741
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.45s
                      Time elapsed: 01:20:08
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.238s, learning 0.112s)
             Mean action noise std: 3.96
          Mean value_function loss: 149.9222
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.9786
                       Mean reward: 851.59
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.5267
     Episode_Reward/lifting_object: 164.3863
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.35s
                      Time elapsed: 01:20:11
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 41161 steps/s (collection: 2.273s, learning 0.115s)
             Mean action noise std: 3.97
          Mean value_function loss: 136.5047
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.9925
                       Mean reward: 844.47
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.5422
     Episode_Reward/lifting_object: 166.2620
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.39s
                      Time elapsed: 01:20:13
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 40876 steps/s (collection: 2.296s, learning 0.109s)
             Mean action noise std: 3.97
          Mean value_function loss: 176.6509
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 72.0106
                       Mean reward: 869.19
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 1.4940
     Episode_Reward/lifting_object: 160.6587
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.40s
                      Time elapsed: 01:20:15
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 39329 steps/s (collection: 2.373s, learning 0.127s)
             Mean action noise std: 3.97
          Mean value_function loss: 145.3503
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.0293
                       Mean reward: 847.34
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.5187
     Episode_Reward/lifting_object: 163.4422
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.50s
                      Time elapsed: 01:20:18
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 40495 steps/s (collection: 2.304s, learning 0.124s)
             Mean action noise std: 3.97
          Mean value_function loss: 180.9105
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.0400
                       Mean reward: 861.23
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 1.5239
     Episode_Reward/lifting_object: 163.8168
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.43s
                      Time elapsed: 01:20:20
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 39573 steps/s (collection: 2.374s, learning 0.110s)
             Mean action noise std: 3.98
          Mean value_function loss: 191.3827
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.0528
                       Mean reward: 845.53
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.4948
     Episode_Reward/lifting_object: 160.3616
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.48s
                      Time elapsed: 01:20:23
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 40855 steps/s (collection: 2.280s, learning 0.127s)
             Mean action noise std: 3.98
          Mean value_function loss: 175.5975
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.0661
                       Mean reward: 819.45
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 1.5235
     Episode_Reward/lifting_object: 163.2466
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.41s
                      Time elapsed: 01:20:25
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 41183 steps/s (collection: 2.270s, learning 0.117s)
             Mean action noise std: 3.98
          Mean value_function loss: 206.7519
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.0867
                       Mean reward: 794.28
               Mean episode length: 216.84
    Episode_Reward/reaching_object: 1.4735
     Episode_Reward/lifting_object: 157.6653
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.39s
                      Time elapsed: 01:20:28
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 41271 steps/s (collection: 2.276s, learning 0.106s)
             Mean action noise std: 3.98
          Mean value_function loss: 194.9925
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.1008
                       Mean reward: 816.80
               Mean episode length: 221.36
    Episode_Reward/reaching_object: 1.4644
     Episode_Reward/lifting_object: 156.5724
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.38s
                      Time elapsed: 01:20:30
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 41107 steps/s (collection: 2.286s, learning 0.105s)
             Mean action noise std: 3.99
          Mean value_function loss: 201.4968
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.1105
                       Mean reward: 816.36
               Mean episode length: 220.88
    Episode_Reward/reaching_object: 1.5216
     Episode_Reward/lifting_object: 163.2321
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.39s
                      Time elapsed: 01:20:32
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 41025 steps/s (collection: 2.293s, learning 0.104s)
             Mean action noise std: 3.99
          Mean value_function loss: 193.0654
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 72.1176
                       Mean reward: 759.12
               Mean episode length: 208.10
    Episode_Reward/reaching_object: 1.4772
     Episode_Reward/lifting_object: 157.7191
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.40s
                      Time elapsed: 01:20:35
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 41065 steps/s (collection: 2.283s, learning 0.111s)
             Mean action noise std: 3.99
          Mean value_function loss: 191.2960
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.1240
                       Mean reward: 805.73
               Mean episode length: 216.90
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 158.7298
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.39s
                      Time elapsed: 01:20:37
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 40482 steps/s (collection: 2.321s, learning 0.107s)
             Mean action noise std: 3.99
          Mean value_function loss: 188.3616
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.1367
                       Mean reward: 841.42
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.5397
     Episode_Reward/lifting_object: 165.0105
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.43s
                      Time elapsed: 01:20:40
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 41396 steps/s (collection: 2.264s, learning 0.111s)
             Mean action noise std: 3.99
          Mean value_function loss: 180.3177
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.1474
                       Mean reward: 825.57
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 1.5189
     Episode_Reward/lifting_object: 162.6802
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.37s
                      Time elapsed: 01:20:42
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 40603 steps/s (collection: 2.303s, learning 0.119s)
             Mean action noise std: 3.99
          Mean value_function loss: 183.9634
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.1615
                       Mean reward: 782.71
               Mean episode length: 213.95
    Episode_Reward/reaching_object: 1.4914
     Episode_Reward/lifting_object: 160.0046
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.42s
                      Time elapsed: 01:20:44
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 41569 steps/s (collection: 2.262s, learning 0.103s)
             Mean action noise std: 4.00
          Mean value_function loss: 172.7356
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.1770
                       Mean reward: 861.49
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.5588
     Episode_Reward/lifting_object: 168.7467
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.36s
                      Time elapsed: 01:20:47
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 41385 steps/s (collection: 2.267s, learning 0.109s)
             Mean action noise std: 4.00
          Mean value_function loss: 146.6719
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.1943
                       Mean reward: 871.96
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 1.5239
     Episode_Reward/lifting_object: 164.1481
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.38s
                      Time elapsed: 01:20:49
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 40828 steps/s (collection: 2.274s, learning 0.134s)
             Mean action noise std: 4.00
          Mean value_function loss: 151.3079
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.2061
                       Mean reward: 855.10
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.5658
     Episode_Reward/lifting_object: 169.8082
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.41s
                      Time elapsed: 01:20:52
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 41355 steps/s (collection: 2.266s, learning 0.111s)
             Mean action noise std: 4.00
          Mean value_function loss: 197.5779
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.2180
                       Mean reward: 852.28
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 1.5095
     Episode_Reward/lifting_object: 163.0185
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.38s
                      Time elapsed: 01:20:54
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 41695 steps/s (collection: 2.243s, learning 0.115s)
             Mean action noise std: 4.01
          Mean value_function loss: 208.3236
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.2332
                       Mean reward: 834.29
               Mean episode length: 223.33
    Episode_Reward/reaching_object: 1.5272
     Episode_Reward/lifting_object: 165.1040
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.36s
                      Time elapsed: 01:20:56
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 40417 steps/s (collection: 2.281s, learning 0.151s)
             Mean action noise std: 4.01
          Mean value_function loss: 177.1972
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.2445
                       Mean reward: 858.40
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.4708
     Episode_Reward/lifting_object: 157.7821
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.43s
                      Time elapsed: 01:20:59
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 40573 steps/s (collection: 2.304s, learning 0.119s)
             Mean action noise std: 4.01
          Mean value_function loss: 174.1840
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.2539
                       Mean reward: 815.46
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.5328
     Episode_Reward/lifting_object: 165.8296
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.42s
                      Time elapsed: 01:21:01
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 38795 steps/s (collection: 2.388s, learning 0.146s)
             Mean action noise std: 4.01
          Mean value_function loss: 190.9668
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.2641
                       Mean reward: 843.50
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 1.4860
     Episode_Reward/lifting_object: 160.9436
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.53s
                      Time elapsed: 01:21:04
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 40489 steps/s (collection: 2.280s, learning 0.148s)
             Mean action noise std: 4.01
          Mean value_function loss: 164.5456
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.2701
                       Mean reward: 799.55
               Mean episode length: 215.82
    Episode_Reward/reaching_object: 1.4888
     Episode_Reward/lifting_object: 161.4153
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.43s
                      Time elapsed: 01:21:06
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 38788 steps/s (collection: 2.382s, learning 0.153s)
             Mean action noise std: 4.01
          Mean value_function loss: 113.7723
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.2842
                       Mean reward: 884.12
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 1.5557
     Episode_Reward/lifting_object: 168.8317
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.53s
                      Time elapsed: 01:21:09
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 39087 steps/s (collection: 2.376s, learning 0.139s)
             Mean action noise std: 4.02
          Mean value_function loss: 106.0839
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.3029
                       Mean reward: 854.98
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 1.5583
     Episode_Reward/lifting_object: 169.7365
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.51s
                      Time elapsed: 01:21:11
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 40017 steps/s (collection: 2.322s, learning 0.135s)
             Mean action noise std: 4.02
          Mean value_function loss: 155.9546
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.3122
                       Mean reward: 844.56
               Mean episode length: 226.60
    Episode_Reward/reaching_object: 1.5697
     Episode_Reward/lifting_object: 171.0771
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.46s
                      Time elapsed: 01:21:14
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 40959 steps/s (collection: 2.284s, learning 0.116s)
             Mean action noise std: 4.02
          Mean value_function loss: 153.0214
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.3257
                       Mean reward: 798.56
               Mean episode length: 215.39
    Episode_Reward/reaching_object: 1.5178
     Episode_Reward/lifting_object: 164.9201
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.40s
                      Time elapsed: 01:21:16
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 36282 steps/s (collection: 2.547s, learning 0.162s)
             Mean action noise std: 4.02
          Mean value_function loss: 179.1181
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.3381
                       Mean reward: 849.67
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.4942
     Episode_Reward/lifting_object: 162.6969
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.71s
                      Time elapsed: 01:21:19
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 40760 steps/s (collection: 2.297s, learning 0.115s)
             Mean action noise std: 4.02
          Mean value_function loss: 179.2377
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.3499
                       Mean reward: 809.58
               Mean episode length: 217.36
    Episode_Reward/reaching_object: 1.5000
     Episode_Reward/lifting_object: 163.6840
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.41s
                      Time elapsed: 01:21:21
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 40483 steps/s (collection: 2.323s, learning 0.106s)
             Mean action noise std: 4.03
          Mean value_function loss: 180.1187
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.3647
                       Mean reward: 830.53
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.5155
     Episode_Reward/lifting_object: 166.1318
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.43s
                      Time elapsed: 01:21:24
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 40849 steps/s (collection: 2.280s, learning 0.126s)
             Mean action noise std: 4.03
          Mean value_function loss: 145.3804
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.3750
                       Mean reward: 846.07
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 1.5204
     Episode_Reward/lifting_object: 166.6779
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.41s
                      Time elapsed: 01:21:26
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 39949 steps/s (collection: 2.337s, learning 0.124s)
             Mean action noise std: 4.03
          Mean value_function loss: 193.3483
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.3891
                       Mean reward: 831.70
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 1.4788
     Episode_Reward/lifting_object: 162.1153
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.46s
                      Time elapsed: 01:21:28
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 39584 steps/s (collection: 2.378s, learning 0.105s)
             Mean action noise std: 4.03
          Mean value_function loss: 169.4088
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.4096
                       Mean reward: 789.37
               Mean episode length: 213.99
    Episode_Reward/reaching_object: 1.4899
     Episode_Reward/lifting_object: 163.2891
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.48s
                      Time elapsed: 01:21:31
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 40713 steps/s (collection: 2.304s, learning 0.110s)
             Mean action noise std: 4.04
          Mean value_function loss: 163.4168
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.4198
                       Mean reward: 844.53
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 1.5070
     Episode_Reward/lifting_object: 165.8378
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.41s
                      Time elapsed: 01:21:33
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 39538 steps/s (collection: 2.367s, learning 0.119s)
             Mean action noise std: 4.04
          Mean value_function loss: 192.8599
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.4286
                       Mean reward: 840.29
               Mean episode length: 226.51
    Episode_Reward/reaching_object: 1.5079
     Episode_Reward/lifting_object: 165.1812
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.49s
                      Time elapsed: 01:21:36
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 38348 steps/s (collection: 2.435s, learning 0.128s)
             Mean action noise std: 4.04
          Mean value_function loss: 194.8965
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.4378
                       Mean reward: 906.96
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 1.4847
     Episode_Reward/lifting_object: 162.9006
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.56s
                      Time elapsed: 01:21:38
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 40728 steps/s (collection: 2.303s, learning 0.111s)
             Mean action noise std: 4.04
          Mean value_function loss: 155.2281
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.4479
                       Mean reward: 795.30
               Mean episode length: 215.82
    Episode_Reward/reaching_object: 1.5081
     Episode_Reward/lifting_object: 165.1628
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.41s
                      Time elapsed: 01:21:41
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 38837 steps/s (collection: 2.403s, learning 0.128s)
             Mean action noise std: 4.04
          Mean value_function loss: 137.2807
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.4619
                       Mean reward: 852.63
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 1.5405
     Episode_Reward/lifting_object: 169.5021
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.53s
                      Time elapsed: 01:21:43
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 38806 steps/s (collection: 2.414s, learning 0.119s)
             Mean action noise std: 4.04
          Mean value_function loss: 210.0095
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.4786
                       Mean reward: 816.84
               Mean episode length: 219.76
    Episode_Reward/reaching_object: 1.4931
     Episode_Reward/lifting_object: 163.7616
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.53s
                      Time elapsed: 01:21:46
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 39644 steps/s (collection: 2.363s, learning 0.117s)
             Mean action noise std: 4.05
          Mean value_function loss: 136.3810
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.4877
                       Mean reward: 817.38
               Mean episode length: 221.06
    Episode_Reward/reaching_object: 1.5006
     Episode_Reward/lifting_object: 164.1653
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.48s
                      Time elapsed: 01:21:48
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 39370 steps/s (collection: 2.363s, learning 0.134s)
             Mean action noise std: 4.05
          Mean value_function loss: 161.6898
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.4970
                       Mean reward: 860.33
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.5225
     Episode_Reward/lifting_object: 167.1767
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.50s
                      Time elapsed: 01:21:51
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 38894 steps/s (collection: 2.416s, learning 0.112s)
             Mean action noise std: 4.05
          Mean value_function loss: 195.1431
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.5064
                       Mean reward: 863.92
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 1.4812
     Episode_Reward/lifting_object: 161.9290
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.53s
                      Time elapsed: 01:21:53
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 38896 steps/s (collection: 2.370s, learning 0.157s)
             Mean action noise std: 4.05
          Mean value_function loss: 184.3230
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.5208
                       Mean reward: 819.46
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 1.5030
     Episode_Reward/lifting_object: 164.5451
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.53s
                      Time elapsed: 01:21:56
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 39166 steps/s (collection: 2.363s, learning 0.147s)
             Mean action noise std: 4.06
          Mean value_function loss: 166.5763
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.5409
                       Mean reward: 820.00
               Mean episode length: 221.65
    Episode_Reward/reaching_object: 1.4937
     Episode_Reward/lifting_object: 163.5580
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.51s
                      Time elapsed: 01:21:58
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 39099 steps/s (collection: 2.386s, learning 0.129s)
             Mean action noise std: 4.06
          Mean value_function loss: 222.1831
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.5580
                       Mean reward: 843.71
               Mean episode length: 225.46
    Episode_Reward/reaching_object: 1.4873
     Episode_Reward/lifting_object: 163.3419
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.51s
                      Time elapsed: 01:22:01
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 39520 steps/s (collection: 2.368s, learning 0.120s)
             Mean action noise std: 4.06
          Mean value_function loss: 230.8337
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 72.5724
                       Mean reward: 758.33
               Mean episode length: 209.00
    Episode_Reward/reaching_object: 1.4118
     Episode_Reward/lifting_object: 153.2952
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.49s
                      Time elapsed: 01:22:03
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 40114 steps/s (collection: 2.322s, learning 0.129s)
             Mean action noise std: 4.06
          Mean value_function loss: 184.4319
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.5855
                       Mean reward: 780.98
               Mean episode length: 211.45
    Episode_Reward/reaching_object: 1.4453
     Episode_Reward/lifting_object: 157.9164
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.45s
                      Time elapsed: 01:22:06
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 40816 steps/s (collection: 2.298s, learning 0.110s)
             Mean action noise std: 4.07
          Mean value_function loss: 147.9690
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.6035
                       Mean reward: 827.96
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 1.5072
     Episode_Reward/lifting_object: 165.4838
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.41s
                      Time elapsed: 01:22:08
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 40837 steps/s (collection: 2.301s, learning 0.107s)
             Mean action noise std: 4.07
          Mean value_function loss: 149.7443
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.6250
                       Mean reward: 828.13
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.5054
     Episode_Reward/lifting_object: 165.9277
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.1016
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.41s
                      Time elapsed: 01:22:11
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 40637 steps/s (collection: 2.309s, learning 0.110s)
             Mean action noise std: 4.07
          Mean value_function loss: 166.6464
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.6376
                       Mean reward: 833.07
               Mean episode length: 222.93
    Episode_Reward/reaching_object: 1.4933
     Episode_Reward/lifting_object: 164.3014
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.1013
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.42s
                      Time elapsed: 01:22:13
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 39957 steps/s (collection: 2.342s, learning 0.118s)
             Mean action noise std: 4.07
          Mean value_function loss: 210.3686
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.6504
                       Mean reward: 802.53
               Mean episode length: 216.53
    Episode_Reward/reaching_object: 1.4736
     Episode_Reward/lifting_object: 161.9861
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.46s
                      Time elapsed: 01:22:16
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 38808 steps/s (collection: 2.410s, learning 0.123s)
             Mean action noise std: 4.07
          Mean value_function loss: 190.6634
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.6616
                       Mean reward: 823.01
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 1.4598
     Episode_Reward/lifting_object: 160.1769
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.53s
                      Time elapsed: 01:22:18
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 38116 steps/s (collection: 2.450s, learning 0.129s)
             Mean action noise std: 4.08
          Mean value_function loss: 247.4370
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.6719
                       Mean reward: 804.60
               Mean episode length: 216.18
    Episode_Reward/reaching_object: 1.4572
     Episode_Reward/lifting_object: 159.9648
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.58s
                      Time elapsed: 01:22:21
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 39100 steps/s (collection: 2.394s, learning 0.121s)
             Mean action noise std: 4.08
          Mean value_function loss: 142.8496
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.6829
                       Mean reward: 864.29
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.5380
     Episode_Reward/lifting_object: 169.5618
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.51s
                      Time elapsed: 01:22:23
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 39003 steps/s (collection: 2.399s, learning 0.122s)
             Mean action noise std: 4.08
          Mean value_function loss: 190.2396
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 72.6930
                       Mean reward: 844.28
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.4581
     Episode_Reward/lifting_object: 160.3373
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.52s
                      Time elapsed: 01:22:26
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 39931 steps/s (collection: 2.342s, learning 0.120s)
             Mean action noise std: 4.08
          Mean value_function loss: 215.2994
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.6966
                       Mean reward: 808.69
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 1.4645
     Episode_Reward/lifting_object: 159.6795
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.46s
                      Time elapsed: 01:22:28
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 38726 steps/s (collection: 2.398s, learning 0.140s)
             Mean action noise std: 4.08
          Mean value_function loss: 178.9418
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.7019
                       Mean reward: 830.11
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 1.5019
     Episode_Reward/lifting_object: 164.8993
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.54s
                      Time elapsed: 01:22:31
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 40915 steps/s (collection: 2.297s, learning 0.106s)
             Mean action noise std: 4.08
          Mean value_function loss: 241.5106
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 72.7117
                       Mean reward: 799.80
               Mean episode length: 217.03
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 153.9405
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.40s
                      Time elapsed: 01:22:33
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 39363 steps/s (collection: 2.381s, learning 0.116s)
             Mean action noise std: 4.08
          Mean value_function loss: 174.6241
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.7178
                       Mean reward: 836.86
               Mean episode length: 227.00
    Episode_Reward/reaching_object: 1.4973
     Episode_Reward/lifting_object: 163.9281
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.50s
                      Time elapsed: 01:22:36
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 37821 steps/s (collection: 2.493s, learning 0.106s)
             Mean action noise std: 4.09
          Mean value_function loss: 175.1405
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.7287
                       Mean reward: 810.62
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 1.4968
     Episode_Reward/lifting_object: 165.2660
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.60s
                      Time elapsed: 01:22:38
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 37761 steps/s (collection: 2.488s, learning 0.115s)
             Mean action noise std: 4.09
          Mean value_function loss: 172.7170
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.7392
                       Mean reward: 847.21
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.5235
     Episode_Reward/lifting_object: 167.8870
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.1022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.60s
                      Time elapsed: 01:22:41
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 40364 steps/s (collection: 2.322s, learning 0.114s)
             Mean action noise std: 4.09
          Mean value_function loss: 211.7219
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.7544
                       Mean reward: 812.74
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.4758
     Episode_Reward/lifting_object: 161.7227
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.44s
                      Time elapsed: 01:22:43
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 39970 steps/s (collection: 2.324s, learning 0.136s)
             Mean action noise std: 4.09
          Mean value_function loss: 209.7887
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.7751
                       Mean reward: 799.91
               Mean episode length: 218.20
    Episode_Reward/reaching_object: 1.5043
     Episode_Reward/lifting_object: 165.8831
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.46s
                      Time elapsed: 01:22:46
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 40462 steps/s (collection: 2.325s, learning 0.105s)
             Mean action noise std: 4.10
          Mean value_function loss: 183.4593
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.7944
                       Mean reward: 839.50
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 1.4877
     Episode_Reward/lifting_object: 163.5314
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.43s
                      Time elapsed: 01:22:48
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 40759 steps/s (collection: 2.281s, learning 0.131s)
             Mean action noise std: 4.10
          Mean value_function loss: 172.6492
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.8043
                       Mean reward: 810.84
               Mean episode length: 219.60
    Episode_Reward/reaching_object: 1.4673
     Episode_Reward/lifting_object: 159.0763
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.41s
                      Time elapsed: 01:22:51
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 39790 steps/s (collection: 2.330s, learning 0.140s)
             Mean action noise std: 4.10
          Mean value_function loss: 163.9396
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.8199
                       Mean reward: 812.15
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 1.4745
     Episode_Reward/lifting_object: 160.5369
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.47s
                      Time elapsed: 01:22:53
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 39879 steps/s (collection: 2.328s, learning 0.137s)
             Mean action noise std: 4.10
          Mean value_function loss: 160.2108
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.8356
                       Mean reward: 821.13
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 1.4688
     Episode_Reward/lifting_object: 160.1940
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.47s
                      Time elapsed: 01:22:55
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 40572 steps/s (collection: 2.301s, learning 0.122s)
             Mean action noise std: 4.11
          Mean value_function loss: 170.6584
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.8505
                       Mean reward: 808.30
               Mean episode length: 217.47
    Episode_Reward/reaching_object: 1.4886
     Episode_Reward/lifting_object: 163.3517
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.42s
                      Time elapsed: 01:22:58
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 40817 steps/s (collection: 2.281s, learning 0.127s)
             Mean action noise std: 4.11
          Mean value_function loss: 157.0347
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.8646
                       Mean reward: 830.51
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.4755
     Episode_Reward/lifting_object: 161.2600
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.41s
                      Time elapsed: 01:23:00
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 37859 steps/s (collection: 2.452s, learning 0.145s)
             Mean action noise std: 4.11
          Mean value_function loss: 155.7186
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.8749
                       Mean reward: 849.21
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.5427
     Episode_Reward/lifting_object: 168.8178
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.60s
                      Time elapsed: 01:23:03
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 41494 steps/s (collection: 2.250s, learning 0.120s)
             Mean action noise std: 4.11
          Mean value_function loss: 226.2230
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.8843
                       Mean reward: 823.63
               Mean episode length: 221.90
    Episode_Reward/reaching_object: 1.4953
     Episode_Reward/lifting_object: 163.1177
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.37s
                      Time elapsed: 01:23:05
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 41154 steps/s (collection: 2.289s, learning 0.100s)
             Mean action noise std: 4.11
          Mean value_function loss: 193.2640
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.8967
                       Mean reward: 793.69
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 1.4961
     Episode_Reward/lifting_object: 162.8404
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.39s
                      Time elapsed: 01:23:08
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 40195 steps/s (collection: 2.320s, learning 0.126s)
             Mean action noise std: 4.12
          Mean value_function loss: 168.0603
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.9101
                       Mean reward: 854.96
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 1.5194
     Episode_Reward/lifting_object: 164.8726
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.45s
                      Time elapsed: 01:23:10
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 40991 steps/s (collection: 2.295s, learning 0.103s)
             Mean action noise std: 4.12
          Mean value_function loss: 204.7610
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.9241
                       Mean reward: 830.79
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.5013
     Episode_Reward/lifting_object: 162.1294
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.40s
                      Time elapsed: 01:23:12
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 40946 steps/s (collection: 2.280s, learning 0.121s)
             Mean action noise std: 4.12
          Mean value_function loss: 172.4458
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.9390
                       Mean reward: 793.83
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 1.4659
     Episode_Reward/lifting_object: 158.5560
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.40s
                      Time elapsed: 01:23:15
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 38148 steps/s (collection: 2.428s, learning 0.149s)
             Mean action noise std: 4.12
          Mean value_function loss: 156.1359
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.9521
                       Mean reward: 821.62
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 1.5023
     Episode_Reward/lifting_object: 162.9996
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.58s
                      Time elapsed: 01:23:17
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 39389 steps/s (collection: 2.387s, learning 0.109s)
             Mean action noise std: 4.12
          Mean value_function loss: 170.3497
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.9693
                       Mean reward: 821.92
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 1.5089
     Episode_Reward/lifting_object: 163.8833
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.50s
                      Time elapsed: 01:23:20
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 40878 steps/s (collection: 2.306s, learning 0.099s)
             Mean action noise std: 4.13
          Mean value_function loss: 229.8582
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.9819
                       Mean reward: 831.62
               Mean episode length: 222.51
    Episode_Reward/reaching_object: 1.5225
     Episode_Reward/lifting_object: 164.8738
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.40s
                      Time elapsed: 01:23:22
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 39668 steps/s (collection: 2.330s, learning 0.149s)
             Mean action noise std: 4.13
          Mean value_function loss: 189.0693
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.9928
                       Mean reward: 809.00
               Mean episode length: 218.37
    Episode_Reward/reaching_object: 1.4857
     Episode_Reward/lifting_object: 160.6577
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.48s
                      Time elapsed: 01:23:25
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 40685 steps/s (collection: 2.302s, learning 0.115s)
             Mean action noise std: 4.13
          Mean value_function loss: 188.9219
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.0061
                       Mean reward: 831.77
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.5216
     Episode_Reward/lifting_object: 164.4727
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.42s
                      Time elapsed: 01:23:27
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 39519 steps/s (collection: 2.329s, learning 0.158s)
             Mean action noise std: 4.13
          Mean value_function loss: 164.0385
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.0137
                       Mean reward: 814.01
               Mean episode length: 220.47
    Episode_Reward/reaching_object: 1.5441
     Episode_Reward/lifting_object: 166.1722
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.49s
                      Time elapsed: 01:23:30
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 40127 steps/s (collection: 2.320s, learning 0.130s)
             Mean action noise std: 4.13
          Mean value_function loss: 210.6575
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.0241
                       Mean reward: 856.34
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.5585
     Episode_Reward/lifting_object: 167.5813
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.1015
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.45s
                      Time elapsed: 01:23:32
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 41303 steps/s (collection: 2.253s, learning 0.127s)
             Mean action noise std: 4.14
          Mean value_function loss: 206.0150
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.0405
                       Mean reward: 825.07
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 1.5065
     Episode_Reward/lifting_object: 162.8928
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.38s
                      Time elapsed: 01:23:35
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 37035 steps/s (collection: 2.543s, learning 0.112s)
             Mean action noise std: 4.14
          Mean value_function loss: 180.6826
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.0534
                       Mean reward: 824.19
               Mean episode length: 222.33
    Episode_Reward/reaching_object: 1.5150
     Episode_Reward/lifting_object: 163.2133
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.65s
                      Time elapsed: 01:23:37
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 40938 steps/s (collection: 2.286s, learning 0.115s)
             Mean action noise std: 4.14
          Mean value_function loss: 174.8867
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.0724
                       Mean reward: 857.70
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 1.5115
     Episode_Reward/lifting_object: 163.7345
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.40s
                      Time elapsed: 01:23:40
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 37094 steps/s (collection: 2.506s, learning 0.144s)
             Mean action noise std: 4.14
          Mean value_function loss: 206.0206
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.0917
                       Mean reward: 804.91
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 1.4912
     Episode_Reward/lifting_object: 161.1561
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.65s
                      Time elapsed: 01:23:42
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 37640 steps/s (collection: 2.480s, learning 0.132s)
             Mean action noise std: 4.15
          Mean value_function loss: 165.9895
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.1061
                       Mean reward: 845.92
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.5146
     Episode_Reward/lifting_object: 163.4842
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.61s
                      Time elapsed: 01:23:45
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 35717 steps/s (collection: 2.622s, learning 0.130s)
             Mean action noise std: 4.15
          Mean value_function loss: 142.1445
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.1226
                       Mean reward: 823.88
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 1.5436
     Episode_Reward/lifting_object: 165.9835
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.75s
                      Time elapsed: 01:23:48
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 38401 steps/s (collection: 2.422s, learning 0.138s)
             Mean action noise std: 4.15
          Mean value_function loss: 200.2022
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.1405
                       Mean reward: 773.37
               Mean episode length: 208.21
    Episode_Reward/reaching_object: 1.4974
     Episode_Reward/lifting_object: 162.3332
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.56s
                      Time elapsed: 01:23:50
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 38262 steps/s (collection: 2.458s, learning 0.112s)
             Mean action noise std: 4.15
          Mean value_function loss: 227.6820
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.1553
                       Mean reward: 807.95
               Mean episode length: 217.22
    Episode_Reward/reaching_object: 1.4798
     Episode_Reward/lifting_object: 160.3549
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.57s
                      Time elapsed: 01:23:53
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 40367 steps/s (collection: 2.301s, learning 0.134s)
             Mean action noise std: 4.16
          Mean value_function loss: 208.8729
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.1625
                       Mean reward: 806.34
               Mean episode length: 219.14
    Episode_Reward/reaching_object: 1.5084
     Episode_Reward/lifting_object: 161.3939
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.1013
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.44s
                      Time elapsed: 01:23:55
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 37241 steps/s (collection: 2.520s, learning 0.120s)
             Mean action noise std: 4.16
          Mean value_function loss: 229.6763
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.1765
                       Mean reward: 812.78
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 1.4819
     Episode_Reward/lifting_object: 160.1814
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.64s
                      Time elapsed: 01:23:58
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 40087 steps/s (collection: 2.328s, learning 0.125s)
             Mean action noise std: 4.16
          Mean value_function loss: 170.5708
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.1846
                       Mean reward: 791.88
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 1.4847
     Episode_Reward/lifting_object: 160.2172
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.45s
                      Time elapsed: 01:24:00
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 40220 steps/s (collection: 2.307s, learning 0.138s)
             Mean action noise std: 4.16
          Mean value_function loss: 211.9727
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.1943
                       Mean reward: 787.92
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 1.5157
     Episode_Reward/lifting_object: 163.4111
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.44s
                      Time elapsed: 01:24:03
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 40938 steps/s (collection: 2.286s, learning 0.115s)
             Mean action noise std: 4.16
          Mean value_function loss: 243.9778
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.2100
                       Mean reward: 821.59
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.5050
     Episode_Reward/lifting_object: 161.9037
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.40s
                      Time elapsed: 01:24:05
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 41389 steps/s (collection: 2.268s, learning 0.107s)
             Mean action noise std: 4.17
          Mean value_function loss: 178.3606
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.2262
                       Mean reward: 849.80
               Mean episode length: 226.60
    Episode_Reward/reaching_object: 1.4723
     Episode_Reward/lifting_object: 157.4746
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.38s
                      Time elapsed: 01:24:08
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 40515 steps/s (collection: 2.300s, learning 0.126s)
             Mean action noise std: 4.17
          Mean value_function loss: 174.4439
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.2389
                       Mean reward: 833.09
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 1.5091
     Episode_Reward/lifting_object: 161.8271
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.43s
                      Time elapsed: 01:24:10
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 41555 steps/s (collection: 2.259s, learning 0.107s)
             Mean action noise std: 4.17
          Mean value_function loss: 171.3858
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.2508
                       Mean reward: 842.27
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 1.5332
     Episode_Reward/lifting_object: 164.4717
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.37s
                      Time elapsed: 01:24:12
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 42039 steps/s (collection: 2.238s, learning 0.101s)
             Mean action noise std: 4.17
          Mean value_function loss: 160.9897
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.2575
                       Mean reward: 836.97
               Mean episode length: 224.51
    Episode_Reward/reaching_object: 1.5325
     Episode_Reward/lifting_object: 163.0839
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.34s
                      Time elapsed: 01:24:15
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 40729 steps/s (collection: 2.283s, learning 0.131s)
             Mean action noise std: 4.17
          Mean value_function loss: 188.8322
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.2673
                       Mean reward: 824.32
               Mean episode length: 221.27
    Episode_Reward/reaching_object: 1.4812
     Episode_Reward/lifting_object: 159.0839
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.41s
                      Time elapsed: 01:24:17
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 41058 steps/s (collection: 2.283s, learning 0.111s)
             Mean action noise std: 4.17
          Mean value_function loss: 193.3962
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.2817
                       Mean reward: 853.05
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 1.5493
     Episode_Reward/lifting_object: 166.8902
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.1016
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.39s
                      Time elapsed: 01:24:19
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 40245 steps/s (collection: 2.337s, learning 0.106s)
             Mean action noise std: 4.18
          Mean value_function loss: 238.4571
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.2958
                       Mean reward: 805.38
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 1.4929
     Episode_Reward/lifting_object: 159.6874
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.44s
                      Time elapsed: 01:24:22
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 39314 steps/s (collection: 2.369s, learning 0.131s)
             Mean action noise std: 4.18
          Mean value_function loss: 166.2237
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.3036
                       Mean reward: 806.45
               Mean episode length: 218.06
    Episode_Reward/reaching_object: 1.5274
     Episode_Reward/lifting_object: 164.6962
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.50s
                      Time elapsed: 01:24:24
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 41647 steps/s (collection: 2.238s, learning 0.123s)
             Mean action noise std: 4.18
          Mean value_function loss: 188.3865
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.3108
                       Mean reward: 858.22
               Mean episode length: 230.24
    Episode_Reward/reaching_object: 1.5103
     Episode_Reward/lifting_object: 162.0927
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.36s
                      Time elapsed: 01:24:27
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 39978 steps/s (collection: 2.336s, learning 0.123s)
             Mean action noise std: 4.18
          Mean value_function loss: 210.1436
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.3227
                       Mean reward: 824.44
               Mean episode length: 223.71
    Episode_Reward/reaching_object: 1.5216
     Episode_Reward/lifting_object: 163.4058
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.46s
                      Time elapsed: 01:24:29
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 41302 steps/s (collection: 2.264s, learning 0.117s)
             Mean action noise std: 4.18
          Mean value_function loss: 162.5741
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.3415
                       Mean reward: 838.99
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 162.8696
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.38s
                      Time elapsed: 01:24:32
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 40116 steps/s (collection: 2.338s, learning 0.112s)
             Mean action noise std: 4.19
          Mean value_function loss: 187.1353
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.3510
                       Mean reward: 811.15
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 1.5063
     Episode_Reward/lifting_object: 161.7719
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.45s
                      Time elapsed: 01:24:34
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 40614 steps/s (collection: 2.300s, learning 0.120s)
             Mean action noise std: 4.19
          Mean value_function loss: 182.1829
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.3625
                       Mean reward: 832.67
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.5043
     Episode_Reward/lifting_object: 159.8982
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.42s
                      Time elapsed: 01:24:36
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 40373 steps/s (collection: 2.309s, learning 0.125s)
             Mean action noise std: 4.19
          Mean value_function loss: 150.7747
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.3762
                       Mean reward: 820.02
               Mean episode length: 221.24
    Episode_Reward/reaching_object: 1.5131
     Episode_Reward/lifting_object: 162.8154
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.43s
                      Time elapsed: 01:24:39
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 38011 steps/s (collection: 2.447s, learning 0.139s)
             Mean action noise std: 4.19
          Mean value_function loss: 158.5286
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.3864
                       Mean reward: 833.88
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 1.5079
     Episode_Reward/lifting_object: 162.5257
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.59s
                      Time elapsed: 01:24:41
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 40780 steps/s (collection: 2.300s, learning 0.111s)
             Mean action noise std: 4.19
          Mean value_function loss: 145.1646
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.3955
                       Mean reward: 859.14
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.5202
     Episode_Reward/lifting_object: 162.6481
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.41s
                      Time elapsed: 01:24:44
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 41694 steps/s (collection: 2.253s, learning 0.105s)
             Mean action noise std: 4.19
          Mean value_function loss: 194.8659
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.4029
                       Mean reward: 782.53
               Mean episode length: 212.51
    Episode_Reward/reaching_object: 1.5527
     Episode_Reward/lifting_object: 167.4888
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.36s
                      Time elapsed: 01:24:46
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 41346 steps/s (collection: 2.280s, learning 0.098s)
             Mean action noise std: 4.20
          Mean value_function loss: 163.2066
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.4199
                       Mean reward: 869.44
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 1.5488
     Episode_Reward/lifting_object: 166.6423
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.38s
                      Time elapsed: 01:24:49
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 40892 steps/s (collection: 2.272s, learning 0.132s)
             Mean action noise std: 4.20
          Mean value_function loss: 149.9853
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.4352
                       Mean reward: 813.02
               Mean episode length: 217.81
    Episode_Reward/reaching_object: 1.5394
     Episode_Reward/lifting_object: 165.9120
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.40s
                      Time elapsed: 01:24:51
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 40495 steps/s (collection: 2.309s, learning 0.119s)
             Mean action noise std: 4.20
          Mean value_function loss: 177.0107
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.4466
                       Mean reward: 805.69
               Mean episode length: 216.69
    Episode_Reward/reaching_object: 1.5210
     Episode_Reward/lifting_object: 163.3171
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.43s
                      Time elapsed: 01:24:53
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 41669 steps/s (collection: 2.250s, learning 0.109s)
             Mean action noise std: 4.20
          Mean value_function loss: 157.0753
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.4575
                       Mean reward: 826.59
               Mean episode length: 223.87
    Episode_Reward/reaching_object: 1.5230
     Episode_Reward/lifting_object: 163.7905
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.36s
                      Time elapsed: 01:24:56
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 40417 steps/s (collection: 2.294s, learning 0.139s)
             Mean action noise std: 4.20
          Mean value_function loss: 197.3114
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.4628
                       Mean reward: 865.55
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.4840
     Episode_Reward/lifting_object: 159.5915
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.43s
                      Time elapsed: 01:24:58
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 41323 steps/s (collection: 2.277s, learning 0.102s)
             Mean action noise std: 4.21
          Mean value_function loss: 200.3164
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.4689
                       Mean reward: 809.52
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 1.5282
     Episode_Reward/lifting_object: 160.9113
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.38s
                      Time elapsed: 01:25:01
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 41501 steps/s (collection: 2.247s, learning 0.122s)
             Mean action noise std: 4.21
          Mean value_function loss: 204.3170
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.4752
                       Mean reward: 793.86
               Mean episode length: 214.76
    Episode_Reward/reaching_object: 1.4995
     Episode_Reward/lifting_object: 161.5097
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.37s
                      Time elapsed: 01:25:03
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 40169 steps/s (collection: 2.307s, learning 0.141s)
             Mean action noise std: 4.21
          Mean value_function loss: 178.7608
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.4891
                       Mean reward: 807.95
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.4748
     Episode_Reward/lifting_object: 158.8772
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.45s
                      Time elapsed: 01:25:05
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 40512 steps/s (collection: 2.307s, learning 0.120s)
             Mean action noise std: 4.21
          Mean value_function loss: 145.7190
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.5024
                       Mean reward: 828.32
               Mean episode length: 222.24
    Episode_Reward/reaching_object: 1.5137
     Episode_Reward/lifting_object: 163.6215
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.1029
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.43s
                      Time elapsed: 01:25:08
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 39602 steps/s (collection: 2.355s, learning 0.127s)
             Mean action noise std: 4.22
          Mean value_function loss: 156.1641
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.5232
                       Mean reward: 833.09
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.5285
     Episode_Reward/lifting_object: 165.4899
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.48s
                      Time elapsed: 01:25:10
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 41276 steps/s (collection: 2.248s, learning 0.134s)
             Mean action noise std: 4.22
          Mean value_function loss: 189.3720
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.5371
                       Mean reward: 776.63
               Mean episode length: 211.32
    Episode_Reward/reaching_object: 1.4893
     Episode_Reward/lifting_object: 161.0510
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.38s
                      Time elapsed: 01:25:13
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 41673 steps/s (collection: 2.251s, learning 0.108s)
             Mean action noise std: 4.22
          Mean value_function loss: 183.3721
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.5534
                       Mean reward: 818.53
               Mean episode length: 220.23
    Episode_Reward/reaching_object: 1.5035
     Episode_Reward/lifting_object: 163.0407
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.36s
                      Time elapsed: 01:25:15
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 40920 steps/s (collection: 2.281s, learning 0.121s)
             Mean action noise std: 4.22
          Mean value_function loss: 183.0164
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.5643
                       Mean reward: 843.32
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.5291
     Episode_Reward/lifting_object: 165.7850
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.40s
                      Time elapsed: 01:25:17
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 40909 steps/s (collection: 2.284s, learning 0.119s)
             Mean action noise std: 4.22
          Mean value_function loss: 129.8016
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.5775
                       Mean reward: 831.82
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.5377
     Episode_Reward/lifting_object: 166.8263
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.40s
                      Time elapsed: 01:25:20
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 40436 steps/s (collection: 2.285s, learning 0.146s)
             Mean action noise std: 4.23
          Mean value_function loss: 172.4605
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.5899
                       Mean reward: 815.08
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.5101
     Episode_Reward/lifting_object: 163.0209
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.43s
                      Time elapsed: 01:25:22
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 40482 steps/s (collection: 2.307s, learning 0.121s)
             Mean action noise std: 4.23
          Mean value_function loss: 160.5728
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.6013
                       Mean reward: 861.77
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.5352
     Episode_Reward/lifting_object: 166.9029
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.43s
                      Time elapsed: 01:25:25
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 40900 steps/s (collection: 2.265s, learning 0.139s)
             Mean action noise std: 4.23
          Mean value_function loss: 171.9724
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.6141
                       Mean reward: 831.97
               Mean episode length: 224.34
    Episode_Reward/reaching_object: 1.5084
     Episode_Reward/lifting_object: 163.7341
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.40s
                      Time elapsed: 01:25:27
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 40883 steps/s (collection: 2.294s, learning 0.110s)
             Mean action noise std: 4.23
          Mean value_function loss: 214.2649
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.6231
                       Mean reward: 781.48
               Mean episode length: 212.29
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 162.4351
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.40s
                      Time elapsed: 01:25:30
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 40372 steps/s (collection: 2.331s, learning 0.104s)
             Mean action noise std: 4.23
          Mean value_function loss: 171.2546
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.6341
                       Mean reward: 870.02
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.5189
     Episode_Reward/lifting_object: 164.9898
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.43s
                      Time elapsed: 01:25:32
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 39902 steps/s (collection: 2.344s, learning 0.120s)
             Mean action noise std: 4.23
          Mean value_function loss: 188.0029
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.6432
                       Mean reward: 813.65
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 1.4693
     Episode_Reward/lifting_object: 159.7452
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.46s
                      Time elapsed: 01:25:34
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 41115 steps/s (collection: 2.262s, learning 0.129s)
             Mean action noise std: 4.24
          Mean value_function loss: 186.1310
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.6545
                       Mean reward: 828.19
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 1.4544
     Episode_Reward/lifting_object: 156.8595
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.39s
                      Time elapsed: 01:25:37
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 40418 steps/s (collection: 2.316s, learning 0.116s)
             Mean action noise std: 4.24
          Mean value_function loss: 178.9619
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.6681
                       Mean reward: 803.42
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.4746
     Episode_Reward/lifting_object: 159.8265
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0991
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.43s
                      Time elapsed: 01:25:39
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 38673 steps/s (collection: 2.397s, learning 0.145s)
             Mean action noise std: 4.24
          Mean value_function loss: 183.9909
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.6806
                       Mean reward: 828.66
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.4806
     Episode_Reward/lifting_object: 160.7068
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0991
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.54s
                      Time elapsed: 01:25:42
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 38886 steps/s (collection: 2.404s, learning 0.124s)
             Mean action noise std: 4.24
          Mean value_function loss: 2063.0969
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 73.6842
                       Mean reward: 841.89
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.5513
     Episode_Reward/lifting_object: 169.2286
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -2.6400
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.53s
                      Time elapsed: 01:25:44
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 40216 steps/s (collection: 2.302s, learning 0.142s)
             Mean action noise std: 4.24
          Mean value_function loss: 209.5029
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.6871
                       Mean reward: 815.02
               Mean episode length: 218.36
    Episode_Reward/reaching_object: 1.4756
     Episode_Reward/lifting_object: 159.8406
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.1029
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.44s
                      Time elapsed: 01:25:47
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 39825 steps/s (collection: 2.327s, learning 0.142s)
             Mean action noise std: 4.24
          Mean value_function loss: 222.9515
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.6951
                       Mean reward: 808.31
               Mean episode length: 219.28
    Episode_Reward/reaching_object: 1.5189
     Episode_Reward/lifting_object: 165.6366
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.47s
                      Time elapsed: 01:25:49
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 41042 steps/s (collection: 2.281s, learning 0.114s)
             Mean action noise std: 4.24
          Mean value_function loss: 164.6773
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.7036
                       Mean reward: 792.77
               Mean episode length: 213.68
    Episode_Reward/reaching_object: 1.4981
     Episode_Reward/lifting_object: 163.3098
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.40s
                      Time elapsed: 01:25:52
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 41724 steps/s (collection: 2.259s, learning 0.097s)
             Mean action noise std: 4.25
          Mean value_function loss: 209.7495
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.7149
                       Mean reward: 759.33
               Mean episode length: 206.76
    Episode_Reward/reaching_object: 1.4600
     Episode_Reward/lifting_object: 158.9222
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.36s
                      Time elapsed: 01:25:54
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 40154 steps/s (collection: 2.323s, learning 0.126s)
             Mean action noise std: 4.25
          Mean value_function loss: 213.3944
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.7284
                       Mean reward: 839.58
               Mean episode length: 225.52
    Episode_Reward/reaching_object: 1.4851
     Episode_Reward/lifting_object: 161.3705
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.45s
                      Time elapsed: 01:25:56
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 39718 steps/s (collection: 2.345s, learning 0.130s)
             Mean action noise std: 4.25
          Mean value_function loss: 169.5419
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.7438
                       Mean reward: 764.06
               Mean episode length: 208.60
    Episode_Reward/reaching_object: 1.4717
     Episode_Reward/lifting_object: 160.6626
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.48s
                      Time elapsed: 01:25:59
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 37589 steps/s (collection: 2.482s, learning 0.134s)
             Mean action noise std: 4.25
          Mean value_function loss: 189.8414
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.7595
                       Mean reward: 743.12
               Mean episode length: 205.94
    Episode_Reward/reaching_object: 1.4333
     Episode_Reward/lifting_object: 156.2036
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.62s
                      Time elapsed: 01:26:02
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 39743 steps/s (collection: 2.344s, learning 0.130s)
             Mean action noise std: 4.26
          Mean value_function loss: 176.4332
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.7703
                       Mean reward: 826.51
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 1.5101
     Episode_Reward/lifting_object: 165.3812
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.47s
                      Time elapsed: 01:26:04
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 39731 steps/s (collection: 2.344s, learning 0.130s)
             Mean action noise std: 4.26
          Mean value_function loss: 208.0518
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.7882
                       Mean reward: 796.87
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 1.4810
     Episode_Reward/lifting_object: 161.8759
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.47s
                      Time elapsed: 01:26:07
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 38651 steps/s (collection: 2.408s, learning 0.136s)
             Mean action noise std: 4.26
          Mean value_function loss: 183.0511
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.8021
                       Mean reward: 836.32
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 1.4875
     Episode_Reward/lifting_object: 162.0407
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.54s
                      Time elapsed: 01:26:09
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 38527 steps/s (collection: 2.430s, learning 0.121s)
             Mean action noise std: 4.26
          Mean value_function loss: 161.3966
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.8122
                       Mean reward: 847.01
               Mean episode length: 227.23
    Episode_Reward/reaching_object: 1.5061
     Episode_Reward/lifting_object: 164.5937
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.55s
                      Time elapsed: 01:26:12
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 39902 steps/s (collection: 2.357s, learning 0.107s)
             Mean action noise std: 4.27
          Mean value_function loss: 167.0555
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.8219
                       Mean reward: 832.15
               Mean episode length: 224.38
    Episode_Reward/reaching_object: 1.5050
     Episode_Reward/lifting_object: 164.6410
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.46s
                      Time elapsed: 01:26:14
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 38449 steps/s (collection: 2.439s, learning 0.118s)
             Mean action noise std: 4.27
          Mean value_function loss: 164.8149
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 73.8352
                       Mean reward: 799.29
               Mean episode length: 215.67
    Episode_Reward/reaching_object: 1.4825
     Episode_Reward/lifting_object: 162.1200
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.56s
                      Time elapsed: 01:26:17
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 39552 steps/s (collection: 2.369s, learning 0.116s)
             Mean action noise std: 4.27
          Mean value_function loss: 196.0660
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.8510
                       Mean reward: 807.09
               Mean episode length: 218.05
    Episode_Reward/reaching_object: 1.4432
     Episode_Reward/lifting_object: 157.0761
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.49s
                      Time elapsed: 01:26:19
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 39614 steps/s (collection: 2.363s, learning 0.119s)
             Mean action noise std: 4.27
          Mean value_function loss: 190.6435
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8604
                       Mean reward: 834.03
               Mean episode length: 224.17
    Episode_Reward/reaching_object: 1.4602
     Episode_Reward/lifting_object: 159.3651
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.48s
                      Time elapsed: 01:26:22
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 39562 steps/s (collection: 2.385s, learning 0.100s)
             Mean action noise std: 4.28
          Mean value_function loss: 238.1000
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.8740
                       Mean reward: 785.88
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 1.4801
     Episode_Reward/lifting_object: 161.6502
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.48s
                      Time elapsed: 01:26:24
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 41329 steps/s (collection: 2.273s, learning 0.106s)
             Mean action noise std: 4.28
          Mean value_function loss: 196.2024
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.8953
                       Mean reward: 862.23
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 1.5190
     Episode_Reward/lifting_object: 165.9936
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.38s
                      Time elapsed: 01:26:26
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 39547 steps/s (collection: 2.371s, learning 0.115s)
             Mean action noise std: 4.28
          Mean value_function loss: 168.8079
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.9060
                       Mean reward: 862.81
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 1.5151
     Episode_Reward/lifting_object: 164.4560
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1038
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.49s
                      Time elapsed: 01:26:29
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 40365 steps/s (collection: 2.290s, learning 0.145s)
             Mean action noise std: 4.28
          Mean value_function loss: 173.5673
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.9165
                       Mean reward: 834.29
               Mean episode length: 223.21
    Episode_Reward/reaching_object: 1.5495
     Episode_Reward/lifting_object: 169.5245
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.44s
                      Time elapsed: 01:26:31
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 40980 steps/s (collection: 2.287s, learning 0.112s)
             Mean action noise std: 4.28
          Mean value_function loss: 205.0005
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.9303
                       Mean reward: 843.92
               Mean episode length: 226.87
    Episode_Reward/reaching_object: 1.4915
     Episode_Reward/lifting_object: 161.8603
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.40s
                      Time elapsed: 01:26:34
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 40732 steps/s (collection: 2.300s, learning 0.113s)
             Mean action noise std: 4.29
          Mean value_function loss: 161.0849
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.9408
                       Mean reward: 811.42
               Mean episode length: 217.85
    Episode_Reward/reaching_object: 1.5021
     Episode_Reward/lifting_object: 163.8155
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.41s
                      Time elapsed: 01:26:36
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 40220 steps/s (collection: 2.326s, learning 0.118s)
             Mean action noise std: 4.29
          Mean value_function loss: 169.7858
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.9521
                       Mean reward: 845.17
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 1.4914
     Episode_Reward/lifting_object: 163.4158
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.44s
                      Time elapsed: 01:26:39
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 40773 steps/s (collection: 2.304s, learning 0.107s)
             Mean action noise std: 4.29
          Mean value_function loss: 196.7630
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.9665
                       Mean reward: 819.65
               Mean episode length: 220.42
    Episode_Reward/reaching_object: 1.4973
     Episode_Reward/lifting_object: 163.3591
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.41s
                      Time elapsed: 01:26:41
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 40798 steps/s (collection: 2.287s, learning 0.123s)
             Mean action noise std: 4.29
          Mean value_function loss: 190.2274
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.9816
                       Mean reward: 848.01
               Mean episode length: 227.03
    Episode_Reward/reaching_object: 1.4526
     Episode_Reward/lifting_object: 158.3323
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.41s
                      Time elapsed: 01:26:43
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 38603 steps/s (collection: 2.439s, learning 0.108s)
             Mean action noise std: 4.29
          Mean value_function loss: 192.6682
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.9914
                       Mean reward: 822.43
               Mean episode length: 219.80
    Episode_Reward/reaching_object: 1.4896
     Episode_Reward/lifting_object: 162.9321
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1027
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.55s
                      Time elapsed: 01:26:46
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 37862 steps/s (collection: 2.452s, learning 0.144s)
             Mean action noise std: 4.30
          Mean value_function loss: 155.5047
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.0080
                       Mean reward: 828.36
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 1.4714
     Episode_Reward/lifting_object: 160.8270
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.60s
                      Time elapsed: 01:26:49
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 40205 steps/s (collection: 2.324s, learning 0.121s)
             Mean action noise std: 4.30
          Mean value_function loss: 156.1871
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0203
                       Mean reward: 840.17
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.4770
     Episode_Reward/lifting_object: 159.5856
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.45s
                      Time elapsed: 01:26:51
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 40225 steps/s (collection: 2.327s, learning 0.117s)
             Mean action noise std: 4.30
          Mean value_function loss: 135.5550
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.0270
                       Mean reward: 852.27
               Mean episode length: 228.03
    Episode_Reward/reaching_object: 1.5262
     Episode_Reward/lifting_object: 167.5400
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.44s
                      Time elapsed: 01:26:53
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 41165 steps/s (collection: 2.276s, learning 0.112s)
             Mean action noise std: 4.30
          Mean value_function loss: 152.5451
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.0351
                       Mean reward: 816.04
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 1.5343
     Episode_Reward/lifting_object: 167.4671
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.1083
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.39s
                      Time elapsed: 01:26:56
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 39691 steps/s (collection: 2.327s, learning 0.150s)
             Mean action noise std: 4.30
          Mean value_function loss: 230.6735
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.0444
                       Mean reward: 851.70
               Mean episode length: 229.66
    Episode_Reward/reaching_object: 1.4301
     Episode_Reward/lifting_object: 155.6531
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.48s
                      Time elapsed: 01:26:58
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 37478 steps/s (collection: 2.470s, learning 0.153s)
             Mean action noise std: 4.31
          Mean value_function loss: 169.8614
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.0560
                       Mean reward: 811.99
               Mean episode length: 219.64
    Episode_Reward/reaching_object: 1.4677
     Episode_Reward/lifting_object: 160.6781
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.62s
                      Time elapsed: 01:27:01
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 37990 steps/s (collection: 2.449s, learning 0.139s)
             Mean action noise std: 4.31
          Mean value_function loss: 156.8073
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.0635
                       Mean reward: 798.71
               Mean episode length: 214.58
    Episode_Reward/reaching_object: 1.5139
     Episode_Reward/lifting_object: 166.5622
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.59s
                      Time elapsed: 01:27:04
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 37542 steps/s (collection: 2.503s, learning 0.115s)
             Mean action noise std: 4.31
          Mean value_function loss: 162.6857
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.0750
                       Mean reward: 833.04
               Mean episode length: 223.99
    Episode_Reward/reaching_object: 1.5304
     Episode_Reward/lifting_object: 168.2863
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.62s
                      Time elapsed: 01:27:06
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 37877 steps/s (collection: 2.461s, learning 0.135s)
             Mean action noise std: 4.31
          Mean value_function loss: 175.6986
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.0845
                       Mean reward: 805.35
               Mean episode length: 216.74
    Episode_Reward/reaching_object: 1.4655
     Episode_Reward/lifting_object: 160.4983
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.60s
                      Time elapsed: 01:27:09
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 37133 steps/s (collection: 2.528s, learning 0.120s)
             Mean action noise std: 4.31
          Mean value_function loss: 179.7083
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.0924
                       Mean reward: 847.54
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 1.4813
     Episode_Reward/lifting_object: 161.7338
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.65s
                      Time elapsed: 01:27:11
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 38120 steps/s (collection: 2.444s, learning 0.135s)
             Mean action noise std: 4.32
          Mean value_function loss: 208.7537
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.1049
                       Mean reward: 790.05
               Mean episode length: 211.82
    Episode_Reward/reaching_object: 1.4665
     Episode_Reward/lifting_object: 161.0489
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.58s
                      Time elapsed: 01:27:14
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 40102 steps/s (collection: 2.327s, learning 0.124s)
             Mean action noise std: 4.32
          Mean value_function loss: 215.7690
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.1188
                       Mean reward: 787.26
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 1.4820
     Episode_Reward/lifting_object: 162.5755
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1034
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.45s
                      Time elapsed: 01:27:16
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 38165 steps/s (collection: 2.427s, learning 0.149s)
             Mean action noise std: 4.32
          Mean value_function loss: 202.7068
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.1341
                       Mean reward: 803.33
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 1.4929
     Episode_Reward/lifting_object: 163.7676
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1041
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.58s
                      Time elapsed: 01:27:19
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 38764 steps/s (collection: 2.394s, learning 0.142s)
             Mean action noise std: 4.32
          Mean value_function loss: 147.3085
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.1449
                       Mean reward: 854.36
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.5075
     Episode_Reward/lifting_object: 165.2412
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.54s
                      Time elapsed: 01:27:22
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 39589 steps/s (collection: 2.376s, learning 0.107s)
             Mean action noise std: 4.33
          Mean value_function loss: 168.8543
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 74.1580
                       Mean reward: 860.29
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.5163
     Episode_Reward/lifting_object: 166.7598
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.1081
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.48s
                      Time elapsed: 01:27:24
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 40426 steps/s (collection: 2.333s, learning 0.099s)
             Mean action noise std: 4.33
          Mean value_function loss: 210.3664
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.1720
                       Mean reward: 851.06
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.4921
     Episode_Reward/lifting_object: 163.1450
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.1084
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.43s
                      Time elapsed: 01:27:26
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 38096 steps/s (collection: 2.431s, learning 0.149s)
             Mean action noise std: 4.33
          Mean value_function loss: 197.4804
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.1846
                       Mean reward: 846.56
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.5105
     Episode_Reward/lifting_object: 164.5360
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.1094
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.58s
                      Time elapsed: 01:27:29
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 37102 steps/s (collection: 2.515s, learning 0.134s)
             Mean action noise std: 4.33
          Mean value_function loss: 260.5356
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.1987
                       Mean reward: 773.63
               Mean episode length: 211.55
    Episode_Reward/reaching_object: 1.4584
     Episode_Reward/lifting_object: 157.3960
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.65s
                      Time elapsed: 01:27:32
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 40058 steps/s (collection: 2.331s, learning 0.123s)
             Mean action noise std: 4.33
          Mean value_function loss: 246.5753
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.2112
                       Mean reward: 794.46
               Mean episode length: 216.31
    Episode_Reward/reaching_object: 1.5163
     Episode_Reward/lifting_object: 162.9081
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.1115
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.45s
                      Time elapsed: 01:27:34
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 38475 steps/s (collection: 2.424s, learning 0.131s)
             Mean action noise std: 4.34
          Mean value_function loss: 141.5594
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.2208
                       Mean reward: 860.26
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.5098
     Episode_Reward/lifting_object: 164.3190
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.1096
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.55s
                      Time elapsed: 01:27:37
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 36217 steps/s (collection: 2.579s, learning 0.136s)
             Mean action noise std: 4.34
          Mean value_function loss: 183.9125
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.2278
                       Mean reward: 800.45
               Mean episode length: 217.99
    Episode_Reward/reaching_object: 1.5331
     Episode_Reward/lifting_object: 166.5609
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.1116
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.71s
                      Time elapsed: 01:27:39
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 37100 steps/s (collection: 2.533s, learning 0.117s)
             Mean action noise std: 4.34
          Mean value_function loss: 198.8076
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.2378
                       Mean reward: 787.80
               Mean episode length: 214.71
    Episode_Reward/reaching_object: 1.4939
     Episode_Reward/lifting_object: 161.3440
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.1097
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.65s
                      Time elapsed: 01:27:42
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 39386 steps/s (collection: 2.379s, learning 0.116s)
             Mean action noise std: 4.34
          Mean value_function loss: 176.2415
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.2555
                       Mean reward: 835.43
               Mean episode length: 223.64
    Episode_Reward/reaching_object: 1.5313
     Episode_Reward/lifting_object: 166.2704
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.1113
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.50s
                      Time elapsed: 01:27:45
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 40213 steps/s (collection: 2.320s, learning 0.124s)
             Mean action noise std: 4.34
          Mean value_function loss: 209.3918
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.2653
                       Mean reward: 789.81
               Mean episode length: 213.93
    Episode_Reward/reaching_object: 1.4854
     Episode_Reward/lifting_object: 160.4785
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.44s
                      Time elapsed: 01:27:47
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 39875 steps/s (collection: 2.345s, learning 0.121s)
             Mean action noise std: 4.34
          Mean value_function loss: 198.1222
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.2750
                       Mean reward: 828.50
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 1.4975
     Episode_Reward/lifting_object: 161.4967
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1044
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.47s
                      Time elapsed: 01:27:49
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 37548 steps/s (collection: 2.482s, learning 0.137s)
             Mean action noise std: 4.35
          Mean value_function loss: 153.5386
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.2843
                       Mean reward: 879.54
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.5065
     Episode_Reward/lifting_object: 161.3090
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.62s
                      Time elapsed: 01:27:52
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 39065 steps/s (collection: 2.393s, learning 0.124s)
             Mean action noise std: 4.35
          Mean value_function loss: 173.5052
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.2993
                       Mean reward: 802.30
               Mean episode length: 217.17
    Episode_Reward/reaching_object: 1.4883
     Episode_Reward/lifting_object: 160.8149
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1041
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.52s
                      Time elapsed: 01:27:55
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 39726 steps/s (collection: 2.359s, learning 0.116s)
             Mean action noise std: 4.35
          Mean value_function loss: 172.7576
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.3109
                       Mean reward: 865.68
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 1.5217
     Episode_Reward/lifting_object: 164.3725
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1065
          Episode_Reward/joint_vel: -0.1106
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.47s
                      Time elapsed: 01:27:57
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 38982 steps/s (collection: 2.393s, learning 0.129s)
             Mean action noise std: 4.35
          Mean value_function loss: 158.9385
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.3214
                       Mean reward: 849.58
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.4915
     Episode_Reward/lifting_object: 161.6603
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.52s
                      Time elapsed: 01:28:00
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 36625 steps/s (collection: 2.553s, learning 0.131s)
             Mean action noise std: 4.35
          Mean value_function loss: 167.7012
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.3283
                       Mean reward: 863.37
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.5453
     Episode_Reward/lifting_object: 167.7413
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.1112
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.68s
                      Time elapsed: 01:28:02
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 36867 steps/s (collection: 2.533s, learning 0.133s)
             Mean action noise std: 4.35
          Mean value_function loss: 205.7123
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.3308
                       Mean reward: 838.65
               Mean episode length: 224.64
    Episode_Reward/reaching_object: 1.5345
     Episode_Reward/lifting_object: 166.9633
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.1108
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.67s
                      Time elapsed: 01:28:05
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 38533 steps/s (collection: 2.420s, learning 0.132s)
             Mean action noise std: 4.35
          Mean value_function loss: 167.4822
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.3358
                       Mean reward: 845.62
               Mean episode length: 226.74
    Episode_Reward/reaching_object: 1.4936
     Episode_Reward/lifting_object: 160.1349
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.1091
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.55s
                      Time elapsed: 01:28:08
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 38581 steps/s (collection: 2.434s, learning 0.114s)
             Mean action noise std: 4.36
          Mean value_function loss: 171.2397
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.3512
                       Mean reward: 768.36
               Mean episode length: 208.52
    Episode_Reward/reaching_object: 1.4557
     Episode_Reward/lifting_object: 158.1154
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1034
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.55s
                      Time elapsed: 01:28:10
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 41112 steps/s (collection: 2.281s, learning 0.110s)
             Mean action noise std: 4.36
          Mean value_function loss: 142.9306
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.3691
                       Mean reward: 869.08
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.5129
     Episode_Reward/lifting_object: 164.9605
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.1107
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.39s
                      Time elapsed: 01:28:12
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 40111 steps/s (collection: 2.326s, learning 0.125s)
             Mean action noise std: 4.36
          Mean value_function loss: 133.0713
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.3800
                       Mean reward: 878.50
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.5165
     Episode_Reward/lifting_object: 165.9877
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.1114
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.45s
                      Time elapsed: 01:28:15
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 39800 steps/s (collection: 2.341s, learning 0.128s)
             Mean action noise std: 4.36
          Mean value_function loss: 133.4702
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.3928
                       Mean reward: 834.19
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 1.5465
     Episode_Reward/lifting_object: 169.8045
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.1123
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.47s
                      Time elapsed: 01:28:17
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 40282 steps/s (collection: 2.335s, learning 0.106s)
             Mean action noise std: 4.37
          Mean value_function loss: 166.7254
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.4102
                       Mean reward: 801.60
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 1.4935
     Episode_Reward/lifting_object: 163.6295
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.1107
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.44s
                      Time elapsed: 01:28:20
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 40557 steps/s (collection: 2.317s, learning 0.107s)
             Mean action noise std: 4.37
          Mean value_function loss: 162.6892
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.4276
                       Mean reward: 875.61
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 1.5297
     Episode_Reward/lifting_object: 167.2643
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.1139
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.42s
                      Time elapsed: 01:28:22
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 40793 steps/s (collection: 2.273s, learning 0.137s)
             Mean action noise std: 4.37
          Mean value_function loss: 131.8871
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.4416
                       Mean reward: 876.45
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 1.5331
     Episode_Reward/lifting_object: 168.2303
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.1136
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.41s
                      Time elapsed: 01:28:25
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 39995 steps/s (collection: 2.333s, learning 0.125s)
             Mean action noise std: 4.37
          Mean value_function loss: 129.1785
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.4508
                       Mean reward: 872.58
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.5440
     Episode_Reward/lifting_object: 169.0140
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.1130
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.46s
                      Time elapsed: 01:28:27
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 41412 steps/s (collection: 2.255s, learning 0.119s)
             Mean action noise std: 4.38
          Mean value_function loss: 141.0722
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.4604
                       Mean reward: 895.49
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.5267
     Episode_Reward/lifting_object: 166.5634
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1089
          Episode_Reward/joint_vel: -0.1127
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.37s
                      Time elapsed: 01:28:29
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 40673 steps/s (collection: 2.283s, learning 0.134s)
             Mean action noise std: 4.38
          Mean value_function loss: 187.6824
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.4720
                       Mean reward: 814.33
               Mean episode length: 219.92
    Episode_Reward/reaching_object: 1.5031
     Episode_Reward/lifting_object: 164.0572
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.1126
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.42s
                      Time elapsed: 01:28:32
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 39016 steps/s (collection: 2.389s, learning 0.130s)
             Mean action noise std: 4.38
          Mean value_function loss: 163.1676
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.4830
                       Mean reward: 825.49
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.4862
     Episode_Reward/lifting_object: 162.0836
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.52s
                      Time elapsed: 01:28:34
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 38890 steps/s (collection: 2.385s, learning 0.143s)
             Mean action noise std: 4.38
          Mean value_function loss: 142.2242
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.4914
                       Mean reward: 832.96
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 166.8994
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.1121
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.53s
                      Time elapsed: 01:28:37
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 39056 steps/s (collection: 2.393s, learning 0.124s)
             Mean action noise std: 4.38
          Mean value_function loss: 152.8354
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.4977
                       Mean reward: 838.04
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.5004
     Episode_Reward/lifting_object: 163.9716
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.1113
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.52s
                      Time elapsed: 01:28:39
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 40476 steps/s (collection: 2.299s, learning 0.130s)
             Mean action noise std: 4.38
          Mean value_function loss: 182.9851
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.5017
                       Mean reward: 786.55
               Mean episode length: 219.37
    Episode_Reward/reaching_object: 1.4863
     Episode_Reward/lifting_object: 161.1229
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.1107
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.43s
                      Time elapsed: 01:28:42
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 40933 steps/s (collection: 2.277s, learning 0.124s)
             Mean action noise std: 4.38
          Mean value_function loss: 163.8619
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.5090
                       Mean reward: 810.91
               Mean episode length: 218.67
    Episode_Reward/reaching_object: 1.5068
     Episode_Reward/lifting_object: 164.8442
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.1111
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.40s
                      Time elapsed: 01:28:44
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 39512 steps/s (collection: 2.362s, learning 0.126s)
             Mean action noise std: 4.39
          Mean value_function loss: 158.0741
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.5152
                       Mean reward: 830.69
               Mean episode length: 222.83
    Episode_Reward/reaching_object: 1.4845
     Episode_Reward/lifting_object: 162.4624
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.1084
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.49s
                      Time elapsed: 01:28:47
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 40865 steps/s (collection: 2.287s, learning 0.118s)
             Mean action noise std: 4.39
          Mean value_function loss: 159.1010
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.5230
                       Mean reward: 827.08
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 1.5243
     Episode_Reward/lifting_object: 167.0294
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.1113
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.41s
                      Time elapsed: 01:28:49
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 41064 steps/s (collection: 2.282s, learning 0.111s)
             Mean action noise std: 4.39
          Mean value_function loss: 206.7208
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.5347
                       Mean reward: 832.36
               Mean episode length: 225.96
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 161.6131
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1065
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.39s
                      Time elapsed: 01:28:52
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 41288 steps/s (collection: 2.269s, learning 0.112s)
             Mean action noise std: 4.39
          Mean value_function loss: 156.8218
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.5477
                       Mean reward: 802.86
               Mean episode length: 218.17
    Episode_Reward/reaching_object: 1.4993
     Episode_Reward/lifting_object: 163.4073
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.1114
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.38s
                      Time elapsed: 01:28:54
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 40732 steps/s (collection: 2.278s, learning 0.135s)
             Mean action noise std: 4.39
          Mean value_function loss: 126.2229
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.5593
                       Mean reward: 886.92
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 1.5421
     Episode_Reward/lifting_object: 168.9115
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.1135
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.41s
                      Time elapsed: 01:28:56
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 40994 steps/s (collection: 2.283s, learning 0.115s)
             Mean action noise std: 4.40
          Mean value_function loss: 207.5085
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.5739
                       Mean reward: 860.64
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 1.5092
     Episode_Reward/lifting_object: 164.9674
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.1116
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.40s
                      Time elapsed: 01:28:59
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 40959 steps/s (collection: 2.276s, learning 0.124s)
             Mean action noise std: 4.40
          Mean value_function loss: 213.0795
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.5902
                       Mean reward: 770.45
               Mean episode length: 217.46
    Episode_Reward/reaching_object: 1.4821
     Episode_Reward/lifting_object: 160.3216
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.1117
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.40s
                      Time elapsed: 01:29:01
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 40152 steps/s (collection: 2.310s, learning 0.138s)
             Mean action noise std: 4.40
          Mean value_function loss: 141.0769
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.6017
                       Mean reward: 859.17
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 1.4916
     Episode_Reward/lifting_object: 162.3073
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.1119
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.45s
                      Time elapsed: 01:29:04
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 41096 steps/s (collection: 2.289s, learning 0.103s)
             Mean action noise std: 4.40
          Mean value_function loss: 143.3271
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.6102
                       Mean reward: 823.12
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.5398
     Episode_Reward/lifting_object: 168.3085
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.1144
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.39s
                      Time elapsed: 01:29:06
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 40606 steps/s (collection: 2.319s, learning 0.102s)
             Mean action noise std: 4.40
          Mean value_function loss: 201.9736
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.6178
                       Mean reward: 776.41
               Mean episode length: 212.68
    Episode_Reward/reaching_object: 1.4968
     Episode_Reward/lifting_object: 162.9924
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.1130
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.42s
                      Time elapsed: 01:29:08
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 40545 steps/s (collection: 2.298s, learning 0.126s)
             Mean action noise std: 4.41
          Mean value_function loss: 170.9458
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.6284
                       Mean reward: 850.77
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.5424
     Episode_Reward/lifting_object: 169.3779
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.1156
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.42s
                      Time elapsed: 01:29:11
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 38746 steps/s (collection: 2.406s, learning 0.131s)
             Mean action noise std: 4.41
          Mean value_function loss: 164.0386
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.6395
                       Mean reward: 819.52
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.5002
     Episode_Reward/lifting_object: 163.6046
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1087
          Episode_Reward/joint_vel: -0.1127
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.54s
                      Time elapsed: 01:29:13
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 39674 steps/s (collection: 2.364s, learning 0.114s)
             Mean action noise std: 4.41
          Mean value_function loss: 158.9040
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 74.6517
                       Mean reward: 832.81
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 1.4854
     Episode_Reward/lifting_object: 162.8183
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.1138
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.48s
                      Time elapsed: 01:29:16
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 39553 steps/s (collection: 2.361s, learning 0.125s)
             Mean action noise std: 4.41
          Mean value_function loss: 164.0624
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.6640
                       Mean reward: 809.09
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 1.4934
     Episode_Reward/lifting_object: 162.0596
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.1139
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.49s
                      Time elapsed: 01:29:18
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 40715 steps/s (collection: 2.314s, learning 0.100s)
             Mean action noise std: 4.41
          Mean value_function loss: 185.0271
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.6751
                       Mean reward: 846.47
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 1.4822
     Episode_Reward/lifting_object: 162.3927
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.1130
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.41s
                      Time elapsed: 01:29:21
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 39739 steps/s (collection: 2.345s, learning 0.129s)
             Mean action noise std: 4.42
          Mean value_function loss: 139.0303
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.6873
                       Mean reward: 840.90
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 1.4874
     Episode_Reward/lifting_object: 162.9775
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.1139
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.47s
                      Time elapsed: 01:29:23
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 41076 steps/s (collection: 2.289s, learning 0.105s)
             Mean action noise std: 4.42
          Mean value_function loss: 168.8241
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.7021
                       Mean reward: 872.26
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 1.5137
     Episode_Reward/lifting_object: 166.6276
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.1158
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.39s
                      Time elapsed: 01:29:26
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 41163 steps/s (collection: 2.277s, learning 0.112s)
             Mean action noise std: 4.42
          Mean value_function loss: 185.1422
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.7116
                       Mean reward: 761.46
               Mean episode length: 210.15
    Episode_Reward/reaching_object: 1.4925
     Episode_Reward/lifting_object: 162.9351
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.1139
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.39s
                      Time elapsed: 01:29:28
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 40707 steps/s (collection: 2.289s, learning 0.126s)
             Mean action noise std: 4.42
          Mean value_function loss: 210.1514
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.7204
                       Mean reward: 805.98
               Mean episode length: 216.38
    Episode_Reward/reaching_object: 1.4323
     Episode_Reward/lifting_object: 156.8775
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.1108
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.41s
                      Time elapsed: 01:29:30
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 41141 steps/s (collection: 2.275s, learning 0.114s)
             Mean action noise std: 4.43
          Mean value_function loss: 130.9779
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.7393
                       Mean reward: 845.91
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 1.5146
     Episode_Reward/lifting_object: 166.3114
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.1159
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.39s
                      Time elapsed: 01:29:33
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 41235 steps/s (collection: 2.271s, learning 0.113s)
             Mean action noise std: 4.43
          Mean value_function loss: 219.2898
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.7552
                       Mean reward: 784.83
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.4769
     Episode_Reward/lifting_object: 161.9379
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.1126
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.38s
                      Time elapsed: 01:29:35
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 40840 steps/s (collection: 2.276s, learning 0.131s)
             Mean action noise std: 4.43
          Mean value_function loss: 224.3096
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.7674
                       Mean reward: 817.10
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 1.4661
     Episode_Reward/lifting_object: 160.5453
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.1126
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.41s
                      Time elapsed: 01:29:38
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 40728 steps/s (collection: 2.305s, learning 0.109s)
             Mean action noise std: 4.43
          Mean value_function loss: 233.6581
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.7818
                       Mean reward: 807.92
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 1.4249
     Episode_Reward/lifting_object: 155.3383
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.41s
                      Time elapsed: 01:29:40
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 41087 steps/s (collection: 2.278s, learning 0.115s)
             Mean action noise std: 4.43
          Mean value_function loss: 176.4470
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.7935
                       Mean reward: 803.87
               Mean episode length: 215.91
    Episode_Reward/reaching_object: 1.4684
     Episode_Reward/lifting_object: 159.3438
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.1124
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.39s
                      Time elapsed: 01:29:42
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 40597 steps/s (collection: 2.290s, learning 0.131s)
             Mean action noise std: 4.44
          Mean value_function loss: 222.5913
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.8047
                       Mean reward: 850.21
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 1.5071
     Episode_Reward/lifting_object: 165.4164
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.1148
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.42s
                      Time elapsed: 01:29:45
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 41245 steps/s (collection: 2.276s, learning 0.108s)
             Mean action noise std: 4.44
          Mean value_function loss: 198.3890
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.8098
                       Mean reward: 804.51
               Mean episode length: 216.57
    Episode_Reward/reaching_object: 1.4733
     Episode_Reward/lifting_object: 161.1863
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.1113
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.38s
                      Time elapsed: 01:29:47
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 41347 steps/s (collection: 2.266s, learning 0.112s)
             Mean action noise std: 4.44
          Mean value_function loss: 218.3965
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.8218
                       Mean reward: 763.77
               Mean episode length: 213.97
    Episode_Reward/reaching_object: 1.5032
     Episode_Reward/lifting_object: 164.2975
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.1135
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.38s
                      Time elapsed: 01:29:50
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 40624 steps/s (collection: 2.286s, learning 0.134s)
             Mean action noise std: 4.44
          Mean value_function loss: 200.4832
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.8353
                       Mean reward: 858.32
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.5285
     Episode_Reward/lifting_object: 167.8016
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.1156
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.42s
                      Time elapsed: 01:29:52
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 39858 steps/s (collection: 2.363s, learning 0.103s)
             Mean action noise std: 4.44
          Mean value_function loss: 169.7380
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.8509
                       Mean reward: 880.52
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.5166
     Episode_Reward/lifting_object: 166.8958
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.1139
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.47s
                      Time elapsed: 01:29:54
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 41157 steps/s (collection: 2.287s, learning 0.102s)
             Mean action noise std: 4.45
          Mean value_function loss: 209.2856
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.8618
                       Mean reward: 857.98
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.5474
     Episode_Reward/lifting_object: 170.9282
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.1155
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.39s
                      Time elapsed: 01:29:57
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 40275 steps/s (collection: 2.313s, learning 0.128s)
             Mean action noise std: 4.45
          Mean value_function loss: 232.7150
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.8733
                       Mean reward: 849.18
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.5139
     Episode_Reward/lifting_object: 166.2789
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.1144
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.44s
                      Time elapsed: 01:29:59
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 41824 steps/s (collection: 2.233s, learning 0.117s)
             Mean action noise std: 4.45
          Mean value_function loss: 215.4454
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.8896
                       Mean reward: 814.20
               Mean episode length: 222.01
    Episode_Reward/reaching_object: 1.5017
     Episode_Reward/lifting_object: 164.4845
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.1136
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.35s
                      Time elapsed: 01:30:02
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 41425 steps/s (collection: 2.256s, learning 0.117s)
             Mean action noise std: 4.45
          Mean value_function loss: 192.9336
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.9004
                       Mean reward: 862.02
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 1.5073
     Episode_Reward/lifting_object: 165.7455
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.1135
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.37s
                      Time elapsed: 01:30:04
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 40245 steps/s (collection: 2.313s, learning 0.130s)
             Mean action noise std: 4.45
          Mean value_function loss: 166.9131
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.9109
                       Mean reward: 794.18
               Mean episode length: 214.78
    Episode_Reward/reaching_object: 1.5197
     Episode_Reward/lifting_object: 167.1869
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -0.1144
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.44s
                      Time elapsed: 01:30:06
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 40522 steps/s (collection: 2.321s, learning 0.105s)
             Mean action noise std: 4.46
          Mean value_function loss: 147.9844
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.9261
                       Mean reward: 808.88
               Mean episode length: 218.13
    Episode_Reward/reaching_object: 1.5071
     Episode_Reward/lifting_object: 164.8220
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.1127
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.43s
                      Time elapsed: 01:30:09
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 39408 steps/s (collection: 2.381s, learning 0.113s)
             Mean action noise std: 4.46
          Mean value_function loss: 144.6278
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.9388
                       Mean reward: 849.81
               Mean episode length: 228.69
    Episode_Reward/reaching_object: 1.5090
     Episode_Reward/lifting_object: 164.2990
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.1134
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.49s
                      Time elapsed: 01:30:11
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 40486 steps/s (collection: 2.323s, learning 0.106s)
             Mean action noise std: 4.46
          Mean value_function loss: 139.5912
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.9507
                       Mean reward: 855.15
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 1.5239
     Episode_Reward/lifting_object: 168.5664
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -0.1136
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.43s
                      Time elapsed: 01:30:14
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 40101 steps/s (collection: 2.333s, learning 0.119s)
             Mean action noise std: 4.46
          Mean value_function loss: 213.2584
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.9696
                       Mean reward: 890.70
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 1.5146
     Episode_Reward/lifting_object: 167.4850
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.1129
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.45s
                      Time elapsed: 01:30:16
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 39019 steps/s (collection: 2.385s, learning 0.135s)
             Mean action noise std: 4.47
          Mean value_function loss: 174.2112
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.9787
                       Mean reward: 846.70
               Mean episode length: 228.12
    Episode_Reward/reaching_object: 1.5205
     Episode_Reward/lifting_object: 167.7753
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.1126
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.52s
                      Time elapsed: 01:30:19
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 38694 steps/s (collection: 2.425s, learning 0.116s)
             Mean action noise std: 4.47
          Mean value_function loss: 161.8032
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.9887
                       Mean reward: 830.24
               Mean episode length: 223.24
    Episode_Reward/reaching_object: 1.4869
     Episode_Reward/lifting_object: 164.0639
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1119
          Episode_Reward/joint_vel: -0.1114
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.54s
                      Time elapsed: 01:30:21
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 38141 steps/s (collection: 2.463s, learning 0.114s)
             Mean action noise std: 4.47
          Mean value_function loss: 160.9924
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.9996
                       Mean reward: 835.55
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 1.4975
     Episode_Reward/lifting_object: 164.7835
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.1117
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.58s
                      Time elapsed: 01:30:24
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 40043 steps/s (collection: 2.332s, learning 0.123s)
             Mean action noise std: 4.47
          Mean value_function loss: 158.9610
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.0089
                       Mean reward: 827.29
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 1.5134
     Episode_Reward/lifting_object: 167.8252
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.1120
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.45s
                      Time elapsed: 01:30:26
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 36437 steps/s (collection: 2.590s, learning 0.108s)
             Mean action noise std: 4.47
          Mean value_function loss: 182.8506
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 75.0190
                       Mean reward: 864.54
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 1.4627
     Episode_Reward/lifting_object: 159.5587
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.1109
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.70s
                      Time elapsed: 01:30:29
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 39308 steps/s (collection: 2.368s, learning 0.133s)
             Mean action noise std: 4.48
          Mean value_function loss: 198.0664
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.0323
                       Mean reward: 827.64
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.5014
     Episode_Reward/lifting_object: 166.6113
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.1134
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.50s
                      Time elapsed: 01:30:32
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 39424 steps/s (collection: 2.350s, learning 0.143s)
             Mean action noise std: 4.48
          Mean value_function loss: 243.1871
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.0439
                       Mean reward: 867.00
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.4612
     Episode_Reward/lifting_object: 159.0008
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.49s
                      Time elapsed: 01:30:34
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 40392 steps/s (collection: 2.308s, learning 0.126s)
             Mean action noise std: 4.48
          Mean value_function loss: 158.3870
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.0564
                       Mean reward: 852.38
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 1.5059
     Episode_Reward/lifting_object: 167.5184
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.1134
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.43s
                      Time elapsed: 01:30:37
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 40050 steps/s (collection: 2.347s, learning 0.108s)
             Mean action noise std: 4.48
          Mean value_function loss: 206.1369
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.0637
                       Mean reward: 809.77
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 1.4715
     Episode_Reward/lifting_object: 163.1363
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.1131
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.45s
                      Time elapsed: 01:30:39
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 39606 steps/s (collection: 2.355s, learning 0.127s)
             Mean action noise std: 4.48
          Mean value_function loss: 165.6562
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.0728
                       Mean reward: 875.89
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.4969
     Episode_Reward/lifting_object: 166.3907
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.1139
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.48s
                      Time elapsed: 01:30:41
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 39673 steps/s (collection: 2.366s, learning 0.112s)
             Mean action noise std: 4.49
          Mean value_function loss: 175.2870
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.0832
                       Mean reward: 817.79
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.4845
     Episode_Reward/lifting_object: 165.2635
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.1132
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.48s
                      Time elapsed: 01:30:44
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 40811 steps/s (collection: 2.295s, learning 0.114s)
             Mean action noise std: 4.49
          Mean value_function loss: 146.9958
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.0956
                       Mean reward: 833.19
               Mean episode length: 224.93
    Episode_Reward/reaching_object: 1.5078
     Episode_Reward/lifting_object: 167.1570
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1150
          Episode_Reward/joint_vel: -0.1147
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.41s
                      Time elapsed: 01:30:46
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 40551 steps/s (collection: 2.289s, learning 0.135s)
             Mean action noise std: 4.49
          Mean value_function loss: 170.3830
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.1050
                       Mean reward: 842.42
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.4983
     Episode_Reward/lifting_object: 166.9044
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.1147
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.42s
                      Time elapsed: 01:30:49
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 39587 steps/s (collection: 2.368s, learning 0.116s)
             Mean action noise std: 4.49
          Mean value_function loss: 143.1447
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.1153
                       Mean reward: 889.47
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.5079
     Episode_Reward/lifting_object: 167.4043
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.1152
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.48s
                      Time elapsed: 01:30:51
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 38325 steps/s (collection: 2.422s, learning 0.143s)
             Mean action noise std: 4.49
          Mean value_function loss: 135.4024
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.1245
                       Mean reward: 840.43
               Mean episode length: 225.36
    Episode_Reward/reaching_object: 1.5144
     Episode_Reward/lifting_object: 168.4640
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.1141
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.56s
                      Time elapsed: 01:30:54
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 36899 steps/s (collection: 2.440s, learning 0.224s)
             Mean action noise std: 4.49
          Mean value_function loss: 170.1136
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.1339
                       Mean reward: 820.79
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 1.5183
     Episode_Reward/lifting_object: 168.0524
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1153
          Episode_Reward/joint_vel: -0.1158
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.66s
                      Time elapsed: 01:30:56
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 32231 steps/s (collection: 2.872s, learning 0.178s)
             Mean action noise std: 4.50
          Mean value_function loss: 148.5870
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.1494
                       Mean reward: 784.14
               Mean episode length: 213.23
    Episode_Reward/reaching_object: 1.4980
     Episode_Reward/lifting_object: 163.3758
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.1147
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 3.05s
                      Time elapsed: 01:31:00
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 36115 steps/s (collection: 2.551s, learning 0.171s)
             Mean action noise std: 4.50
          Mean value_function loss: 139.0495
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 75.1589
                       Mean reward: 902.85
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 1.5338
     Episode_Reward/lifting_object: 169.9502
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.1159
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.72s
                      Time elapsed: 01:31:02
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 31773 steps/s (collection: 2.807s, learning 0.287s)
             Mean action noise std: 4.50
          Mean value_function loss: 198.9554
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.1656
                       Mean reward: 842.71
               Mean episode length: 226.18
    Episode_Reward/reaching_object: 1.4922
     Episode_Reward/lifting_object: 162.7318
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.1136
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 3.09s
                      Time elapsed: 01:31:05
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 32711 steps/s (collection: 2.779s, learning 0.227s)
             Mean action noise std: 4.50
          Mean value_function loss: 236.0214
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 75.1690
                       Mean reward: 846.76
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 1.4784
     Episode_Reward/lifting_object: 161.8667
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.1130
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 3.01s
                      Time elapsed: 01:31:08
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 31451 steps/s (collection: 2.929s, learning 0.196s)
             Mean action noise std: 4.50
          Mean value_function loss: 184.3405
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 75.1766
                       Mean reward: 799.99
               Mean episode length: 216.23
    Episode_Reward/reaching_object: 1.4804
     Episode_Reward/lifting_object: 162.5415
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1129
          Episode_Reward/joint_vel: -0.1138
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 3.13s
                      Time elapsed: 01:31:11
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 33781 steps/s (collection: 2.704s, learning 0.206s)
             Mean action noise std: 4.50
          Mean value_function loss: 160.2228
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.1908
                       Mean reward: 862.16
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 1.5327
     Episode_Reward/lifting_object: 168.5796
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1164
          Episode_Reward/joint_vel: -0.1167
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.91s
                      Time elapsed: 01:31:14
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 38950 steps/s (collection: 2.415s, learning 0.109s)
             Mean action noise std: 4.51
          Mean value_function loss: 196.7749
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.2026
                       Mean reward: 753.45
               Mean episode length: 204.89
    Episode_Reward/reaching_object: 1.4789
     Episode_Reward/lifting_object: 161.4545
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.1142
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.52s
                      Time elapsed: 01:31:17
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 36051 steps/s (collection: 2.527s, learning 0.200s)
             Mean action noise std: 4.51
          Mean value_function loss: 197.4639
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.2185
                       Mean reward: 805.01
               Mean episode length: 218.31
    Episode_Reward/reaching_object: 1.4754
     Episode_Reward/lifting_object: 161.0295
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.1135
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.73s
                      Time elapsed: 01:31:20
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 34670 steps/s (collection: 2.590s, learning 0.245s)
             Mean action noise std: 4.51
          Mean value_function loss: 140.0513
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 75.2317
                       Mean reward: 818.65
               Mean episode length: 219.78
    Episode_Reward/reaching_object: 1.4859
     Episode_Reward/lifting_object: 162.7794
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1143
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.84s
                      Time elapsed: 01:31:22
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 36348 steps/s (collection: 2.570s, learning 0.135s)
             Mean action noise std: 4.51
          Mean value_function loss: 190.5430
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.2456
                       Mean reward: 784.99
               Mean episode length: 212.26
    Episode_Reward/reaching_object: 1.4770
     Episode_Reward/lifting_object: 160.9858
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.1136
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.70s
                      Time elapsed: 01:31:25
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 37380 steps/s (collection: 2.502s, learning 0.128s)
             Mean action noise std: 4.51
          Mean value_function loss: 196.6606
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.2516
                       Mean reward: 749.79
               Mean episode length: 205.74
    Episode_Reward/reaching_object: 1.5034
     Episode_Reward/lifting_object: 164.1775
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.1150
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.63s
                      Time elapsed: 01:31:28
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 32950 steps/s (collection: 2.736s, learning 0.247s)
             Mean action noise std: 4.52
          Mean value_function loss: 150.8741
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.2619
                       Mean reward: 818.75
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.5203
     Episode_Reward/lifting_object: 165.7247
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.1149
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.98s
                      Time elapsed: 01:31:31
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 35787 steps/s (collection: 2.569s, learning 0.178s)
             Mean action noise std: 4.52
          Mean value_function loss: 186.6154
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.2808
                       Mean reward: 850.78
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.5287
     Episode_Reward/lifting_object: 164.5190
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.1170
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.75s
                      Time elapsed: 01:31:34
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 35888 steps/s (collection: 2.582s, learning 0.157s)
             Mean action noise std: 4.52
          Mean value_function loss: 137.0581
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.2914
                       Mean reward: 788.74
               Mean episode length: 213.30
    Episode_Reward/reaching_object: 1.4784
     Episode_Reward/lifting_object: 159.5611
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.1126
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.74s
                      Time elapsed: 01:31:36
                               ETA: 00:00:02

