################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10846 steps/s (collection: 8.800s, learning 0.264s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 25.5708
                       Mean reward: 0.00
               Mean episode length: 21.94
    Episode_Reward/reaching_object: 0.0005
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0002
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.06s
                      Time elapsed: 00:00:09
                               ETA: 05:02:07

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14673 steps/s (collection: 6.547s, learning 0.152s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 25.6336
                       Mean reward: 0.00
               Mean episode length: 45.00
    Episode_Reward/reaching_object: 0.0014
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.70s
                      Time elapsed: 00:00:15
                               ETA: 04:22:34

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14758 steps/s (collection: 6.509s, learning 0.152s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 25.6488
                       Mean reward: 0.01
               Mean episode length: 69.37
    Episode_Reward/reaching_object: 0.0024
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.66s
                      Time elapsed: 00:00:22
                               ETA: 04:08:54

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14714 steps/s (collection: 6.536s, learning 0.145s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0011
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.6604
                       Mean reward: 0.01
               Mean episode length: 93.77
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.68s
                      Time elapsed: 00:00:29
                               ETA: 04:02:10

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14809 steps/s (collection: 6.493s, learning 0.146s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.6986
                       Mean reward: 0.02
               Mean episode length: 117.34
    Episode_Reward/reaching_object: 0.0050
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.64s
                      Time elapsed: 00:00:35
                               ETA: 03:57:48

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14676 steps/s (collection: 6.561s, learning 0.137s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 25.7330
                       Mean reward: 0.02
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.0066
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.70s
                      Time elapsed: 00:00:42
                               ETA: 03:55:11

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14766 steps/s (collection: 6.519s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 25.7435
                       Mean reward: 0.03
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.0088
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.66s
                      Time elapsed: 00:00:49
                               ETA: 03:53:05

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14623 steps/s (collection: 6.559s, learning 0.163s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 25.7537
                       Mean reward: 0.04
               Mean episode length: 189.52
    Episode_Reward/reaching_object: 0.0108
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0030
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.72s
                      Time elapsed: 00:00:55
                               ETA: 03:51:46

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 18739 steps/s (collection: 5.148s, learning 0.098s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 25.7808
                       Mean reward: 0.06
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 0.0151
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0034
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.25s
                      Time elapsed: 00:01:01
                               ETA: 03:45:15

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 59780 steps/s (collection: 1.555s, learning 0.089s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 25.7937
                       Mean reward: 0.07
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.0178
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.64s
                      Time elapsed: 00:01:02
                               ETA: 03:28:05

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 61384 steps/s (collection: 1.507s, learning 0.095s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 25.8153
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0212
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.60s
                      Time elapsed: 00:01:04
                               ETA: 03:13:54

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 60977 steps/s (collection: 1.516s, learning 0.096s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 25.8497
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0258
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.61s
                      Time elapsed: 00:01:05
                               ETA: 03:02:06

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 60862 steps/s (collection: 1.520s, learning 0.095s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 25.9129
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0322
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.62s
                      Time elapsed: 00:01:07
                               ETA: 02:52:08

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 60853 steps/s (collection: 1.521s, learning 0.095s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 25.8988
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0378
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.62s
                      Time elapsed: 00:01:09
                               ETA: 02:43:35

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 57638 steps/s (collection: 1.602s, learning 0.104s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 25.9447
                       Mean reward: 0.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0452
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.71s
                      Time elapsed: 00:01:10
                               ETA: 02:36:21

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 58098 steps/s (collection: 1.593s, learning 0.099s)
             Mean action noise std: 1.03
          Mean value_function loss: 1.9661
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.0548
                       Mean reward: -1.49
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.0574
     Episode_Reward/lifting_object: -0.1246
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.69s
                      Time elapsed: 00:01:12
                               ETA: 02:30:01

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 56007 steps/s (collection: 1.642s, learning 0.113s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.3334
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.1256
                       Mean reward: -0.44
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.0713
     Episode_Reward/lifting_object: -0.0901
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.76s
                      Time elapsed: 00:01:14
                               ETA: 02:24:32

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 54416 steps/s (collection: 1.692s, learning 0.115s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.2353
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 26.3231
                       Mean reward: -1.00
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.0923
     Episode_Reward/lifting_object: -0.1431
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.81s
                      Time elapsed: 00:01:16
                               ETA: 02:19:45

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 54070 steps/s (collection: 1.717s, learning 0.101s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.3921
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.4369
                       Mean reward: 0.54
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.0983
     Episode_Reward/lifting_object: -0.1406
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.82s
                      Time elapsed: 00:01:17
                               ETA: 02:15:29

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 55796 steps/s (collection: 1.665s, learning 0.097s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.2735
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.6175
                       Mean reward: 0.48
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.1049
     Episode_Reward/lifting_object: -0.0800
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.76s
                      Time elapsed: 00:01:19
                               ETA: 02:11:33

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 56907 steps/s (collection: 1.623s, learning 0.104s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.7055
                       Mean reward: 0.58
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.1136
     Episode_Reward/lifting_object: -0.0370
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.73s
                      Time elapsed: 00:01:21
                               ETA: 02:07:56

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 54874 steps/s (collection: 1.690s, learning 0.101s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 26.7566
                       Mean reward: 0.62
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.1275
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.79s
                      Time elapsed: 00:01:23
                               ETA: 02:04:45

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 56812 steps/s (collection: 1.630s, learning 0.100s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 26.7855
                       Mean reward: 0.50
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.1247
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.73s
                      Time elapsed: 00:01:24
                               ETA: 02:01:45

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 56495 steps/s (collection: 1.637s, learning 0.103s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.2518
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.8466
                       Mean reward: 0.58
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.1259
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.74s
                      Time elapsed: 00:01:26
                               ETA: 01:59:00

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 55686 steps/s (collection: 1.673s, learning 0.093s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.9113
                       Mean reward: 0.50
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.1109
     Episode_Reward/lifting_object: -0.0309
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.77s
                      Time elapsed: 00:01:28
                               ETA: 01:56:30

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 57103 steps/s (collection: 1.608s, learning 0.114s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 27.0910
                       Mean reward: 0.50
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.1069
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.72s
                      Time elapsed: 00:01:30
                               ETA: 01:54:09

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 55567 steps/s (collection: 1.659s, learning 0.110s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 27.0388
                       Mean reward: 0.50
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.1049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.77s
                      Time elapsed: 00:01:31
                               ETA: 01:52:01

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 56043 steps/s (collection: 1.638s, learning 0.116s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2504
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.0981
                       Mean reward: 0.48
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 0.1078
     Episode_Reward/lifting_object: -0.0522
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.75s
                      Time elapsed: 00:01:33
                               ETA: 01:50:02

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 54664 steps/s (collection: 1.687s, learning 0.112s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.4715
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.1429
                       Mean reward: 0.54
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 0.1123
     Episode_Reward/lifting_object: -0.1278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.80s
                      Time elapsed: 00:01:35
                               ETA: 01:48:13

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 53197 steps/s (collection: 1.731s, learning 0.117s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.4379
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.2513
                       Mean reward: -0.81
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.1199
     Episode_Reward/lifting_object: -0.0623
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.85s
                      Time elapsed: 00:01:37
                               ETA: 01:46:35

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 53165 steps/s (collection: 1.719s, learning 0.131s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0819
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.3518
                       Mean reward: 0.63
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 0.1340
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.85s
                      Time elapsed: 00:01:39
                               ETA: 01:45:03

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 54214 steps/s (collection: 1.697s, learning 0.116s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.4623
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.4769
                       Mean reward: 0.76
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 0.1432
     Episode_Reward/lifting_object: -0.0496
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.81s
                      Time elapsed: 00:01:41
                               ETA: 01:43:34

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 55102 steps/s (collection: 1.676s, learning 0.108s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.2446
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.5550
                       Mean reward: 0.72
               Mean episode length: 229.31
    Episode_Reward/reaching_object: 0.1472
     Episode_Reward/lifting_object: -0.0664
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.78s
                      Time elapsed: 00:01:42
                               ETA: 01:42:09

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 54992 steps/s (collection: 1.679s, learning 0.109s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1196
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.6372
                       Mean reward: 0.77
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 0.1587
     Episode_Reward/lifting_object: -0.0261
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.79s
                      Time elapsed: 00:01:44
                               ETA: 01:40:49

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 55444 steps/s (collection: 1.664s, learning 0.109s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1703
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.7066
                       Mean reward: 0.78
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 0.1678
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.77s
                      Time elapsed: 00:01:46
                               ETA: 01:39:33

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 54958 steps/s (collection: 1.684s, learning 0.105s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1367
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.7890
                       Mean reward: 0.44
               Mean episode length: 224.93
    Episode_Reward/reaching_object: 0.1612
     Episode_Reward/lifting_object: -0.0367
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.79s
                      Time elapsed: 00:01:48
                               ETA: 01:38:22

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 55084 steps/s (collection: 1.678s, learning 0.106s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 27.8561
                       Mean reward: 0.85
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 0.1786
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.78s
                      Time elapsed: 00:01:49
                               ETA: 01:37:14

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 56238 steps/s (collection: 1.642s, learning 0.106s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 27.9301
                       Mean reward: 0.81
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 0.1677
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.75s
                      Time elapsed: 00:01:51
                               ETA: 01:36:08

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 55344 steps/s (collection: 1.667s, learning 0.109s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.9600
                       Mean reward: 0.76
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 0.1660
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.78s
                      Time elapsed: 00:01:53
                               ETA: 01:35:07

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 55077 steps/s (collection: 1.673s, learning 0.112s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.0318
                       Mean reward: 0.75
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 0.1535
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.78s
                      Time elapsed: 00:01:55
                               ETA: 01:34:09

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 55265 steps/s (collection: 1.663s, learning 0.116s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 28.0613
                       Mean reward: 0.71
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.1550
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.78s
                      Time elapsed: 00:01:57
                               ETA: 01:33:13

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 55731 steps/s (collection: 1.658s, learning 0.106s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 28.1236
                       Mean reward: 0.72
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 0.1479
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.76s
                      Time elapsed: 00:01:58
                               ETA: 01:32:19

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 55394 steps/s (collection: 1.667s, learning 0.108s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 28.1649
                       Mean reward: 0.69
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.1571
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.77s
                      Time elapsed: 00:02:00
                               ETA: 01:31:29

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 53803 steps/s (collection: 1.723s, learning 0.104s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 28.2177
                       Mean reward: 0.75
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.1645
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.83s
                      Time elapsed: 00:02:02
                               ETA: 01:30:42

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 54929 steps/s (collection: 1.680s, learning 0.110s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.2613
                       Mean reward: 0.66
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 0.1750
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.79s
                      Time elapsed: 00:02:04
                               ETA: 01:29:56

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 53949 steps/s (collection: 1.710s, learning 0.112s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0401
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.2996
                       Mean reward: 0.93
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 0.1898
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.82s
                      Time elapsed: 00:02:05
                               ETA: 01:29:14

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 54103 steps/s (collection: 1.717s, learning 0.100s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.3661
                       Mean reward: 1.04
               Mean episode length: 203.64
    Episode_Reward/reaching_object: 0.1985
     Episode_Reward/lifting_object: -0.0217
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.82s
                      Time elapsed: 00:02:07
                               ETA: 01:28:33

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 53206 steps/s (collection: 1.732s, learning 0.116s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1826
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.4385
                       Mean reward: 0.53
               Mean episode length: 203.29
    Episode_Reward/reaching_object: 0.2177
     Episode_Reward/lifting_object: -0.0198
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.85s
                      Time elapsed: 00:02:09
                               ETA: 01:27:55

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 54144 steps/s (collection: 1.707s, learning 0.109s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0419
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.4608
                       Mean reward: 1.16
               Mean episode length: 215.42
    Episode_Reward/reaching_object: 0.2551
     Episode_Reward/lifting_object: -0.0424
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.82s
                      Time elapsed: 00:02:11
                               ETA: 01:27:17

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 52781 steps/s (collection: 1.749s, learning 0.114s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 28.5295
                       Mean reward: 1.57
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 0.3009
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.86s
                      Time elapsed: 00:02:13
                               ETA: 01:26:42

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 54139 steps/s (collection: 1.720s, learning 0.095s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0354
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.5766
                       Mean reward: 1.81
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.82s
                      Time elapsed: 00:02:15
                               ETA: 01:26:07

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 52854 steps/s (collection: 1.754s, learning 0.106s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.1220
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.6651
                       Mean reward: 1.70
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 0.3728
     Episode_Reward/lifting_object: -0.0451
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.86s
                      Time elapsed: 00:02:17
                               ETA: 01:25:34

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 52896 steps/s (collection: 1.750s, learning 0.108s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.6842
                       Mean reward: 1.95
               Mean episode length: 222.55
    Episode_Reward/reaching_object: 0.3918
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.86s
                      Time elapsed: 00:02:18
                               ETA: 01:25:03

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 53634 steps/s (collection: 1.728s, learning 0.105s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2654
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.7270
                       Mean reward: 1.85
               Mean episode length: 213.83
    Episode_Reward/reaching_object: 0.3978
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.83s
                      Time elapsed: 00:02:20
                               ETA: 01:24:32

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 53598 steps/s (collection: 1.728s, learning 0.106s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1102
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.7464
                       Mean reward: 2.36
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 0.4483
     Episode_Reward/lifting_object: -0.0188
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.83s
                      Time elapsed: 00:02:22
                               ETA: 01:24:02

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 53118 steps/s (collection: 1.742s, learning 0.109s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.8114
                       Mean reward: 1.78
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 0.4650
     Episode_Reward/lifting_object: -0.0417
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.85s
                      Time elapsed: 00:02:24
                               ETA: 01:23:34

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 52912 steps/s (collection: 1.752s, learning 0.106s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.4473
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.8934
                       Mean reward: 1.71
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.4717
     Episode_Reward/lifting_object: -0.0376
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.86s
                      Time elapsed: 00:02:26
                               ETA: 01:23:07

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 53755 steps/s (collection: 1.726s, learning 0.103s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0334
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.9124
                       Mean reward: 1.69
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 0.4793
     Episode_Reward/lifting_object: -0.0530
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.83s
                      Time elapsed: 00:02:28
                               ETA: 01:22:40

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 52802 steps/s (collection: 1.754s, learning 0.107s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0537
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.9723
                       Mean reward: 2.42
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 0.4702
     Episode_Reward/lifting_object: -0.0454
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.86s
                      Time elapsed: 00:02:29
                               ETA: 01:22:14

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 53074 steps/s (collection: 1.742s, learning 0.110s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.0792
                       Mean reward: 2.42
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 0.4842
     Episode_Reward/lifting_object: -0.0186
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.85s
                      Time elapsed: 00:02:31
                               ETA: 01:21:50

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 52236 steps/s (collection: 1.776s, learning 0.106s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.5323
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.1193
                       Mean reward: 1.85
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 0.5149
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.88s
                      Time elapsed: 00:02:33
                               ETA: 01:21:26

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52953 steps/s (collection: 1.751s, learning 0.105s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0168
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.1381
                       Mean reward: 2.61
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.5149
     Episode_Reward/lifting_object: -0.0463
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.86s
                      Time elapsed: 00:02:35
                               ETA: 01:21:03

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 53489 steps/s (collection: 1.753s, learning 0.085s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.3646
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.2033
                       Mean reward: 2.03
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 0.5522
     Episode_Reward/lifting_object: -0.0502
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.84s
                      Time elapsed: 00:02:37
                               ETA: 01:20:40

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 52090 steps/s (collection: 1.759s, learning 0.128s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.1148
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.2240
                       Mean reward: 2.70
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.5467
     Episode_Reward/lifting_object: -0.0503
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 19.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.89s
                      Time elapsed: 00:02:39
                               ETA: 01:20:19

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 53108 steps/s (collection: 1.731s, learning 0.120s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 29.2676
                       Mean reward: 2.41
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.5640
     Episode_Reward/lifting_object: -0.0431
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.85s
                      Time elapsed: 00:02:41
                               ETA: 01:19:58

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 52172 steps/s (collection: 1.775s, learning 0.109s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0596
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.2844
                       Mean reward: 2.45
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 0.5608
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.88s
                      Time elapsed: 00:02:42
                               ETA: 01:19:38

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 52293 steps/s (collection: 1.777s, learning 0.103s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.2174
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.3379
                       Mean reward: 2.77
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.5665
     Episode_Reward/lifting_object: -0.0356
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.88s
                      Time elapsed: 00:02:44
                               ETA: 01:19:18

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 51606 steps/s (collection: 1.800s, learning 0.105s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.5123
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.3586
                       Mean reward: 1.95
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.5674
     Episode_Reward/lifting_object: -0.0414
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 19.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.90s
                      Time elapsed: 00:02:46
                               ETA: 01:19:00

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 52805 steps/s (collection: 1.756s, learning 0.105s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.3865
                       Mean reward: 2.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5935
     Episode_Reward/lifting_object: -0.0513
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.86s
                      Time elapsed: 00:02:48
                               ETA: 01:18:41

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 52675 steps/s (collection: 1.760s, learning 0.106s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 29.4223
                       Mean reward: 2.89
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.5836
     Episode_Reward/lifting_object: -0.0303
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.87s
                      Time elapsed: 00:02:50
                               ETA: 01:18:23

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 52674 steps/s (collection: 1.752s, learning 0.114s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1233
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 29.4261
                       Mean reward: 2.76
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.5901
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.87s
                      Time elapsed: 00:02:52
                               ETA: 01:18:05

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 52985 steps/s (collection: 1.745s, learning 0.110s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0999
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.4357
                       Mean reward: 2.90
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.6027
     Episode_Reward/lifting_object: -0.0132
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.86s
                      Time elapsed: 00:02:54
                               ETA: 01:17:47

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 52343 steps/s (collection: 1.768s, learning 0.110s)
             Mean action noise std: 1.25
          Mean value_function loss: 1.0334
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.4667
                       Mean reward: 3.10
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.6186
     Episode_Reward/lifting_object: -0.0172
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.88s
                      Time elapsed: 00:02:56
                               ETA: 01:17:30

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 53165 steps/s (collection: 1.752s, learning 0.097s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 29.4782
                       Mean reward: 2.96
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.6039
     Episode_Reward/lifting_object: -0.1050
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 18.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.85s
                      Time elapsed: 00:02:57
                               ETA: 01:17:13

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 51981 steps/s (collection: 1.779s, learning 0.112s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0310
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.5331
                       Mean reward: 2.56
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 18.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.89s
                      Time elapsed: 00:02:59
                               ETA: 01:16:58

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 51882 steps/s (collection: 1.780s, learning 0.115s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.9387
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.6184
                       Mean reward: 2.94
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.5876
     Episode_Reward/lifting_object: -0.1103
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.89s
                      Time elapsed: 00:03:01
                               ETA: 01:16:42

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 51847 steps/s (collection: 1.785s, learning 0.112s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 29.6289
                       Mean reward: 2.35
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: -0.0731
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.90s
                      Time elapsed: 00:03:03
                               ETA: 01:16:28

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 51949 steps/s (collection: 1.782s, learning 0.111s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.6567
                       Mean reward: 3.15
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.6202
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.89s
                      Time elapsed: 00:03:05
                               ETA: 01:16:13

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 51418 steps/s (collection: 1.801s, learning 0.111s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.3305
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.7051
                       Mean reward: 2.01
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 0.6108
     Episode_Reward/lifting_object: -0.0802
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.91s
                      Time elapsed: 00:03:07
                               ETA: 01:15:59

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 51478 steps/s (collection: 1.787s, learning 0.123s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0703
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.7227
                       Mean reward: 2.91
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.6274
     Episode_Reward/lifting_object: -0.0326
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.91s
                      Time elapsed: 00:03:09
                               ETA: 01:15:46

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 52578 steps/s (collection: 1.775s, learning 0.095s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.7903
                       Mean reward: 3.18
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.6517
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.87s
                      Time elapsed: 00:03:11
                               ETA: 01:15:32

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 52045 steps/s (collection: 1.789s, learning 0.100s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.3744
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.8505
                       Mean reward: 3.13
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.6390
     Episode_Reward/lifting_object: 0.0092
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.89s
                      Time elapsed: 00:03:13
                               ETA: 01:15:18

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 52343 steps/s (collection: 1.765s, learning 0.113s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0312
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.8636
                       Mean reward: 3.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6417
     Episode_Reward/lifting_object: -0.0566
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.88s
                      Time elapsed: 00:03:14
                               ETA: 01:15:05

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 50624 steps/s (collection: 1.828s, learning 0.114s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 29.8914
                       Mean reward: 3.28
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: -0.0360
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.94s
                      Time elapsed: 00:03:16
                               ETA: 01:14:53

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 50687 steps/s (collection: 1.828s, learning 0.112s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1344
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.9149
                       Mean reward: 3.39
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.6662
     Episode_Reward/lifting_object: -0.0089
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 20.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.94s
                      Time elapsed: 00:03:18
                               ETA: 01:14:42

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 51505 steps/s (collection: 1.796s, learning 0.113s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2214
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.9583
                       Mean reward: 3.20
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.6701
     Episode_Reward/lifting_object: -0.0175
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.91s
                      Time elapsed: 00:03:20
                               ETA: 01:14:30

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51201 steps/s (collection: 1.814s, learning 0.106s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0671
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.9750
                       Mean reward: 3.41
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.6664
     Episode_Reward/lifting_object: -0.0068
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.92s
                      Time elapsed: 00:03:22
                               ETA: 01:14:18

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 51956 steps/s (collection: 1.803s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0474
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.0262
                       Mean reward: 3.32
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 0.6636
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 18.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.89s
                      Time elapsed: 00:03:24
                               ETA: 01:14:07

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 51347 steps/s (collection: 1.803s, learning 0.111s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0377
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.0975
                       Mean reward: 3.11
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.6766
     Episode_Reward/lifting_object: -0.0081
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.91s
                      Time elapsed: 00:03:26
                               ETA: 01:13:56

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 51327 steps/s (collection: 1.810s, learning 0.106s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.4008
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.1389
                       Mean reward: 3.03
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.6678
     Episode_Reward/lifting_object: -0.0080
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.92s
                      Time elapsed: 00:03:28
                               ETA: 01:13:45

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 52363 steps/s (collection: 1.782s, learning 0.096s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1487
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.1529
                       Mean reward: 3.12
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.6541
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.88s
                      Time elapsed: 00:03:30
                               ETA: 01:13:33

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 51764 steps/s (collection: 1.779s, learning 0.120s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.1955
                       Mean reward: 3.20
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 0.6821
     Episode_Reward/lifting_object: 0.0100
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.90s
                      Time elapsed: 00:03:32
                               ETA: 01:13:22

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 52520 steps/s (collection: 1.768s, learning 0.104s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0852
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.2397
                       Mean reward: 3.54
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.6790
     Episode_Reward/lifting_object: 0.0026
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.87s
                      Time elapsed: 00:03:34
                               ETA: 01:13:11

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 51641 steps/s (collection: 1.794s, learning 0.110s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0303
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.2621
                       Mean reward: 3.19
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.6918
     Episode_Reward/lifting_object: 0.0023
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.90s
                      Time elapsed: 00:03:35
                               ETA: 01:13:01

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 51993 steps/s (collection: 1.790s, learning 0.101s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.4108
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.3008
                       Mean reward: 3.47
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7022
     Episode_Reward/lifting_object: -0.0189
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 19.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.89s
                      Time elapsed: 00:03:37
                               ETA: 01:12:50

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 51263 steps/s (collection: 1.802s, learning 0.115s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 30.3101
                       Mean reward: 3.48
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.6887
     Episode_Reward/lifting_object: -0.0040
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 19.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.92s
                      Time elapsed: 00:03:39
                               ETA: 01:12:40

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 51127 steps/s (collection: 1.801s, learning 0.122s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0599
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.3260
                       Mean reward: 3.22
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.7073
     Episode_Reward/lifting_object: -0.0293
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.92s
                      Time elapsed: 00:03:41
                               ETA: 01:12:31

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 51806 steps/s (collection: 1.789s, learning 0.108s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.3681
                       Mean reward: 3.42
               Mean episode length: 240.42
    Episode_Reward/reaching_object: 0.6963
     Episode_Reward/lifting_object: -0.0478
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.90s
                      Time elapsed: 00:03:43
                               ETA: 01:12:21

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 50893 steps/s (collection: 1.815s, learning 0.117s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.9273
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.4127
                       Mean reward: 3.67
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7156
     Episode_Reward/lifting_object: -0.0423
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 18.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.93s
                      Time elapsed: 00:03:45
                               ETA: 01:12:12

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 50935 steps/s (collection: 1.805s, learning 0.125s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.4115
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.4250
                       Mean reward: 3.46
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.7040
     Episode_Reward/lifting_object: -0.1438
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.93s
                      Time elapsed: 00:03:47
                               ETA: 01:12:03

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 52120 steps/s (collection: 1.767s, learning 0.119s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0399
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.4815
                       Mean reward: 3.36
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 0.7068
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.89s
                      Time elapsed: 00:03:49
                               ETA: 01:11:54

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 51012 steps/s (collection: 1.832s, learning 0.095s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0905
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.5588
                       Mean reward: 3.66
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.7196
     Episode_Reward/lifting_object: -0.0432
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.93s
                      Time elapsed: 00:03:51
                               ETA: 01:11:45

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 51965 steps/s (collection: 1.780s, learning 0.112s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0196
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.6150
                       Mean reward: 3.76
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.7330
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.89s
                      Time elapsed: 00:03:53
                               ETA: 01:11:36

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 51345 steps/s (collection: 1.806s, learning 0.108s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.7027
                       Mean reward: 3.73
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 0.0058
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.91s
                      Time elapsed: 00:03:55
                               ETA: 01:11:27

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 51225 steps/s (collection: 1.806s, learning 0.113s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1401
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 30.7559
                       Mean reward: 3.37
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: -0.0042
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.92s
                      Time elapsed: 00:03:56
                               ETA: 01:11:19

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 51716 steps/s (collection: 1.795s, learning 0.106s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.7641
                       Mean reward: 3.62
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 0.7398
     Episode_Reward/lifting_object: -0.0459
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 19.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.90s
                      Time elapsed: 00:03:58
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 50291 steps/s (collection: 1.844s, learning 0.111s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.9328
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.7811
                       Mean reward: 3.29
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.7310
     Episode_Reward/lifting_object: -0.0150
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.95s
                      Time elapsed: 00:04:00
                               ETA: 01:11:03

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 51368 steps/s (collection: 1.805s, learning 0.109s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0994
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.7920
                       Mean reward: 3.51
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.7288
     Episode_Reward/lifting_object: 0.0018
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.91s
                      Time elapsed: 00:04:02
                               ETA: 01:10:54

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 51881 steps/s (collection: 1.799s, learning 0.096s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2938
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.8151
                       Mean reward: 3.88
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 0.0071
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.89s
                      Time elapsed: 00:04:04
                               ETA: 01:10:46

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 52063 steps/s (collection: 1.776s, learning 0.112s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 30.8223
                       Mean reward: 2.70
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: -0.0379
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.89s
                      Time elapsed: 00:04:06
                               ETA: 01:10:38

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 51510 steps/s (collection: 1.795s, learning 0.113s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0440
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.8558
                       Mean reward: 3.25
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7345
     Episode_Reward/lifting_object: -0.0593
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.91s
                      Time elapsed: 00:04:08
                               ETA: 01:10:30

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 51135 steps/s (collection: 1.810s, learning 0.112s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0444
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.9113
                       Mean reward: 3.86
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 0.0133
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.92s
                      Time elapsed: 00:04:10
                               ETA: 01:10:22

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 50936 steps/s (collection: 1.806s, learning 0.124s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.4810
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.9471
                       Mean reward: 3.39
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.7227
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.93s
                      Time elapsed: 00:04:12
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 51199 steps/s (collection: 1.808s, learning 0.112s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0313
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.9567
                       Mean reward: 3.46
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.92s
                      Time elapsed: 00:04:14
                               ETA: 01:10:07

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 50709 steps/s (collection: 1.823s, learning 0.115s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1111
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.9808
                       Mean reward: 3.60
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.94s
                      Time elapsed: 00:04:16
                               ETA: 01:10:00

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 50749 steps/s (collection: 1.829s, learning 0.108s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3720
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.0070
                       Mean reward: 3.80
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: -0.0061
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 19.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.94s
                      Time elapsed: 00:04:18
                               ETA: 01:09:53

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 50859 steps/s (collection: 1.826s, learning 0.107s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0574
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 31.0163
                       Mean reward: 3.81
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: -0.0490
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 19.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.93s
                      Time elapsed: 00:04:20
                               ETA: 01:09:47

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 51509 steps/s (collection: 1.813s, learning 0.096s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0340
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.0441
                       Mean reward: 3.93
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: -0.0140
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.91s
                      Time elapsed: 00:04:21
                               ETA: 01:09:39

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 51713 steps/s (collection: 1.788s, learning 0.113s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1705
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.0811
                       Mean reward: 3.57
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 0.0015
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.90s
                      Time elapsed: 00:04:23
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 51098 steps/s (collection: 1.817s, learning 0.107s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.4015
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.0919
                       Mean reward: 3.62
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 0.0169
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.92s
                      Time elapsed: 00:04:25
                               ETA: 01:09:25

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 50699 steps/s (collection: 1.827s, learning 0.112s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0611
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.1303
                       Mean reward: 4.12
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: -0.0100
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.94s
                      Time elapsed: 00:04:27
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 51102 steps/s (collection: 1.808s, learning 0.115s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0974
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.1751
                       Mean reward: 3.83
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 0.0060
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.92s
                      Time elapsed: 00:04:29
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 51039 steps/s (collection: 1.813s, learning 0.113s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.0643
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.1955
                       Mean reward: 4.20
               Mean episode length: 247.32
    Episode_Reward/reaching_object: 0.7877
     Episode_Reward/lifting_object: 0.0319
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.93s
                      Time elapsed: 00:04:31
                               ETA: 01:09:06

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 51423 steps/s (collection: 1.797s, learning 0.115s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0838
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.2045
                       Mean reward: 4.00
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 0.0393
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.91s
                      Time elapsed: 00:04:33
                               ETA: 01:08:59

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 52135 steps/s (collection: 1.770s, learning 0.116s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0860
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.2273
                       Mean reward: 3.98
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: -0.0567
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.89s
                      Time elapsed: 00:04:35
                               ETA: 01:08:52

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 52490 steps/s (collection: 1.765s, learning 0.108s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1456
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.2584
                       Mean reward: 3.71
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 0.0154
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.87s
                      Time elapsed: 00:04:37
                               ETA: 01:08:45

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 52305 steps/s (collection: 1.770s, learning 0.109s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.2089
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 31.2959
                       Mean reward: 3.90
               Mean episode length: 244.37
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 0.0459
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 20.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.88s
                      Time elapsed: 00:04:39
                               ETA: 01:08:38

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 50712 steps/s (collection: 1.819s, learning 0.119s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.7517
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.3283
                       Mean reward: 3.87
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 0.0133
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.94s
                      Time elapsed: 00:04:41
                               ETA: 01:08:32

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 51314 steps/s (collection: 1.801s, learning 0.115s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.2397
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.3713
                       Mean reward: 4.54
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 0.7419
     Episode_Reward/lifting_object: 0.0639
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.92s
                      Time elapsed: 00:04:42
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 51743 steps/s (collection: 1.787s, learning 0.113s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.9469
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.3903
                       Mean reward: 2.82
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: -0.0073
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.90s
                      Time elapsed: 00:04:44
                               ETA: 01:08:19

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 52186 steps/s (collection: 1.773s, learning 0.110s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3470
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.4007
                       Mean reward: 3.81
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 0.0299
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.88s
                      Time elapsed: 00:04:46
                               ETA: 01:08:13

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 51268 steps/s (collection: 1.800s, learning 0.117s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3089
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.4362
                       Mean reward: 3.84
               Mean episode length: 237.75
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 0.1129
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.92s
                      Time elapsed: 00:04:48
                               ETA: 01:08:07

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51533 steps/s (collection: 1.788s, learning 0.120s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.5677
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.4892
                       Mean reward: 3.49
               Mean episode length: 234.32
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 0.0211
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.91s
                      Time elapsed: 00:04:50
                               ETA: 01:08:00

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 52008 steps/s (collection: 1.774s, learning 0.116s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.4717
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 31.5560
                       Mean reward: 4.68
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 0.1306
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.89s
                      Time elapsed: 00:04:52
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 51580 steps/s (collection: 1.785s, learning 0.121s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.9372
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.5846
                       Mean reward: 3.81
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 0.7256
     Episode_Reward/lifting_object: 0.1513
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.91s
                      Time elapsed: 00:04:54
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 51032 steps/s (collection: 1.817s, learning 0.109s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.5847
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.6130
                       Mean reward: 4.63
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.7195
     Episode_Reward/lifting_object: 0.2443
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.93s
                      Time elapsed: 00:04:56
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 51596 steps/s (collection: 1.789s, learning 0.116s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.8153
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 31.6416
                       Mean reward: 4.22
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 0.7350
     Episode_Reward/lifting_object: 0.1783
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 19.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.91s
                      Time elapsed: 00:04:58
                               ETA: 01:07:37

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 52023 steps/s (collection: 1.788s, learning 0.102s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.9416
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.6786
                       Mean reward: 5.33
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.6946
     Episode_Reward/lifting_object: 0.2657
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.89s
                      Time elapsed: 00:05:00
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 52101 steps/s (collection: 1.767s, learning 0.120s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.6005
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 31.7331
                       Mean reward: 4.09
               Mean episode length: 234.88
    Episode_Reward/reaching_object: 0.7116
     Episode_Reward/lifting_object: 0.2684
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.89s
                      Time elapsed: 00:05:01
                               ETA: 01:07:24

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 52778 steps/s (collection: 1.751s, learning 0.112s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.0195
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.7712
                       Mean reward: 5.39
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 0.6951
     Episode_Reward/lifting_object: 0.3777
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.86s
                      Time elapsed: 00:05:03
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 53540 steps/s (collection: 1.742s, learning 0.095s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.1654
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.8228
                       Mean reward: 4.65
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 0.6887
     Episode_Reward/lifting_object: 0.2688
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.84s
                      Time elapsed: 00:05:05
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 52132 steps/s (collection: 1.769s, learning 0.117s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.2678
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 31.8790
                       Mean reward: 4.42
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 0.6853
     Episode_Reward/lifting_object: 0.3405
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.89s
                      Time elapsed: 00:05:07
                               ETA: 01:07:06

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 52627 steps/s (collection: 1.760s, learning 0.108s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.3812
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.9416
                       Mean reward: 4.77
               Mean episode length: 223.78
    Episode_Reward/reaching_object: 0.6495
     Episode_Reward/lifting_object: 0.4328
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.87s
                      Time elapsed: 00:05:09
                               ETA: 01:07:00

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 53210 steps/s (collection: 1.740s, learning 0.107s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.1223
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.9855
                       Mean reward: 5.92
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 0.6391
     Episode_Reward/lifting_object: 0.4953
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.85s
                      Time elapsed: 00:05:11
                               ETA: 01:06:53

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 52528 steps/s (collection: 1.759s, learning 0.113s)
             Mean action noise std: 1.44
          Mean value_function loss: 2.5515
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.0445
                       Mean reward: 6.83
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 0.6640
     Episode_Reward/lifting_object: 0.4951
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.87s
                      Time elapsed: 00:05:13
                               ETA: 01:06:48

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 52837 steps/s (collection: 1.746s, learning 0.115s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.7929
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 32.0930
                       Mean reward: 6.50
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 0.6603
     Episode_Reward/lifting_object: 0.5931
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.86s
                      Time elapsed: 00:05:14
                               ETA: 01:06:42

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 51903 steps/s (collection: 1.775s, learning 0.119s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.9090
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 32.1412
                       Mean reward: 6.53
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.6519
     Episode_Reward/lifting_object: 0.5909
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.89s
                      Time elapsed: 00:05:16
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 52859 steps/s (collection: 1.741s, learning 0.119s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.8462
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 32.1843
                       Mean reward: 6.52
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.6355
     Episode_Reward/lifting_object: 0.8432
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.86s
                      Time elapsed: 00:05:18
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 53303 steps/s (collection: 1.732s, learning 0.112s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.9252
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 32.2410
                       Mean reward: 7.68
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.6148
     Episode_Reward/lifting_object: 0.9012
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.84s
                      Time elapsed: 00:05:20
                               ETA: 01:06:24

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 53748 steps/s (collection: 1.700s, learning 0.129s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.1495
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 32.2904
                       Mean reward: 6.74
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 0.5825
     Episode_Reward/lifting_object: 0.7029
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.83s
                      Time elapsed: 00:05:22
                               ETA: 01:06:18

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 54118 steps/s (collection: 1.706s, learning 0.110s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.1568
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 32.3272
                       Mean reward: 7.00
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 0.5636
     Episode_Reward/lifting_object: 0.8786
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.82s
                      Time elapsed: 00:05:24
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 52133 steps/s (collection: 1.772s, learning 0.114s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.4384
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 32.3798
                       Mean reward: 6.79
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 0.5359
     Episode_Reward/lifting_object: 0.8383
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.89s
                      Time elapsed: 00:05:26
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 54133 steps/s (collection: 1.710s, learning 0.106s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.1031
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 32.4399
                       Mean reward: 6.52
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 0.4872
     Episode_Reward/lifting_object: 1.0617
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.82s
                      Time elapsed: 00:05:27
                               ETA: 01:06:00

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 54456 steps/s (collection: 1.712s, learning 0.094s)
             Mean action noise std: 1.48
          Mean value_function loss: 1.9350
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 32.4783
                       Mean reward: 5.50
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 0.4858
     Episode_Reward/lifting_object: 0.9532
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.81s
                      Time elapsed: 00:05:29
                               ETA: 01:05:54

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 53260 steps/s (collection: 1.736s, learning 0.110s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.2396
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 32.5105
                       Mean reward: 7.34
               Mean episode length: 198.04
    Episode_Reward/reaching_object: 0.4687
     Episode_Reward/lifting_object: 0.8985
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.85s
                      Time elapsed: 00:05:31
                               ETA: 01:05:49

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 52563 steps/s (collection: 1.751s, learning 0.119s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.3125
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.5476
                       Mean reward: 8.24
               Mean episode length: 211.57
    Episode_Reward/reaching_object: 0.4368
     Episode_Reward/lifting_object: 0.9779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.87s
                      Time elapsed: 00:05:33
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 52863 steps/s (collection: 1.748s, learning 0.112s)
             Mean action noise std: 1.49
          Mean value_function loss: 2.1667
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.5869
                       Mean reward: 9.11
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 0.4148
     Episode_Reward/lifting_object: 0.9108
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.86s
                      Time elapsed: 00:05:35
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 52226 steps/s (collection: 1.774s, learning 0.108s)
             Mean action noise std: 1.49
          Mean value_function loss: 2.2109
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 32.6336
                       Mean reward: 6.04
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 0.4258
     Episode_Reward/lifting_object: 0.7551
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.88s
                      Time elapsed: 00:05:37
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 53405 steps/s (collection: 1.746s, learning 0.095s)
             Mean action noise std: 1.50
          Mean value_function loss: 2.3709
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 32.6895
                       Mean reward: 4.95
               Mean episode length: 188.67
    Episode_Reward/reaching_object: 0.4255
     Episode_Reward/lifting_object: 0.8127
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.84s
                      Time elapsed: 00:05:39
                               ETA: 01:05:27

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 53216 steps/s (collection: 1.722s, learning 0.125s)
             Mean action noise std: 1.50
          Mean value_function loss: 2.2464
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 32.7346
                       Mean reward: 7.53
               Mean episode length: 200.40
    Episode_Reward/reaching_object: 0.4092
     Episode_Reward/lifting_object: 0.9426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.85s
                      Time elapsed: 00:05:40
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 53070 steps/s (collection: 1.725s, learning 0.128s)
             Mean action noise std: 1.50
          Mean value_function loss: 2.2443
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 32.7625
                       Mean reward: 7.73
               Mean episode length: 202.95
    Episode_Reward/reaching_object: 0.4297
     Episode_Reward/lifting_object: 1.1371
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.85s
                      Time elapsed: 00:05:42
                               ETA: 01:05:17

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 52365 steps/s (collection: 1.756s, learning 0.122s)
             Mean action noise std: 1.51
          Mean value_function loss: 2.5364
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 32.8088
                       Mean reward: 7.88
               Mean episode length: 186.36
    Episode_Reward/reaching_object: 0.4034
     Episode_Reward/lifting_object: 0.9527
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.88s
                      Time elapsed: 00:05:44
                               ETA: 01:05:12

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 52675 steps/s (collection: 1.744s, learning 0.122s)
             Mean action noise std: 1.51
          Mean value_function loss: 2.4141
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 32.8476
                       Mean reward: 7.27
               Mean episode length: 196.35
    Episode_Reward/reaching_object: 0.4037
     Episode_Reward/lifting_object: 0.9654
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.87s
                      Time elapsed: 00:05:46
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 52309 steps/s (collection: 1.774s, learning 0.106s)
             Mean action noise std: 1.51
          Mean value_function loss: 2.9375
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 32.8774
                       Mean reward: 5.45
               Mean episode length: 180.08
    Episode_Reward/reaching_object: 0.3940
     Episode_Reward/lifting_object: 0.8926
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.88s
                      Time elapsed: 00:05:48
                               ETA: 01:05:02

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 52614 steps/s (collection: 1.767s, learning 0.101s)
             Mean action noise std: 1.52
          Mean value_function loss: 2.4133
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 32.9092
                       Mean reward: 8.54
               Mean episode length: 186.36
    Episode_Reward/reaching_object: 0.4088
     Episode_Reward/lifting_object: 0.9785
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.87s
                      Time elapsed: 00:05:50
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 51866 steps/s (collection: 1.772s, learning 0.124s)
             Mean action noise std: 1.52
          Mean value_function loss: 3.4263
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.9400
                       Mean reward: 7.37
               Mean episode length: 183.22
    Episode_Reward/reaching_object: 0.4260
     Episode_Reward/lifting_object: 1.0976
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.90s
                      Time elapsed: 00:05:52
                               ETA: 01:04:52

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 51544 steps/s (collection: 1.797s, learning 0.110s)
             Mean action noise std: 1.52
          Mean value_function loss: 3.2724
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 32.9712
                       Mean reward: 7.44
               Mean episode length: 182.96
    Episode_Reward/reaching_object: 0.4048
     Episode_Reward/lifting_object: 1.0511
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.91s
                      Time elapsed: 00:05:54
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 51219 steps/s (collection: 1.809s, learning 0.111s)
             Mean action noise std: 1.52
          Mean value_function loss: 3.1369
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 32.9992
                       Mean reward: 7.98
               Mean episode length: 194.56
    Episode_Reward/reaching_object: 0.4411
     Episode_Reward/lifting_object: 1.2005
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.92s
                      Time elapsed: 00:05:55
                               ETA: 01:04:43

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 51447 steps/s (collection: 1.796s, learning 0.115s)
             Mean action noise std: 1.53
          Mean value_function loss: 4.0713
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 33.0226
                       Mean reward: 9.99
               Mean episode length: 185.39
    Episode_Reward/reaching_object: 0.4639
     Episode_Reward/lifting_object: 1.2002
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.91s
                      Time elapsed: 00:05:57
                               ETA: 01:04:39

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 51467 steps/s (collection: 1.796s, learning 0.114s)
             Mean action noise std: 1.53
          Mean value_function loss: 3.5326
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.0559
                       Mean reward: 8.19
               Mean episode length: 177.04
    Episode_Reward/reaching_object: 0.4274
     Episode_Reward/lifting_object: 1.1336
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.91s
                      Time elapsed: 00:05:59
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 51887 steps/s (collection: 1.787s, learning 0.108s)
             Mean action noise std: 1.53
          Mean value_function loss: 3.3287
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.0827
                       Mean reward: 11.33
               Mean episode length: 166.09
    Episode_Reward/reaching_object: 0.4143
     Episode_Reward/lifting_object: 1.3531
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.89s
                      Time elapsed: 00:06:01
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 52633 steps/s (collection: 1.767s, learning 0.100s)
             Mean action noise std: 1.53
          Mean value_function loss: 3.4895
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 33.1038
                       Mean reward: 9.61
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 0.4977
     Episode_Reward/lifting_object: 1.4189
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.87s
                      Time elapsed: 00:06:03
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 51256 steps/s (collection: 1.809s, learning 0.109s)
             Mean action noise std: 1.53
          Mean value_function loss: 4.5129
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.1135
                       Mean reward: 9.55
               Mean episode length: 177.31
    Episode_Reward/reaching_object: 0.4681
     Episode_Reward/lifting_object: 1.2972
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.92s
                      Time elapsed: 00:06:05
                               ETA: 01:04:21

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 50940 steps/s (collection: 1.813s, learning 0.117s)
             Mean action noise std: 1.54
          Mean value_function loss: 4.4388
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.1374
                       Mean reward: 9.00
               Mean episode length: 176.32
    Episode_Reward/reaching_object: 0.4893
     Episode_Reward/lifting_object: 1.4468
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.93s
                      Time elapsed: 00:06:07
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 50464 steps/s (collection: 1.834s, learning 0.114s)
             Mean action noise std: 1.54
          Mean value_function loss: 5.2385
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 33.1734
                       Mean reward: 7.51
               Mean episode length: 179.17
    Episode_Reward/reaching_object: 0.4640
     Episode_Reward/lifting_object: 1.2521
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.95s
                      Time elapsed: 00:06:09
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 51676 steps/s (collection: 1.786s, learning 0.117s)
             Mean action noise std: 1.54
          Mean value_function loss: 6.4032
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.2011
                       Mean reward: 11.60
               Mean episode length: 187.39
    Episode_Reward/reaching_object: 0.4803
     Episode_Reward/lifting_object: 1.4248
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.90s
                      Time elapsed: 00:06:11
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 51770 steps/s (collection: 1.783s, learning 0.116s)
             Mean action noise std: 1.54
          Mean value_function loss: 5.2634
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 33.2334
                       Mean reward: 11.08
               Mean episode length: 199.63
    Episode_Reward/reaching_object: 0.4851
     Episode_Reward/lifting_object: 1.5273
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.90s
                      Time elapsed: 00:06:13
                               ETA: 01:04:05

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 52347 steps/s (collection: 1.785s, learning 0.093s)
             Mean action noise std: 1.55
          Mean value_function loss: 4.9733
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.2604
                       Mean reward: 11.13
               Mean episode length: 170.61
    Episode_Reward/reaching_object: 0.4602
     Episode_Reward/lifting_object: 1.6359
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.88s
                      Time elapsed: 00:06:15
                               ETA: 01:04:00

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 53003 steps/s (collection: 1.757s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.3779
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.2915
                       Mean reward: 9.80
               Mean episode length: 189.15
    Episode_Reward/reaching_object: 0.4464
     Episode_Reward/lifting_object: 1.5778
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.85s
                      Time elapsed: 00:06:16
                               ETA: 01:03:56

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 52424 steps/s (collection: 1.760s, learning 0.115s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.2408
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 33.3142
                       Mean reward: 11.49
               Mean episode length: 190.63
    Episode_Reward/reaching_object: 0.4384
     Episode_Reward/lifting_object: 1.7164
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.88s
                      Time elapsed: 00:06:18
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 51956 steps/s (collection: 1.766s, learning 0.126s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.8668
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 33.3347
                       Mean reward: 11.10
               Mean episode length: 171.11
    Episode_Reward/reaching_object: 0.4130
     Episode_Reward/lifting_object: 1.5848
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.89s
                      Time elapsed: 00:06:20
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 52352 steps/s (collection: 1.761s, learning 0.116s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.5526
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 33.3531
                       Mean reward: 12.06
               Mean episode length: 169.65
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: 1.7516
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.88s
                      Time elapsed: 00:06:22
                               ETA: 01:03:43

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 50870 steps/s (collection: 1.817s, learning 0.115s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.8775
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.3645
                       Mean reward: 11.97
               Mean episode length: 160.18
    Episode_Reward/reaching_object: 0.3776
     Episode_Reward/lifting_object: 1.6390
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.93s
                      Time elapsed: 00:06:24
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 51609 steps/s (collection: 1.799s, learning 0.106s)
             Mean action noise std: 1.56
          Mean value_function loss: 4.9277
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 33.3890
                       Mean reward: 11.10
               Mean episode length: 169.29
    Episode_Reward/reaching_object: 0.3883
     Episode_Reward/lifting_object: 1.7404
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.90s
                      Time elapsed: 00:06:26
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 52148 steps/s (collection: 1.787s, learning 0.098s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.7949
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 33.3956
                       Mean reward: 9.81
               Mean episode length: 178.09
    Episode_Reward/reaching_object: 0.3807
     Episode_Reward/lifting_object: 1.7790
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.89s
                      Time elapsed: 00:06:28
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 51883 steps/s (collection: 1.774s, learning 0.121s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.1903
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.4032
                       Mean reward: 11.74
               Mean episode length: 181.40
    Episode_Reward/reaching_object: 0.4284
     Episode_Reward/lifting_object: 1.7126
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.89s
                      Time elapsed: 00:06:30
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 51497 steps/s (collection: 1.798s, learning 0.111s)
             Mean action noise std: 1.56
          Mean value_function loss: 6.3245
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 33.4204
                       Mean reward: 9.93
               Mean episode length: 168.23
    Episode_Reward/reaching_object: 0.3964
     Episode_Reward/lifting_object: 1.8260
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.91s
                      Time elapsed: 00:06:32
                               ETA: 01:03:22

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 52117 steps/s (collection: 1.777s, learning 0.109s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.5920
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.4415
                       Mean reward: 12.90
               Mean episode length: 176.36
    Episode_Reward/reaching_object: 0.3950
     Episode_Reward/lifting_object: 1.8138
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.89s
                      Time elapsed: 00:06:33
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 52957 steps/s (collection: 1.765s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 5.9707
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 33.4724
                       Mean reward: 11.37
               Mean episode length: 183.25
    Episode_Reward/reaching_object: 0.3987
     Episode_Reward/lifting_object: 1.8418
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.86s
                      Time elapsed: 00:06:35
                               ETA: 01:03:14

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 52461 steps/s (collection: 1.758s, learning 0.116s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.3761
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.5068
                       Mean reward: 12.69
               Mean episode length: 187.73
    Episode_Reward/reaching_object: 0.4134
     Episode_Reward/lifting_object: 2.1184
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.87s
                      Time elapsed: 00:06:37
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 52866 steps/s (collection: 1.754s, learning 0.106s)
             Mean action noise std: 1.57
          Mean value_function loss: 5.6016
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 33.5442
                       Mean reward: 13.76
               Mean episode length: 177.37
    Episode_Reward/reaching_object: 0.3996
     Episode_Reward/lifting_object: 2.0825
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.86s
                      Time elapsed: 00:06:39
                               ETA: 01:03:05

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 52676 steps/s (collection: 1.752s, learning 0.115s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.1728
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 33.5623
                       Mean reward: 9.74
               Mean episode length: 183.17
    Episode_Reward/reaching_object: 0.4080
     Episode_Reward/lifting_object: 2.0244
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.87s
                      Time elapsed: 00:06:41
                               ETA: 01:03:01

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 52449 steps/s (collection: 1.761s, learning 0.113s)
             Mean action noise std: 1.58
          Mean value_function loss: 5.7843
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.5790
                       Mean reward: 14.53
               Mean episode length: 185.93
    Episode_Reward/reaching_object: 0.4042
     Episode_Reward/lifting_object: 2.1571
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.87s
                      Time elapsed: 00:06:43
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 51160 steps/s (collection: 1.804s, learning 0.118s)
             Mean action noise std: 1.58
          Mean value_function loss: 7.1514
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.6136
                       Mean reward: 12.70
               Mean episode length: 185.61
    Episode_Reward/reaching_object: 0.4154
     Episode_Reward/lifting_object: 1.8898
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.92s
                      Time elapsed: 00:06:45
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 51827 steps/s (collection: 1.779s, learning 0.118s)
             Mean action noise std: 1.58
          Mean value_function loss: 6.4915
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.6444
                       Mean reward: 13.40
               Mean episode length: 176.77
    Episode_Reward/reaching_object: 0.3980
     Episode_Reward/lifting_object: 2.0001
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.90s
                      Time elapsed: 00:06:47
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 52322 steps/s (collection: 1.770s, learning 0.109s)
             Mean action noise std: 1.58
          Mean value_function loss: 9.9600
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 33.6648
                       Mean reward: 13.33
               Mean episode length: 174.98
    Episode_Reward/reaching_object: 0.3972
     Episode_Reward/lifting_object: 2.1488
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.88s
                      Time elapsed: 00:06:48
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 52020 steps/s (collection: 1.774s, learning 0.115s)
             Mean action noise std: 1.59
          Mean value_function loss: 8.3557
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.6917
                       Mean reward: 12.55
               Mean episode length: 166.92
    Episode_Reward/reaching_object: 0.4047
     Episode_Reward/lifting_object: 2.2728
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.89s
                      Time elapsed: 00:06:50
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 52872 steps/s (collection: 1.748s, learning 0.111s)
             Mean action noise std: 1.59
          Mean value_function loss: 9.6492
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 33.7092
                       Mean reward: 14.01
               Mean episode length: 159.79
    Episode_Reward/reaching_object: 0.3857
     Episode_Reward/lifting_object: 2.2369
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.86s
                      Time elapsed: 00:06:52
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 52417 steps/s (collection: 1.770s, learning 0.105s)
             Mean action noise std: 1.59
          Mean value_function loss: 8.7039
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 33.7230
                       Mean reward: 14.35
               Mean episode length: 167.51
    Episode_Reward/reaching_object: 0.4074
     Episode_Reward/lifting_object: 2.4148
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.88s
                      Time elapsed: 00:06:54
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 51683 steps/s (collection: 1.782s, learning 0.120s)
             Mean action noise std: 1.59
          Mean value_function loss: 7.5900
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 33.7364
                       Mean reward: 20.03
               Mean episode length: 171.15
    Episode_Reward/reaching_object: 0.4074
     Episode_Reward/lifting_object: 2.5346
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.90s
                      Time elapsed: 00:06:56
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 51346 steps/s (collection: 1.804s, learning 0.111s)
             Mean action noise std: 1.59
          Mean value_function loss: 9.3759
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 33.7449
                       Mean reward: 15.17
               Mean episode length: 158.80
    Episode_Reward/reaching_object: 0.3962
     Episode_Reward/lifting_object: 2.6171
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.91s
                      Time elapsed: 00:06:58
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 50481 steps/s (collection: 1.836s, learning 0.112s)
             Mean action noise std: 1.59
          Mean value_function loss: 11.8883
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.7644
                       Mean reward: 16.20
               Mean episode length: 165.68
    Episode_Reward/reaching_object: 0.4090
     Episode_Reward/lifting_object: 2.6518
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.95s
                      Time elapsed: 00:07:00
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 50901 steps/s (collection: 1.842s, learning 0.090s)
             Mean action noise std: 1.60
          Mean value_function loss: 9.1088
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.7909
                       Mean reward: 18.31
               Mean episode length: 155.03
    Episode_Reward/reaching_object: 0.3797
     Episode_Reward/lifting_object: 2.7322
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.93s
                      Time elapsed: 00:07:02
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 51225 steps/s (collection: 1.810s, learning 0.110s)
             Mean action noise std: 1.60
          Mean value_function loss: 12.6956
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 33.8146
                       Mean reward: 17.11
               Mean episode length: 166.79
    Episode_Reward/reaching_object: 0.4031
     Episode_Reward/lifting_object: 2.9197
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.92s
                      Time elapsed: 00:07:04
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 51376 steps/s (collection: 1.816s, learning 0.098s)
             Mean action noise std: 1.60
          Mean value_function loss: 8.9591
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 33.8446
                       Mean reward: 18.65
               Mean episode length: 149.74
    Episode_Reward/reaching_object: 0.4031
     Episode_Reward/lifting_object: 3.2203
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.91s
                      Time elapsed: 00:07:06
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 51243 steps/s (collection: 1.832s, learning 0.086s)
             Mean action noise std: 1.60
          Mean value_function loss: 8.7663
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 33.8610
                       Mean reward: 16.12
               Mean episode length: 150.25
    Episode_Reward/reaching_object: 0.3820
     Episode_Reward/lifting_object: 2.8215
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.92s
                      Time elapsed: 00:07:08
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 51190 steps/s (collection: 1.832s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 10.5093
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 33.8660
                       Mean reward: 20.45
               Mean episode length: 164.02
    Episode_Reward/reaching_object: 0.3917
     Episode_Reward/lifting_object: 3.1547
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.92s
                      Time elapsed: 00:07:09
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 52124 steps/s (collection: 1.800s, learning 0.086s)
             Mean action noise std: 1.60
          Mean value_function loss: 10.8156
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 33.8773
                       Mean reward: 18.55
               Mean episode length: 152.94
    Episode_Reward/reaching_object: 0.3951
     Episode_Reward/lifting_object: 3.2549
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.89s
                      Time elapsed: 00:07:11
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 52098 steps/s (collection: 1.792s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 12.5728
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 33.8899
                       Mean reward: 17.26
               Mean episode length: 143.63
    Episode_Reward/reaching_object: 0.3849
     Episode_Reward/lifting_object: 3.1338
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.89s
                      Time elapsed: 00:07:13
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 52670 steps/s (collection: 1.779s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 10.4040
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 33.9036
                       Mean reward: 18.20
               Mean episode length: 140.79
    Episode_Reward/reaching_object: 0.3777
     Episode_Reward/lifting_object: 3.2515
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.87s
                      Time elapsed: 00:07:15
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 51659 steps/s (collection: 1.804s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 10.1267
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.9100
                       Mean reward: 19.72
               Mean episode length: 144.09
    Episode_Reward/reaching_object: 0.3794
     Episode_Reward/lifting_object: 3.2439
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.90s
                      Time elapsed: 00:07:17
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 51281 steps/s (collection: 1.810s, learning 0.107s)
             Mean action noise std: 1.61
          Mean value_function loss: 10.0574
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.9243
                       Mean reward: 18.20
               Mean episode length: 150.53
    Episode_Reward/reaching_object: 0.3899
     Episode_Reward/lifting_object: 3.4802
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.92s
                      Time elapsed: 00:07:19
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 50698 steps/s (collection: 1.827s, learning 0.112s)
             Mean action noise std: 1.61
          Mean value_function loss: 12.7951
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 33.9383
                       Mean reward: 15.30
               Mean episode length: 139.20
    Episode_Reward/reaching_object: 0.3755
     Episode_Reward/lifting_object: 3.3462
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.94s
                      Time elapsed: 00:07:21
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 52210 steps/s (collection: 1.782s, learning 0.101s)
             Mean action noise std: 1.61
          Mean value_function loss: 13.0830
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 33.9597
                       Mean reward: 19.05
               Mean episode length: 146.48
    Episode_Reward/reaching_object: 0.3835
     Episode_Reward/lifting_object: 3.3303
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.88s
                      Time elapsed: 00:07:23
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 52057 steps/s (collection: 1.798s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 11.5476
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 33.9774
                       Mean reward: 18.11
               Mean episode length: 138.14
    Episode_Reward/reaching_object: 0.3796
     Episode_Reward/lifting_object: 3.3201
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.89s
                      Time elapsed: 00:07:25
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 51816 steps/s (collection: 1.806s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 14.8561
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 33.9861
                       Mean reward: 22.76
               Mean episode length: 155.74
    Episode_Reward/reaching_object: 0.3978
     Episode_Reward/lifting_object: 3.5702
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.90s
                      Time elapsed: 00:07:27
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 51691 steps/s (collection: 1.791s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 10.6675
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.0029
                       Mean reward: 22.68
               Mean episode length: 147.92
    Episode_Reward/reaching_object: 0.4065
     Episode_Reward/lifting_object: 3.9429
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.90s
                      Time elapsed: 00:07:28
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 52143 steps/s (collection: 1.779s, learning 0.106s)
             Mean action noise std: 1.62
          Mean value_function loss: 13.5914
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.0178
                       Mean reward: 19.94
               Mean episode length: 145.31
    Episode_Reward/reaching_object: 0.3885
     Episode_Reward/lifting_object: 3.7047
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.89s
                      Time elapsed: 00:07:30
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 51399 steps/s (collection: 1.797s, learning 0.116s)
             Mean action noise std: 1.62
          Mean value_function loss: 12.0935
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.0364
                       Mean reward: 22.19
               Mean episode length: 154.13
    Episode_Reward/reaching_object: 0.4086
     Episode_Reward/lifting_object: 3.7182
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.91s
                      Time elapsed: 00:07:32
                               ETA: 01:01:23

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 51855 steps/s (collection: 1.786s, learning 0.110s)
             Mean action noise std: 1.62
          Mean value_function loss: 12.5244
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.0551
                       Mean reward: 22.23
               Mean episode length: 146.72
    Episode_Reward/reaching_object: 0.3845
     Episode_Reward/lifting_object: 4.0787
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.90s
                      Time elapsed: 00:07:34
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 51827 steps/s (collection: 1.786s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 14.5396
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.0683
                       Mean reward: 26.11
               Mean episode length: 155.32
    Episode_Reward/reaching_object: 0.3915
     Episode_Reward/lifting_object: 4.2299
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.90s
                      Time elapsed: 00:07:36
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 51524 steps/s (collection: 1.796s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 12.1385
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 34.0792
                       Mean reward: 20.71
               Mean episode length: 151.52
    Episode_Reward/reaching_object: 0.4046
     Episode_Reward/lifting_object: 4.3494
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.91s
                      Time elapsed: 00:07:38
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 51469 steps/s (collection: 1.803s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 12.2523
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.0898
                       Mean reward: 23.68
               Mean episode length: 146.20
    Episode_Reward/reaching_object: 0.4007
     Episode_Reward/lifting_object: 4.2724
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.91s
                      Time elapsed: 00:07:40
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 52533 steps/s (collection: 1.784s, learning 0.087s)
             Mean action noise std: 1.63
          Mean value_function loss: 12.1540
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.1056
                       Mean reward: 22.69
               Mean episode length: 141.21
    Episode_Reward/reaching_object: 0.3886
     Episode_Reward/lifting_object: 4.4482
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.87s
                      Time elapsed: 00:07:42
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 51191 steps/s (collection: 1.799s, learning 0.121s)
             Mean action noise std: 1.63
          Mean value_function loss: 15.2127
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.1206
                       Mean reward: 29.11
               Mean episode length: 135.48
    Episode_Reward/reaching_object: 0.3852
     Episode_Reward/lifting_object: 4.5717
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.92s
                      Time elapsed: 00:07:44
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 50377 steps/s (collection: 1.819s, learning 0.133s)
             Mean action noise std: 1.63
          Mean value_function loss: 14.6486
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.1308
                       Mean reward: 22.98
               Mean episode length: 148.88
    Episode_Reward/reaching_object: 0.3846
     Episode_Reward/lifting_object: 4.1906
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.95s
                      Time elapsed: 00:07:46
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 52222 steps/s (collection: 1.786s, learning 0.097s)
             Mean action noise std: 1.63
          Mean value_function loss: 15.3382
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.1373
                       Mean reward: 24.09
               Mean episode length: 134.80
    Episode_Reward/reaching_object: 0.3750
     Episode_Reward/lifting_object: 4.0507
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.88s
                      Time elapsed: 00:07:47
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 50677 steps/s (collection: 1.818s, learning 0.122s)
             Mean action noise std: 1.63
          Mean value_function loss: 14.6601
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.1433
                       Mean reward: 24.32
               Mean episode length: 136.13
    Episode_Reward/reaching_object: 0.3542
     Episode_Reward/lifting_object: 4.4998
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.94s
                      Time elapsed: 00:07:49
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 51897 steps/s (collection: 1.787s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 14.8152
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 34.1521
                       Mean reward: 23.07
               Mean episode length: 139.82
    Episode_Reward/reaching_object: 0.3860
     Episode_Reward/lifting_object: 4.7028
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.89s
                      Time elapsed: 00:07:51
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 51486 steps/s (collection: 1.800s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 16.6400
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.1640
                       Mean reward: 21.62
               Mean episode length: 143.33
    Episode_Reward/reaching_object: 0.3723
     Episode_Reward/lifting_object: 4.7088
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.91s
                      Time elapsed: 00:07:53
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 51683 steps/s (collection: 1.795s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 15.4977
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.1755
                       Mean reward: 28.26
               Mean episode length: 140.75
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 4.7067
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.90s
                      Time elapsed: 00:07:55
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 51019 steps/s (collection: 1.829s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 17.4224
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.1859
                       Mean reward: 25.80
               Mean episode length: 128.85
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: 4.5315
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.93s
                      Time elapsed: 00:07:57
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 50717 steps/s (collection: 1.841s, learning 0.097s)
             Mean action noise std: 1.64
          Mean value_function loss: 17.6177
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.1945
                       Mean reward: 28.18
               Mean episode length: 143.30
    Episode_Reward/reaching_object: 0.3684
     Episode_Reward/lifting_object: 4.7139
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.94s
                      Time elapsed: 00:07:59
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49018 steps/s (collection: 1.885s, learning 0.120s)
             Mean action noise std: 1.64
          Mean value_function loss: 18.2869
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.1996
                       Mean reward: 28.98
               Mean episode length: 136.70
    Episode_Reward/reaching_object: 0.3615
     Episode_Reward/lifting_object: 5.1148
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.01s
                      Time elapsed: 00:08:01
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 49345 steps/s (collection: 1.879s, learning 0.114s)
             Mean action noise std: 1.64
          Mean value_function loss: 17.7328
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.2071
                       Mean reward: 28.75
               Mean episode length: 144.61
    Episode_Reward/reaching_object: 0.3676
     Episode_Reward/lifting_object: 4.8381
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.99s
                      Time elapsed: 00:08:03
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 48850 steps/s (collection: 1.907s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 19.9776
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 34.2137
                       Mean reward: 25.80
               Mean episode length: 138.26
    Episode_Reward/reaching_object: 0.3711
     Episode_Reward/lifting_object: 5.0132
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.01s
                      Time elapsed: 00:08:05
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 42871 steps/s (collection: 2.109s, learning 0.184s)
             Mean action noise std: 1.64
          Mean value_function loss: 16.9271
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.2174
                       Mean reward: 25.34
               Mean episode length: 124.13
    Episode_Reward/reaching_object: 0.3618
     Episode_Reward/lifting_object: 5.2899
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.29s
                      Time elapsed: 00:08:07
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 39198 steps/s (collection: 2.412s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 17.3192
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.2227
                       Mean reward: 29.26
               Mean episode length: 141.81
    Episode_Reward/reaching_object: 0.3567
     Episode_Reward/lifting_object: 5.3343
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.51s
                      Time elapsed: 00:08:10
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 48353 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 20.1055
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.2294
                       Mean reward: 31.68
               Mean episode length: 138.09
    Episode_Reward/reaching_object: 0.3598
     Episode_Reward/lifting_object: 5.4709
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.03s
                      Time elapsed: 00:08:12
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 46156 steps/s (collection: 2.021s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 20.8896
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.2373
                       Mean reward: 30.36
               Mean episode length: 140.66
    Episode_Reward/reaching_object: 0.3334
     Episode_Reward/lifting_object: 5.1641
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.13s
                      Time elapsed: 00:08:14
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 50122 steps/s (collection: 1.839s, learning 0.123s)
             Mean action noise std: 1.64
          Mean value_function loss: 24.0767
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.2406
                       Mean reward: 29.34
               Mean episode length: 129.98
    Episode_Reward/reaching_object: 0.3252
     Episode_Reward/lifting_object: 5.4353
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.96s
                      Time elapsed: 00:08:16
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 50307 steps/s (collection: 1.839s, learning 0.115s)
             Mean action noise std: 1.64
          Mean value_function loss: 21.6889
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.2490
                       Mean reward: 29.92
               Mean episode length: 121.33
    Episode_Reward/reaching_object: 0.3264
     Episode_Reward/lifting_object: 5.5817
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.95s
                      Time elapsed: 00:08:18
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 51087 steps/s (collection: 1.812s, learning 0.113s)
             Mean action noise std: 1.64
          Mean value_function loss: 22.9164
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.2565
                       Mean reward: 29.90
               Mean episode length: 118.57
    Episode_Reward/reaching_object: 0.3290
     Episode_Reward/lifting_object: 5.6134
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.92s
                      Time elapsed: 00:08:20
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 51221 steps/s (collection: 1.818s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 21.1287
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.2635
                       Mean reward: 28.00
               Mean episode length: 109.49
    Episode_Reward/reaching_object: 0.3282
     Episode_Reward/lifting_object: 5.8612
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 1.92s
                      Time elapsed: 00:08:22
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 49225 steps/s (collection: 1.882s, learning 0.116s)
             Mean action noise std: 1.64
          Mean value_function loss: 22.6150
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.2666
                       Mean reward: 30.20
               Mean episode length: 122.06
    Episode_Reward/reaching_object: 0.3196
     Episode_Reward/lifting_object: 5.5058
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.00s
                      Time elapsed: 00:08:24
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 50982 steps/s (collection: 1.833s, learning 0.095s)
             Mean action noise std: 1.65
          Mean value_function loss: 22.8717
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.2793
                       Mean reward: 29.78
               Mean episode length: 120.48
    Episode_Reward/reaching_object: 0.3257
     Episode_Reward/lifting_object: 5.8069
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.93s
                      Time elapsed: 00:08:26
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 50973 steps/s (collection: 1.805s, learning 0.124s)
             Mean action noise std: 1.65
          Mean value_function loss: 24.7910
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.2986
                       Mean reward: 32.24
               Mean episode length: 121.45
    Episode_Reward/reaching_object: 0.3316
     Episode_Reward/lifting_object: 5.8718
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.93s
                      Time elapsed: 00:08:28
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 50337 steps/s (collection: 1.835s, learning 0.118s)
             Mean action noise std: 1.65
          Mean value_function loss: 23.2961
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.3102
                       Mean reward: 31.55
               Mean episode length: 117.07
    Episode_Reward/reaching_object: 0.3287
     Episode_Reward/lifting_object: 5.8776
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.95s
                      Time elapsed: 00:08:29
                               ETA: 01:00:04

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 50714 steps/s (collection: 1.819s, learning 0.119s)
             Mean action noise std: 1.65
          Mean value_function loss: 25.4822
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 34.3174
                       Mean reward: 30.94
               Mean episode length: 120.27
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 5.7999
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.94s
                      Time elapsed: 00:08:31
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 50582 steps/s (collection: 1.827s, learning 0.116s)
             Mean action noise std: 1.65
          Mean value_function loss: 21.5699
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.3239
                       Mean reward: 32.57
               Mean episode length: 117.82
    Episode_Reward/reaching_object: 0.3295
     Episode_Reward/lifting_object: 6.0991
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.94s
                      Time elapsed: 00:08:33
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 46658 steps/s (collection: 1.975s, learning 0.132s)
             Mean action noise std: 1.65
          Mean value_function loss: 20.7207
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.3285
                       Mean reward: 26.64
               Mean episode length: 110.45
    Episode_Reward/reaching_object: 0.3192
     Episode_Reward/lifting_object: 5.8523
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.11s
                      Time elapsed: 00:08:35
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.939s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 23.8347
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.3324
                       Mean reward: 29.60
               Mean episode length: 119.83
    Episode_Reward/reaching_object: 0.3175
     Episode_Reward/lifting_object: 5.9766
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.05s
                      Time elapsed: 00:08:38
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 48828 steps/s (collection: 1.896s, learning 0.117s)
             Mean action noise std: 1.65
          Mean value_function loss: 22.1509
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.3414
                       Mean reward: 33.10
               Mean episode length: 113.24
    Episode_Reward/reaching_object: 0.3228
     Episode_Reward/lifting_object: 6.2054
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 32.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.01s
                      Time elapsed: 00:08:40
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 49276 steps/s (collection: 1.883s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 26.2532
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.3517
                       Mean reward: 34.04
               Mean episode length: 115.01
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 6.1226
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 34.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.99s
                      Time elapsed: 00:08:42
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 49218 steps/s (collection: 1.874s, learning 0.123s)
             Mean action noise std: 1.65
          Mean value_function loss: 23.2985
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.3595
                       Mean reward: 32.94
               Mean episode length: 108.85
    Episode_Reward/reaching_object: 0.3122
     Episode_Reward/lifting_object: 6.1323
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.00s
                      Time elapsed: 00:08:44
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 48120 steps/s (collection: 1.918s, learning 0.125s)
             Mean action noise std: 1.65
          Mean value_function loss: 30.2234
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.3728
                       Mean reward: 30.69
               Mean episode length: 107.23
    Episode_Reward/reaching_object: 0.3069
     Episode_Reward/lifting_object: 6.0503
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.04s
                      Time elapsed: 00:08:46
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 45164 steps/s (collection: 2.053s, learning 0.124s)
             Mean action noise std: 1.66
          Mean value_function loss: 29.0726
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.3869
                       Mean reward: 33.34
               Mean episode length: 114.82
    Episode_Reward/reaching_object: 0.3144
     Episode_Reward/lifting_object: 6.2167
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 35.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.18s
                      Time elapsed: 00:08:48
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 48877 steps/s (collection: 1.893s, learning 0.119s)
             Mean action noise std: 1.66
          Mean value_function loss: 26.7417
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.4000
                       Mean reward: 38.36
               Mean episode length: 119.27
    Episode_Reward/reaching_object: 0.3089
     Episode_Reward/lifting_object: 6.5719
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.01s
                      Time elapsed: 00:08:50
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 50064 steps/s (collection: 1.847s, learning 0.117s)
             Mean action noise std: 1.66
          Mean value_function loss: 25.5293
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.4076
                       Mean reward: 31.95
               Mean episode length: 106.41
    Episode_Reward/reaching_object: 0.3088
     Episode_Reward/lifting_object: 6.7168
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.96s
                      Time elapsed: 00:08:52
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 49557 steps/s (collection: 1.858s, learning 0.126s)
             Mean action noise std: 1.66
          Mean value_function loss: 29.0565
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.4096
                       Mean reward: 36.24
               Mean episode length: 107.56
    Episode_Reward/reaching_object: 0.3106
     Episode_Reward/lifting_object: 6.8213
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.98s
                      Time elapsed: 00:08:54
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 50405 steps/s (collection: 1.855s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 26.3359
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 34.4104
                       Mean reward: 40.91
               Mean episode length: 104.18
    Episode_Reward/reaching_object: 0.3067
     Episode_Reward/lifting_object: 6.8217
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.95s
                      Time elapsed: 00:08:56
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 48324 steps/s (collection: 1.919s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 28.9633
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.4111
                       Mean reward: 39.35
               Mean episode length: 107.64
    Episode_Reward/reaching_object: 0.3072
     Episode_Reward/lifting_object: 7.0729
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.03s
                      Time elapsed: 00:08:58
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 47790 steps/s (collection: 1.926s, learning 0.131s)
             Mean action noise std: 1.66
          Mean value_function loss: 31.1989
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 34.4144
                       Mean reward: 38.80
               Mean episode length: 116.23
    Episode_Reward/reaching_object: 0.3127
     Episode_Reward/lifting_object: 7.2543
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.06s
                      Time elapsed: 00:09:00
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 49127 steps/s (collection: 1.894s, learning 0.107s)
             Mean action noise std: 1.66
          Mean value_function loss: 31.2989
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.4214
                       Mean reward: 34.33
               Mean episode length: 104.64
    Episode_Reward/reaching_object: 0.2950
     Episode_Reward/lifting_object: 6.8668
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.00s
                      Time elapsed: 00:09:02
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 50450 steps/s (collection: 1.850s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 30.1039
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 34.4259
                       Mean reward: 36.97
               Mean episode length: 99.41
    Episode_Reward/reaching_object: 0.2887
     Episode_Reward/lifting_object: 6.7769
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.95s
                      Time elapsed: 00:09:04
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 47697 steps/s (collection: 1.939s, learning 0.122s)
             Mean action noise std: 1.66
          Mean value_function loss: 31.1948
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 34.4300
                       Mean reward: 39.01
               Mean episode length: 108.34
    Episode_Reward/reaching_object: 0.2942
     Episode_Reward/lifting_object: 7.1983
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.06s
                      Time elapsed: 00:09:06
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 49308 steps/s (collection: 1.878s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 27.5681
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 34.4323
                       Mean reward: 36.87
               Mean episode length: 102.69
    Episode_Reward/reaching_object: 0.3006
     Episode_Reward/lifting_object: 7.3425
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.99s
                      Time elapsed: 00:09:08
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 48340 steps/s (collection: 1.916s, learning 0.118s)
             Mean action noise std: 1.66
          Mean value_function loss: 37.0507
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 34.4328
                       Mean reward: 37.11
               Mean episode length: 98.37
    Episode_Reward/reaching_object: 0.3008
     Episode_Reward/lifting_object: 7.6652
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.03s
                      Time elapsed: 00:09:10
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 48632 steps/s (collection: 1.904s, learning 0.117s)
             Mean action noise std: 1.66
          Mean value_function loss: 30.6355
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.4330
                       Mean reward: 35.91
               Mean episode length: 109.03
    Episode_Reward/reaching_object: 0.2965
     Episode_Reward/lifting_object: 7.4572
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.02s
                      Time elapsed: 00:09:12
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 49258 steps/s (collection: 1.894s, learning 0.102s)
             Mean action noise std: 1.66
          Mean value_function loss: 34.5490
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.4341
                       Mean reward: 39.42
               Mean episode length: 103.29
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 7.3529
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.00s
                      Time elapsed: 00:09:14
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 48825 steps/s (collection: 1.909s, learning 0.104s)
             Mean action noise std: 1.66
          Mean value_function loss: 31.4604
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.4365
                       Mean reward: 34.71
               Mean episode length: 101.62
    Episode_Reward/reaching_object: 0.3028
     Episode_Reward/lifting_object: 7.3281
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.01s
                      Time elapsed: 00:09:16
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 46127 steps/s (collection: 2.026s, learning 0.106s)
             Mean action noise std: 1.66
          Mean value_function loss: 35.7391
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.4409
                       Mean reward: 41.12
               Mean episode length: 107.20
    Episode_Reward/reaching_object: 0.3131
     Episode_Reward/lifting_object: 7.5906
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.13s
                      Time elapsed: 00:09:18
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 48742 steps/s (collection: 1.907s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 32.4732
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.4459
                       Mean reward: 42.55
               Mean episode length: 105.60
    Episode_Reward/reaching_object: 0.3065
     Episode_Reward/lifting_object: 7.7202
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.02s
                      Time elapsed: 00:09:20
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 49448 steps/s (collection: 1.890s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 32.8220
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.4478
                       Mean reward: 39.90
               Mean episode length: 100.04
    Episode_Reward/reaching_object: 0.3099
     Episode_Reward/lifting_object: 7.6091
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 39.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.99s
                      Time elapsed: 00:09:22
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 50000 steps/s (collection: 1.859s, learning 0.108s)
             Mean action noise std: 1.66
          Mean value_function loss: 36.0436
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.4508
                       Mean reward: 43.52
               Mean episode length: 102.19
    Episode_Reward/reaching_object: 0.2985
     Episode_Reward/lifting_object: 7.9351
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.97s
                      Time elapsed: 00:09:24
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 49301 steps/s (collection: 1.883s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 41.0262
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 34.4556
                       Mean reward: 42.57
               Mean episode length: 104.80
    Episode_Reward/reaching_object: 0.3043
     Episode_Reward/lifting_object: 7.8368
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.99s
                      Time elapsed: 00:09:26
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 48421 steps/s (collection: 1.902s, learning 0.128s)
             Mean action noise std: 1.66
          Mean value_function loss: 43.0430
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.4589
                       Mean reward: 44.25
               Mean episode length: 103.06
    Episode_Reward/reaching_object: 0.2989
     Episode_Reward/lifting_object: 8.0697
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.03s
                      Time elapsed: 00:09:28
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 49810 steps/s (collection: 1.864s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 42.1678
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.4629
                       Mean reward: 38.02
               Mean episode length: 99.40
    Episode_Reward/reaching_object: 0.3004
     Episode_Reward/lifting_object: 7.8813
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 39.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.97s
                      Time elapsed: 00:09:30
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 49476 steps/s (collection: 1.868s, learning 0.119s)
             Mean action noise std: 1.66
          Mean value_function loss: 36.1688
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.4688
                       Mean reward: 41.69
               Mean episode length: 97.89
    Episode_Reward/reaching_object: 0.2993
     Episode_Reward/lifting_object: 8.4160
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.99s
                      Time elapsed: 00:09:32
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 47914 steps/s (collection: 1.928s, learning 0.124s)
             Mean action noise std: 1.67
          Mean value_function loss: 37.1541
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.4733
                       Mean reward: 46.76
               Mean episode length: 105.73
    Episode_Reward/reaching_object: 0.3007
     Episode_Reward/lifting_object: 8.1735
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.05s
                      Time elapsed: 00:09:34
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 45434 steps/s (collection: 2.030s, learning 0.134s)
             Mean action noise std: 1.67
          Mean value_function loss: 43.0303
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.4779
                       Mean reward: 43.44
               Mean episode length: 101.05
    Episode_Reward/reaching_object: 0.2900
     Episode_Reward/lifting_object: 8.2853
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.16s
                      Time elapsed: 00:09:36
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.915s, learning 0.127s)
             Mean action noise std: 1.67
          Mean value_function loss: 40.0961
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.4843
                       Mean reward: 37.49
               Mean episode length: 100.97
    Episode_Reward/reaching_object: 0.2959
     Episode_Reward/lifting_object: 8.0226
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.04s
                      Time elapsed: 00:09:38
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 49014 steps/s (collection: 1.883s, learning 0.123s)
             Mean action noise std: 1.67
          Mean value_function loss: 39.7704
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.4898
                       Mean reward: 46.82
               Mean episode length: 103.04
    Episode_Reward/reaching_object: 0.3023
     Episode_Reward/lifting_object: 8.5302
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.01s
                      Time elapsed: 00:09:40
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 49960 steps/s (collection: 1.850s, learning 0.117s)
             Mean action noise std: 1.67
          Mean value_function loss: 37.1156
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.4978
                       Mean reward: 43.85
               Mean episode length: 99.28
    Episode_Reward/reaching_object: 0.2958
     Episode_Reward/lifting_object: 8.6853
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.97s
                      Time elapsed: 00:09:42
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 49688 steps/s (collection: 1.860s, learning 0.118s)
             Mean action noise std: 1.67
          Mean value_function loss: 38.9027
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.5031
                       Mean reward: 41.57
               Mean episode length: 96.09
    Episode_Reward/reaching_object: 0.2942
     Episode_Reward/lifting_object: 8.6012
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 42.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.98s
                      Time elapsed: 00:09:44
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 49131 steps/s (collection: 1.880s, learning 0.121s)
             Mean action noise std: 1.67
          Mean value_function loss: 41.5233
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.5069
                       Mean reward: 41.87
               Mean episode length: 95.33
    Episode_Reward/reaching_object: 0.2853
     Episode_Reward/lifting_object: 8.7741
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.00s
                      Time elapsed: 00:09:46
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 49904 steps/s (collection: 1.854s, learning 0.116s)
             Mean action noise std: 1.67
          Mean value_function loss: 39.6568
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 34.5098
                       Mean reward: 48.91
               Mean episode length: 96.88
    Episode_Reward/reaching_object: 0.2979
     Episode_Reward/lifting_object: 8.9260
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 44.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 1.97s
                      Time elapsed: 00:09:48
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 49552 steps/s (collection: 1.857s, learning 0.127s)
             Mean action noise std: 1.67
          Mean value_function loss: 38.4549
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.5131
                       Mean reward: 44.87
               Mean episode length: 101.81
    Episode_Reward/reaching_object: 0.2858
     Episode_Reward/lifting_object: 8.8730
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.98s
                      Time elapsed: 00:09:50
                               ETA: 00:58:32

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 48857 steps/s (collection: 1.895s, learning 0.118s)
             Mean action noise std: 1.67
          Mean value_function loss: 40.0699
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 34.5181
                       Mean reward: 46.22
               Mean episode length: 99.41
    Episode_Reward/reaching_object: 0.2872
     Episode_Reward/lifting_object: 8.8431
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 39.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.01s
                      Time elapsed: 00:09:52
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 49962 steps/s (collection: 1.850s, learning 0.118s)
             Mean action noise std: 1.67
          Mean value_function loss: 48.6352
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.5218
                       Mean reward: 47.49
               Mean episode length: 100.54
    Episode_Reward/reaching_object: 0.2940
     Episode_Reward/lifting_object: 9.2570
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 42.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.97s
                      Time elapsed: 00:09:54
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 49564 steps/s (collection: 1.864s, learning 0.120s)
             Mean action noise std: 1.67
          Mean value_function loss: 47.9081
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.5278
                       Mean reward: 47.19
               Mean episode length: 93.15
    Episode_Reward/reaching_object: 0.2918
     Episode_Reward/lifting_object: 8.7938
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.98s
                      Time elapsed: 00:09:56
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 50489 steps/s (collection: 1.854s, learning 0.093s)
             Mean action noise std: 1.67
          Mean value_function loss: 43.7508
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.5392
                       Mean reward: 44.48
               Mean episode length: 92.41
    Episode_Reward/reaching_object: 0.2799
     Episode_Reward/lifting_object: 8.8569
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 42.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.95s
                      Time elapsed: 00:09:58
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 49829 steps/s (collection: 1.876s, learning 0.097s)
             Mean action noise std: 1.67
          Mean value_function loss: 48.2501
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.5456
                       Mean reward: 52.34
               Mean episode length: 99.85
    Episode_Reward/reaching_object: 0.2858
     Episode_Reward/lifting_object: 9.0822
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 43.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.97s
                      Time elapsed: 00:10:00
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 49333 steps/s (collection: 1.860s, learning 0.133s)
             Mean action noise std: 1.67
          Mean value_function loss: 51.5649
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.5502
                       Mean reward: 44.09
               Mean episode length: 94.81
    Episode_Reward/reaching_object: 0.2808
     Episode_Reward/lifting_object: 9.3763
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.99s
                      Time elapsed: 00:10:02
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 49492 steps/s (collection: 1.851s, learning 0.136s)
             Mean action noise std: 1.67
          Mean value_function loss: 44.2672
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 34.5557
                       Mean reward: 49.71
               Mean episode length: 96.40
    Episode_Reward/reaching_object: 0.2804
     Episode_Reward/lifting_object: 9.0299
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 42.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.99s
                      Time elapsed: 00:10:04
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 49111 steps/s (collection: 1.874s, learning 0.128s)
             Mean action noise std: 1.67
          Mean value_function loss: 41.8335
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.5575
                       Mean reward: 46.06
               Mean episode length: 87.52
    Episode_Reward/reaching_object: 0.2766
     Episode_Reward/lifting_object: 9.2218
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.00s
                      Time elapsed: 00:10:06
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 49523 steps/s (collection: 1.861s, learning 0.124s)
             Mean action noise std: 1.67
          Mean value_function loss: 52.7234
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.5601
                       Mean reward: 48.72
               Mean episode length: 93.09
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 9.3480
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.98s
                      Time elapsed: 00:10:08
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 49098 steps/s (collection: 1.882s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 44.1783
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.5667
                       Mean reward: 49.78
               Mean episode length: 98.48
    Episode_Reward/reaching_object: 0.2702
     Episode_Reward/lifting_object: 9.1581
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.00s
                      Time elapsed: 00:10:10
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 48507 steps/s (collection: 1.899s, learning 0.128s)
             Mean action noise std: 1.68
          Mean value_function loss: 48.7534
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.5757
                       Mean reward: 42.99
               Mean episode length: 88.91
    Episode_Reward/reaching_object: 0.2700
     Episode_Reward/lifting_object: 9.3970
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.03s
                      Time elapsed: 00:10:12
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 48345 steps/s (collection: 1.908s, learning 0.126s)
             Mean action noise std: 1.68
          Mean value_function loss: 43.5243
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.5809
                       Mean reward: 44.76
               Mean episode length: 85.74
    Episode_Reward/reaching_object: 0.2717
     Episode_Reward/lifting_object: 9.4417
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.03s
                      Time elapsed: 00:10:14
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 47293 steps/s (collection: 1.955s, learning 0.124s)
             Mean action noise std: 1.68
          Mean value_function loss: 55.9595
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.5814
                       Mean reward: 50.19
               Mean episode length: 91.81
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 9.2083
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 47.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.08s
                      Time elapsed: 00:10:16
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 49059 steps/s (collection: 1.893s, learning 0.111s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.3259
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.5867
                       Mean reward: 50.53
               Mean episode length: 91.27
    Episode_Reward/reaching_object: 0.2709
     Episode_Reward/lifting_object: 9.6168
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 46.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.00s
                      Time elapsed: 00:10:18
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 48498 steps/s (collection: 1.897s, learning 0.130s)
             Mean action noise std: 1.68
          Mean value_function loss: 44.8909
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.5903
                       Mean reward: 45.11
               Mean episode length: 83.77
    Episode_Reward/reaching_object: 0.2700
     Episode_Reward/lifting_object: 9.9462
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 44.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.03s
                      Time elapsed: 00:10:20
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 48369 steps/s (collection: 1.913s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.2216
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.5910
                       Mean reward: 49.17
               Mean episode length: 87.19
    Episode_Reward/reaching_object: 0.2667
     Episode_Reward/lifting_object: 9.8646
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.03s
                      Time elapsed: 00:10:22
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 48647 steps/s (collection: 1.902s, learning 0.119s)
             Mean action noise std: 1.68
          Mean value_function loss: 54.6032
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.5909
                       Mean reward: 60.24
               Mean episode length: 97.05
    Episode_Reward/reaching_object: 0.2732
     Episode_Reward/lifting_object: 10.4031
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 47.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.02s
                      Time elapsed: 00:10:24
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 48390 steps/s (collection: 1.896s, learning 0.136s)
             Mean action noise std: 1.68
          Mean value_function loss: 54.6993
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.5910
                       Mean reward: 45.26
               Mean episode length: 83.35
    Episode_Reward/reaching_object: 0.2628
     Episode_Reward/lifting_object: 9.8253
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 45.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.03s
                      Time elapsed: 00:10:26
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 48635 steps/s (collection: 1.896s, learning 0.125s)
             Mean action noise std: 1.68
          Mean value_function loss: 56.8216
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.5921
                       Mean reward: 52.24
               Mean episode length: 85.31
    Episode_Reward/reaching_object: 0.2634
     Episode_Reward/lifting_object: 10.0061
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 49.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.02s
                      Time elapsed: 00:10:28
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 48102 steps/s (collection: 1.923s, learning 0.121s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.8257
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.5955
                       Mean reward: 45.64
               Mean episode length: 83.71
    Episode_Reward/reaching_object: 0.2647
     Episode_Reward/lifting_object: 9.9373
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 46.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.04s
                      Time elapsed: 00:10:30
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 49207 steps/s (collection: 1.878s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 52.7908
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.5979
                       Mean reward: 51.50
               Mean episode length: 89.04
    Episode_Reward/reaching_object: 0.2558
     Episode_Reward/lifting_object: 9.8076
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.00s
                      Time elapsed: 00:10:32
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 48646 steps/s (collection: 1.898s, learning 0.123s)
             Mean action noise std: 1.68
          Mean value_function loss: 60.2677
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.6028
                       Mean reward: 55.14
               Mean episode length: 87.44
    Episode_Reward/reaching_object: 0.2519
     Episode_Reward/lifting_object: 10.1495
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 47.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.02s
                      Time elapsed: 00:10:34
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 49297 steps/s (collection: 1.870s, learning 0.124s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.1292
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.6091
                       Mean reward: 51.60
               Mean episode length: 86.51
    Episode_Reward/reaching_object: 0.2519
     Episode_Reward/lifting_object: 10.0982
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.99s
                      Time elapsed: 00:10:36
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 48278 steps/s (collection: 1.913s, learning 0.124s)
             Mean action noise std: 1.68
          Mean value_function loss: 49.0114
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.6124
                       Mean reward: 54.21
               Mean episode length: 86.73
    Episode_Reward/reaching_object: 0.2527
     Episode_Reward/lifting_object: 10.4358
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 47.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.04s
                      Time elapsed: 00:10:38
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 48576 steps/s (collection: 1.904s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 49.3376
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 34.6155
                       Mean reward: 56.11
               Mean episode length: 87.33
    Episode_Reward/reaching_object: 0.2593
     Episode_Reward/lifting_object: 10.5043
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.02s
                      Time elapsed: 00:10:40
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 48758 steps/s (collection: 1.896s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.5435
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 34.6177
                       Mean reward: 59.27
               Mean episode length: 91.81
    Episode_Reward/reaching_object: 0.2559
     Episode_Reward/lifting_object: 10.2879
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.02s
                      Time elapsed: 00:10:42
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 49350 steps/s (collection: 1.868s, learning 0.124s)
             Mean action noise std: 1.68
          Mean value_function loss: 49.4407
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.6184
                       Mean reward: 55.13
               Mean episode length: 88.28
    Episode_Reward/reaching_object: 0.2554
     Episode_Reward/lifting_object: 10.3016
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 1.99s
                      Time elapsed: 00:10:44
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 49013 steps/s (collection: 1.888s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 52.8887
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 34.6203
                       Mean reward: 50.92
               Mean episode length: 85.51
    Episode_Reward/reaching_object: 0.2667
     Episode_Reward/lifting_object: 10.8373
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 46.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.01s
                      Time elapsed: 00:10:46
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 48190 steps/s (collection: 1.914s, learning 0.126s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.3920
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.6209
                       Mean reward: 55.61
               Mean episode length: 89.96
    Episode_Reward/reaching_object: 0.2609
     Episode_Reward/lifting_object: 10.5830
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 46.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.04s
                      Time elapsed: 00:10:48
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 48957 steps/s (collection: 1.889s, learning 0.119s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.1036
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 34.6216
                       Mean reward: 58.80
               Mean episode length: 90.39
    Episode_Reward/reaching_object: 0.2634
     Episode_Reward/lifting_object: 10.8262
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.01s
                      Time elapsed: 00:10:50
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 49516 steps/s (collection: 1.888s, learning 0.098s)
             Mean action noise std: 1.68
          Mean value_function loss: 52.9545
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.6227
                       Mean reward: 48.56
               Mean episode length: 78.33
    Episode_Reward/reaching_object: 0.2571
     Episode_Reward/lifting_object: 10.7510
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 47.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 1.99s
                      Time elapsed: 00:10:52
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 48992 steps/s (collection: 1.885s, learning 0.121s)
             Mean action noise std: 1.68
          Mean value_function loss: 52.4625
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.6234
                       Mean reward: 57.46
               Mean episode length: 88.86
    Episode_Reward/reaching_object: 0.2631
     Episode_Reward/lifting_object: 10.9746
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.01s
                      Time elapsed: 00:10:54
                               ETA: 00:57:20

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 49196 steps/s (collection: 1.882s, learning 0.117s)
             Mean action noise std: 1.68
          Mean value_function loss: 58.5820
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.6256
                       Mean reward: 52.15
               Mean episode length: 85.23
    Episode_Reward/reaching_object: 0.2607
     Episode_Reward/lifting_object: 10.9552
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 46.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.00s
                      Time elapsed: 00:10:56
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 48467 steps/s (collection: 1.916s, learning 0.112s)
             Mean action noise std: 1.68
          Mean value_function loss: 62.0402
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.6288
                       Mean reward: 52.04
               Mean episode length: 84.36
    Episode_Reward/reaching_object: 0.2541
     Episode_Reward/lifting_object: 10.7565
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 46.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.03s
                      Time elapsed: 00:10:58
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 47959 steps/s (collection: 1.931s, learning 0.119s)
             Mean action noise std: 1.68
          Mean value_function loss: 59.8652
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.6312
                       Mean reward: 57.74
               Mean episode length: 92.36
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: 11.1927
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 48.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.05s
                      Time elapsed: 00:11:00
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 48370 steps/s (collection: 1.920s, learning 0.113s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.6009
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.6326
                       Mean reward: 58.57
               Mean episode length: 86.89
    Episode_Reward/reaching_object: 0.2549
     Episode_Reward/lifting_object: 11.1383
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 47.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.03s
                      Time elapsed: 00:11:02
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 48642 steps/s (collection: 1.892s, learning 0.129s)
             Mean action noise std: 1.68
          Mean value_function loss: 53.5486
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.6335
                       Mean reward: 53.24
               Mean episode length: 78.38
    Episode_Reward/reaching_object: 0.2514
     Episode_Reward/lifting_object: 10.8880
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.02s
                      Time elapsed: 00:11:04
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 48485 steps/s (collection: 1.903s, learning 0.124s)
             Mean action noise std: 1.68
          Mean value_function loss: 56.7266
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.6350
                       Mean reward: 57.05
               Mean episode length: 84.66
    Episode_Reward/reaching_object: 0.2540
     Episode_Reward/lifting_object: 11.2102
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 46.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.03s
                      Time elapsed: 00:11:07
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 49045 steps/s (collection: 1.875s, learning 0.129s)
             Mean action noise std: 1.68
          Mean value_function loss: 51.2728
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.6357
                       Mean reward: 61.54
               Mean episode length: 84.74
    Episode_Reward/reaching_object: 0.2602
     Episode_Reward/lifting_object: 11.4563
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 47.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.00s
                      Time elapsed: 00:11:09
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 49392 steps/s (collection: 1.874s, learning 0.117s)
             Mean action noise std: 1.68
          Mean value_function loss: 57.6990
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.6329
                       Mean reward: 58.06
               Mean episode length: 82.74
    Episode_Reward/reaching_object: 0.2538
     Episode_Reward/lifting_object: 11.4062
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 1.99s
                      Time elapsed: 00:11:11
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 49227 steps/s (collection: 1.875s, learning 0.122s)
             Mean action noise std: 1.68
          Mean value_function loss: 56.7026
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.6333
                       Mean reward: 53.32
               Mean episode length: 86.85
    Episode_Reward/reaching_object: 0.2673
     Episode_Reward/lifting_object: 11.8719
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.00s
                      Time elapsed: 00:11:13
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 48361 steps/s (collection: 1.919s, learning 0.114s)
             Mean action noise std: 1.68
          Mean value_function loss: 53.0355
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.6345
                       Mean reward: 56.44
               Mean episode length: 78.95
    Episode_Reward/reaching_object: 0.2571
     Episode_Reward/lifting_object: 11.4561
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 48.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.03s
                      Time elapsed: 00:11:15
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 48089 steps/s (collection: 1.923s, learning 0.122s)
             Mean action noise std: 1.68
          Mean value_function loss: 52.0289
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.6380
                       Mean reward: 58.79
               Mean episode length: 84.52
    Episode_Reward/reaching_object: 0.2638
     Episode_Reward/lifting_object: 11.6823
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 49.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.04s
                      Time elapsed: 00:11:17
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 48233 steps/s (collection: 1.911s, learning 0.128s)
             Mean action noise std: 1.69
          Mean value_function loss: 58.1908
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.6421
                       Mean reward: 63.40
               Mean episode length: 84.37
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 12.1084
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 49.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.04s
                      Time elapsed: 00:11:19
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 48050 steps/s (collection: 1.915s, learning 0.131s)
             Mean action noise std: 1.69
          Mean value_function loss: 58.9691
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 34.6448
                       Mean reward: 55.09
               Mean episode length: 78.86
    Episode_Reward/reaching_object: 0.2579
     Episode_Reward/lifting_object: 11.6435
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 49.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.05s
                      Time elapsed: 00:11:21
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 19203 steps/s (collection: 4.973s, learning 0.146s)
             Mean action noise std: 1.69
          Mean value_function loss: 54.5655
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 34.6451
                       Mean reward: 63.06
               Mean episode length: 82.22
    Episode_Reward/reaching_object: 0.2568
     Episode_Reward/lifting_object: 11.8180
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 49.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.12s
                      Time elapsed: 00:11:26
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14840 steps/s (collection: 6.479s, learning 0.146s)
             Mean action noise std: 1.69
          Mean value_function loss: 52.0480
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 34.6458
                       Mean reward: 60.76
               Mean episode length: 85.55
    Episode_Reward/reaching_object: 0.2630
     Episode_Reward/lifting_object: 11.9400
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 49.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.62s
                      Time elapsed: 00:11:32
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14353 steps/s (collection: 6.714s, learning 0.134s)
             Mean action noise std: 1.69
          Mean value_function loss: 57.0760
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.6462
                       Mean reward: 53.95
               Mean episode length: 77.20
    Episode_Reward/reaching_object: 0.2578
     Episode_Reward/lifting_object: 11.8121
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 52.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.85s
                      Time elapsed: 00:11:39
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14674 steps/s (collection: 6.583s, learning 0.116s)
             Mean action noise std: 1.69
          Mean value_function loss: 57.6647
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 34.6479
                       Mean reward: 61.88
               Mean episode length: 81.27
    Episode_Reward/reaching_object: 0.2568
     Episode_Reward/lifting_object: 11.8015
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 48.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.70s
                      Time elapsed: 00:11:46
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14360 steps/s (collection: 6.725s, learning 0.120s)
             Mean action noise std: 1.69
          Mean value_function loss: 56.1500
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 34.6493
                       Mean reward: 59.48
               Mean episode length: 81.47
    Episode_Reward/reaching_object: 0.2522
     Episode_Reward/lifting_object: 11.9072
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 50.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.85s
                      Time elapsed: 00:11:53
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14714 steps/s (collection: 6.537s, learning 0.143s)
             Mean action noise std: 1.69
          Mean value_function loss: 62.3910
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.6500
                       Mean reward: 64.14
               Mean episode length: 79.25
    Episode_Reward/reaching_object: 0.2590
     Episode_Reward/lifting_object: 11.8454
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 48.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.68s
                      Time elapsed: 00:11:59
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14795 steps/s (collection: 6.526s, learning 0.119s)
             Mean action noise std: 1.69
          Mean value_function loss: 52.9496
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.6508
                       Mean reward: 59.71
               Mean episode length: 74.85
    Episode_Reward/reaching_object: 0.2578
     Episode_Reward/lifting_object: 11.8765
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.64s
                      Time elapsed: 00:12:06
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14412 steps/s (collection: 6.673s, learning 0.148s)
             Mean action noise std: 1.69
          Mean value_function loss: 58.4512
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.6524
                       Mean reward: 60.78
               Mean episode length: 81.42
    Episode_Reward/reaching_object: 0.2635
     Episode_Reward/lifting_object: 12.0242
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.82s
                      Time elapsed: 00:12:13
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13634 steps/s (collection: 7.081s, learning 0.130s)
             Mean action noise std: 1.69
          Mean value_function loss: 59.7296
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.6561
                       Mean reward: 63.56
               Mean episode length: 79.01
    Episode_Reward/reaching_object: 0.2571
     Episode_Reward/lifting_object: 11.8324
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 51.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.21s
                      Time elapsed: 00:12:20
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 46497 steps/s (collection: 2.006s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 67.8199
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 34.6607
                       Mean reward: 65.19
               Mean episode length: 79.80
    Episode_Reward/reaching_object: 0.2498
     Episode_Reward/lifting_object: 12.1419
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 52.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.11s
                      Time elapsed: 00:12:22
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 49130 steps/s (collection: 1.899s, learning 0.102s)
             Mean action noise std: 1.69
          Mean value_function loss: 62.4094
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.6614
                       Mean reward: 65.30
               Mean episode length: 80.30
    Episode_Reward/reaching_object: 0.2519
     Episode_Reward/lifting_object: 11.9851
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 50.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.00s
                      Time elapsed: 00:12:24
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 51233 steps/s (collection: 1.808s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 62.8263
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.6606
                       Mean reward: 62.33
               Mean episode length: 77.28
    Episode_Reward/reaching_object: 0.2475
     Episode_Reward/lifting_object: 11.8329
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 54.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.92s
                      Time elapsed: 00:12:26
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 50681 steps/s (collection: 1.824s, learning 0.116s)
             Mean action noise std: 1.69
          Mean value_function loss: 65.0624
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.6636
                       Mean reward: 59.72
               Mean episode length: 77.52
    Episode_Reward/reaching_object: 0.2469
     Episode_Reward/lifting_object: 11.8540
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 52.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 1.94s
                      Time elapsed: 00:12:28
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 50078 steps/s (collection: 1.852s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 65.8413
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.6676
                       Mean reward: 64.60
               Mean episode length: 79.01
    Episode_Reward/reaching_object: 0.2445
     Episode_Reward/lifting_object: 11.8903
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 1.96s
                      Time elapsed: 00:12:30
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 50356 steps/s (collection: 1.838s, learning 0.115s)
             Mean action noise std: 1.69
          Mean value_function loss: 68.8201
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.6694
                       Mean reward: 53.83
               Mean episode length: 71.83
    Episode_Reward/reaching_object: 0.2415
     Episode_Reward/lifting_object: 11.6149
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 52.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 1.95s
                      Time elapsed: 00:12:32
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 50390 steps/s (collection: 1.843s, learning 0.107s)
             Mean action noise std: 1.69
          Mean value_function loss: 73.4423
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.6731
                       Mean reward: 62.62
               Mean episode length: 78.26
    Episode_Reward/reaching_object: 0.2449
     Episode_Reward/lifting_object: 11.9178
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 52.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.95s
                      Time elapsed: 00:12:34
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 51264 steps/s (collection: 1.800s, learning 0.117s)
             Mean action noise std: 1.69
          Mean value_function loss: 86.4356
               Mean surrogate loss: 0.0123
                 Mean entropy loss: 34.6767
                       Mean reward: 59.63
               Mean episode length: 73.78
    Episode_Reward/reaching_object: 0.2519
     Episode_Reward/lifting_object: 12.1932
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 52.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.92s
                      Time elapsed: 00:12:36
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 51606 steps/s (collection: 1.789s, learning 0.116s)
             Mean action noise std: 1.69
          Mean value_function loss: 78.7584
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.6777
                       Mean reward: 61.88
               Mean episode length: 80.43
    Episode_Reward/reaching_object: 0.2497
     Episode_Reward/lifting_object: 11.5581
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 52.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 1.90s
                      Time elapsed: 00:12:38
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 50747 steps/s (collection: 1.823s, learning 0.114s)
             Mean action noise std: 1.69
          Mean value_function loss: 70.7793
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 34.6804
                       Mean reward: 57.68
               Mean episode length: 75.94
    Episode_Reward/reaching_object: 0.2545
     Episode_Reward/lifting_object: 12.2223
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 53.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.94s
                      Time elapsed: 00:12:40
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 51500 steps/s (collection: 1.783s, learning 0.126s)
             Mean action noise std: 1.69
          Mean value_function loss: 72.3194
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.6818
                       Mean reward: 64.45
               Mean episode length: 78.08
    Episode_Reward/reaching_object: 0.2485
     Episode_Reward/lifting_object: 11.7420
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.91s
                      Time elapsed: 00:12:42
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 51085 steps/s (collection: 1.802s, learning 0.122s)
             Mean action noise std: 1.69
          Mean value_function loss: 75.1820
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.6827
                       Mean reward: 64.19
               Mean episode length: 78.69
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: 12.0995
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 1.92s
                      Time elapsed: 00:12:44
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 51743 steps/s (collection: 1.799s, learning 0.101s)
             Mean action noise std: 1.69
          Mean value_function loss: 71.2934
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 34.6840
                       Mean reward: 71.69
               Mean episode length: 81.70
    Episode_Reward/reaching_object: 0.2458
     Episode_Reward/lifting_object: 12.0877
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.90s
                      Time elapsed: 00:12:46
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 51408 steps/s (collection: 1.794s, learning 0.118s)
             Mean action noise std: 1.69
          Mean value_function loss: 67.8725
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.6856
                       Mean reward: 68.19
               Mean episode length: 81.21
    Episode_Reward/reaching_object: 0.2578
     Episode_Reward/lifting_object: 12.7252
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 51.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 1.91s
                      Time elapsed: 00:12:47
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 51396 steps/s (collection: 1.804s, learning 0.109s)
             Mean action noise std: 1.69
          Mean value_function loss: 68.1568
               Mean surrogate loss: 0.0166
                 Mean entropy loss: 34.6885
                       Mean reward: 66.20
               Mean episode length: 79.43
    Episode_Reward/reaching_object: 0.2531
     Episode_Reward/lifting_object: 12.6465
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 53.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.91s
                      Time elapsed: 00:12:49
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 51506 steps/s (collection: 1.801s, learning 0.107s)
             Mean action noise std: 1.69
          Mean value_function loss: 61.3248
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 34.6890
                       Mean reward: 59.31
               Mean episode length: 80.45
    Episode_Reward/reaching_object: 0.2535
     Episode_Reward/lifting_object: 12.6273
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 49.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.91s
                      Time elapsed: 00:12:51
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 51941 steps/s (collection: 1.807s, learning 0.086s)
             Mean action noise std: 1.69
          Mean value_function loss: 66.8451
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 34.6891
                       Mean reward: 65.77
               Mean episode length: 77.72
    Episode_Reward/reaching_object: 0.2501
     Episode_Reward/lifting_object: 12.6701
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.89s
                      Time elapsed: 00:12:53
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 51171 steps/s (collection: 1.808s, learning 0.113s)
             Mean action noise std: 1.69
          Mean value_function loss: 74.8142
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 34.6891
                       Mean reward: 61.94
               Mean episode length: 74.97
    Episode_Reward/reaching_object: 0.2593
     Episode_Reward/lifting_object: 12.8977
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.92s
                      Time elapsed: 00:12:55
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 50530 steps/s (collection: 1.852s, learning 0.094s)
             Mean action noise std: 1.69
          Mean value_function loss: 64.9513
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 34.6891
                       Mean reward: 60.74
               Mean episode length: 81.57
    Episode_Reward/reaching_object: 0.2546
     Episode_Reward/lifting_object: 12.4429
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.95s
                      Time elapsed: 00:12:57
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 52670 steps/s (collection: 1.779s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 72.0535
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 34.6891
                       Mean reward: 69.04
               Mean episode length: 81.51
    Episode_Reward/reaching_object: 0.2568
     Episode_Reward/lifting_object: 13.0804
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 50.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.87s
                      Time elapsed: 00:12:59
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 51878 steps/s (collection: 1.808s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 63.4341
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.6892
                       Mean reward: 65.47
               Mean episode length: 78.32
    Episode_Reward/reaching_object: 0.2483
     Episode_Reward/lifting_object: 12.5393
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.89s
                      Time elapsed: 00:13:01
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 52333 steps/s (collection: 1.793s, learning 0.086s)
             Mean action noise std: 1.69
          Mean value_function loss: 61.9752
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 34.6892
                       Mean reward: 64.21
               Mean episode length: 83.19
    Episode_Reward/reaching_object: 0.2528
     Episode_Reward/lifting_object: 12.4220
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.88s
                      Time elapsed: 00:13:03
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 51122 steps/s (collection: 1.834s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 68.4688
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.6894
                       Mean reward: 63.69
               Mean episode length: 78.40
    Episode_Reward/reaching_object: 0.2541
     Episode_Reward/lifting_object: 12.8291
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 54.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.92s
                      Time elapsed: 00:13:05
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 51810 steps/s (collection: 1.789s, learning 0.109s)
             Mean action noise std: 1.69
          Mean value_function loss: 65.5218
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.6882
                       Mean reward: 64.24
               Mean episode length: 81.00
    Episode_Reward/reaching_object: 0.2533
     Episode_Reward/lifting_object: 12.6504
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 51.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.90s
                      Time elapsed: 00:13:06
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 50859 steps/s (collection: 1.843s, learning 0.090s)
             Mean action noise std: 1.69
          Mean value_function loss: 75.1254
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.6854
                       Mean reward: 61.51
               Mean episode length: 69.01
    Episode_Reward/reaching_object: 0.2513
     Episode_Reward/lifting_object: 12.4505
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 52.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.93s
                      Time elapsed: 00:13:08
                               ETA: 00:58:32

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 51691 steps/s (collection: 1.794s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 68.4434
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.6849
                       Mean reward: 62.34
               Mean episode length: 76.64
    Episode_Reward/reaching_object: 0.2534
     Episode_Reward/lifting_object: 12.4726
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 51.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.90s
                      Time elapsed: 00:13:10
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 51445 steps/s (collection: 1.809s, learning 0.102s)
             Mean action noise std: 1.69
          Mean value_function loss: 67.8876
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.6867
                       Mean reward: 60.93
               Mean episode length: 73.33
    Episode_Reward/reaching_object: 0.2570
     Episode_Reward/lifting_object: 12.5419
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.91s
                      Time elapsed: 00:13:12
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 50925 steps/s (collection: 1.827s, learning 0.104s)
             Mean action noise std: 1.69
          Mean value_function loss: 67.3174
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.6887
                       Mean reward: 66.16
               Mean episode length: 79.33
    Episode_Reward/reaching_object: 0.2572
     Episode_Reward/lifting_object: 12.6817
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 53.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.93s
                      Time elapsed: 00:13:14
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 51947 steps/s (collection: 1.803s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 85.7022
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.6920
                       Mean reward: 65.21
               Mean episode length: 78.99
    Episode_Reward/reaching_object: 0.2524
     Episode_Reward/lifting_object: 12.6545
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 54.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.89s
                      Time elapsed: 00:13:16
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 51174 steps/s (collection: 1.825s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 73.1547
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.6937
                       Mean reward: 66.93
               Mean episode length: 76.52
    Episode_Reward/reaching_object: 0.2577
     Episode_Reward/lifting_object: 12.6271
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.92s
                      Time elapsed: 00:13:18
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 50376 steps/s (collection: 1.863s, learning 0.088s)
             Mean action noise std: 1.69
          Mean value_function loss: 78.0431
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.6948
                       Mean reward: 58.70
               Mean episode length: 76.30
    Episode_Reward/reaching_object: 0.2606
     Episode_Reward/lifting_object: 12.8843
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 51.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.95s
                      Time elapsed: 00:13:20
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 50973 steps/s (collection: 1.824s, learning 0.105s)
             Mean action noise std: 1.69
          Mean value_function loss: 75.5896
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.6980
                       Mean reward: 60.34
               Mean episode length: 81.31
    Episode_Reward/reaching_object: 0.2705
     Episode_Reward/lifting_object: 13.1592
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 1.93s
                      Time elapsed: 00:13:22
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 51200 steps/s (collection: 1.823s, learning 0.097s)
             Mean action noise std: 1.69
          Mean value_function loss: 70.8740
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.7019
                       Mean reward: 71.00
               Mean episode length: 83.71
    Episode_Reward/reaching_object: 0.2728
     Episode_Reward/lifting_object: 13.3693
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 50.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.92s
                      Time elapsed: 00:13:24
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 50714 steps/s (collection: 1.822s, learning 0.117s)
             Mean action noise std: 1.69
          Mean value_function loss: 70.4211
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.7070
                       Mean reward: 69.17
               Mean episode length: 81.05
    Episode_Reward/reaching_object: 0.2736
     Episode_Reward/lifting_object: 13.0912
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 51.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.94s
                      Time elapsed: 00:13:26
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 51528 steps/s (collection: 1.786s, learning 0.122s)
             Mean action noise std: 1.69
          Mean value_function loss: 66.0167
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.7089
                       Mean reward: 69.16
               Mean episode length: 81.11
    Episode_Reward/reaching_object: 0.2688
     Episode_Reward/lifting_object: 13.1298
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 49.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.91s
                      Time elapsed: 00:13:28
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 50849 steps/s (collection: 1.817s, learning 0.117s)
             Mean action noise std: 1.69
          Mean value_function loss: 76.8841
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 34.7098
                       Mean reward: 71.56
               Mean episode length: 83.63
    Episode_Reward/reaching_object: 0.2746
     Episode_Reward/lifting_object: 13.5903
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 1.93s
                      Time elapsed: 00:13:30
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 51074 steps/s (collection: 1.816s, learning 0.109s)
             Mean action noise std: 1.69
          Mean value_function loss: 67.2501
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.7127
                       Mean reward: 69.17
               Mean episode length: 77.52
    Episode_Reward/reaching_object: 0.2776
     Episode_Reward/lifting_object: 13.5556
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 51.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.92s
                      Time elapsed: 00:13:31
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 51609 steps/s (collection: 1.792s, learning 0.113s)
             Mean action noise std: 1.70
          Mean value_function loss: 84.0967
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.7174
                       Mean reward: 71.62
               Mean episode length: 79.04
    Episode_Reward/reaching_object: 0.2702
     Episode_Reward/lifting_object: 13.7709
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 52.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.90s
                      Time elapsed: 00:13:33
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 51025 steps/s (collection: 1.818s, learning 0.109s)
             Mean action noise std: 1.70
          Mean value_function loss: 71.7953
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.7178
                       Mean reward: 67.85
               Mean episode length: 79.74
    Episode_Reward/reaching_object: 0.2651
     Episode_Reward/lifting_object: 13.2824
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 1.93s
                      Time elapsed: 00:13:35
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 51154 steps/s (collection: 1.807s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 75.7924
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.7181
                       Mean reward: 66.73
               Mean episode length: 74.10
    Episode_Reward/reaching_object: 0.2661
     Episode_Reward/lifting_object: 13.2474
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 53.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.92s
                      Time elapsed: 00:13:37
                               ETA: 00:57:45

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 50707 steps/s (collection: 1.784s, learning 0.154s)
             Mean action noise std: 1.70
          Mean value_function loss: 71.1452
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.7192
                       Mean reward: 69.77
               Mean episode length: 78.17
    Episode_Reward/reaching_object: 0.2573
     Episode_Reward/lifting_object: 13.3528
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 52.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 1.94s
                      Time elapsed: 00:13:39
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 44908 steps/s (collection: 2.031s, learning 0.158s)
             Mean action noise std: 1.70
          Mean value_function loss: 84.6371
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.7196
                       Mean reward: 68.10
               Mean episode length: 78.21
    Episode_Reward/reaching_object: 0.2562
     Episode_Reward/lifting_object: 13.0925
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 54.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.19s
                      Time elapsed: 00:13:41
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 46061 steps/s (collection: 2.014s, learning 0.120s)
             Mean action noise std: 1.70
          Mean value_function loss: 79.7109
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.7207
                       Mean reward: 64.86
               Mean episode length: 81.65
    Episode_Reward/reaching_object: 0.2577
     Episode_Reward/lifting_object: 13.3334
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 55.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.13s
                      Time elapsed: 00:13:43
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 48036 steps/s (collection: 1.934s, learning 0.112s)
             Mean action noise std: 1.70
          Mean value_function loss: 83.4479
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.7235
                       Mean reward: 65.81
               Mean episode length: 75.03
    Episode_Reward/reaching_object: 0.2518
     Episode_Reward/lifting_object: 13.0106
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 56.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.05s
                      Time elapsed: 00:13:46
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.935s, learning 0.104s)
             Mean action noise std: 1.70
          Mean value_function loss: 77.9963
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.7258
                       Mean reward: 67.00
               Mean episode length: 81.01
    Episode_Reward/reaching_object: 0.2467
     Episode_Reward/lifting_object: 12.8486
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.04s
                      Time elapsed: 00:13:48
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 50731 steps/s (collection: 1.807s, learning 0.131s)
             Mean action noise std: 1.70
          Mean value_function loss: 69.9715
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.7258
                       Mean reward: 65.66
               Mean episode length: 75.59
    Episode_Reward/reaching_object: 0.2451
     Episode_Reward/lifting_object: 13.0277
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 54.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 1.94s
                      Time elapsed: 00:13:49
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 48359 steps/s (collection: 1.916s, learning 0.117s)
             Mean action noise std: 1.70
          Mean value_function loss: 70.2422
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 34.7259
                       Mean reward: 68.53
               Mean episode length: 74.67
    Episode_Reward/reaching_object: 0.2473
     Episode_Reward/lifting_object: 13.1157
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.03s
                      Time elapsed: 00:13:52
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 50835 steps/s (collection: 1.829s, learning 0.104s)
             Mean action noise std: 1.70
          Mean value_function loss: 77.6779
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.7258
                       Mean reward: 65.15
               Mean episode length: 73.64
    Episode_Reward/reaching_object: 0.2496
     Episode_Reward/lifting_object: 13.1236
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 1.93s
                      Time elapsed: 00:13:53
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 50693 steps/s (collection: 1.831s, learning 0.108s)
             Mean action noise std: 1.70
          Mean value_function loss: 85.3737
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.7277
                       Mean reward: 69.29
               Mean episode length: 77.29
    Episode_Reward/reaching_object: 0.2482
     Episode_Reward/lifting_object: 12.9581
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 1.94s
                      Time elapsed: 00:13:55
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 44864 steps/s (collection: 2.045s, learning 0.147s)
             Mean action noise std: 1.70
          Mean value_function loss: 85.2379
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.7326
                       Mean reward: 67.02
               Mean episode length: 74.52
    Episode_Reward/reaching_object: 0.2453
     Episode_Reward/lifting_object: 13.2204
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 56.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.19s
                      Time elapsed: 00:13:58
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 49366 steps/s (collection: 1.882s, learning 0.109s)
             Mean action noise std: 1.70
          Mean value_function loss: 72.5248
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.7373
                       Mean reward: 66.21
               Mean episode length: 76.28
    Episode_Reward/reaching_object: 0.2425
     Episode_Reward/lifting_object: 12.7078
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 1.99s
                      Time elapsed: 00:14:00
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 49516 steps/s (collection: 1.852s, learning 0.134s)
             Mean action noise std: 1.70
          Mean value_function loss: 80.7499
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7403
                       Mean reward: 70.72
               Mean episode length: 75.65
    Episode_Reward/reaching_object: 0.2460
     Episode_Reward/lifting_object: 12.8958
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 53.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 1.99s
                      Time elapsed: 00:14:02
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 48408 steps/s (collection: 1.913s, learning 0.118s)
             Mean action noise std: 1.70
          Mean value_function loss: 73.4883
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.7409
                       Mean reward: 72.61
               Mean episode length: 75.71
    Episode_Reward/reaching_object: 0.2500
     Episode_Reward/lifting_object: 13.3669
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.03s
                      Time elapsed: 00:14:04
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 50010 steps/s (collection: 1.847s, learning 0.119s)
             Mean action noise std: 1.70
          Mean value_function loss: 71.6855
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.7429
                       Mean reward: 75.46
               Mean episode length: 78.89
    Episode_Reward/reaching_object: 0.2538
     Episode_Reward/lifting_object: 13.6452
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 52.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 1.97s
                      Time elapsed: 00:14:06
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 49136 steps/s (collection: 1.883s, learning 0.118s)
             Mean action noise std: 1.70
          Mean value_function loss: 78.3218
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.7444
                       Mean reward: 75.48
               Mean episode length: 76.08
    Episode_Reward/reaching_object: 0.2648
     Episode_Reward/lifting_object: 14.1809
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 52.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.00s
                      Time elapsed: 00:14:08
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 48911 steps/s (collection: 1.891s, learning 0.119s)
             Mean action noise std: 1.70
          Mean value_function loss: 77.1464
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.7459
                       Mean reward: 71.62
               Mean episode length: 76.31
    Episode_Reward/reaching_object: 0.2592
     Episode_Reward/lifting_object: 13.9418
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.01s
                      Time elapsed: 00:14:10
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 49319 steps/s (collection: 1.879s, learning 0.114s)
             Mean action noise std: 1.70
          Mean value_function loss: 81.8080
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.7472
                       Mean reward: 70.76
               Mean episode length: 77.52
    Episode_Reward/reaching_object: 0.2595
     Episode_Reward/lifting_object: 14.0849
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 51.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 1.99s
                      Time elapsed: 00:14:12
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 50033 steps/s (collection: 1.858s, learning 0.107s)
             Mean action noise std: 1.70
          Mean value_function loss: 73.3650
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.7508
                       Mean reward: 71.42
               Mean episode length: 72.80
    Episode_Reward/reaching_object: 0.2550
     Episode_Reward/lifting_object: 13.8654
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 55.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 1.96s
                      Time elapsed: 00:14:14
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 48772 steps/s (collection: 1.852s, learning 0.164s)
             Mean action noise std: 1.70
          Mean value_function loss: 70.8417
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 34.7524
                       Mean reward: 69.62
               Mean episode length: 74.45
    Episode_Reward/reaching_object: 0.2581
     Episode_Reward/lifting_object: 13.7343
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 52.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.02s
                      Time elapsed: 00:14:16
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 48749 steps/s (collection: 1.896s, learning 0.120s)
             Mean action noise std: 1.70
          Mean value_function loss: 79.2668
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.7536
                       Mean reward: 63.06
               Mean episode length: 74.30
    Episode_Reward/reaching_object: 0.2555
     Episode_Reward/lifting_object: 13.6525
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.02s
                      Time elapsed: 00:14:18
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 50301 steps/s (collection: 1.854s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 72.6377
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.7560
                       Mean reward: 68.14
               Mean episode length: 76.86
    Episode_Reward/reaching_object: 0.2522
     Episode_Reward/lifting_object: 13.6322
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 54.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.95s
                      Time elapsed: 00:14:20
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 49367 steps/s (collection: 1.877s, learning 0.114s)
             Mean action noise std: 1.70
          Mean value_function loss: 76.0143
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.7580
                       Mean reward: 73.83
               Mean episode length: 76.44
    Episode_Reward/reaching_object: 0.2555
     Episode_Reward/lifting_object: 14.1817
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 57.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.99s
                      Time elapsed: 00:14:22
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 49418 steps/s (collection: 1.898s, learning 0.091s)
             Mean action noise std: 1.70
          Mean value_function loss: 74.0601
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.7608
                       Mean reward: 68.45
               Mean episode length: 75.21
    Episode_Reward/reaching_object: 0.2501
     Episode_Reward/lifting_object: 13.8327
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 1.99s
                      Time elapsed: 00:14:23
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 49312 steps/s (collection: 1.878s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 78.6407
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.7651
                       Mean reward: 72.00
               Mean episode length: 76.88
    Episode_Reward/reaching_object: 0.2551
     Episode_Reward/lifting_object: 13.9049
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 54.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 1.99s
                      Time elapsed: 00:14:25
                               ETA: 00:56:42

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 49460 steps/s (collection: 1.890s, learning 0.097s)
             Mean action noise std: 1.70
          Mean value_function loss: 84.5733
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 34.7683
                       Mean reward: 70.23
               Mean episode length: 75.77
    Episode_Reward/reaching_object: 0.2495
     Episode_Reward/lifting_object: 13.9298
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 55.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 1.99s
                      Time elapsed: 00:14:27
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 50320 steps/s (collection: 1.868s, learning 0.086s)
             Mean action noise std: 1.70
          Mean value_function loss: 80.7631
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.7695
                       Mean reward: 68.51
               Mean episode length: 74.93
    Episode_Reward/reaching_object: 0.2494
     Episode_Reward/lifting_object: 13.6929
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 56.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 1.95s
                      Time elapsed: 00:14:29
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 48316 steps/s (collection: 1.866s, learning 0.169s)
             Mean action noise std: 1.70
          Mean value_function loss: 90.9029
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7711
                       Mean reward: 69.84
               Mean episode length: 75.02
    Episode_Reward/reaching_object: 0.2494
     Episode_Reward/lifting_object: 13.8939
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 55.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.03s
                      Time elapsed: 00:14:31
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 48551 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 85.6866
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.7731
                       Mean reward: 72.54
               Mean episode length: 74.35
    Episode_Reward/reaching_object: 0.2513
     Episode_Reward/lifting_object: 13.7219
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 55.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.02s
                      Time elapsed: 00:14:33
                               ETA: 00:56:31

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 50241 steps/s (collection: 1.842s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 85.2679
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.7745
                       Mean reward: 72.26
               Mean episode length: 73.80
    Episode_Reward/reaching_object: 0.2487
     Episode_Reward/lifting_object: 13.6098
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 55.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 1.96s
                      Time elapsed: 00:14:35
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 50032 steps/s (collection: 1.864s, learning 0.101s)
             Mean action noise std: 1.70
          Mean value_function loss: 79.5616
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.7775
                       Mean reward: 73.44
               Mean episode length: 77.01
    Episode_Reward/reaching_object: 0.2475
     Episode_Reward/lifting_object: 13.7235
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 54.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 1.96s
                      Time elapsed: 00:14:37
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 50401 steps/s (collection: 1.837s, learning 0.113s)
             Mean action noise std: 1.70
          Mean value_function loss: 90.0415
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 34.7798
                       Mean reward: 71.50
               Mean episode length: 72.97
    Episode_Reward/reaching_object: 0.2543
     Episode_Reward/lifting_object: 14.0646
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 53.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 1.95s
                      Time elapsed: 00:14:39
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 51155 steps/s (collection: 1.813s, learning 0.108s)
             Mean action noise std: 1.70
          Mean value_function loss: 89.9269
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.7801
                       Mean reward: 74.83
               Mean episode length: 73.10
    Episode_Reward/reaching_object: 0.2582
     Episode_Reward/lifting_object: 14.3667
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 1.92s
                      Time elapsed: 00:14:41
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 50206 steps/s (collection: 1.844s, learning 0.114s)
             Mean action noise std: 1.70
          Mean value_function loss: 88.0406
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.7809
                       Mean reward: 64.09
               Mean episode length: 70.84
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: 14.0087
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 56.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 1.96s
                      Time elapsed: 00:14:43
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 50447 steps/s (collection: 1.862s, learning 0.086s)
             Mean action noise std: 1.70
          Mean value_function loss: 92.4834
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.7815
                       Mean reward: 74.71
               Mean episode length: 72.64
    Episode_Reward/reaching_object: 0.2454
     Episode_Reward/lifting_object: 13.8179
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 1.95s
                      Time elapsed: 00:14:45
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 51390 steps/s (collection: 1.827s, learning 0.086s)
             Mean action noise std: 1.70
          Mean value_function loss: 78.4457
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.7815
                       Mean reward: 69.59
               Mean episode length: 71.69
    Episode_Reward/reaching_object: 0.2505
     Episode_Reward/lifting_object: 13.9158
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 55.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 1.91s
                      Time elapsed: 00:14:47
                               ETA: 00:56:11

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 51027 steps/s (collection: 1.835s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 82.8605
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.7816
                       Mean reward: 73.35
               Mean episode length: 74.65
    Episode_Reward/reaching_object: 0.2520
     Episode_Reward/lifting_object: 14.1584
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 54.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 1.93s
                      Time elapsed: 00:14:49
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 47487 steps/s (collection: 1.922s, learning 0.148s)
             Mean action noise std: 1.70
          Mean value_function loss: 86.2610
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 34.7837
                       Mean reward: 75.10
               Mean episode length: 79.30
    Episode_Reward/reaching_object: 0.2568
     Episode_Reward/lifting_object: 14.4260
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 53.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.07s
                      Time elapsed: 00:14:51
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 48630 steps/s (collection: 1.909s, learning 0.113s)
             Mean action noise std: 1.70
          Mean value_function loss: 90.5110
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.7857
                       Mean reward: 71.95
               Mean episode length: 73.95
    Episode_Reward/reaching_object: 0.2467
     Episode_Reward/lifting_object: 14.0746
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 56.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.02s
                      Time elapsed: 00:14:53
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 50462 steps/s (collection: 1.796s, learning 0.152s)
             Mean action noise std: 1.70
          Mean value_function loss: 88.1835
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.7868
                       Mean reward: 71.17
               Mean episode length: 72.30
    Episode_Reward/reaching_object: 0.2481
     Episode_Reward/lifting_object: 14.2081
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 55.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 1.95s
                      Time elapsed: 00:14:55
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 50294 steps/s (collection: 1.862s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 80.6951
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 34.7871
                       Mean reward: 72.24
               Mean episode length: 70.84
    Episode_Reward/reaching_object: 0.2462
     Episode_Reward/lifting_object: 14.0432
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.95s
                      Time elapsed: 00:14:57
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 49189 steps/s (collection: 1.883s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 82.4705
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.7876
                       Mean reward: 73.35
               Mean episode length: 75.31
    Episode_Reward/reaching_object: 0.2437
     Episode_Reward/lifting_object: 14.1273
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.00s
                      Time elapsed: 00:14:59
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 50436 steps/s (collection: 1.829s, learning 0.121s)
             Mean action noise std: 1.71
          Mean value_function loss: 83.4468
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 34.7892
                       Mean reward: 69.06
               Mean episode length: 72.13
    Episode_Reward/reaching_object: 0.2458
     Episode_Reward/lifting_object: 14.2777
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 56.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 1.95s
                      Time elapsed: 00:15:01
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 50533 steps/s (collection: 1.838s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 83.1589
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7899
                       Mean reward: 75.03
               Mean episode length: 73.41
    Episode_Reward/reaching_object: 0.2495
     Episode_Reward/lifting_object: 14.4210
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 54.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 1.95s
                      Time elapsed: 00:15:03
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 51088 steps/s (collection: 1.816s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.2050
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 34.7895
                       Mean reward: 70.94
               Mean episode length: 72.85
    Episode_Reward/reaching_object: 0.2485
     Episode_Reward/lifting_object: 14.4376
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 1.92s
                      Time elapsed: 00:15:05
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 50919 steps/s (collection: 1.811s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 91.3060
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.7903
                       Mean reward: 79.89
               Mean episode length: 80.69
    Episode_Reward/reaching_object: 0.2528
     Episode_Reward/lifting_object: 14.6153
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 52.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 1.93s
                      Time elapsed: 00:15:07
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 51766 steps/s (collection: 1.786s, learning 0.113s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.8302
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.7910
                       Mean reward: 75.45
               Mean episode length: 77.44
    Episode_Reward/reaching_object: 0.2496
     Episode_Reward/lifting_object: 14.3667
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 54.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 1.90s
                      Time elapsed: 00:15:09
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 51109 steps/s (collection: 1.824s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 85.9985
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.7909
                       Mean reward: 69.10
               Mean episode length: 72.45
    Episode_Reward/reaching_object: 0.2457
     Episode_Reward/lifting_object: 14.2766
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 54.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 1.92s
                      Time elapsed: 00:15:11
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 50492 steps/s (collection: 1.847s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 80.3428
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.7908
                       Mean reward: 71.20
               Mean episode length: 71.02
    Episode_Reward/reaching_object: 0.2409
     Episode_Reward/lifting_object: 14.1684
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 1.95s
                      Time elapsed: 00:15:13
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 50524 steps/s (collection: 1.838s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 82.7792
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.7908
                       Mean reward: 70.13
               Mean episode length: 73.01
    Episode_Reward/reaching_object: 0.2416
     Episode_Reward/lifting_object: 14.3700
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 57.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 1.95s
                      Time elapsed: 00:15:14
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 51070 steps/s (collection: 1.813s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 84.3331
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.7915
                       Mean reward: 74.51
               Mean episode length: 77.36
    Episode_Reward/reaching_object: 0.2415
     Episode_Reward/lifting_object: 14.1755
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 54.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 1.92s
                      Time elapsed: 00:15:16
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 51567 steps/s (collection: 1.795s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.3116
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.7903
                       Mean reward: 71.99
               Mean episode length: 74.03
    Episode_Reward/reaching_object: 0.2407
     Episode_Reward/lifting_object: 14.3316
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 54.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.91s
                      Time elapsed: 00:15:18
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 51237 steps/s (collection: 1.806s, learning 0.113s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.0581
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 34.7903
                       Mean reward: 78.65
               Mean episode length: 79.38
    Episode_Reward/reaching_object: 0.2454
     Episode_Reward/lifting_object: 14.4162
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 55.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.92s
                      Time elapsed: 00:15:20
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 50609 steps/s (collection: 1.832s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.7363
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 34.7907
                       Mean reward: 72.65
               Mean episode length: 73.91
    Episode_Reward/reaching_object: 0.2439
     Episode_Reward/lifting_object: 14.3819
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 52.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 1.94s
                      Time elapsed: 00:15:22
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 50093 steps/s (collection: 1.850s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 82.6048
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 34.7912
                       Mean reward: 76.74
               Mean episode length: 75.95
    Episode_Reward/reaching_object: 0.2526
     Episode_Reward/lifting_object: 14.7641
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 54.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 1.96s
                      Time elapsed: 00:15:24
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 51062 steps/s (collection: 1.810s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.6910
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 34.7920
                       Mean reward: 76.00
               Mean episode length: 76.15
    Episode_Reward/reaching_object: 0.2496
     Episode_Reward/lifting_object: 14.6383
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 1.93s
                      Time elapsed: 00:15:26
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 50117 steps/s (collection: 1.836s, learning 0.125s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.7088
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.7927
                       Mean reward: 70.44
               Mean episode length: 73.44
    Episode_Reward/reaching_object: 0.2466
     Episode_Reward/lifting_object: 14.5788
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 52.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 1.96s
                      Time elapsed: 00:15:28
                               ETA: 00:55:13

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 51070 steps/s (collection: 1.820s, learning 0.105s)
             Mean action noise std: 1.71
          Mean value_function loss: 82.3423
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.7931
                       Mean reward: 76.16
               Mean episode length: 74.26
    Episode_Reward/reaching_object: 0.2530
     Episode_Reward/lifting_object: 14.7807
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 51.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 1.92s
                      Time elapsed: 00:15:30
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 51482 steps/s (collection: 1.782s, learning 0.128s)
             Mean action noise std: 1.71
          Mean value_function loss: 84.1554
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.7936
                       Mean reward: 77.88
               Mean episode length: 79.07
    Episode_Reward/reaching_object: 0.2647
     Episode_Reward/lifting_object: 15.2939
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 49.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 1.91s
                      Time elapsed: 00:15:32
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 49442 steps/s (collection: 1.871s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.3181
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.7942
                       Mean reward: 69.70
               Mean episode length: 80.65
    Episode_Reward/reaching_object: 0.2655
     Episode_Reward/lifting_object: 15.2455
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.99s
                      Time elapsed: 00:15:34
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 51157 steps/s (collection: 1.813s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 95.8406
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.7943
                       Mean reward: 75.68
               Mean episode length: 80.67
    Episode_Reward/reaching_object: 0.2725
     Episode_Reward/lifting_object: 15.6977
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 1.92s
                      Time elapsed: 00:15:36
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 50697 steps/s (collection: 1.811s, learning 0.128s)
             Mean action noise std: 1.71
          Mean value_function loss: 85.7450
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.7951
                       Mean reward: 84.84
               Mean episode length: 82.57
    Episode_Reward/reaching_object: 0.2748
     Episode_Reward/lifting_object: 15.7403
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 49.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 1.94s
                      Time elapsed: 00:15:38
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 51349 steps/s (collection: 1.798s, learning 0.116s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.7879
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.7988
                       Mean reward: 76.58
               Mean episode length: 77.21
    Episode_Reward/reaching_object: 0.2661
     Episode_Reward/lifting_object: 15.4967
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 52.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 1.91s
                      Time elapsed: 00:15:40
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 51291 steps/s (collection: 1.806s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 83.9138
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.8004
                       Mean reward: 83.20
               Mean episode length: 81.13
    Episode_Reward/reaching_object: 0.2683
     Episode_Reward/lifting_object: 15.7387
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 52.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 1.92s
                      Time elapsed: 00:15:42
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 51863 steps/s (collection: 1.799s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 81.6951
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.8030
                       Mean reward: 73.46
               Mean episode length: 75.36
    Episode_Reward/reaching_object: 0.2596
     Episode_Reward/lifting_object: 15.2485
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 52.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 1.90s
                      Time elapsed: 00:15:43
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 51085 steps/s (collection: 1.813s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.6690
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.8034
                       Mean reward: 75.81
               Mean episode length: 78.35
    Episode_Reward/reaching_object: 0.2651
     Episode_Reward/lifting_object: 15.4486
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 49.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 1.92s
                      Time elapsed: 00:15:45
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 51042 steps/s (collection: 1.815s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 85.8079
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 34.8024
                       Mean reward: 78.48
               Mean episode length: 75.39
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: 15.8414
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 50.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 1.93s
                      Time elapsed: 00:15:47
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 51825 steps/s (collection: 1.784s, learning 0.113s)
             Mean action noise std: 1.71
          Mean value_function loss: 83.8980
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.8028
                       Mean reward: 82.36
               Mean episode length: 81.76
    Episode_Reward/reaching_object: 0.2722
     Episode_Reward/lifting_object: 16.2004
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 49.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 1.90s
                      Time elapsed: 00:15:49
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 51044 steps/s (collection: 1.818s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.9669
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.8040
                       Mean reward: 84.39
               Mean episode length: 82.21
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 16.4258
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 1.93s
                      Time elapsed: 00:15:51
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 52237 steps/s (collection: 1.775s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 84.5984
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.8039
                       Mean reward: 75.96
               Mean episode length: 73.44
    Episode_Reward/reaching_object: 0.2708
     Episode_Reward/lifting_object: 15.9411
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 51.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 1.88s
                      Time elapsed: 00:15:53
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 51245 steps/s (collection: 1.810s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.3017
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.8042
                       Mean reward: 84.03
               Mean episode length: 79.76
    Episode_Reward/reaching_object: 0.2744
     Episode_Reward/lifting_object: 16.2848
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 1.92s
                      Time elapsed: 00:15:55
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 49454 steps/s (collection: 1.869s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.8287
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 34.8049
                       Mean reward: 82.99
               Mean episode length: 83.81
    Episode_Reward/reaching_object: 0.2769
     Episode_Reward/lifting_object: 16.2824
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 49.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 1.99s
                      Time elapsed: 00:15:57
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 50101 steps/s (collection: 1.854s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 86.5306
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.8052
                       Mean reward: 84.78
               Mean episode length: 83.95
    Episode_Reward/reaching_object: 0.2760
     Episode_Reward/lifting_object: 16.2804
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 1.96s
                      Time elapsed: 00:15:59
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 52451 steps/s (collection: 1.787s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.5304
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.8063
                       Mean reward: 86.23
               Mean episode length: 86.20
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 16.4911
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 1.87s
                      Time elapsed: 00:16:01
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 51209 steps/s (collection: 1.797s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.9419
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.8079
                       Mean reward: 83.50
               Mean episode length: 79.24
    Episode_Reward/reaching_object: 0.2878
     Episode_Reward/lifting_object: 16.9769
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 48.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 1.92s
                      Time elapsed: 00:16:03
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 51776 steps/s (collection: 1.788s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.6676
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 34.8089
                       Mean reward: 87.55
               Mean episode length: 83.26
    Episode_Reward/reaching_object: 0.2832
     Episode_Reward/lifting_object: 16.4368
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 48.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.90s
                      Time elapsed: 00:16:05
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 51392 steps/s (collection: 1.796s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 99.3313
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.8101
                       Mean reward: 84.07
               Mean episode length: 84.45
    Episode_Reward/reaching_object: 0.2892
     Episode_Reward/lifting_object: 16.9704
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 52.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 1.91s
                      Time elapsed: 00:16:06
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 50674 steps/s (collection: 1.820s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.6896
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 34.8116
                       Mean reward: 81.13
               Mean episode length: 82.73
    Episode_Reward/reaching_object: 0.2760
     Episode_Reward/lifting_object: 16.1960
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 50.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 1.94s
                      Time elapsed: 00:16:08
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 50925 steps/s (collection: 1.809s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.8220
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.8131
                       Mean reward: 88.63
               Mean episode length: 85.53
    Episode_Reward/reaching_object: 0.2852
     Episode_Reward/lifting_object: 17.0806
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 1.93s
                      Time elapsed: 00:16:10
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 46866 steps/s (collection: 1.936s, learning 0.162s)
             Mean action noise std: 1.71
          Mean value_function loss: 91.4117
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8151
                       Mean reward: 86.14
               Mean episode length: 85.97
    Episode_Reward/reaching_object: 0.2921
     Episode_Reward/lifting_object: 17.0743
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.10s
                      Time elapsed: 00:16:12
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 48514 steps/s (collection: 1.917s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.7082
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.8154
                       Mean reward: 90.77
               Mean episode length: 88.88
    Episode_Reward/reaching_object: 0.3009
     Episode_Reward/lifting_object: 17.3944
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 43.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.03s
                      Time elapsed: 00:16:14
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 51530 steps/s (collection: 1.794s, learning 0.114s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.9990
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.8150
                       Mean reward: 96.95
               Mean episode length: 91.27
    Episode_Reward/reaching_object: 0.3041
     Episode_Reward/lifting_object: 17.8582
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 43.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 1.91s
                      Time elapsed: 00:16:16
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 51474 steps/s (collection: 1.795s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 86.3065
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.8163
                       Mean reward: 97.07
               Mean episode length: 97.67
    Episode_Reward/reaching_object: 0.3157
     Episode_Reward/lifting_object: 17.9141
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 45.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 1.91s
                      Time elapsed: 00:16:18
                               ETA: 00:54:02

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 51703 steps/s (collection: 1.800s, learning 0.101s)
             Mean action noise std: 1.71
          Mean value_function loss: 94.7897
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.8184
                       Mean reward: 97.78
               Mean episode length: 95.87
    Episode_Reward/reaching_object: 0.3260
     Episode_Reward/lifting_object: 18.8539
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 42.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 1.90s
                      Time elapsed: 00:16:20
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 51498 steps/s (collection: 1.800s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.8961
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 34.8209
                       Mean reward: 95.38
               Mean episode length: 91.61
    Episode_Reward/reaching_object: 0.3307
     Episode_Reward/lifting_object: 18.9486
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 42.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 1.91s
                      Time elapsed: 00:16:22
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 50171 steps/s (collection: 1.833s, learning 0.126s)
             Mean action noise std: 1.71
          Mean value_function loss: 91.3098
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.8221
                       Mean reward: 102.39
               Mean episode length: 93.99
    Episode_Reward/reaching_object: 0.3271
     Episode_Reward/lifting_object: 19.4657
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 1.96s
                      Time elapsed: 00:16:24
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 51143 steps/s (collection: 1.802s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.6660
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.8219
                       Mean reward: 99.93
               Mean episode length: 95.10
    Episode_Reward/reaching_object: 0.3296
     Episode_Reward/lifting_object: 19.4144
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 1.92s
                      Time elapsed: 00:16:26
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 51418 steps/s (collection: 1.800s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.9932
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.8233
                       Mean reward: 92.68
               Mean episode length: 90.77
    Episode_Reward/reaching_object: 0.3234
     Episode_Reward/lifting_object: 18.8602
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 45.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 1.91s
                      Time elapsed: 00:16:28
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 51226 steps/s (collection: 1.802s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.4756
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.8238
                       Mean reward: 99.55
               Mean episode length: 94.09
    Episode_Reward/reaching_object: 0.3191
     Episode_Reward/lifting_object: 18.6672
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 43.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 1.92s
                      Time elapsed: 00:16:30
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 51735 steps/s (collection: 1.801s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.5935
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8251
                       Mean reward: 98.71
               Mean episode length: 92.64
    Episode_Reward/reaching_object: 0.3328
     Episode_Reward/lifting_object: 19.6474
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 1.90s
                      Time elapsed: 00:16:32
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 51798 steps/s (collection: 1.802s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 84.1929
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8243
                       Mean reward: 99.56
               Mean episode length: 93.20
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: 19.1579
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 42.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 1.90s
                      Time elapsed: 00:16:34
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 51248 steps/s (collection: 1.812s, learning 0.106s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.7665
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.8218
                       Mean reward: 97.09
               Mean episode length: 93.68
    Episode_Reward/reaching_object: 0.3248
     Episode_Reward/lifting_object: 19.5331
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 42.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 1.92s
                      Time elapsed: 00:16:36
                               ETA: 00:53:37

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 51309 steps/s (collection: 1.808s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.8693
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.8200
                       Mean reward: 98.04
               Mean episode length: 93.57
    Episode_Reward/reaching_object: 0.3280
     Episode_Reward/lifting_object: 19.6971
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 44.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 1.92s
                      Time elapsed: 00:16:37
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 51119 steps/s (collection: 1.806s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.8951
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.8179
                       Mean reward: 112.95
               Mean episode length: 105.00
    Episode_Reward/reaching_object: 0.3284
     Episode_Reward/lifting_object: 19.8548
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 43.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 1.92s
                      Time elapsed: 00:16:39
                               ETA: 00:53:32

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 50944 steps/s (collection: 1.816s, learning 0.114s)
             Mean action noise std: 1.71
          Mean value_function loss: 97.8181
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.8167
                       Mean reward: 108.73
               Mean episode length: 95.14
    Episode_Reward/reaching_object: 0.3284
     Episode_Reward/lifting_object: 19.8520
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 1.93s
                      Time elapsed: 00:16:41
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 49344 steps/s (collection: 1.863s, learning 0.129s)
             Mean action noise std: 1.71
          Mean value_function loss: 88.0467
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.8160
                       Mean reward: 91.87
               Mean episode length: 87.19
    Episode_Reward/reaching_object: 0.3275
     Episode_Reward/lifting_object: 19.9334
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 1.99s
                      Time elapsed: 00:16:43
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 50398 steps/s (collection: 1.826s, learning 0.124s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.3197
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 34.8147
                       Mean reward: 100.86
               Mean episode length: 93.13
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: 20.5355
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 41.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 1.95s
                      Time elapsed: 00:16:45
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 50544 steps/s (collection: 1.827s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 88.3653
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.8147
                       Mean reward: 110.38
               Mean episode length: 101.14
    Episode_Reward/reaching_object: 0.3365
     Episode_Reward/lifting_object: 20.5345
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 1.94s
                      Time elapsed: 00:16:47
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 51111 steps/s (collection: 1.798s, learning 0.126s)
             Mean action noise std: 1.71
          Mean value_function loss: 88.2578
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.8142
                       Mean reward: 108.08
               Mean episode length: 102.77
    Episode_Reward/reaching_object: 0.3482
     Episode_Reward/lifting_object: 21.2358
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 1.92s
                      Time elapsed: 00:16:49
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 51704 steps/s (collection: 1.797s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.2736
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.8155
                       Mean reward: 112.37
               Mean episode length: 103.73
    Episode_Reward/reaching_object: 0.3477
     Episode_Reward/lifting_object: 21.2893
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 41.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 1.90s
                      Time elapsed: 00:16:51
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 51222 steps/s (collection: 1.798s, learning 0.121s)
             Mean action noise std: 1.71
          Mean value_function loss: 94.6061
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.8172
                       Mean reward: 109.19
               Mean episode length: 99.94
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: 21.6860
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 1.92s
                      Time elapsed: 00:16:53
                               ETA: 00:53:13

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 52039 steps/s (collection: 1.774s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.5208
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.8174
                       Mean reward: 110.86
               Mean episode length: 100.77
    Episode_Reward/reaching_object: 0.3523
     Episode_Reward/lifting_object: 21.4834
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 1.89s
                      Time elapsed: 00:16:55
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 51381 steps/s (collection: 1.795s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.4602
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.8166
                       Mean reward: 105.52
               Mean episode length: 101.56
    Episode_Reward/reaching_object: 0.3510
     Episode_Reward/lifting_object: 21.5759
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 1.91s
                      Time elapsed: 00:16:57
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 51308 steps/s (collection: 1.801s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.4699
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.8163
                       Mean reward: 110.62
               Mean episode length: 101.01
    Episode_Reward/reaching_object: 0.3574
     Episode_Reward/lifting_object: 22.0796
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 1.92s
                      Time elapsed: 00:16:59
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 51263 steps/s (collection: 1.798s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 86.6926
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.8165
                       Mean reward: 121.92
               Mean episode length: 105.68
    Episode_Reward/reaching_object: 0.3711
     Episode_Reward/lifting_object: 22.6419
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 1.92s
                      Time elapsed: 00:17:01
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 50554 steps/s (collection: 1.843s, learning 0.102s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.4135
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 34.8173
                       Mean reward: 110.53
               Mean episode length: 103.23
    Episode_Reward/reaching_object: 0.3604
     Episode_Reward/lifting_object: 22.6784
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 1.94s
                      Time elapsed: 00:17:02
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 50562 steps/s (collection: 1.845s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.3851
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 34.8186
                       Mean reward: 114.64
               Mean episode length: 105.06
    Episode_Reward/reaching_object: 0.3873
     Episode_Reward/lifting_object: 24.1191
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 1.94s
                      Time elapsed: 00:17:04
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 50653 steps/s (collection: 1.826s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 88.9837
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.8188
                       Mean reward: 125.46
               Mean episode length: 110.77
    Episode_Reward/reaching_object: 0.3834
     Episode_Reward/lifting_object: 23.9061
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 1.94s
                      Time elapsed: 00:17:06
                               ETA: 00:52:55

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 51174 steps/s (collection: 1.822s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 91.6589
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.8180
                       Mean reward: 124.72
               Mean episode length: 111.83
    Episode_Reward/reaching_object: 0.3970
     Episode_Reward/lifting_object: 24.4309
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 1.92s
                      Time elapsed: 00:17:08
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 51469 steps/s (collection: 1.792s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 89.0824
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.8166
                       Mean reward: 124.38
               Mean episode length: 115.27
    Episode_Reward/reaching_object: 0.3896
     Episode_Reward/lifting_object: 24.1696
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 1.91s
                      Time elapsed: 00:17:10
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 50907 steps/s (collection: 1.799s, learning 0.133s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.3242
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.8145
                       Mean reward: 135.87
               Mean episode length: 121.88
    Episode_Reward/reaching_object: 0.3988
     Episode_Reward/lifting_object: 25.0204
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 1.93s
                      Time elapsed: 00:17:12
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 50325 steps/s (collection: 1.820s, learning 0.133s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.3550
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.8136
                       Mean reward: 117.62
               Mean episode length: 106.74
    Episode_Reward/reaching_object: 0.3901
     Episode_Reward/lifting_object: 24.3898
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 1.95s
                      Time elapsed: 00:17:14
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 49542 steps/s (collection: 1.864s, learning 0.121s)
             Mean action noise std: 1.71
          Mean value_function loss: 95.4423
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.8129
                       Mean reward: 129.36
               Mean episode length: 116.93
    Episode_Reward/reaching_object: 0.4006
     Episode_Reward/lifting_object: 25.2350
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 1.98s
                      Time elapsed: 00:17:16
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 50992 steps/s (collection: 1.811s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.2236
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 34.8132
                       Mean reward: 119.75
               Mean episode length: 104.79
    Episode_Reward/reaching_object: 0.3977
     Episode_Reward/lifting_object: 24.8580
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 1.93s
                      Time elapsed: 00:17:18
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 50915 steps/s (collection: 1.815s, learning 0.116s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.6696
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.8137
                       Mean reward: 123.23
               Mean episode length: 107.29
    Episode_Reward/reaching_object: 0.3841
     Episode_Reward/lifting_object: 24.3347
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 1.93s
                      Time elapsed: 00:17:20
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 50082 steps/s (collection: 1.844s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 97.5201
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.8120
                       Mean reward: 124.39
               Mean episode length: 109.64
    Episode_Reward/reaching_object: 0.3816
     Episode_Reward/lifting_object: 24.2471
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 1.96s
                      Time elapsed: 00:17:22
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 50214 steps/s (collection: 1.860s, learning 0.097s)
             Mean action noise std: 1.71
          Mean value_function loss: 97.4238
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.8109
                       Mean reward: 118.22
               Mean episode length: 106.54
    Episode_Reward/reaching_object: 0.3879
     Episode_Reward/lifting_object: 24.5332
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 1.96s
                      Time elapsed: 00:17:24
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 49760 steps/s (collection: 1.872s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 91.6262
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 34.8098
                       Mean reward: 132.02
               Mean episode length: 112.42
    Episode_Reward/reaching_object: 0.3822
     Episode_Reward/lifting_object: 24.8303
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 1.98s
                      Time elapsed: 00:17:26
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 50830 steps/s (collection: 1.812s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 94.3532
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 34.8089
                       Mean reward: 129.44
               Mean episode length: 115.47
    Episode_Reward/reaching_object: 0.3882
     Episode_Reward/lifting_object: 25.1301
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 1.93s
                      Time elapsed: 00:17:28
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 51100 steps/s (collection: 1.807s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.3108
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.8077
                       Mean reward: 132.83
               Mean episode length: 109.91
    Episode_Reward/reaching_object: 0.3826
     Episode_Reward/lifting_object: 25.0237
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 1.92s
                      Time elapsed: 00:17:30
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 49731 steps/s (collection: 1.864s, learning 0.113s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.0352
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 34.8066
                       Mean reward: 110.83
               Mean episode length: 99.93
    Episode_Reward/reaching_object: 0.3689
     Episode_Reward/lifting_object: 24.1474
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 1.98s
                      Time elapsed: 00:17:32
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 50448 steps/s (collection: 1.830s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 97.7011
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 34.8074
                       Mean reward: 127.62
               Mean episode length: 109.74
    Episode_Reward/reaching_object: 0.3857
     Episode_Reward/lifting_object: 25.2136
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 1.95s
                      Time elapsed: 00:17:34
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 50982 steps/s (collection: 1.819s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 92.5832
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.8080
                       Mean reward: 133.40
               Mean episode length: 114.50
    Episode_Reward/reaching_object: 0.3846
     Episode_Reward/lifting_object: 25.1615
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 1.93s
                      Time elapsed: 00:17:36
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 50159 steps/s (collection: 1.841s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.2210
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.8084
                       Mean reward: 125.34
               Mean episode length: 107.66
    Episode_Reward/reaching_object: 0.3766
     Episode_Reward/lifting_object: 24.6331
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 1.96s
                      Time elapsed: 00:17:38
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 50513 steps/s (collection: 1.816s, learning 0.130s)
             Mean action noise std: 1.71
          Mean value_function loss: 97.0436
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8083
                       Mean reward: 126.66
               Mean episode length: 105.67
    Episode_Reward/reaching_object: 0.3736
     Episode_Reward/lifting_object: 24.8245
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 1.95s
                      Time elapsed: 00:17:39
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 50785 steps/s (collection: 1.814s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 101.1412
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.8079
                       Mean reward: 121.96
               Mean episode length: 109.47
    Episode_Reward/reaching_object: 0.3750
     Episode_Reward/lifting_object: 24.5834
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 1.94s
                      Time elapsed: 00:17:41
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 51785 steps/s (collection: 1.804s, learning 0.095s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.6568
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.8066
                       Mean reward: 132.13
               Mean episode length: 112.16
    Episode_Reward/reaching_object: 0.3778
     Episode_Reward/lifting_object: 25.0082
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 1.90s
                      Time elapsed: 00:17:43
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 50498 steps/s (collection: 1.807s, learning 0.140s)
             Mean action noise std: 1.71
          Mean value_function loss: 103.5909
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.8064
                       Mean reward: 116.10
               Mean episode length: 108.73
    Episode_Reward/reaching_object: 0.3815
     Episode_Reward/lifting_object: 25.3200
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 1.95s
                      Time elapsed: 00:17:45
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 51283 steps/s (collection: 1.798s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 96.0980
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 34.8062
                       Mean reward: 124.44
               Mean episode length: 107.21
    Episode_Reward/reaching_object: 0.3814
     Episode_Reward/lifting_object: 25.5476
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 1.92s
                      Time elapsed: 00:17:47
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 50594 steps/s (collection: 1.822s, learning 0.121s)
             Mean action noise std: 1.71
          Mean value_function loss: 100.5842
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.8056
                       Mean reward: 121.95
               Mean episode length: 106.08
    Episode_Reward/reaching_object: 0.3745
     Episode_Reward/lifting_object: 25.2155
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.94s
                      Time elapsed: 00:17:49
                               ETA: 00:51:58

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 50208 steps/s (collection: 1.838s, learning 0.120s)
             Mean action noise std: 1.71
          Mean value_function loss: 101.9594
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.8054
                       Mean reward: 132.16
               Mean episode length: 113.38
    Episode_Reward/reaching_object: 0.3766
     Episode_Reward/lifting_object: 24.9405
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 1.96s
                      Time elapsed: 00:17:51
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 50780 steps/s (collection: 1.820s, learning 0.116s)
             Mean action noise std: 1.71
          Mean value_function loss: 103.6064
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.8039
                       Mean reward: 115.65
               Mean episode length: 100.65
    Episode_Reward/reaching_object: 0.3752
     Episode_Reward/lifting_object: 25.1711
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 1.94s
                      Time elapsed: 00:17:53
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 50579 steps/s (collection: 1.823s, learning 0.121s)
             Mean action noise std: 1.71
          Mean value_function loss: 105.3528
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.8018
                       Mean reward: 129.49
               Mean episode length: 109.94
    Episode_Reward/reaching_object: 0.3786
     Episode_Reward/lifting_object: 25.3656
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 1.94s
                      Time elapsed: 00:17:55
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 50449 steps/s (collection: 1.831s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 98.9587
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.8010
                       Mean reward: 132.86
               Mean episode length: 111.31
    Episode_Reward/reaching_object: 0.3898
     Episode_Reward/lifting_object: 25.9997
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 1.95s
                      Time elapsed: 00:17:57
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 49647 steps/s (collection: 1.870s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 101.9339
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.8019
                       Mean reward: 138.53
               Mean episode length: 116.81
    Episode_Reward/reaching_object: 0.4024
     Episode_Reward/lifting_object: 26.9062
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 1.98s
                      Time elapsed: 00:17:59
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 49434 steps/s (collection: 1.865s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 100.2258
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.8039
                       Mean reward: 130.94
               Mean episode length: 108.87
    Episode_Reward/reaching_object: 0.3979
     Episode_Reward/lifting_object: 26.7290
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 1.99s
                      Time elapsed: 00:18:01
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 51050 steps/s (collection: 1.825s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 109.3199
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.8060
                       Mean reward: 147.44
               Mean episode length: 121.78
    Episode_Reward/reaching_object: 0.3988
     Episode_Reward/lifting_object: 26.4592
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 1.93s
                      Time elapsed: 00:18:03
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 49962 steps/s (collection: 1.866s, learning 0.101s)
             Mean action noise std: 1.71
          Mean value_function loss: 98.6315
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.8076
                       Mean reward: 132.71
               Mean episode length: 111.62
    Episode_Reward/reaching_object: 0.4023
     Episode_Reward/lifting_object: 26.6627
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 1.97s
                      Time elapsed: 00:18:05
                               ETA: 00:51:38

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 49997 steps/s (collection: 1.848s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.7715
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 34.8071
                       Mean reward: 144.47
               Mean episode length: 121.81
    Episode_Reward/reaching_object: 0.4275
     Episode_Reward/lifting_object: 28.0232
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 1.97s
                      Time elapsed: 00:18:07
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 50153 steps/s (collection: 1.843s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.4133
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.8076
                       Mean reward: 147.39
               Mean episode length: 129.59
    Episode_Reward/reaching_object: 0.4304
     Episode_Reward/lifting_object: 27.5344
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 1.96s
                      Time elapsed: 00:18:09
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 49343 steps/s (collection: 1.868s, learning 0.125s)
             Mean action noise std: 1.71
          Mean value_function loss: 93.8091
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 34.8072
                       Mean reward: 138.49
               Mean episode length: 120.49
    Episode_Reward/reaching_object: 0.4368
     Episode_Reward/lifting_object: 28.0271
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 1.99s
                      Time elapsed: 00:18:11
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 48833 steps/s (collection: 1.883s, learning 0.130s)
             Mean action noise std: 1.71
          Mean value_function loss: 97.8140
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 34.8078
                       Mean reward: 145.97
               Mean episode length: 124.15
    Episode_Reward/reaching_object: 0.4544
     Episode_Reward/lifting_object: 29.2761
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.01s
                      Time elapsed: 00:18:13
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 49298 steps/s (collection: 1.864s, learning 0.130s)
             Mean action noise std: 1.71
          Mean value_function loss: 94.9733
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 34.8087
                       Mean reward: 143.26
               Mean episode length: 120.67
    Episode_Reward/reaching_object: 0.4496
     Episode_Reward/lifting_object: 28.9492
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 1.99s
                      Time elapsed: 00:18:15
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 50386 steps/s (collection: 1.826s, learning 0.125s)
             Mean action noise std: 1.71
          Mean value_function loss: 95.9203
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.8091
                       Mean reward: 146.50
               Mean episode length: 124.41
    Episode_Reward/reaching_object: 0.4530
     Episode_Reward/lifting_object: 29.1536
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 1.95s
                      Time elapsed: 00:18:17
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 50085 steps/s (collection: 1.840s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 90.2488
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.8106
                       Mean reward: 143.27
               Mean episode length: 129.79
    Episode_Reward/reaching_object: 0.4648
     Episode_Reward/lifting_object: 29.7308
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 1.96s
                      Time elapsed: 00:18:19
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 49702 steps/s (collection: 1.859s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 98.6853
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 34.8127
                       Mean reward: 146.49
               Mean episode length: 125.94
    Episode_Reward/reaching_object: 0.4793
     Episode_Reward/lifting_object: 30.7438
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 1.98s
                      Time elapsed: 00:18:21
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 50165 steps/s (collection: 1.855s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 98.8875
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.8136
                       Mean reward: 145.21
               Mean episode length: 121.86
    Episode_Reward/reaching_object: 0.4666
     Episode_Reward/lifting_object: 29.9878
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 1.96s
                      Time elapsed: 00:18:23
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 50575 steps/s (collection: 1.843s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 103.3712
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 34.8140
                       Mean reward: 157.61
               Mean episode length: 133.66
    Episode_Reward/reaching_object: 0.4600
     Episode_Reward/lifting_object: 29.4152
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 1.94s
                      Time elapsed: 00:18:24
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 49369 steps/s (collection: 1.883s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 97.9537
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 34.8144
                       Mean reward: 146.04
               Mean episode length: 119.39
    Episode_Reward/reaching_object: 0.4503
     Episode_Reward/lifting_object: 29.3632
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 1.99s
                      Time elapsed: 00:18:26
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 50733 steps/s (collection: 1.842s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 102.7990
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 34.8146
                       Mean reward: 147.48
               Mean episode length: 121.54
    Episode_Reward/reaching_object: 0.4463
     Episode_Reward/lifting_object: 29.0302
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 1.94s
                      Time elapsed: 00:18:28
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 50245 steps/s (collection: 1.830s, learning 0.126s)
             Mean action noise std: 1.72
          Mean value_function loss: 99.5347
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.8141
                       Mean reward: 146.31
               Mean episode length: 122.92
    Episode_Reward/reaching_object: 0.4549
     Episode_Reward/lifting_object: 29.5730
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 1.96s
                      Time elapsed: 00:18:30
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 50322 steps/s (collection: 1.824s, learning 0.130s)
             Mean action noise std: 1.72
          Mean value_function loss: 100.1542
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 34.8128
                       Mean reward: 144.88
               Mean episode length: 120.77
    Episode_Reward/reaching_object: 0.4432
     Episode_Reward/lifting_object: 28.8760
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 1.95s
                      Time elapsed: 00:18:32
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 50174 steps/s (collection: 1.840s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 108.9829
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 34.8118
                       Mean reward: 156.90
               Mean episode length: 131.10
    Episode_Reward/reaching_object: 0.4488
     Episode_Reward/lifting_object: 29.2684
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 1.96s
                      Time elapsed: 00:18:34
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 50385 steps/s (collection: 1.834s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 99.5253
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.8117
                       Mean reward: 150.13
               Mean episode length: 125.75
    Episode_Reward/reaching_object: 0.4413
     Episode_Reward/lifting_object: 28.6405
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 1.95s
                      Time elapsed: 00:18:36
                               ETA: 00:50:59

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 49452 steps/s (collection: 1.865s, learning 0.123s)
             Mean action noise std: 1.72
          Mean value_function loss: 100.5478
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.8117
                       Mean reward: 158.09
               Mean episode length: 136.44
    Episode_Reward/reaching_object: 0.4458
     Episode_Reward/lifting_object: 28.9389
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 1.99s
                      Time elapsed: 00:18:38
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 49468 steps/s (collection: 1.859s, learning 0.128s)
             Mean action noise std: 1.72
          Mean value_function loss: 100.7382
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 34.8120
                       Mean reward: 157.23
               Mean episode length: 133.40
    Episode_Reward/reaching_object: 0.4574
     Episode_Reward/lifting_object: 29.6754
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 1.99s
                      Time elapsed: 00:18:40
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 49584 steps/s (collection: 1.863s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 107.9300
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.8127
                       Mean reward: 167.03
               Mean episode length: 135.06
    Episode_Reward/reaching_object: 0.4524
     Episode_Reward/lifting_object: 29.5415
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 1.98s
                      Time elapsed: 00:18:42
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 50122 steps/s (collection: 1.834s, learning 0.127s)
             Mean action noise std: 1.72
          Mean value_function loss: 101.1164
               Mean surrogate loss: 0.0212
                 Mean entropy loss: 34.8138
                       Mean reward: 149.17
               Mean episode length: 124.03
    Episode_Reward/reaching_object: 0.4257
     Episode_Reward/lifting_object: 28.1032
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 1.96s
                      Time elapsed: 00:18:44
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 49282 steps/s (collection: 1.859s, learning 0.136s)
             Mean action noise std: 1.72
          Mean value_function loss: 101.7529
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.8141
                       Mean reward: 148.12
               Mean episode length: 124.25
    Episode_Reward/reaching_object: 0.4554
     Episode_Reward/lifting_object: 30.0023
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 1.99s
                      Time elapsed: 00:18:46
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 49860 steps/s (collection: 1.851s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 102.3413
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.8140
                       Mean reward: 146.87
               Mean episode length: 120.76
    Episode_Reward/reaching_object: 0.4348
     Episode_Reward/lifting_object: 29.0002
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 1.97s
                      Time elapsed: 00:18:48
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 49869 steps/s (collection: 1.853s, learning 0.118s)
             Mean action noise std: 1.72
          Mean value_function loss: 104.8664
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 34.8139
                       Mean reward: 150.74
               Mean episode length: 123.24
    Episode_Reward/reaching_object: 0.4419
     Episode_Reward/lifting_object: 29.2202
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 1.97s
                      Time elapsed: 00:18:50
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 48564 steps/s (collection: 1.900s, learning 0.124s)
             Mean action noise std: 1.72
          Mean value_function loss: 103.7385
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 34.8142
                       Mean reward: 155.77
               Mean episode length: 127.88
    Episode_Reward/reaching_object: 0.4581
     Episode_Reward/lifting_object: 30.5883
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.02s
                      Time elapsed: 00:18:52
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 50019 steps/s (collection: 1.870s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 97.0496
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 34.8148
                       Mean reward: 153.16
               Mean episode length: 123.04
    Episode_Reward/reaching_object: 0.4543
     Episode_Reward/lifting_object: 30.3872
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 1.97s
                      Time elapsed: 00:18:54
                               ETA: 00:50:38

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 48800 steps/s (collection: 1.915s, learning 0.100s)
             Mean action noise std: 1.72
          Mean value_function loss: 98.3598
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 34.8152
                       Mean reward: 150.29
               Mean episode length: 125.51
    Episode_Reward/reaching_object: 0.4552
     Episode_Reward/lifting_object: 30.0611
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.01s
                      Time elapsed: 00:18:56
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 50070 steps/s (collection: 1.868s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 96.9188
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 34.8153
                       Mean reward: 156.26
               Mean episode length: 128.13
    Episode_Reward/reaching_object: 0.4735
     Episode_Reward/lifting_object: 31.0933
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 1.96s
                      Time elapsed: 00:18:58
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 49214 steps/s (collection: 1.880s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 97.3330
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 34.8154
                       Mean reward: 157.39
               Mean episode length: 128.01
    Episode_Reward/reaching_object: 0.4752
     Episode_Reward/lifting_object: 31.3311
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.00s
                      Time elapsed: 00:19:00
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 44442 steps/s (collection: 2.087s, learning 0.125s)
             Mean action noise std: 1.72
          Mean value_function loss: 100.4121
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.8156
                       Mean reward: 148.77
               Mean episode length: 123.94
    Episode_Reward/reaching_object: 0.4727
     Episode_Reward/lifting_object: 30.9720
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.21s
                      Time elapsed: 00:19:02
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 47430 steps/s (collection: 1.977s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 103.0658
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.8166
                       Mean reward: 161.33
               Mean episode length: 138.73
    Episode_Reward/reaching_object: 0.4789
     Episode_Reward/lifting_object: 31.3405
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.07s
                      Time elapsed: 00:19:04
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 45871 steps/s (collection: 2.004s, learning 0.139s)
             Mean action noise std: 1.72
          Mean value_function loss: 103.6549
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 34.8172
                       Mean reward: 153.34
               Mean episode length: 130.33
    Episode_Reward/reaching_object: 0.4698
     Episode_Reward/lifting_object: 30.3740
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.14s
                      Time elapsed: 00:19:06
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 47305 steps/s (collection: 1.946s, learning 0.133s)
             Mean action noise std: 1.72
          Mean value_function loss: 102.2072
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 34.8179
                       Mean reward: 164.21
               Mean episode length: 138.21
    Episode_Reward/reaching_object: 0.4793
     Episode_Reward/lifting_object: 31.0777
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.08s
                      Time elapsed: 00:19:09
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 48178 steps/s (collection: 1.937s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 106.8173
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.8179
                       Mean reward: 148.61
               Mean episode length: 121.29
    Episode_Reward/reaching_object: 0.4788
     Episode_Reward/lifting_object: 31.5557
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.04s
                      Time elapsed: 00:19:11
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 47401 steps/s (collection: 1.932s, learning 0.142s)
             Mean action noise std: 1.72
          Mean value_function loss: 111.5935
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.8169
                       Mean reward: 148.09
               Mean episode length: 126.33
    Episode_Reward/reaching_object: 0.4700
     Episode_Reward/lifting_object: 30.5443
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.07s
                      Time elapsed: 00:19:13
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 48176 steps/s (collection: 1.929s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 104.1234
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.8192
                       Mean reward: 146.67
               Mean episode length: 121.97
    Episode_Reward/reaching_object: 0.4595
     Episode_Reward/lifting_object: 30.0162
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.04s
                      Time elapsed: 00:19:15
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 45268 steps/s (collection: 2.066s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 109.1306
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.8220
                       Mean reward: 148.42
               Mean episode length: 121.25
    Episode_Reward/reaching_object: 0.4659
     Episode_Reward/lifting_object: 30.4844
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.17s
                      Time elapsed: 00:19:17
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 49500 steps/s (collection: 1.878s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 109.2596
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 34.8247
                       Mean reward: 148.80
               Mean episode length: 129.21
    Episode_Reward/reaching_object: 0.4458
     Episode_Reward/lifting_object: 29.0466
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 1.99s
                      Time elapsed: 00:19:19
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 49007 steps/s (collection: 1.889s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 105.2936
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.8264
                       Mean reward: 156.10
               Mean episode length: 131.54
    Episode_Reward/reaching_object: 0.4564
     Episode_Reward/lifting_object: 29.4420
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.01s
                      Time elapsed: 00:19:21
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 47979 steps/s (collection: 1.948s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 104.4219
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.8274
                       Mean reward: 155.27
               Mean episode length: 128.45
    Episode_Reward/reaching_object: 0.4678
     Episode_Reward/lifting_object: 30.7019
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.05s
                      Time elapsed: 00:19:23
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 49332 steps/s (collection: 1.895s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 102.1119
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 34.8303
                       Mean reward: 152.00
               Mean episode length: 126.97
    Episode_Reward/reaching_object: 0.4687
     Episode_Reward/lifting_object: 30.3863
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 1.99s
                      Time elapsed: 00:19:25
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 49527 steps/s (collection: 1.879s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 96.9760
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.8308
                       Mean reward: 161.95
               Mean episode length: 131.61
    Episode_Reward/reaching_object: 0.4737
     Episode_Reward/lifting_object: 31.2547
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 1.98s
                      Time elapsed: 00:19:27
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 49755 steps/s (collection: 1.881s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 96.4371
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 34.8310
                       Mean reward: 161.73
               Mean episode length: 131.92
    Episode_Reward/reaching_object: 0.4833
     Episode_Reward/lifting_object: 31.1172
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 1.98s
                      Time elapsed: 00:19:29
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 49635 steps/s (collection: 1.877s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 98.5762
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 34.8318
                       Mean reward: 159.60
               Mean episode length: 130.88
    Episode_Reward/reaching_object: 0.4783
     Episode_Reward/lifting_object: 31.2516
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 1.98s
                      Time elapsed: 00:19:31
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 49544 steps/s (collection: 1.883s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 103.5874
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.8322
                       Mean reward: 168.99
               Mean episode length: 140.45
    Episode_Reward/reaching_object: 0.4900
     Episode_Reward/lifting_object: 31.6557
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 1.98s
                      Time elapsed: 00:19:33
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 49668 steps/s (collection: 1.873s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 102.1585
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.8320
                       Mean reward: 166.24
               Mean episode length: 138.42
    Episode_Reward/reaching_object: 0.4901
     Episode_Reward/lifting_object: 32.0312
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 1.98s
                      Time elapsed: 00:19:35
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 49151 steps/s (collection: 1.908s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 105.8672
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8320
                       Mean reward: 165.23
               Mean episode length: 142.08
    Episode_Reward/reaching_object: 0.4884
     Episode_Reward/lifting_object: 31.3464
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.00s
                      Time elapsed: 00:19:37
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 48245 steps/s (collection: 1.904s, learning 0.134s)
             Mean action noise std: 1.72
          Mean value_function loss: 104.2059
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 34.8316
                       Mean reward: 168.99
               Mean episode length: 135.79
    Episode_Reward/reaching_object: 0.4783
     Episode_Reward/lifting_object: 31.8270
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.04s
                      Time elapsed: 00:19:39
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 49115 steps/s (collection: 1.881s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 107.7902
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 34.8307
                       Mean reward: 172.04
               Mean episode length: 143.18
    Episode_Reward/reaching_object: 0.4893
     Episode_Reward/lifting_object: 32.0590
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.00s
                      Time elapsed: 00:19:41
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 49411 steps/s (collection: 1.884s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 104.0956
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.8308
                       Mean reward: 172.39
               Mean episode length: 142.45
    Episode_Reward/reaching_object: 0.4735
     Episode_Reward/lifting_object: 31.1396
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 1.99s
                      Time elapsed: 00:19:43
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 50630 steps/s (collection: 1.849s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 101.1115
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8294
                       Mean reward: 165.88
               Mean episode length: 136.95
    Episode_Reward/reaching_object: 0.4827
     Episode_Reward/lifting_object: 31.6358
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 1.94s
                      Time elapsed: 00:19:45
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 49091 steps/s (collection: 1.890s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 113.5527
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 34.8284
                       Mean reward: 160.18
               Mean episode length: 132.36
    Episode_Reward/reaching_object: 0.4731
     Episode_Reward/lifting_object: 31.5128
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.00s
                      Time elapsed: 00:19:47
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 48397 steps/s (collection: 1.893s, learning 0.138s)
             Mean action noise std: 1.72
          Mean value_function loss: 105.1808
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.8297
                       Mean reward: 155.22
               Mean episode length: 123.34
    Episode_Reward/reaching_object: 0.4750
     Episode_Reward/lifting_object: 31.6533
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.03s
                      Time elapsed: 00:19:49
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 50161 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 112.5970
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 34.8313
                       Mean reward: 162.69
               Mean episode length: 132.15
    Episode_Reward/reaching_object: 0.4587
     Episode_Reward/lifting_object: 30.8601
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 1.96s
                      Time elapsed: 00:19:51
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 49035 steps/s (collection: 1.886s, learning 0.119s)
             Mean action noise std: 1.72
          Mean value_function loss: 108.0366
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.8315
                       Mean reward: 160.07
               Mean episode length: 126.74
    Episode_Reward/reaching_object: 0.4549
     Episode_Reward/lifting_object: 30.8740
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.00s
                      Time elapsed: 00:19:53
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 48905 steps/s (collection: 1.891s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 109.3083
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.8317
                       Mean reward: 163.82
               Mean episode length: 132.78
    Episode_Reward/reaching_object: 0.4494
     Episode_Reward/lifting_object: 30.6346
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.0417
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.01s
                      Time elapsed: 00:19:55
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 48026 steps/s (collection: 1.920s, learning 0.127s)
             Mean action noise std: 1.72
          Mean value_function loss: 113.7326
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.8342
                       Mean reward: 155.23
               Mean episode length: 125.31
    Episode_Reward/reaching_object: 0.4437
     Episode_Reward/lifting_object: 30.4197
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.05s
                      Time elapsed: 00:19:57
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 49324 steps/s (collection: 1.897s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 115.8458
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 34.8378
                       Mean reward: 153.27
               Mean episode length: 124.45
    Episode_Reward/reaching_object: 0.4567
     Episode_Reward/lifting_object: 31.2910
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 1.99s
                      Time elapsed: 00:19:59
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 49011 steps/s (collection: 1.884s, learning 0.122s)
             Mean action noise std: 1.72
          Mean value_function loss: 109.3882
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 34.8398
                       Mean reward: 150.50
               Mean episode length: 119.45
    Episode_Reward/reaching_object: 0.4395
     Episode_Reward/lifting_object: 30.0922
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.01s
                      Time elapsed: 00:20:01
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 49671 steps/s (collection: 1.868s, learning 0.111s)
             Mean action noise std: 1.72
          Mean value_function loss: 114.1317
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 34.8411
                       Mean reward: 159.38
               Mean episode length: 127.98
    Episode_Reward/reaching_object: 0.4360
     Episode_Reward/lifting_object: 30.1064
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 1.98s
                      Time elapsed: 00:20:03
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 48973 steps/s (collection: 1.874s, learning 0.134s)
             Mean action noise std: 1.72
          Mean value_function loss: 111.9185
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 34.8415
                       Mean reward: 157.04
               Mean episode length: 126.97
    Episode_Reward/reaching_object: 0.4552
     Episode_Reward/lifting_object: 31.4569
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.01s
                      Time elapsed: 00:20:05
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 48384 steps/s (collection: 1.885s, learning 0.147s)
             Mean action noise std: 1.72
          Mean value_function loss: 108.8839
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.8417
                       Mean reward: 155.51
               Mean episode length: 120.82
    Episode_Reward/reaching_object: 0.4573
     Episode_Reward/lifting_object: 31.2942
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.03s
                      Time elapsed: 00:20:07
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 49806 steps/s (collection: 1.879s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 110.9349
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.8405
                       Mean reward: 161.89
               Mean episode length: 124.03
    Episode_Reward/reaching_object: 0.4470
     Episode_Reward/lifting_object: 31.2272
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.97s
                      Time elapsed: 00:20:09
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 49141 steps/s (collection: 1.880s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 112.8793
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.8406
                       Mean reward: 165.26
               Mean episode length: 128.53
    Episode_Reward/reaching_object: 0.4463
     Episode_Reward/lifting_object: 31.4899
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.00s
                      Time elapsed: 00:20:11
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 49143 steps/s (collection: 1.892s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 114.3082
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.8434
                       Mean reward: 146.18
               Mean episode length: 113.00
    Episode_Reward/reaching_object: 0.4282
     Episode_Reward/lifting_object: 29.9972
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.00s
                      Time elapsed: 00:20:13
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 47597 steps/s (collection: 1.908s, learning 0.158s)
             Mean action noise std: 1.72
          Mean value_function loss: 111.1938
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 34.8454
                       Mean reward: 171.04
               Mean episode length: 136.20
    Episode_Reward/reaching_object: 0.4658
     Episode_Reward/lifting_object: 32.7564
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.07s
                      Time elapsed: 00:20:15
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 44776 steps/s (collection: 2.031s, learning 0.164s)
             Mean action noise std: 1.72
          Mean value_function loss: 117.2885
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.8479
                       Mean reward: 166.67
               Mean episode length: 128.78
    Episode_Reward/reaching_object: 0.4561
     Episode_Reward/lifting_object: 31.9462
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.20s
                      Time elapsed: 00:20:17
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 45868 steps/s (collection: 1.987s, learning 0.156s)
             Mean action noise std: 1.72
          Mean value_function loss: 113.9922
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.8537
                       Mean reward: 163.48
               Mean episode length: 128.47
    Episode_Reward/reaching_object: 0.4557
     Episode_Reward/lifting_object: 32.0347
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.14s
                      Time elapsed: 00:20:19
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 48270 steps/s (collection: 1.910s, learning 0.127s)
             Mean action noise std: 1.72
          Mean value_function loss: 118.3648
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 34.8590
                       Mean reward: 165.43
               Mean episode length: 127.22
    Episode_Reward/reaching_object: 0.4535
     Episode_Reward/lifting_object: 32.2741
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.04s
                      Time elapsed: 00:20:21
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 46998 steps/s (collection: 1.957s, learning 0.135s)
             Mean action noise std: 1.72
          Mean value_function loss: 114.6437
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 34.8613
                       Mean reward: 168.44
               Mean episode length: 130.62
    Episode_Reward/reaching_object: 0.4594
     Episode_Reward/lifting_object: 32.7375
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.09s
                      Time elapsed: 00:20:23
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 48614 steps/s (collection: 1.914s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 113.8264
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.8629
                       Mean reward: 154.22
               Mean episode length: 119.81
    Episode_Reward/reaching_object: 0.4359
     Episode_Reward/lifting_object: 31.6873
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.02s
                      Time elapsed: 00:20:25
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 48412 steps/s (collection: 1.933s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 116.5029
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.8650
                       Mean reward: 159.24
               Mean episode length: 117.39
    Episode_Reward/reaching_object: 0.4271
     Episode_Reward/lifting_object: 31.2365
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.03s
                      Time elapsed: 00:20:27
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 46616 steps/s (collection: 1.943s, learning 0.166s)
             Mean action noise std: 1.72
          Mean value_function loss: 116.3116
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 34.8661
                       Mean reward: 162.33
               Mean episode length: 126.50
    Episode_Reward/reaching_object: 0.4550
     Episode_Reward/lifting_object: 33.0749
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.11s
                      Time elapsed: 00:20:30
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 45062 steps/s (collection: 2.051s, learning 0.131s)
             Mean action noise std: 1.72
          Mean value_function loss: 123.3938
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.8667
                       Mean reward: 155.81
               Mean episode length: 124.42
    Episode_Reward/reaching_object: 0.4447
     Episode_Reward/lifting_object: 32.1274
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.18s
                      Time elapsed: 00:20:32
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 48301 steps/s (collection: 1.894s, learning 0.142s)
             Mean action noise std: 1.72
          Mean value_function loss: 107.8001
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.8680
                       Mean reward: 181.46
               Mean episode length: 138.31
    Episode_Reward/reaching_object: 0.4522
     Episode_Reward/lifting_object: 32.6942
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.04s
                      Time elapsed: 00:20:34
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 48885 steps/s (collection: 1.890s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 115.9282
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 34.8698
                       Mean reward: 155.52
               Mean episode length: 121.88
    Episode_Reward/reaching_object: 0.4364
     Episode_Reward/lifting_object: 31.5719
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.01s
                      Time elapsed: 00:20:36
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 46035 steps/s (collection: 1.959s, learning 0.177s)
             Mean action noise std: 1.72
          Mean value_function loss: 113.9235
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8705
                       Mean reward: 162.38
               Mean episode length: 122.33
    Episode_Reward/reaching_object: 0.4548
     Episode_Reward/lifting_object: 32.6857
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.14s
                      Time elapsed: 00:20:38
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 46407 steps/s (collection: 1.957s, learning 0.161s)
             Mean action noise std: 1.72
          Mean value_function loss: 111.9574
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8717
                       Mean reward: 180.84
               Mean episode length: 139.93
    Episode_Reward/reaching_object: 0.4864
     Episode_Reward/lifting_object: 34.9727
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.12s
                      Time elapsed: 00:20:40
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 48315 steps/s (collection: 1.913s, learning 0.122s)
             Mean action noise std: 1.73
          Mean value_function loss: 125.0958
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.8750
                       Mean reward: 165.80
               Mean episode length: 128.36
    Episode_Reward/reaching_object: 0.4589
     Episode_Reward/lifting_object: 33.0532
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.03s
                      Time elapsed: 00:20:42
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 49262 steps/s (collection: 1.901s, learning 0.095s)
             Mean action noise std: 1.73
          Mean value_function loss: 118.1040
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.8780
                       Mean reward: 171.30
               Mean episode length: 134.76
    Episode_Reward/reaching_object: 0.4763
     Episode_Reward/lifting_object: 34.3190
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.00s
                      Time elapsed: 00:20:44
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 49518 steps/s (collection: 1.882s, learning 0.103s)
             Mean action noise std: 1.73
          Mean value_function loss: 116.8601
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 34.8798
                       Mean reward: 170.01
               Mean episode length: 124.88
    Episode_Reward/reaching_object: 0.4587
     Episode_Reward/lifting_object: 33.4984
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 1.99s
                      Time elapsed: 00:20:46
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 48866 steps/s (collection: 1.908s, learning 0.104s)
             Mean action noise std: 1.73
          Mean value_function loss: 122.4650
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 34.8811
                       Mean reward: 173.18
               Mean episode length: 125.93
    Episode_Reward/reaching_object: 0.4528
     Episode_Reward/lifting_object: 33.3027
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.01s
                      Time elapsed: 00:20:48
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 50004 steps/s (collection: 1.869s, learning 0.097s)
             Mean action noise std: 1.73
          Mean value_function loss: 121.8654
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.8835
                       Mean reward: 171.34
               Mean episode length: 127.92
    Episode_Reward/reaching_object: 0.4400
     Episode_Reward/lifting_object: 32.5402
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 1.97s
                      Time elapsed: 00:20:50
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 48186 steps/s (collection: 1.943s, learning 0.097s)
             Mean action noise std: 1.73
          Mean value_function loss: 123.8056
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.8851
                       Mean reward: 157.87
               Mean episode length: 119.88
    Episode_Reward/reaching_object: 0.4416
     Episode_Reward/lifting_object: 32.8470
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.04s
                      Time elapsed: 00:20:52
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 49431 steps/s (collection: 1.881s, learning 0.108s)
             Mean action noise std: 1.73
          Mean value_function loss: 122.8634
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.8857
                       Mean reward: 169.39
               Mean episode length: 126.52
    Episode_Reward/reaching_object: 0.4279
     Episode_Reward/lifting_object: 31.9311
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 1.99s
                      Time elapsed: 00:20:54
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 47404 steps/s (collection: 1.944s, learning 0.130s)
             Mean action noise std: 1.73
          Mean value_function loss: 119.3509
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 34.8880
                       Mean reward: 168.57
               Mean episode length: 123.44
    Episode_Reward/reaching_object: 0.4200
     Episode_Reward/lifting_object: 31.6064
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.07s
                      Time elapsed: 00:20:56
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 47882 steps/s (collection: 1.923s, learning 0.130s)
             Mean action noise std: 1.73
          Mean value_function loss: 118.9415
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.8928
                       Mean reward: 149.99
               Mean episode length: 111.93
    Episode_Reward/reaching_object: 0.4343
     Episode_Reward/lifting_object: 32.4998
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.05s
                      Time elapsed: 00:20:58
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 49425 steps/s (collection: 1.871s, learning 0.118s)
             Mean action noise std: 1.73
          Mean value_function loss: 119.2168
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 34.8988
                       Mean reward: 171.57
               Mean episode length: 124.46
    Episode_Reward/reaching_object: 0.4331
     Episode_Reward/lifting_object: 32.7421
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 1.99s
                      Time elapsed: 00:21:00
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 48784 steps/s (collection: 1.898s, learning 0.117s)
             Mean action noise std: 1.73
          Mean value_function loss: 120.7223
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 34.9007
                       Mean reward: 178.29
               Mean episode length: 132.24
    Episode_Reward/reaching_object: 0.4405
     Episode_Reward/lifting_object: 32.8894
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.02s
                      Time elapsed: 00:21:02
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 47733 steps/s (collection: 1.912s, learning 0.147s)
             Mean action noise std: 1.73
          Mean value_function loss: 117.6361
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 34.9010
                       Mean reward: 168.64
               Mean episode length: 120.11
    Episode_Reward/reaching_object: 0.4442
     Episode_Reward/lifting_object: 33.6533
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.06s
                      Time elapsed: 00:21:04
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 46212 steps/s (collection: 1.901s, learning 0.226s)
             Mean action noise std: 1.73
          Mean value_function loss: 117.0068
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.9014
                       Mean reward: 180.34
               Mean episode length: 130.49
    Episode_Reward/reaching_object: 0.4622
     Episode_Reward/lifting_object: 34.8614
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.13s
                      Time elapsed: 00:21:06
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 47941 steps/s (collection: 1.939s, learning 0.111s)
             Mean action noise std: 1.73
          Mean value_function loss: 127.8573
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 34.9019
                       Mean reward: 167.61
               Mean episode length: 123.58
    Episode_Reward/reaching_object: 0.4548
     Episode_Reward/lifting_object: 34.4519
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.05s
                      Time elapsed: 00:21:08
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 48765 steps/s (collection: 1.907s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 119.1290
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.9032
                       Mean reward: 179.57
               Mean episode length: 127.73
    Episode_Reward/reaching_object: 0.4675
     Episode_Reward/lifting_object: 35.0134
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.02s
                      Time elapsed: 00:21:10
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 50285 steps/s (collection: 1.860s, learning 0.095s)
             Mean action noise std: 1.73
          Mean value_function loss: 134.2859
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.9056
                       Mean reward: 163.94
               Mean episode length: 118.96
    Episode_Reward/reaching_object: 0.4685
     Episode_Reward/lifting_object: 35.3137
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 1.95s
                      Time elapsed: 00:21:12
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 50517 steps/s (collection: 1.850s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 132.0224
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.9075
                       Mean reward: 157.95
               Mean episode length: 117.40
    Episode_Reward/reaching_object: 0.4514
     Episode_Reward/lifting_object: 33.5675
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 1.95s
                      Time elapsed: 00:21:14
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 46084 steps/s (collection: 2.040s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 121.1027
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.9107
                       Mean reward: 176.53
               Mean episode length: 129.75
    Episode_Reward/reaching_object: 0.4579
     Episode_Reward/lifting_object: 34.1936
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.13s
                      Time elapsed: 00:21:16
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 46043 steps/s (collection: 1.951s, learning 0.184s)
             Mean action noise std: 1.73
          Mean value_function loss: 132.6015
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 34.9122
                       Mean reward: 165.70
               Mean episode length: 123.00
    Episode_Reward/reaching_object: 0.4290
     Episode_Reward/lifting_object: 31.6764
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.14s
                      Time elapsed: 00:21:19
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 47692 steps/s (collection: 1.954s, learning 0.108s)
             Mean action noise std: 1.73
          Mean value_function loss: 124.7254
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 34.9111
                       Mean reward: 157.52
               Mean episode length: 117.69
    Episode_Reward/reaching_object: 0.4432
     Episode_Reward/lifting_object: 32.9015
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.06s
                      Time elapsed: 00:21:21
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 48118 steps/s (collection: 1.921s, learning 0.122s)
             Mean action noise std: 1.73
          Mean value_function loss: 115.4012
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.9117
                       Mean reward: 181.88
               Mean episode length: 134.55
    Episode_Reward/reaching_object: 0.4530
     Episode_Reward/lifting_object: 33.5570
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.04s
                      Time elapsed: 00:21:23
                               ETA: 00:47:58

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 48010 steps/s (collection: 1.901s, learning 0.147s)
             Mean action noise std: 1.73
          Mean value_function loss: 112.8002
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.9135
                       Mean reward: 165.63
               Mean episode length: 121.68
    Episode_Reward/reaching_object: 0.4381
     Episode_Reward/lifting_object: 32.9055
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.05s
                      Time elapsed: 00:21:25
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 46519 steps/s (collection: 1.914s, learning 0.199s)
             Mean action noise std: 1.73
          Mean value_function loss: 116.0306
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.9133
                       Mean reward: 172.28
               Mean episode length: 126.19
    Episode_Reward/reaching_object: 0.4562
     Episode_Reward/lifting_object: 33.9100
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.11s
                      Time elapsed: 00:21:27
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 47936 steps/s (collection: 1.939s, learning 0.112s)
             Mean action noise std: 1.73
          Mean value_function loss: 123.3673
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.9133
                       Mean reward: 172.84
               Mean episode length: 132.58
    Episode_Reward/reaching_object: 0.4824
     Episode_Reward/lifting_object: 35.1699
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.05s
                      Time elapsed: 00:21:29
                               ETA: 00:47:51

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 48737 steps/s (collection: 1.915s, learning 0.102s)
             Mean action noise std: 1.73
          Mean value_function loss: 117.5055
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 34.9154
                       Mean reward: 183.63
               Mean episode length: 133.71
    Episode_Reward/reaching_object: 0.4694
     Episode_Reward/lifting_object: 34.7868
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.02s
                      Time elapsed: 00:21:31
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 47427 steps/s (collection: 1.952s, learning 0.120s)
             Mean action noise std: 1.73
          Mean value_function loss: 123.5978
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 34.9184
                       Mean reward: 186.58
               Mean episode length: 139.42
    Episode_Reward/reaching_object: 0.4851
     Episode_Reward/lifting_object: 35.3605
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.07s
                      Time elapsed: 00:21:33
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 49290 steps/s (collection: 1.886s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 122.0099
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.9222
                       Mean reward: 186.24
               Mean episode length: 134.51
    Episode_Reward/reaching_object: 0.5021
     Episode_Reward/lifting_object: 37.0823
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 1.99s
                      Time elapsed: 00:21:35
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 49650 steps/s (collection: 1.854s, learning 0.126s)
             Mean action noise std: 1.73
          Mean value_function loss: 136.7845
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 34.9244
                       Mean reward: 195.41
               Mean episode length: 142.22
    Episode_Reward/reaching_object: 0.4891
     Episode_Reward/lifting_object: 36.4325
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 1.98s
                      Time elapsed: 00:21:37
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 48140 steps/s (collection: 1.910s, learning 0.132s)
             Mean action noise std: 1.73
          Mean value_function loss: 148.4601
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 34.9266
                       Mean reward: 175.58
               Mean episode length: 126.77
    Episode_Reward/reaching_object: 0.4687
     Episode_Reward/lifting_object: 34.8633
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.04s
                      Time elapsed: 00:21:39
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 48429 steps/s (collection: 1.888s, learning 0.142s)
             Mean action noise std: 1.73
          Mean value_function loss: 129.2504
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.9299
                       Mean reward: 173.01
               Mean episode length: 124.72
    Episode_Reward/reaching_object: 0.4523
     Episode_Reward/lifting_object: 33.8512
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.03s
                      Time elapsed: 00:21:41
                               ETA: 00:47:38

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 48033 steps/s (collection: 1.949s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 123.2281
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 34.9310
                       Mean reward: 172.99
               Mean episode length: 123.64
    Episode_Reward/reaching_object: 0.4508
     Episode_Reward/lifting_object: 34.3675
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.05s
                      Time elapsed: 00:21:43
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 48520 steps/s (collection: 1.902s, learning 0.125s)
             Mean action noise std: 1.73
          Mean value_function loss: 120.9710
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 34.9314
                       Mean reward: 161.85
               Mean episode length: 118.37
    Episode_Reward/reaching_object: 0.4412
     Episode_Reward/lifting_object: 33.7157
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.03s
                      Time elapsed: 00:21:45
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 49188 steps/s (collection: 1.887s, learning 0.112s)
             Mean action noise std: 1.73
          Mean value_function loss: 119.0373
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 34.9316
                       Mean reward: 163.88
               Mean episode length: 119.80
    Episode_Reward/reaching_object: 0.4533
     Episode_Reward/lifting_object: 34.1507
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.00s
                      Time elapsed: 00:21:47
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 48719 steps/s (collection: 1.905s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 114.0437
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 34.9322
                       Mean reward: 182.03
               Mean episode length: 127.48
    Episode_Reward/reaching_object: 0.4717
     Episode_Reward/lifting_object: 35.4614
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.02s
                      Time elapsed: 00:21:49
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 48909 steps/s (collection: 1.880s, learning 0.130s)
             Mean action noise std: 1.73
          Mean value_function loss: 113.6218
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.9321
                       Mean reward: 181.43
               Mean episode length: 136.47
    Episode_Reward/reaching_object: 0.4935
     Episode_Reward/lifting_object: 36.8027
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.01s
                      Time elapsed: 00:21:51
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 48311 steps/s (collection: 1.897s, learning 0.138s)
             Mean action noise std: 1.73
          Mean value_function loss: 113.0114
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.9325
                       Mean reward: 198.48
               Mean episode length: 141.84
    Episode_Reward/reaching_object: 0.5157
     Episode_Reward/lifting_object: 38.7501
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.03s
                      Time elapsed: 00:21:53
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 48890 steps/s (collection: 1.909s, learning 0.102s)
             Mean action noise std: 1.73
          Mean value_function loss: 112.2137
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.9338
                       Mean reward: 210.35
               Mean episode length: 154.07
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 39.4629
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.01s
                      Time elapsed: 00:21:55
                               ETA: 00:47:23

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 47200 steps/s (collection: 1.980s, learning 0.103s)
             Mean action noise std: 1.73
          Mean value_function loss: 113.0565
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.9334
                       Mean reward: 211.63
               Mean episode length: 152.52
    Episode_Reward/reaching_object: 0.5427
     Episode_Reward/lifting_object: 40.4714
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.08s
                      Time elapsed: 00:21:57
                               ETA: 00:47:21

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 47568 steps/s (collection: 1.940s, learning 0.127s)
             Mean action noise std: 1.73
          Mean value_function loss: 105.0213
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.9349
                       Mean reward: 224.78
               Mean episode length: 165.26
    Episode_Reward/reaching_object: 0.5872
     Episode_Reward/lifting_object: 43.3611
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.07s
                      Time elapsed: 00:21:59
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 48525 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 105.3029
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 34.9377
                       Mean reward: 233.78
               Mean episode length: 167.31
    Episode_Reward/reaching_object: 0.6049
     Episode_Reward/lifting_object: 44.6363
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.03s
                      Time elapsed: 00:22:01
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 47468 steps/s (collection: 1.946s, learning 0.125s)
             Mean action noise std: 1.73
          Mean value_function loss: 108.5764
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.9402
                       Mean reward: 224.51
               Mean episode length: 163.85
    Episode_Reward/reaching_object: 0.5858
     Episode_Reward/lifting_object: 43.8676
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.07s
                      Time elapsed: 00:22:03
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 45795 steps/s (collection: 2.040s, learning 0.107s)
             Mean action noise std: 1.73
          Mean value_function loss: 106.3277
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 34.9430
                       Mean reward: 230.28
               Mean episode length: 163.92
    Episode_Reward/reaching_object: 0.5946
     Episode_Reward/lifting_object: 44.5545
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.15s
                      Time elapsed: 00:22:06
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 47404 steps/s (collection: 1.941s, learning 0.133s)
             Mean action noise std: 1.73
          Mean value_function loss: 111.2865
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.9436
                       Mean reward: 234.27
               Mean episode length: 170.77
    Episode_Reward/reaching_object: 0.6274
     Episode_Reward/lifting_object: 46.5348
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.07s
                      Time elapsed: 00:22:08
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 48281 steps/s (collection: 1.922s, learning 0.114s)
             Mean action noise std: 1.73
          Mean value_function loss: 104.3171
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 34.9438
                       Mean reward: 225.05
               Mean episode length: 160.89
    Episode_Reward/reaching_object: 0.6057
     Episode_Reward/lifting_object: 45.5905
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.04s
                      Time elapsed: 00:22:10
                               ETA: 00:47:08

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 48827 steps/s (collection: 1.894s, learning 0.120s)
             Mean action noise std: 1.73
          Mean value_function loss: 108.8109
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.9454
                       Mean reward: 218.18
               Mean episode length: 157.36
    Episode_Reward/reaching_object: 0.6042
     Episode_Reward/lifting_object: 45.8394
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.01s
                      Time elapsed: 00:22:12
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 48100 steps/s (collection: 1.934s, learning 0.110s)
             Mean action noise std: 1.73
          Mean value_function loss: 109.4299
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.9461
                       Mean reward: 240.24
               Mean episode length: 166.55
    Episode_Reward/reaching_object: 0.5988
     Episode_Reward/lifting_object: 45.7052
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.04s
                      Time elapsed: 00:22:14
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 48218 steps/s (collection: 1.924s, learning 0.115s)
             Mean action noise std: 1.74
          Mean value_function loss: 111.0383
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.9453
                       Mean reward: 242.95
               Mean episode length: 169.33
    Episode_Reward/reaching_object: 0.6013
     Episode_Reward/lifting_object: 46.4052
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.04s
                      Time elapsed: 00:22:16
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 47940 steps/s (collection: 1.940s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 112.6600
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 34.9448
                       Mean reward: 219.74
               Mean episode length: 150.18
    Episode_Reward/reaching_object: 0.5793
     Episode_Reward/lifting_object: 45.0822
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.05s
                      Time elapsed: 00:22:18
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 46304 steps/s (collection: 1.971s, learning 0.152s)
             Mean action noise std: 1.74
          Mean value_function loss: 110.3083
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 34.9456
                       Mean reward: 223.39
               Mean episode length: 158.76
    Episode_Reward/reaching_object: 0.5932
     Episode_Reward/lifting_object: 46.0371
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.12s
                      Time elapsed: 00:22:20
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 46980 steps/s (collection: 1.953s, learning 0.139s)
             Mean action noise std: 1.74
          Mean value_function loss: 107.4379
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.9455
                       Mean reward: 236.59
               Mean episode length: 163.44
    Episode_Reward/reaching_object: 0.5910
     Episode_Reward/lifting_object: 46.0105
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.09s
                      Time elapsed: 00:22:22
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 47884 steps/s (collection: 1.898s, learning 0.155s)
             Mean action noise std: 1.74
          Mean value_function loss: 104.2168
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 34.9477
                       Mean reward: 230.70
               Mean episode length: 160.57
    Episode_Reward/reaching_object: 0.5995
     Episode_Reward/lifting_object: 46.7764
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.05s
                      Time elapsed: 00:22:24
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 49529 steps/s (collection: 1.879s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 104.5887
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.9538
                       Mean reward: 234.65
               Mean episode length: 162.26
    Episode_Reward/reaching_object: 0.6185
     Episode_Reward/lifting_object: 48.1049
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 1.98s
                      Time elapsed: 00:22:26
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 48043 steps/s (collection: 1.915s, learning 0.131s)
             Mean action noise std: 1.74
          Mean value_function loss: 106.4273
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.9569
                       Mean reward: 249.92
               Mean episode length: 174.83
    Episode_Reward/reaching_object: 0.6185
     Episode_Reward/lifting_object: 47.6252
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.05s
                      Time elapsed: 00:22:28
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 49327 steps/s (collection: 1.887s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 102.0819
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 34.9566
                       Mean reward: 243.42
               Mean episode length: 171.70
    Episode_Reward/reaching_object: 0.6122
     Episode_Reward/lifting_object: 47.3247
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 1.99s
                      Time elapsed: 00:22:30
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 48189 steps/s (collection: 1.897s, learning 0.143s)
             Mean action noise std: 1.74
          Mean value_function loss: 112.3970
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 34.9574
                       Mean reward: 256.65
               Mean episode length: 175.76
    Episode_Reward/reaching_object: 0.6363
     Episode_Reward/lifting_object: 49.8196
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.04s
                      Time elapsed: 00:22:32
                               ETA: 00:46:45

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 47551 steps/s (collection: 1.970s, learning 0.097s)
             Mean action noise std: 1.74
          Mean value_function loss: 116.5673
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.9592
                       Mean reward: 230.35
               Mean episode length: 158.85
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 48.9767
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.07s
                      Time elapsed: 00:22:34
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 48448 steps/s (collection: 1.920s, learning 0.109s)
             Mean action noise std: 1.74
          Mean value_function loss: 110.4532
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.9584
                       Mean reward: 256.71
               Mean episode length: 177.54
    Episode_Reward/reaching_object: 0.6363
     Episode_Reward/lifting_object: 49.4427
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.03s
                      Time elapsed: 00:22:36
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 48412 steps/s (collection: 1.917s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 108.6516
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.9573
                       Mean reward: 245.20
               Mean episode length: 172.41
    Episode_Reward/reaching_object: 0.6259
     Episode_Reward/lifting_object: 48.6574
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.03s
                      Time elapsed: 00:22:38
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 49118 steps/s (collection: 1.883s, learning 0.118s)
             Mean action noise std: 1.74
          Mean value_function loss: 131.1009
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 34.9582
                       Mean reward: 246.85
               Mean episode length: 168.58
    Episode_Reward/reaching_object: 0.6357
     Episode_Reward/lifting_object: 50.0584
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.00s
                      Time elapsed: 00:22:40
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 48840 steps/s (collection: 1.908s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 127.9570
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.9582
                       Mean reward: 233.58
               Mean episode length: 158.58
    Episode_Reward/reaching_object: 0.6141
     Episode_Reward/lifting_object: 49.0624
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.01s
                      Time elapsed: 00:22:42
                               ETA: 00:46:34

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 49017 steps/s (collection: 1.898s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 119.6283
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.9591
                       Mean reward: 252.68
               Mean episode length: 165.52
    Episode_Reward/reaching_object: 0.5956
     Episode_Reward/lifting_object: 48.2257
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.01s
                      Time elapsed: 00:22:44
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 47856 steps/s (collection: 1.933s, learning 0.122s)
             Mean action noise std: 1.74
          Mean value_function loss: 124.9743
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.9611
                       Mean reward: 254.88
               Mean episode length: 167.29
    Episode_Reward/reaching_object: 0.6042
     Episode_Reward/lifting_object: 48.9422
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.05s
                      Time elapsed: 00:22:46
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 49347 steps/s (collection: 1.896s, learning 0.096s)
             Mean action noise std: 1.74
          Mean value_function loss: 122.8989
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.9622
                       Mean reward: 225.56
               Mean episode length: 152.84
    Episode_Reward/reaching_object: 0.5799
     Episode_Reward/lifting_object: 46.7267
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 1.99s
                      Time elapsed: 00:22:48
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 50475 steps/s (collection: 1.857s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 119.8529
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 34.9629
                       Mean reward: 257.63
               Mean episode length: 171.42
    Episode_Reward/reaching_object: 0.5984
     Episode_Reward/lifting_object: 47.9948
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 1.95s
                      Time elapsed: 00:22:50
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 49921 steps/s (collection: 1.867s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 119.5899
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.9653
                       Mean reward: 254.91
               Mean episode length: 169.96
    Episode_Reward/reaching_object: 0.6105
     Episode_Reward/lifting_object: 49.7688
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 1.97s
                      Time elapsed: 00:22:52
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 50140 steps/s (collection: 1.849s, learning 0.112s)
             Mean action noise std: 1.74
          Mean value_function loss: 119.8127
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 34.9685
                       Mean reward: 266.62
               Mean episode length: 173.13
    Episode_Reward/reaching_object: 0.6072
     Episode_Reward/lifting_object: 49.9321
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 1.96s
                      Time elapsed: 00:22:54
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 48176 steps/s (collection: 1.935s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 118.7801
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.9689
                       Mean reward: 264.46
               Mean episode length: 169.33
    Episode_Reward/reaching_object: 0.6206
     Episode_Reward/lifting_object: 51.1681
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.04s
                      Time elapsed: 00:22:56
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 50147 steps/s (collection: 1.864s, learning 0.097s)
             Mean action noise std: 1.74
          Mean value_function loss: 124.1754
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.9688
                       Mean reward: 265.64
               Mean episode length: 168.19
    Episode_Reward/reaching_object: 0.5960
     Episode_Reward/lifting_object: 49.0656
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 1.96s
                      Time elapsed: 00:22:58
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 50837 steps/s (collection: 1.827s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 127.3395
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.9702
                       Mean reward: 256.99
               Mean episode length: 166.22
    Episode_Reward/reaching_object: 0.5990
     Episode_Reward/lifting_object: 49.7655
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 1.93s
                      Time elapsed: 00:23:00
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 49051 steps/s (collection: 1.828s, learning 0.176s)
             Mean action noise std: 1.74
          Mean value_function loss: 123.8230
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 34.9720
                       Mean reward: 260.24
               Mean episode length: 166.68
    Episode_Reward/reaching_object: 0.5955
     Episode_Reward/lifting_object: 49.1581
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.00s
                      Time elapsed: 00:23:02
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 28492 steps/s (collection: 3.340s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 139.0824
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.9718
                       Mean reward: 257.25
               Mean episode length: 164.27
    Episode_Reward/reaching_object: 0.6185
     Episode_Reward/lifting_object: 51.4096
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.45s
                      Time elapsed: 00:23:06
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14059 steps/s (collection: 6.863s, learning 0.130s)
             Mean action noise std: 1.74
          Mean value_function loss: 143.4993
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.9696
                       Mean reward: 251.20
               Mean episode length: 161.80
    Episode_Reward/reaching_object: 0.5910
     Episode_Reward/lifting_object: 49.3689
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.99s
                      Time elapsed: 00:23:13
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14098 steps/s (collection: 6.801s, learning 0.172s)
             Mean action noise std: 1.74
          Mean value_function loss: 130.9472
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 34.9694
                       Mean reward: 269.03
               Mean episode length: 169.51
    Episode_Reward/reaching_object: 0.5987
     Episode_Reward/lifting_object: 50.4107
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.97s
                      Time elapsed: 00:23:20
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 13871 steps/s (collection: 6.949s, learning 0.138s)
             Mean action noise std: 1.74
          Mean value_function loss: 133.0949
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 34.9686
                       Mean reward: 255.93
               Mean episode length: 157.81
    Episode_Reward/reaching_object: 0.5728
     Episode_Reward/lifting_object: 48.6269
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.09s
                      Time elapsed: 00:23:27
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14450 steps/s (collection: 6.669s, learning 0.134s)
             Mean action noise std: 1.74
          Mean value_function loss: 127.1537
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 34.9686
                       Mean reward: 234.70
               Mean episode length: 154.34
    Episode_Reward/reaching_object: 0.5553
     Episode_Reward/lifting_object: 46.4237
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.80s
                      Time elapsed: 00:23:33
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14216 steps/s (collection: 6.777s, learning 0.138s)
             Mean action noise std: 1.74
          Mean value_function loss: 139.8035
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 34.9685
                       Mean reward: 251.31
               Mean episode length: 163.50
    Episode_Reward/reaching_object: 0.5825
     Episode_Reward/lifting_object: 48.8493
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.91s
                      Time elapsed: 00:23:40
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13868 steps/s (collection: 6.962s, learning 0.127s)
             Mean action noise std: 1.74
          Mean value_function loss: 128.0838
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.9699
                       Mean reward: 242.65
               Mean episode length: 155.44
    Episode_Reward/reaching_object: 0.5904
     Episode_Reward/lifting_object: 49.8087
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.09s
                      Time elapsed: 00:23:47
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14723 steps/s (collection: 6.546s, learning 0.131s)
             Mean action noise std: 1.74
          Mean value_function loss: 136.9642
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 34.9710
                       Mean reward: 257.56
               Mean episode length: 161.91
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 49.4969
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.68s
                      Time elapsed: 00:23:54
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14934 steps/s (collection: 6.450s, learning 0.132s)
             Mean action noise std: 1.74
          Mean value_function loss: 124.1145
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.9725
                       Mean reward: 241.94
               Mean episode length: 155.92
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 48.8638
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.58s
                      Time elapsed: 00:24:01
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 24514 steps/s (collection: 3.894s, learning 0.116s)
             Mean action noise std: 1.74
          Mean value_function loss: 119.8411
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.9748
                       Mean reward: 243.86
               Mean episode length: 151.59
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 49.7381
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.01s
                      Time elapsed: 00:24:05
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 50908 steps/s (collection: 1.806s, learning 0.125s)
             Mean action noise std: 1.74
          Mean value_function loss: 131.8334
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.9758
                       Mean reward: 260.87
               Mean episode length: 165.16
    Episode_Reward/reaching_object: 0.6061
     Episode_Reward/lifting_object: 50.8414
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 1.93s
                      Time elapsed: 00:24:07
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 52043 steps/s (collection: 1.787s, learning 0.102s)
             Mean action noise std: 1.74
          Mean value_function loss: 126.1475
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 34.9783
                       Mean reward: 274.93
               Mean episode length: 175.40
    Episode_Reward/reaching_object: 0.6308
     Episode_Reward/lifting_object: 52.9899
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 1.89s
                      Time elapsed: 00:24:09
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 52505 steps/s (collection: 1.773s, learning 0.099s)
             Mean action noise std: 1.74
          Mean value_function loss: 113.5630
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 34.9808
                       Mean reward: 282.65
               Mean episode length: 178.50
    Episode_Reward/reaching_object: 0.6450
     Episode_Reward/lifting_object: 54.6158
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 1.87s
                      Time elapsed: 00:24:10
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 51168 steps/s (collection: 1.804s, learning 0.118s)
             Mean action noise std: 1.74
          Mean value_function loss: 128.3907
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.9811
                       Mean reward: 276.25
               Mean episode length: 174.22
    Episode_Reward/reaching_object: 0.6361
     Episode_Reward/lifting_object: 53.3815
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 1.92s
                      Time elapsed: 00:24:12
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 49528 steps/s (collection: 1.869s, learning 0.116s)
             Mean action noise std: 1.74
          Mean value_function loss: 132.8989
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 34.9793
                       Mean reward: 278.05
               Mean episode length: 171.41
    Episode_Reward/reaching_object: 0.6312
     Episode_Reward/lifting_object: 53.9060
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 1.98s
                      Time elapsed: 00:24:14
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 50493 steps/s (collection: 1.839s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 114.8154
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 34.9786
                       Mean reward: 260.10
               Mean episode length: 163.87
    Episode_Reward/reaching_object: 0.6295
     Episode_Reward/lifting_object: 53.2239
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 1.95s
                      Time elapsed: 00:24:16
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 50004 steps/s (collection: 1.850s, learning 0.116s)
             Mean action noise std: 1.74
          Mean value_function loss: 126.6510
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 34.9822
                       Mean reward: 288.58
               Mean episode length: 177.67
    Episode_Reward/reaching_object: 0.6386
     Episode_Reward/lifting_object: 54.9263
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 1.97s
                      Time elapsed: 00:24:18
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 50987 steps/s (collection: 1.819s, learning 0.109s)
             Mean action noise std: 1.74
          Mean value_function loss: 124.3392
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 34.9849
                       Mean reward: 272.36
               Mean episode length: 170.40
    Episode_Reward/reaching_object: 0.6447
     Episode_Reward/lifting_object: 55.4982
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 1.93s
                      Time elapsed: 00:24:20
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 51344 steps/s (collection: 1.806s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 127.9595
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.9855
                       Mean reward: 242.35
               Mean episode length: 153.09
    Episode_Reward/reaching_object: 0.6075
     Episode_Reward/lifting_object: 53.0309
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 1.91s
                      Time elapsed: 00:24:22
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 51028 steps/s (collection: 1.815s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 173.8504
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 34.9858
                       Mean reward: 259.31
               Mean episode length: 167.81
    Episode_Reward/reaching_object: 0.6069
     Episode_Reward/lifting_object: 52.8123
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 1.93s
                      Time elapsed: 00:24:24
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 52021 steps/s (collection: 1.794s, learning 0.096s)
             Mean action noise std: 1.74
          Mean value_function loss: 143.2686
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 34.9861
                       Mean reward: 268.12
               Mean episode length: 167.29
    Episode_Reward/reaching_object: 0.6044
     Episode_Reward/lifting_object: 52.3813
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 1.89s
                      Time elapsed: 00:24:26
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 50757 steps/s (collection: 1.814s, learning 0.123s)
             Mean action noise std: 1.74
          Mean value_function loss: 138.1147
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 34.9862
                       Mean reward: 298.27
               Mean episode length: 181.98
    Episode_Reward/reaching_object: 0.6303
     Episode_Reward/lifting_object: 54.9013
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 1.94s
                      Time elapsed: 00:24:28
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 50205 steps/s (collection: 1.837s, learning 0.121s)
             Mean action noise std: 1.74
          Mean value_function loss: 144.0765
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 34.9859
                       Mean reward: 290.09
               Mean episode length: 173.88
    Episode_Reward/reaching_object: 0.6210
     Episode_Reward/lifting_object: 54.9737
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 1.96s
                      Time elapsed: 00:24:30
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 50961 steps/s (collection: 1.810s, learning 0.119s)
             Mean action noise std: 1.74
          Mean value_function loss: 131.4104
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.9873
                       Mean reward: 282.63
               Mean episode length: 170.82
    Episode_Reward/reaching_object: 0.6259
     Episode_Reward/lifting_object: 55.2568
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 1.93s
                      Time elapsed: 00:24:32
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 51361 steps/s (collection: 1.792s, learning 0.122s)
             Mean action noise std: 1.74
          Mean value_function loss: 124.8817
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 34.9879
                       Mean reward: 280.92
               Mean episode length: 168.96
    Episode_Reward/reaching_object: 0.6242
     Episode_Reward/lifting_object: 55.1054
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 1.91s
                      Time elapsed: 00:24:34
                               ETA: 00:46:34

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 49817 steps/s (collection: 1.853s, learning 0.121s)
             Mean action noise std: 1.74
          Mean value_function loss: 132.3822
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 34.9850
                       Mean reward: 268.64
               Mean episode length: 164.98
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: 56.4520
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 1.97s
                      Time elapsed: 00:24:36
                               ETA: 00:46:32

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 50703 steps/s (collection: 1.819s, learning 0.120s)
             Mean action noise std: 1.74
          Mean value_function loss: 135.9952
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 34.9829
                       Mean reward: 272.04
               Mean episode length: 166.53
    Episode_Reward/reaching_object: 0.6339
     Episode_Reward/lifting_object: 55.5886
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 1.94s
                      Time elapsed: 00:24:38
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 51469 steps/s (collection: 1.799s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 137.0553
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 34.9820
                       Mean reward: 280.38
               Mean episode length: 169.59
    Episode_Reward/reaching_object: 0.6562
     Episode_Reward/lifting_object: 57.7423
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 1.91s
                      Time elapsed: 00:24:39
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 51603 steps/s (collection: 1.799s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 135.7823
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 34.9817
                       Mean reward: 293.47
               Mean episode length: 177.31
    Episode_Reward/reaching_object: 0.6673
     Episode_Reward/lifting_object: 58.5065
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 1.90s
                      Time elapsed: 00:24:41
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 51549 steps/s (collection: 1.788s, learning 0.119s)
             Mean action noise std: 1.74
          Mean value_function loss: 128.6469
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 34.9821
                       Mean reward: 283.60
               Mean episode length: 177.06
    Episode_Reward/reaching_object: 0.6704
     Episode_Reward/lifting_object: 58.0625
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 1.91s
                      Time elapsed: 00:24:43
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 51590 steps/s (collection: 1.793s, learning 0.112s)
             Mean action noise std: 1.74
          Mean value_function loss: 142.4383
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 34.9823
                       Mean reward: 307.69
               Mean episode length: 190.58
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 58.1975
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 1.91s
                      Time elapsed: 00:24:45
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 51690 steps/s (collection: 1.798s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 135.3390
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 34.9830
                       Mean reward: 277.27
               Mean episode length: 167.01
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 58.1622
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 1.90s
                      Time elapsed: 00:24:47
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 50949 steps/s (collection: 1.819s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 119.0654
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.9870
                       Mean reward: 304.66
               Mean episode length: 182.99
    Episode_Reward/reaching_object: 0.6617
     Episode_Reward/lifting_object: 57.8869
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 1.93s
                      Time elapsed: 00:24:49
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 51154 steps/s (collection: 1.816s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 122.0663
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 34.9902
                       Mean reward: 290.95
               Mean episode length: 175.42
    Episode_Reward/reaching_object: 0.6503
     Episode_Reward/lifting_object: 56.4063
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 1.92s
                      Time elapsed: 00:24:51
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 51764 steps/s (collection: 1.811s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 115.8073
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 34.9911
                       Mean reward: 275.01
               Mean episode length: 171.24
    Episode_Reward/reaching_object: 0.6464
     Episode_Reward/lifting_object: 56.5178
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 1.90s
                      Time elapsed: 00:24:53
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 50414 steps/s (collection: 1.831s, learning 0.119s)
             Mean action noise std: 1.74
          Mean value_function loss: 127.3630
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 34.9912
                       Mean reward: 282.54
               Mean episode length: 170.01
    Episode_Reward/reaching_object: 0.6595
     Episode_Reward/lifting_object: 57.3839
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 1.95s
                      Time elapsed: 00:24:55
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 51877 steps/s (collection: 1.782s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 149.2016
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 34.9912
                       Mean reward: 291.94
               Mean episode length: 172.95
    Episode_Reward/reaching_object: 0.6567
     Episode_Reward/lifting_object: 57.7640
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 1.89s
                      Time elapsed: 00:24:57
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 52473 steps/s (collection: 1.785s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 156.8269
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 34.9932
                       Mean reward: 285.44
               Mean episode length: 171.61
    Episode_Reward/reaching_object: 0.6672
     Episode_Reward/lifting_object: 58.5371
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 1.87s
                      Time elapsed: 00:24:59
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 51456 steps/s (collection: 1.807s, learning 0.103s)
             Mean action noise std: 1.75
          Mean value_function loss: 163.7982
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 34.9944
                       Mean reward: 294.65
               Mean episode length: 174.38
    Episode_Reward/reaching_object: 0.6748
     Episode_Reward/lifting_object: 60.3832
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 1.91s
                      Time elapsed: 00:25:00
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 50089 steps/s (collection: 1.867s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 191.1644
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.9953
                       Mean reward: 291.36
               Mean episode length: 172.67
    Episode_Reward/reaching_object: 0.6277
     Episode_Reward/lifting_object: 56.7854
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 1.96s
                      Time elapsed: 00:25:02
                               ETA: 00:45:56

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 49081 steps/s (collection: 1.909s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 172.8453
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.9973
                       Mean reward: 278.40
               Mean episode length: 168.47
    Episode_Reward/reaching_object: 0.6290
     Episode_Reward/lifting_object: 56.1538
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.00s
                      Time elapsed: 00:25:04
                               ETA: 00:45:54

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 50182 steps/s (collection: 1.859s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 169.9797
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 34.9988
                       Mean reward: 265.34
               Mean episode length: 161.71
    Episode_Reward/reaching_object: 0.6027
     Episode_Reward/lifting_object: 54.5688
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.96s
                      Time elapsed: 00:25:06
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 50932 steps/s (collection: 1.841s, learning 0.089s)
             Mean action noise std: 1.75
          Mean value_function loss: 149.4333
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.0000
                       Mean reward: 250.45
               Mean episode length: 151.47
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 52.3929
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 1.93s
                      Time elapsed: 00:25:08
                               ETA: 00:45:49

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 51568 steps/s (collection: 1.816s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 149.8378
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.0020
                       Mean reward: 271.83
               Mean episode length: 160.27
    Episode_Reward/reaching_object: 0.5806
     Episode_Reward/lifting_object: 53.5231
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 1.91s
                      Time elapsed: 00:25:10
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 50541 steps/s (collection: 1.851s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 132.3721
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.0058
                       Mean reward: 282.70
               Mean episode length: 168.45
    Episode_Reward/reaching_object: 0.6081
     Episode_Reward/lifting_object: 55.1462
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 1.94s
                      Time elapsed: 00:25:12
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 51175 steps/s (collection: 1.825s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 146.3864
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.0097
                       Mean reward: 265.10
               Mean episode length: 162.24
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 54.3502
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 1.92s
                      Time elapsed: 00:25:14
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 50855 steps/s (collection: 1.826s, learning 0.107s)
             Mean action noise std: 1.75
          Mean value_function loss: 137.8097
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.0154
                       Mean reward: 299.00
               Mean episode length: 176.33
    Episode_Reward/reaching_object: 0.6263
     Episode_Reward/lifting_object: 56.5766
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 1.93s
                      Time elapsed: 00:25:16
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 51674 steps/s (collection: 1.808s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 149.5493
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.0198
                       Mean reward: 307.47
               Mean episode length: 176.34
    Episode_Reward/reaching_object: 0.6462
     Episode_Reward/lifting_object: 58.9099
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 1.90s
                      Time elapsed: 00:25:18
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 50079 steps/s (collection: 1.863s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 147.3361
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.0213
                       Mean reward: 299.78
               Mean episode length: 175.78
    Episode_Reward/reaching_object: 0.6790
     Episode_Reward/lifting_object: 61.4121
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 1.96s
                      Time elapsed: 00:25:20
                               ETA: 00:45:34

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 51083 steps/s (collection: 1.829s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 137.7094
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.0212
                       Mean reward: 323.66
               Mean episode length: 193.27
    Episode_Reward/reaching_object: 0.6851
     Episode_Reward/lifting_object: 61.9167
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 1.92s
                      Time elapsed: 00:25:22
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 50837 steps/s (collection: 1.814s, learning 0.120s)
             Mean action noise std: 1.75
          Mean value_function loss: 135.6836
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.0226
                       Mean reward: 288.33
               Mean episode length: 171.89
    Episode_Reward/reaching_object: 0.6864
     Episode_Reward/lifting_object: 61.9745
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 1.93s
                      Time elapsed: 00:25:24
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 50694 steps/s (collection: 1.828s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 151.8072
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.0244
                       Mean reward: 309.09
               Mean episode length: 176.48
    Episode_Reward/reaching_object: 0.6753
     Episode_Reward/lifting_object: 61.5394
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.94s
                      Time elapsed: 00:25:26
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 50308 steps/s (collection: 1.846s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 143.4561
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.0256
                       Mean reward: 292.02
               Mean episode length: 169.29
    Episode_Reward/reaching_object: 0.6732
     Episode_Reward/lifting_object: 61.6881
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 1.95s
                      Time elapsed: 00:25:28
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 51194 steps/s (collection: 1.828s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 145.5867
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.0262
                       Mean reward: 288.25
               Mean episode length: 170.55
    Episode_Reward/reaching_object: 0.6756
     Episode_Reward/lifting_object: 61.4264
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.92s
                      Time elapsed: 00:25:30
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 51602 steps/s (collection: 1.800s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 152.0478
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.0272
                       Mean reward: 320.33
               Mean episode length: 185.21
    Episode_Reward/reaching_object: 0.6690
     Episode_Reward/lifting_object: 61.3110
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 1.91s
                      Time elapsed: 00:25:31
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 51154 steps/s (collection: 1.805s, learning 0.117s)
             Mean action noise std: 1.75
          Mean value_function loss: 146.7128
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.0301
                       Mean reward: 302.61
               Mean episode length: 174.30
    Episode_Reward/reaching_object: 0.6536
     Episode_Reward/lifting_object: 60.3038
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 1.92s
                      Time elapsed: 00:25:33
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 50328 steps/s (collection: 1.838s, learning 0.115s)
             Mean action noise std: 1.75
          Mean value_function loss: 149.7808
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.0323
                       Mean reward: 308.47
               Mean episode length: 176.26
    Episode_Reward/reaching_object: 0.6506
     Episode_Reward/lifting_object: 60.1922
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 1.95s
                      Time elapsed: 00:25:35
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 50128 steps/s (collection: 1.852s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 146.2325
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.0342
                       Mean reward: 316.89
               Mean episode length: 176.84
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 59.5324
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 1.96s
                      Time elapsed: 00:25:37
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 50994 steps/s (collection: 1.815s, learning 0.113s)
             Mean action noise std: 1.75
          Mean value_function loss: 145.2173
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.0362
                       Mean reward: 326.10
               Mean episode length: 182.65
    Episode_Reward/reaching_object: 0.6567
     Episode_Reward/lifting_object: 61.0179
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 1.93s
                      Time elapsed: 00:25:39
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 50151 steps/s (collection: 1.849s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 163.0784
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.0405
                       Mean reward: 331.81
               Mean episode length: 187.27
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 59.3461
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 1.96s
                      Time elapsed: 00:25:41
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 50417 steps/s (collection: 1.840s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 149.7798
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.0443
                       Mean reward: 291.03
               Mean episode length: 164.64
    Episode_Reward/reaching_object: 0.6607
     Episode_Reward/lifting_object: 61.7958
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 1.95s
                      Time elapsed: 00:25:43
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 50470 steps/s (collection: 1.847s, learning 0.101s)
             Mean action noise std: 1.75
          Mean value_function loss: 145.7032
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.0474
                       Mean reward: 289.12
               Mean episode length: 163.15
    Episode_Reward/reaching_object: 0.6506
     Episode_Reward/lifting_object: 60.5745
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 1.95s
                      Time elapsed: 00:25:45
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 51269 steps/s (collection: 1.821s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 148.7632
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.0514
                       Mean reward: 294.55
               Mean episode length: 168.86
    Episode_Reward/reaching_object: 0.6484
     Episode_Reward/lifting_object: 60.8611
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.92s
                      Time elapsed: 00:25:47
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 49563 steps/s (collection: 1.872s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 139.7560
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.0527
                       Mean reward: 315.70
               Mean episode length: 176.28
    Episode_Reward/reaching_object: 0.6465
     Episode_Reward/lifting_object: 60.9827
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 1.98s
                      Time elapsed: 00:25:49
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 50302 steps/s (collection: 1.841s, learning 0.113s)
             Mean action noise std: 1.75
          Mean value_function loss: 138.4125
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.0530
                       Mean reward: 321.23
               Mean episode length: 182.00
    Episode_Reward/reaching_object: 0.6418
     Episode_Reward/lifting_object: 59.8854
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 1.95s
                      Time elapsed: 00:25:51
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 50330 steps/s (collection: 1.843s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 146.3468
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 35.0559
                       Mean reward: 307.99
               Mean episode length: 170.96
    Episode_Reward/reaching_object: 0.6374
     Episode_Reward/lifting_object: 59.8454
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 1.95s
                      Time elapsed: 00:25:53
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 49757 steps/s (collection: 1.851s, learning 0.125s)
             Mean action noise std: 1.75
          Mean value_function loss: 133.4415
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.0569
                       Mean reward: 321.23
               Mean episode length: 178.82
    Episode_Reward/reaching_object: 0.6687
     Episode_Reward/lifting_object: 63.0699
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 1.98s
                      Time elapsed: 00:25:55
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 50504 steps/s (collection: 1.854s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 147.8818
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.0576
                       Mean reward: 308.68
               Mean episode length: 173.74
    Episode_Reward/reaching_object: 0.6777
     Episode_Reward/lifting_object: 63.1482
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 1.95s
                      Time elapsed: 00:25:57
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 50398 steps/s (collection: 1.842s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 141.5846
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.0582
                       Mean reward: 311.79
               Mean episode length: 176.46
    Episode_Reward/reaching_object: 0.6859
     Episode_Reward/lifting_object: 64.5016
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 1.95s
                      Time elapsed: 00:25:59
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 50741 steps/s (collection: 1.830s, learning 0.107s)
             Mean action noise std: 1.75
          Mean value_function loss: 150.1550
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.0593
                       Mean reward: 325.71
               Mean episode length: 181.55
    Episode_Reward/reaching_object: 0.6714
     Episode_Reward/lifting_object: 63.0325
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 1.94s
                      Time elapsed: 00:26:01
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 50301 steps/s (collection: 1.844s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 130.9215
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.0612
                       Mean reward: 327.85
               Mean episode length: 182.59
    Episode_Reward/reaching_object: 0.6969
     Episode_Reward/lifting_object: 65.6712
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 1.95s
                      Time elapsed: 00:26:03
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 49125 steps/s (collection: 1.882s, learning 0.119s)
             Mean action noise std: 1.76
          Mean value_function loss: 144.5986
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.0614
                       Mean reward: 335.38
               Mean episode length: 185.24
    Episode_Reward/reaching_object: 0.6830
     Episode_Reward/lifting_object: 64.1129
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.00s
                      Time elapsed: 00:26:05
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 49857 steps/s (collection: 1.851s, learning 0.121s)
             Mean action noise std: 1.76
          Mean value_function loss: 162.9788
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.0627
                       Mean reward: 339.54
               Mean episode length: 187.51
    Episode_Reward/reaching_object: 0.6947
     Episode_Reward/lifting_object: 65.6286
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 1.97s
                      Time elapsed: 00:26:07
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 50280 steps/s (collection: 1.843s, learning 0.113s)
             Mean action noise std: 1.76
          Mean value_function loss: 168.1644
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.0657
                       Mean reward: 295.36
               Mean episode length: 164.47
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 62.8263
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 1.96s
                      Time elapsed: 00:26:09
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 51022 steps/s (collection: 1.831s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 169.5076
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.0678
                       Mean reward: 280.97
               Mean episode length: 157.69
    Episode_Reward/reaching_object: 0.6385
     Episode_Reward/lifting_object: 61.4306
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 1.93s
                      Time elapsed: 00:26:11
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 49899 steps/s (collection: 1.847s, learning 0.123s)
             Mean action noise std: 1.76
          Mean value_function loss: 170.8769
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.0686
                       Mean reward: 308.88
               Mean episode length: 170.05
    Episode_Reward/reaching_object: 0.6441
     Episode_Reward/lifting_object: 62.0082
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 1.97s
                      Time elapsed: 00:26:12
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 50070 steps/s (collection: 1.843s, learning 0.120s)
             Mean action noise std: 1.76
          Mean value_function loss: 162.4082
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.0689
                       Mean reward: 310.79
               Mean episode length: 166.60
    Episode_Reward/reaching_object: 0.6524
     Episode_Reward/lifting_object: 62.9282
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 1.96s
                      Time elapsed: 00:26:14
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 49929 steps/s (collection: 1.862s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 160.5113
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.0681
                       Mean reward: 299.69
               Mean episode length: 167.89
    Episode_Reward/reaching_object: 0.6516
     Episode_Reward/lifting_object: 62.5611
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 1.97s
                      Time elapsed: 00:26:16
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 50020 steps/s (collection: 1.859s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 144.1009
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.0681
                       Mean reward: 322.60
               Mean episode length: 175.64
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: 64.3096
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 1.97s
                      Time elapsed: 00:26:18
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 50332 steps/s (collection: 1.845s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 163.2706
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.0683
                       Mean reward: 346.48
               Mean episode length: 189.11
    Episode_Reward/reaching_object: 0.6677
     Episode_Reward/lifting_object: 65.6582
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 1.95s
                      Time elapsed: 00:26:20
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 50492 steps/s (collection: 1.840s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 155.5336
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.0696
                       Mean reward: 307.75
               Mean episode length: 168.84
    Episode_Reward/reaching_object: 0.6363
     Episode_Reward/lifting_object: 61.6917
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 1.95s
                      Time elapsed: 00:26:22
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 49985 steps/s (collection: 1.856s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 155.2883
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.0711
                       Mean reward: 333.80
               Mean episode length: 179.72
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 65.3750
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 1.97s
                      Time elapsed: 00:26:24
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 50174 steps/s (collection: 1.853s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 147.2765
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.0740
                       Mean reward: 330.70
               Mean episode length: 178.52
    Episode_Reward/reaching_object: 0.6673
     Episode_Reward/lifting_object: 65.1553
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 1.96s
                      Time elapsed: 00:26:26
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 50216 steps/s (collection: 1.848s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 147.5866
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.0774
                       Mean reward: 320.28
               Mean episode length: 173.61
    Episode_Reward/reaching_object: 0.6631
     Episode_Reward/lifting_object: 65.1946
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 1.96s
                      Time elapsed: 00:26:28
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 50362 steps/s (collection: 1.845s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 131.8326
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.0810
                       Mean reward: 331.87
               Mean episode length: 180.44
    Episode_Reward/reaching_object: 0.6885
     Episode_Reward/lifting_object: 67.8584
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 1.95s
                      Time elapsed: 00:26:30
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 49679 steps/s (collection: 1.853s, learning 0.126s)
             Mean action noise std: 1.76
          Mean value_function loss: 145.9089
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.0823
                       Mean reward: 346.55
               Mean episode length: 189.61
    Episode_Reward/reaching_object: 0.6756
     Episode_Reward/lifting_object: 65.7546
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 1.98s
                      Time elapsed: 00:26:32
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 50089 steps/s (collection: 1.845s, learning 0.118s)
             Mean action noise std: 1.76
          Mean value_function loss: 153.0966
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.0808
                       Mean reward: 352.66
               Mean episode length: 189.92
    Episode_Reward/reaching_object: 0.6795
     Episode_Reward/lifting_object: 66.3711
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 1.96s
                      Time elapsed: 00:26:34
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 49877 steps/s (collection: 1.854s, learning 0.117s)
             Mean action noise std: 1.76
          Mean value_function loss: 163.0431
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.0788
                       Mean reward: 348.73
               Mean episode length: 187.17
    Episode_Reward/reaching_object: 0.6934
     Episode_Reward/lifting_object: 68.0563
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 1.97s
                      Time elapsed: 00:26:36
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 50525 steps/s (collection: 1.826s, learning 0.120s)
             Mean action noise std: 1.76
          Mean value_function loss: 149.3926
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.0816
                       Mean reward: 328.56
               Mean episode length: 177.52
    Episode_Reward/reaching_object: 0.6767
     Episode_Reward/lifting_object: 66.7904
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 1.95s
                      Time elapsed: 00:26:38
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 49830 steps/s (collection: 1.847s, learning 0.126s)
             Mean action noise std: 1.76
          Mean value_function loss: 170.8919
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.0842
                       Mean reward: 324.24
               Mean episode length: 176.68
    Episode_Reward/reaching_object: 0.6710
     Episode_Reward/lifting_object: 66.0750
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 1.97s
                      Time elapsed: 00:26:40
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 50182 steps/s (collection: 1.846s, learning 0.113s)
             Mean action noise std: 1.76
          Mean value_function loss: 165.0267
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.0890
                       Mean reward: 315.48
               Mean episode length: 166.62
    Episode_Reward/reaching_object: 0.6738
     Episode_Reward/lifting_object: 66.9721
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 1.96s
                      Time elapsed: 00:26:42
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 49835 steps/s (collection: 1.859s, learning 0.113s)
             Mean action noise std: 1.76
          Mean value_function loss: 192.8101
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.0968
                       Mean reward: 328.69
               Mean episode length: 174.48
    Episode_Reward/reaching_object: 0.6518
     Episode_Reward/lifting_object: 65.1482
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 1.97s
                      Time elapsed: 00:26:44
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 50605 steps/s (collection: 1.827s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 182.8005
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.1058
                       Mean reward: 332.35
               Mean episode length: 180.63
    Episode_Reward/reaching_object: 0.6548
     Episode_Reward/lifting_object: 65.2401
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 1.94s
                      Time elapsed: 00:26:46
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 49968 steps/s (collection: 1.860s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 161.4389
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.1077
                       Mean reward: 336.23
               Mean episode length: 176.24
    Episode_Reward/reaching_object: 0.6387
     Episode_Reward/lifting_object: 64.0632
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 1.97s
                      Time elapsed: 00:26:48
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 50301 steps/s (collection: 1.838s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 184.8591
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.1074
                       Mean reward: 326.99
               Mean episode length: 172.65
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 64.9305
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 1.95s
                      Time elapsed: 00:26:50
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 50947 steps/s (collection: 1.841s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 157.4318
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.1086
                       Mean reward: 351.91
               Mean episode length: 185.83
    Episode_Reward/reaching_object: 0.6536
     Episode_Reward/lifting_object: 64.5856
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 1.93s
                      Time elapsed: 00:26:52
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 50383 steps/s (collection: 1.862s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 177.1971
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.1099
                       Mean reward: 343.11
               Mean episode length: 180.37
    Episode_Reward/reaching_object: 0.6531
     Episode_Reward/lifting_object: 65.9039
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 1.95s
                      Time elapsed: 00:26:54
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 49317 steps/s (collection: 1.886s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 172.8620
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.1105
                       Mean reward: 343.20
               Mean episode length: 182.71
    Episode_Reward/reaching_object: 0.6788
     Episode_Reward/lifting_object: 67.8914
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 1.99s
                      Time elapsed: 00:26:56
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 49410 steps/s (collection: 1.874s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 157.3998
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 35.1108
                       Mean reward: 343.80
               Mean episode length: 180.56
    Episode_Reward/reaching_object: 0.6858
     Episode_Reward/lifting_object: 68.8782
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 1.99s
                      Time elapsed: 00:26:58
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 50223 steps/s (collection: 1.847s, learning 0.111s)
             Mean action noise std: 1.76
          Mean value_function loss: 171.0812
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.1104
                       Mean reward: 345.32
               Mean episode length: 182.61
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 66.1502
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 1.96s
                      Time elapsed: 00:27:00
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 48026 steps/s (collection: 1.931s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 161.1210
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 35.1107
                       Mean reward: 336.63
               Mean episode length: 182.00
    Episode_Reward/reaching_object: 0.6822
     Episode_Reward/lifting_object: 67.6419
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.05s
                      Time elapsed: 00:27:02
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 49781 steps/s (collection: 1.865s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 156.2951
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.1121
                       Mean reward: 334.43
               Mean episode length: 177.05
    Episode_Reward/reaching_object: 0.6878
     Episode_Reward/lifting_object: 68.9303
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 1.97s
                      Time elapsed: 00:27:04
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 49895 steps/s (collection: 1.856s, learning 0.114s)
             Mean action noise std: 1.76
          Mean value_function loss: 153.4971
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.1130
                       Mean reward: 347.99
               Mean episode length: 182.32
    Episode_Reward/reaching_object: 0.6903
     Episode_Reward/lifting_object: 69.9513
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 1.97s
                      Time elapsed: 00:27:06
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 49766 steps/s (collection: 1.855s, learning 0.120s)
             Mean action noise std: 1.76
          Mean value_function loss: 162.2604
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.1136
                       Mean reward: 349.67
               Mean episode length: 180.25
    Episode_Reward/reaching_object: 0.6896
     Episode_Reward/lifting_object: 69.7049
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 1.98s
                      Time elapsed: 00:27:08
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 50388 steps/s (collection: 1.836s, learning 0.115s)
             Mean action noise std: 1.76
          Mean value_function loss: 169.1256
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.1142
                       Mean reward: 347.28
               Mean episode length: 179.55
    Episode_Reward/reaching_object: 0.6807
     Episode_Reward/lifting_object: 69.0240
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 1.95s
                      Time elapsed: 00:27:09
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 49313 steps/s (collection: 1.875s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 188.7434
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.1152
                       Mean reward: 355.32
               Mean episode length: 184.36
    Episode_Reward/reaching_object: 0.6687
     Episode_Reward/lifting_object: 67.9603
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 1.99s
                      Time elapsed: 00:27:11
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 49746 steps/s (collection: 1.865s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 199.2776
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.1186
                       Mean reward: 364.94
               Mean episode length: 186.24
    Episode_Reward/reaching_object: 0.6588
     Episode_Reward/lifting_object: 67.0841
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 1.98s
                      Time elapsed: 00:27:13
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 49660 steps/s (collection: 1.868s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 194.7242
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.1235
                       Mean reward: 337.30
               Mean episode length: 173.89
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: 66.3306
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 1.98s
                      Time elapsed: 00:27:15
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 49434 steps/s (collection: 1.890s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 196.2379
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.1270
                       Mean reward: 318.32
               Mean episode length: 167.96
    Episode_Reward/reaching_object: 0.6436
     Episode_Reward/lifting_object: 65.6065
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 1.99s
                      Time elapsed: 00:27:17
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 49926 steps/s (collection: 1.871s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 182.7162
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.1278
                       Mean reward: 332.82
               Mean episode length: 172.57
    Episode_Reward/reaching_object: 0.6356
     Episode_Reward/lifting_object: 65.2375
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 1.97s
                      Time elapsed: 00:27:19
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 49875 steps/s (collection: 1.872s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 211.6462
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.1284
                       Mean reward: 328.93
               Mean episode length: 169.70
    Episode_Reward/reaching_object: 0.6407
     Episode_Reward/lifting_object: 65.6587
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 1.97s
                      Time elapsed: 00:27:21
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 50129 steps/s (collection: 1.854s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 208.4229
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.1292
                       Mean reward: 321.94
               Mean episode length: 171.17
    Episode_Reward/reaching_object: 0.6338
     Episode_Reward/lifting_object: 64.1613
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 1.96s
                      Time elapsed: 00:27:23
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 49417 steps/s (collection: 1.874s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 216.2993
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.1345
                       Mean reward: 311.99
               Mean episode length: 161.18
    Episode_Reward/reaching_object: 0.6160
     Episode_Reward/lifting_object: 63.4884
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 1.99s
                      Time elapsed: 00:27:25
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 50100 steps/s (collection: 1.852s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 223.5001
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.1384
                       Mean reward: 321.27
               Mean episode length: 167.90
    Episode_Reward/reaching_object: 0.6221
     Episode_Reward/lifting_object: 62.8643
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 1.96s
                      Time elapsed: 00:27:27
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 49796 steps/s (collection: 1.857s, learning 0.117s)
             Mean action noise std: 1.77
          Mean value_function loss: 191.0838
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.1414
                       Mean reward: 352.43
               Mean episode length: 178.42
    Episode_Reward/reaching_object: 0.6483
     Episode_Reward/lifting_object: 66.5918
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 1.97s
                      Time elapsed: 00:27:29
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 49822 steps/s (collection: 1.860s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 185.9714
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.1424
                       Mean reward: 300.35
               Mean episode length: 155.05
    Episode_Reward/reaching_object: 0.6444
     Episode_Reward/lifting_object: 65.8593
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 1.97s
                      Time elapsed: 00:27:31
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 50714 steps/s (collection: 1.843s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 186.1788
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.1441
                       Mean reward: 337.63
               Mean episode length: 174.90
    Episode_Reward/reaching_object: 0.6395
     Episode_Reward/lifting_object: 64.9496
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 1.94s
                      Time elapsed: 00:27:33
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 50384 steps/s (collection: 1.854s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 190.0880
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.1450
                       Mean reward: 346.68
               Mean episode length: 179.78
    Episode_Reward/reaching_object: 0.6717
     Episode_Reward/lifting_object: 68.4554
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 1.95s
                      Time elapsed: 00:27:35
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 49801 steps/s (collection: 1.852s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 182.6039
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.1458
                       Mean reward: 351.28
               Mean episode length: 179.02
    Episode_Reward/reaching_object: 0.6603
     Episode_Reward/lifting_object: 68.1577
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 1.97s
                      Time elapsed: 00:27:37
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 49095 steps/s (collection: 1.876s, learning 0.126s)
             Mean action noise std: 1.77
          Mean value_function loss: 186.9031
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.1472
                       Mean reward: 373.90
               Mean episode length: 189.63
    Episode_Reward/reaching_object: 0.6774
     Episode_Reward/lifting_object: 70.2137
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.00s
                      Time elapsed: 00:27:39
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 48833 steps/s (collection: 1.909s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 191.8371
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.1487
                       Mean reward: 349.65
               Mean episode length: 176.98
    Episode_Reward/reaching_object: 0.6755
     Episode_Reward/lifting_object: 69.1919
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.01s
                      Time elapsed: 00:27:41
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 48575 steps/s (collection: 1.910s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 192.7844
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.1493
                       Mean reward: 345.67
               Mean episode length: 174.00
    Episode_Reward/reaching_object: 0.6552
     Episode_Reward/lifting_object: 67.7567
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.02s
                      Time elapsed: 00:27:43
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 50326 steps/s (collection: 1.839s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 186.9352
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 35.1509
                       Mean reward: 339.23
               Mean episode length: 171.97
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 70.3512
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 1.95s
                      Time elapsed: 00:27:45
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 50299 steps/s (collection: 1.846s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 186.2631
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.1510
                       Mean reward: 373.58
               Mean episode length: 187.24
    Episode_Reward/reaching_object: 0.6612
     Episode_Reward/lifting_object: 68.6515
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 1.95s
                      Time elapsed: 00:27:47
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 49808 steps/s (collection: 1.856s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 176.9316
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 35.1516
                       Mean reward: 351.71
               Mean episode length: 180.71
    Episode_Reward/reaching_object: 0.6736
     Episode_Reward/lifting_object: 69.5783
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 1.97s
                      Time elapsed: 00:27:49
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 49682 steps/s (collection: 1.856s, learning 0.123s)
             Mean action noise std: 1.77
          Mean value_function loss: 178.1011
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 35.1521
                       Mean reward: 335.39
               Mean episode length: 171.19
    Episode_Reward/reaching_object: 0.6850
     Episode_Reward/lifting_object: 71.0130
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 1.98s
                      Time elapsed: 00:27:51
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 49959 steps/s (collection: 1.856s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 175.2921
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.1523
                       Mean reward: 362.10
               Mean episode length: 183.24
    Episode_Reward/reaching_object: 0.6667
     Episode_Reward/lifting_object: 69.2213
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 1.97s
                      Time elapsed: 00:27:53
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 48740 steps/s (collection: 1.890s, learning 0.127s)
             Mean action noise std: 1.77
          Mean value_function loss: 172.2705
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.1519
                       Mean reward: 328.87
               Mean episode length: 169.68
    Episode_Reward/reaching_object: 0.6899
     Episode_Reward/lifting_object: 71.9053
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.02s
                      Time elapsed: 00:27:55
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 49650 steps/s (collection: 1.867s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 176.1889
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.1524
                       Mean reward: 353.68
               Mean episode length: 180.11
    Episode_Reward/reaching_object: 0.6711
     Episode_Reward/lifting_object: 69.5733
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 1.98s
                      Time elapsed: 00:27:57
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 50178 steps/s (collection: 1.848s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 194.4299
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.1540
                       Mean reward: 362.28
               Mean episode length: 182.54
    Episode_Reward/reaching_object: 0.6722
     Episode_Reward/lifting_object: 69.5308
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 1.96s
                      Time elapsed: 00:27:59
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 49619 steps/s (collection: 1.870s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 171.4808
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.1587
                       Mean reward: 373.67
               Mean episode length: 185.84
    Episode_Reward/reaching_object: 0.6853
     Episode_Reward/lifting_object: 70.9949
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 1.98s
                      Time elapsed: 00:28:01
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 49811 steps/s (collection: 1.860s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 168.2190
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.1622
                       Mean reward: 348.27
               Mean episode length: 175.43
    Episode_Reward/reaching_object: 0.6720
     Episode_Reward/lifting_object: 70.1224
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 1.97s
                      Time elapsed: 00:28:03
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 49726 steps/s (collection: 1.869s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 182.4732
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.1643
                       Mean reward: 392.58
               Mean episode length: 196.73
    Episode_Reward/reaching_object: 0.7076
     Episode_Reward/lifting_object: 74.3441
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 1.98s
                      Time elapsed: 00:28:05
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 49870 steps/s (collection: 1.859s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 178.0580
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.1653
                       Mean reward: 382.22
               Mean episode length: 190.19
    Episode_Reward/reaching_object: 0.6959
     Episode_Reward/lifting_object: 72.6526
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 1.97s
                      Time elapsed: 00:28:07
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 50475 steps/s (collection: 1.846s, learning 0.102s)
             Mean action noise std: 1.77
          Mean value_function loss: 182.1075
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.1657
                       Mean reward: 364.95
               Mean episode length: 180.33
    Episode_Reward/reaching_object: 0.6923
     Episode_Reward/lifting_object: 72.9322
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 1.95s
                      Time elapsed: 00:28:09
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 48457 steps/s (collection: 1.914s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 289.8369
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.1662
                       Mean reward: 352.43
               Mean episode length: 179.65
    Episode_Reward/reaching_object: 0.6766
     Episode_Reward/lifting_object: 71.0581
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.03s
                      Time elapsed: 00:28:11
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 47742 steps/s (collection: 1.953s, learning 0.106s)
             Mean action noise std: 1.77
          Mean value_function loss: 192.6392
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.1698
                       Mean reward: 317.27
               Mean episode length: 161.89
    Episode_Reward/reaching_object: 0.6783
     Episode_Reward/lifting_object: 70.8727
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.06s
                      Time elapsed: 00:28:13
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 49213 steps/s (collection: 1.889s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 188.1962
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.1723
                       Mean reward: 368.07
               Mean episode length: 181.39
    Episode_Reward/reaching_object: 0.6701
     Episode_Reward/lifting_object: 70.1554
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.00s
                      Time elapsed: 00:28:15
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 49933 steps/s (collection: 1.862s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 175.0952
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.1736
                       Mean reward: 364.06
               Mean episode length: 180.68
    Episode_Reward/reaching_object: 0.6847
     Episode_Reward/lifting_object: 72.1427
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 1.97s
                      Time elapsed: 00:28:17
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 49701 steps/s (collection: 1.859s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 190.8767
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.1734
                       Mean reward: 370.70
               Mean episode length: 179.77
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 71.5856
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 1.98s
                      Time elapsed: 00:28:19
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 50161 steps/s (collection: 1.851s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 176.2454
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.1743
                       Mean reward: 382.58
               Mean episode length: 186.01
    Episode_Reward/reaching_object: 0.6742
     Episode_Reward/lifting_object: 70.9552
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 1.96s
                      Time elapsed: 00:28:21
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 49178 steps/s (collection: 1.882s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 177.8567
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.1777
                       Mean reward: 346.62
               Mean episode length: 171.58
    Episode_Reward/reaching_object: 0.6761
     Episode_Reward/lifting_object: 71.3171
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.00s
                      Time elapsed: 00:28:23
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 49208 steps/s (collection: 1.882s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.9625
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.1824
                       Mean reward: 381.41
               Mean episode length: 185.35
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 73.2470
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.00s
                      Time elapsed: 00:28:25
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 49416 steps/s (collection: 1.879s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.8439
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.1829
                       Mean reward: 380.67
               Mean episode length: 190.15
    Episode_Reward/reaching_object: 0.7192
     Episode_Reward/lifting_object: 75.4494
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 1.99s
                      Time elapsed: 00:28:27
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 50029 steps/s (collection: 1.856s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 186.0081
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.1833
                       Mean reward: 358.60
               Mean episode length: 176.68
    Episode_Reward/reaching_object: 0.6978
     Episode_Reward/lifting_object: 73.8491
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 1.96s
                      Time elapsed: 00:28:29
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 49578 steps/s (collection: 1.871s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 171.5850
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.1841
                       Mean reward: 369.75
               Mean episode length: 182.89
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 75.0928
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 1.98s
                      Time elapsed: 00:28:31
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 49808 steps/s (collection: 1.864s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 200.9981
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.1849
                       Mean reward: 378.76
               Mean episode length: 187.20
    Episode_Reward/reaching_object: 0.7066
     Episode_Reward/lifting_object: 74.2170
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 1.97s
                      Time elapsed: 00:28:33
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 49342 steps/s (collection: 1.885s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.4358
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.1873
                       Mean reward: 375.20
               Mean episode length: 189.08
    Episode_Reward/reaching_object: 0.7144
     Episode_Reward/lifting_object: 74.5579
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.99s
                      Time elapsed: 00:28:35
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 50194 steps/s (collection: 1.856s, learning 0.103s)
             Mean action noise std: 1.78
          Mean value_function loss: 191.3145
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.1928
                       Mean reward: 365.22
               Mean episode length: 178.91
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 74.7625
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 1.96s
                      Time elapsed: 00:28:37
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 48498 steps/s (collection: 1.912s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.0775
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 35.1942
                       Mean reward: 369.34
               Mean episode length: 183.16
    Episode_Reward/reaching_object: 0.6992
     Episode_Reward/lifting_object: 73.4905
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.03s
                      Time elapsed: 00:28:39
                               ETA: 00:41:36

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 49168 steps/s (collection: 1.883s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.1816
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.1946
                       Mean reward: 395.44
               Mean episode length: 193.51
    Episode_Reward/reaching_object: 0.7152
     Episode_Reward/lifting_object: 75.8172
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.00s
                      Time elapsed: 00:28:41
                               ETA: 00:41:34

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 49920 steps/s (collection: 1.869s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 177.1311
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.1934
                       Mean reward: 398.31
               Mean episode length: 194.14
    Episode_Reward/reaching_object: 0.6990
     Episode_Reward/lifting_object: 73.8015
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 1.97s
                      Time elapsed: 00:28:43
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 49390 steps/s (collection: 1.877s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 196.2812
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.1935
                       Mean reward: 379.55
               Mean episode length: 185.15
    Episode_Reward/reaching_object: 0.6884
     Episode_Reward/lifting_object: 72.8243
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 1.99s
                      Time elapsed: 00:28:45
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 49462 steps/s (collection: 1.880s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 201.9561
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.1970
                       Mean reward: 389.39
               Mean episode length: 189.42
    Episode_Reward/reaching_object: 0.7059
     Episode_Reward/lifting_object: 74.9159
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 1.99s
                      Time elapsed: 00:28:47
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 49634 steps/s (collection: 1.872s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 172.0229
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.1959
                       Mean reward: 375.99
               Mean episode length: 185.22
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: 75.5454
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 1.98s
                      Time elapsed: 00:28:49
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 49834 steps/s (collection: 1.859s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.3234
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.1948
                       Mean reward: 388.11
               Mean episode length: 187.16
    Episode_Reward/reaching_object: 0.7252
     Episode_Reward/lifting_object: 76.8200
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 1.97s
                      Time elapsed: 00:28:51
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 49925 steps/s (collection: 1.856s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.2329
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.2000
                       Mean reward: 385.87
               Mean episode length: 187.82
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 77.2227
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 1.97s
                      Time elapsed: 00:28:52
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 48792 steps/s (collection: 1.904s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 187.6298
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.2036
                       Mean reward: 353.54
               Mean episode length: 174.37
    Episode_Reward/reaching_object: 0.6887
     Episode_Reward/lifting_object: 72.5390
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.01s
                      Time elapsed: 00:28:55
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 49719 steps/s (collection: 1.870s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.9675
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.2053
                       Mean reward: 391.68
               Mean episode length: 190.61
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 76.7434
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 1.98s
                      Time elapsed: 00:28:56
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 49168 steps/s (collection: 1.892s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 164.5206
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.2057
                       Mean reward: 389.71
               Mean episode length: 189.46
    Episode_Reward/reaching_object: 0.7385
     Episode_Reward/lifting_object: 77.9009
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.00s
                      Time elapsed: 00:28:58
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 49284 steps/s (collection: 1.885s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 197.9197
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.2061
                       Mean reward: 383.90
               Mean episode length: 184.81
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 76.1782
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 1.99s
                      Time elapsed: 00:29:00
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 50059 steps/s (collection: 1.869s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.5623
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.2085
                       Mean reward: 386.62
               Mean episode length: 187.26
    Episode_Reward/reaching_object: 0.7221
     Episode_Reward/lifting_object: 75.9189
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.96s
                      Time elapsed: 00:29:02
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 48670 steps/s (collection: 1.902s, learning 0.118s)
             Mean action noise std: 1.78
          Mean value_function loss: 162.8886
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.2124
                       Mean reward: 370.00
               Mean episode length: 181.56
    Episode_Reward/reaching_object: 0.6929
     Episode_Reward/lifting_object: 72.8757
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.02s
                      Time elapsed: 00:29:04
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 44777 steps/s (collection: 1.999s, learning 0.197s)
             Mean action noise std: 1.78
          Mean value_function loss: 158.1050
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.2183
                       Mean reward: 394.55
               Mean episode length: 192.11
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 77.3862
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.20s
                      Time elapsed: 00:29:07
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 43821 steps/s (collection: 2.098s, learning 0.145s)
             Mean action noise std: 1.78
          Mean value_function loss: 186.4606
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.2217
                       Mean reward: 381.38
               Mean episode length: 183.64
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: 78.1392
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.24s
                      Time elapsed: 00:29:09
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 49208 steps/s (collection: 1.902s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 154.5976
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.2236
                       Mean reward: 383.66
               Mean episode length: 189.94
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 79.6503
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.00s
                      Time elapsed: 00:29:11
                               ETA: 00:41:00

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 50907 steps/s (collection: 1.844s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 153.4547
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.2284
                       Mean reward: 405.40
               Mean episode length: 196.79
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 79.7836
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 1.93s
                      Time elapsed: 00:29:13
                               ETA: 00:40:58

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 48073 steps/s (collection: 1.918s, learning 0.127s)
             Mean action noise std: 1.78
          Mean value_function loss: 159.6395
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.2346
                       Mean reward: 412.80
               Mean episode length: 200.41
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 80.2616
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.04s
                      Time elapsed: 00:29:15
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 49445 steps/s (collection: 1.894s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 155.9244
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.2389
                       Mean reward: 416.03
               Mean episode length: 200.09
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 81.0224
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 1.99s
                      Time elapsed: 00:29:17
                               ETA: 00:40:53

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 49110 steps/s (collection: 1.915s, learning 0.087s)
             Mean action noise std: 1.79
          Mean value_function loss: 154.9766
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.2403
                       Mean reward: 371.57
               Mean episode length: 178.47
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 77.7986
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.00s
                      Time elapsed: 00:29:19
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 47015 steps/s (collection: 1.910s, learning 0.181s)
             Mean action noise std: 1.79
          Mean value_function loss: 178.8123
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.2421
                       Mean reward: 442.95
               Mean episode length: 211.46
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 82.6733
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.09s
                      Time elapsed: 00:29:21
                               ETA: 00:40:49

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 49787 steps/s (collection: 1.880s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 157.3291
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.2425
                       Mean reward: 384.30
               Mean episode length: 192.37
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 80.6891
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 1.97s
                      Time elapsed: 00:29:23
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 48788 steps/s (collection: 1.914s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 175.6459
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.2433
                       Mean reward: 421.28
               Mean episode length: 201.14
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 81.0498
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.01s
                      Time elapsed: 00:29:25
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 49431 steps/s (collection: 1.879s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 170.9157
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.2482
                       Mean reward: 402.64
               Mean episode length: 192.43
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 80.3560
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 1.99s
                      Time elapsed: 00:29:27
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 48583 steps/s (collection: 1.918s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 175.3392
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.2539
                       Mean reward: 419.24
               Mean episode length: 200.71
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 81.3073
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.02s
                      Time elapsed: 00:29:29
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 49215 steps/s (collection: 1.901s, learning 0.096s)
             Mean action noise std: 1.79
          Mean value_function loss: 185.8522
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.2599
                       Mean reward: 394.00
               Mean episode length: 187.71
    Episode_Reward/reaching_object: 0.7275
     Episode_Reward/lifting_object: 78.2163
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.00s
                      Time elapsed: 00:29:31
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 48854 steps/s (collection: 1.921s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 191.4556
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.2659
                       Mean reward: 389.53
               Mean episode length: 184.55
    Episode_Reward/reaching_object: 0.7105
     Episode_Reward/lifting_object: 76.8454
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.01s
                      Time elapsed: 00:29:33
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 48301 steps/s (collection: 1.946s, learning 0.089s)
             Mean action noise std: 1.79
          Mean value_function loss: 185.1015
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.2729
                       Mean reward: 389.45
               Mean episode length: 184.86
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 80.7224
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.04s
                      Time elapsed: 00:29:35
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 49340 steps/s (collection: 1.888s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 174.8147
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2785
                       Mean reward: 412.52
               Mean episode length: 193.69
    Episode_Reward/reaching_object: 0.7255
     Episode_Reward/lifting_object: 78.2692
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 1.99s
                      Time elapsed: 00:29:37
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 49630 steps/s (collection: 1.890s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 201.1590
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.2854
                       Mean reward: 408.98
               Mean episode length: 190.35
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 79.8472
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 1.98s
                      Time elapsed: 00:29:39
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 48919 steps/s (collection: 1.917s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 199.0528
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.2936
                       Mean reward: 422.64
               Mean episode length: 197.81
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 80.3121
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.01s
                      Time elapsed: 00:29:41
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 48825 steps/s (collection: 1.908s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 229.9859
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.3013
                       Mean reward: 402.59
               Mean episode length: 191.67
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 81.9741
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.01s
                      Time elapsed: 00:29:43
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 49882 steps/s (collection: 1.875s, learning 0.096s)
             Mean action noise std: 1.79
          Mean value_function loss: 199.0023
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.3082
                       Mean reward: 414.40
               Mean episode length: 195.31
    Episode_Reward/reaching_object: 0.7289
     Episode_Reward/lifting_object: 78.1133
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 1.97s
                      Time elapsed: 00:29:45
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 49315 steps/s (collection: 1.891s, learning 0.102s)
             Mean action noise std: 1.79
          Mean value_function loss: 182.5119
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.3139
                       Mean reward: 390.74
               Mean episode length: 187.54
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 81.9124
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 1.99s
                      Time elapsed: 00:29:47
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 49993 steps/s (collection: 1.873s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 167.7636
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.3184
                       Mean reward: 408.58
               Mean episode length: 195.74
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 81.4324
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 1.97s
                      Time elapsed: 00:29:49
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 48973 steps/s (collection: 1.909s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 184.8208
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.3206
                       Mean reward: 414.23
               Mean episode length: 196.65
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 82.7423
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.01s
                      Time elapsed: 00:29:51
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 47205 steps/s (collection: 1.949s, learning 0.134s)
             Mean action noise std: 1.79
          Mean value_function loss: 168.5952
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.3218
                       Mean reward: 409.32
               Mean episode length: 192.40
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 81.8759
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.08s
                      Time elapsed: 00:29:53
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 47325 steps/s (collection: 1.975s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 188.4742
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.3228
                       Mean reward: 377.14
               Mean episode length: 184.73
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 81.6762
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.08s
                      Time elapsed: 00:29:55
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 48857 steps/s (collection: 1.920s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 170.7081
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.3250
                       Mean reward: 382.97
               Mean episode length: 185.33
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 82.5644
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.01s
                      Time elapsed: 00:29:57
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 49291 steps/s (collection: 1.885s, learning 0.109s)
             Mean action noise std: 1.80
          Mean value_function loss: 171.7968
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.3312
                       Mean reward: 426.74
               Mean episode length: 199.76
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 85.4695
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 1.99s
                      Time elapsed: 00:29:59
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 48877 steps/s (collection: 1.913s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 171.0637
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.3376
                       Mean reward: 432.06
               Mean episode length: 203.36
    Episode_Reward/reaching_object: 0.7894
     Episode_Reward/lifting_object: 85.4180
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.01s
                      Time elapsed: 00:30:01
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 48744 steps/s (collection: 1.899s, learning 0.118s)
             Mean action noise std: 1.80
          Mean value_function loss: 232.6234
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.3412
                       Mean reward: 417.17
               Mean episode length: 196.34
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 81.0587
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.02s
                      Time elapsed: 00:30:03
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 49331 steps/s (collection: 1.879s, learning 0.114s)
             Mean action noise std: 1.80
          Mean value_function loss: 180.3295
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.3449
                       Mean reward: 401.18
               Mean episode length: 186.55
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 82.2349
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 1.99s
                      Time elapsed: 00:30:05
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 48795 steps/s (collection: 1.890s, learning 0.125s)
             Mean action noise std: 1.80
          Mean value_function loss: 174.6008
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.3490
                       Mean reward: 398.44
               Mean episode length: 188.26
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 80.4730
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.01s
                      Time elapsed: 00:30:07
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 48298 steps/s (collection: 1.944s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 184.3084
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.3559
                       Mean reward: 391.38
               Mean episode length: 186.48
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 79.1771
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.04s
                      Time elapsed: 00:30:09
                               ETA: 00:39:56

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 49522 steps/s (collection: 1.894s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 164.7145
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.3617
                       Mean reward: 425.19
               Mean episode length: 196.68
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 83.0869
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 1.99s
                      Time elapsed: 00:30:11
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 49173 steps/s (collection: 1.898s, learning 0.101s)
             Mean action noise std: 1.80
          Mean value_function loss: 177.9363
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.3656
                       Mean reward: 432.74
               Mean episode length: 200.70
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 84.4039
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.00s
                      Time elapsed: 00:30:13
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 46606 steps/s (collection: 1.968s, learning 0.142s)
             Mean action noise std: 1.80
          Mean value_function loss: 183.7833
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.3727
                       Mean reward: 406.86
               Mean episode length: 189.22
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 83.5572
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.11s
                      Time elapsed: 00:30:15
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 46722 steps/s (collection: 1.962s, learning 0.142s)
             Mean action noise std: 1.80
          Mean value_function loss: 191.9212
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.3798
                       Mean reward: 427.38
               Mean episode length: 194.84
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 82.9138
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.10s
                      Time elapsed: 00:30:17
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 46042 steps/s (collection: 2.046s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 186.8241
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.3804
                       Mean reward: 414.30
               Mean episode length: 188.41
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 82.4820
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.14s
                      Time elapsed: 00:30:20
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 49532 steps/s (collection: 1.889s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 194.3674
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.3816
                       Mean reward: 430.65
               Mean episode length: 197.44
    Episode_Reward/reaching_object: 0.7254
     Episode_Reward/lifting_object: 79.6792
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 1.98s
                      Time elapsed: 00:30:21
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 48395 steps/s (collection: 1.924s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 201.9122
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.3851
                       Mean reward: 411.52
               Mean episode length: 191.36
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 81.9300
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.03s
                      Time elapsed: 00:30:24
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 50196 steps/s (collection: 1.850s, learning 0.109s)
             Mean action noise std: 1.80
          Mean value_function loss: 208.3854
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.3922
                       Mean reward: 435.56
               Mean episode length: 198.13
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 84.3113
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 1.96s
                      Time elapsed: 00:30:25
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 47960 steps/s (collection: 1.944s, learning 0.106s)
             Mean action noise std: 1.81
          Mean value_function loss: 187.1558
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 35.3961
                       Mean reward: 450.13
               Mean episode length: 206.14
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 86.5680
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.05s
                      Time elapsed: 00:30:28
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 47244 steps/s (collection: 1.912s, learning 0.169s)
             Mean action noise std: 1.81
          Mean value_function loss: 197.3548
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.4010
                       Mean reward: 398.08
               Mean episode length: 180.51
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 84.2714
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.08s
                      Time elapsed: 00:30:30
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 49105 steps/s (collection: 1.888s, learning 0.114s)
             Mean action noise std: 1.81
          Mean value_function loss: 179.3991
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.4070
                       Mean reward: 423.16
               Mean episode length: 192.98
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 83.4438
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.00s
                      Time elapsed: 00:30:32
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 46598 steps/s (collection: 1.948s, learning 0.162s)
             Mean action noise std: 1.81
          Mean value_function loss: 189.2643
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.4096
                       Mean reward: 412.67
               Mean episode length: 187.20
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 83.9072
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.11s
                      Time elapsed: 00:30:34
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 49246 steps/s (collection: 1.872s, learning 0.124s)
             Mean action noise std: 1.81
          Mean value_function loss: 188.7299
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.4142
                       Mean reward: 432.14
               Mean episode length: 195.66
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 84.0543
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.00s
                      Time elapsed: 00:30:36
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 47297 steps/s (collection: 1.967s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 212.6425
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.4161
                       Mean reward: 444.01
               Mean episode length: 201.44
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 86.6661
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.08s
                      Time elapsed: 00:30:38
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 49355 steps/s (collection: 1.882s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 195.3248
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 35.4176
                       Mean reward: 448.55
               Mean episode length: 199.30
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 86.6606
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 1.99s
                      Time elapsed: 00:30:40
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 49195 steps/s (collection: 1.884s, learning 0.114s)
             Mean action noise std: 1.81
          Mean value_function loss: 195.0316
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.4183
                       Mean reward: 426.22
               Mean episode length: 193.16
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 85.8495
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.00s
                      Time elapsed: 00:30:42
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 49611 steps/s (collection: 1.875s, learning 0.106s)
             Mean action noise std: 1.81
          Mean value_function loss: 262.0749
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 35.4188
                       Mean reward: 426.08
               Mean episode length: 196.77
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 82.4120
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 1.98s
                      Time elapsed: 00:30:44
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 49998 steps/s (collection: 1.871s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 226.1102
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.4198
                       Mean reward: 421.03
               Mean episode length: 189.77
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 87.1165
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 1.97s
                      Time elapsed: 00:30:46
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 48835 steps/s (collection: 1.875s, learning 0.138s)
             Mean action noise std: 1.81
          Mean value_function loss: 188.3996
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.4214
                       Mean reward: 421.80
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 84.0384
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.01s
                      Time elapsed: 00:30:48
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 49250 steps/s (collection: 1.869s, learning 0.127s)
             Mean action noise std: 1.81
          Mean value_function loss: 193.8055
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.4224
                       Mean reward: 435.68
               Mean episode length: 197.30
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 85.7679
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.00s
                      Time elapsed: 00:30:50
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 49798 steps/s (collection: 1.887s, learning 0.087s)
             Mean action noise std: 1.81
          Mean value_function loss: 261.9210
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.4246
                       Mean reward: 423.98
               Mean episode length: 192.18
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 82.6361
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 1.97s
                      Time elapsed: 00:30:52
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 46889 steps/s (collection: 2.000s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 233.4907
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.4288
                       Mean reward: 422.30
               Mean episode length: 192.54
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 82.2978
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.10s
                      Time elapsed: 00:30:54
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 46458 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 222.4994
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.4376
                       Mean reward: 420.20
               Mean episode length: 190.25
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 86.7038
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.12s
                      Time elapsed: 00:30:56
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 46925 steps/s (collection: 1.965s, learning 0.130s)
             Mean action noise std: 1.81
          Mean value_function loss: 215.0628
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.4482
                       Mean reward: 426.21
               Mean episode length: 191.18
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 84.3461
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.09s
                      Time elapsed: 00:30:58
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 47768 steps/s (collection: 1.963s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 218.3633
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.4529
                       Mean reward: 412.29
               Mean episode length: 187.58
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 82.7860
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.06s
                      Time elapsed: 00:31:00
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 48583 steps/s (collection: 1.936s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 218.6831
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.4572
                       Mean reward: 426.21
               Mean episode length: 188.50
    Episode_Reward/reaching_object: 0.7365
     Episode_Reward/lifting_object: 81.7382
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.02s
                      Time elapsed: 00:31:02
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 48547 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 217.3519
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.4640
                       Mean reward: 424.01
               Mean episode length: 186.51
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 83.6374
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.02s
                      Time elapsed: 00:31:04
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 46590 steps/s (collection: 1.992s, learning 0.118s)
             Mean action noise std: 1.81
          Mean value_function loss: 205.5224
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.4664
                       Mean reward: 397.89
               Mean episode length: 179.24
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 83.3742
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.11s
                      Time elapsed: 00:31:06
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 44419 steps/s (collection: 2.098s, learning 0.116s)
             Mean action noise std: 1.81
          Mean value_function loss: 230.7863
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.4698
                       Mean reward: 427.25
               Mean episode length: 190.93
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 82.5820
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.21s
                      Time elapsed: 00:31:08
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 45723 steps/s (collection: 2.023s, learning 0.127s)
             Mean action noise std: 1.82
          Mean value_function loss: 206.1584
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.4776
                       Mean reward: 426.74
               Mean episode length: 193.47
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 81.6296
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.15s
                      Time elapsed: 00:31:11
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 47988 steps/s (collection: 1.940s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 196.1621
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.4857
                       Mean reward: 459.10
               Mean episode length: 201.65
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 85.3926
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.05s
                      Time elapsed: 00:31:13
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 47004 steps/s (collection: 2.001s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 195.0993
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.4939
                       Mean reward: 419.94
               Mean episode length: 189.65
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 87.3347
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.09s
                      Time elapsed: 00:31:15
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 46003 steps/s (collection: 2.008s, learning 0.129s)
             Mean action noise std: 1.82
          Mean value_function loss: 182.6699
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.5016
                       Mean reward: 456.93
               Mean episode length: 204.16
    Episode_Reward/reaching_object: 0.8078
     Episode_Reward/lifting_object: 90.2829
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.14s
                      Time elapsed: 00:31:17
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 47139 steps/s (collection: 1.995s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 175.7995
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.5106
                       Mean reward: 430.38
               Mean episode length: 194.25
    Episode_Reward/reaching_object: 0.7950
     Episode_Reward/lifting_object: 88.3504
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.09s
                      Time elapsed: 00:31:19
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 44658 steps/s (collection: 2.103s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 198.9382
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.5237
                       Mean reward: 445.76
               Mean episode length: 201.47
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 87.9059
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.20s
                      Time elapsed: 00:31:21
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 47807 steps/s (collection: 1.948s, learning 0.109s)
             Mean action noise std: 1.82
          Mean value_function loss: 189.8067
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.5328
                       Mean reward: 461.35
               Mean episode length: 206.72
    Episode_Reward/reaching_object: 0.7961
     Episode_Reward/lifting_object: 88.9743
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.06s
                      Time elapsed: 00:31:23
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 46860 steps/s (collection: 2.001s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 199.8408
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.5403
                       Mean reward: 450.64
               Mean episode length: 200.23
    Episode_Reward/reaching_object: 0.8019
     Episode_Reward/lifting_object: 90.0200
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.10s
                      Time elapsed: 00:31:25
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 46440 steps/s (collection: 2.010s, learning 0.107s)
             Mean action noise std: 1.82
          Mean value_function loss: 189.5603
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.5486
                       Mean reward: 429.40
               Mean episode length: 193.25
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 86.7420
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.12s
                      Time elapsed: 00:31:27
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 49078 steps/s (collection: 1.904s, learning 0.099s)
             Mean action noise std: 1.82
          Mean value_function loss: 199.8373
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.5518
                       Mean reward: 427.48
               Mean episode length: 192.75
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 87.3117
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.00s
                      Time elapsed: 00:31:29
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 47749 steps/s (collection: 1.930s, learning 0.128s)
             Mean action noise std: 1.82
          Mean value_function loss: 190.2266
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.5529
                       Mean reward: 415.20
               Mean episode length: 185.57
    Episode_Reward/reaching_object: 0.7795
     Episode_Reward/lifting_object: 87.6183
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.06s
                      Time elapsed: 00:31:31
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 48479 steps/s (collection: 1.923s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 195.5243
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.5537
                       Mean reward: 440.01
               Mean episode length: 196.39
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: 87.4539
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.03s
                      Time elapsed: 00:31:34
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 46848 steps/s (collection: 1.977s, learning 0.121s)
             Mean action noise std: 1.82
          Mean value_function loss: 196.5328
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.5557
                       Mean reward: 434.49
               Mean episode length: 193.92
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 87.7332
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.10s
                      Time elapsed: 00:31:36
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 48837 steps/s (collection: 1.910s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 167.2775
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.5592
                       Mean reward: 424.12
               Mean episode length: 190.98
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 88.1414
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.01s
                      Time elapsed: 00:31:38
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 48655 steps/s (collection: 1.910s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 170.4335
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.5598
                       Mean reward: 456.69
               Mean episode length: 202.72
    Episode_Reward/reaching_object: 0.8026
     Episode_Reward/lifting_object: 89.8290
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.02s
                      Time elapsed: 00:31:40
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 47968 steps/s (collection: 1.929s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 177.7805
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.5612
                       Mean reward: 451.79
               Mean episode length: 200.75
    Episode_Reward/reaching_object: 0.7980
     Episode_Reward/lifting_object: 89.4641
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.05s
                      Time elapsed: 00:31:42
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 47055 steps/s (collection: 1.943s, learning 0.146s)
             Mean action noise std: 1.83
          Mean value_function loss: 175.5867
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.5642
                       Mean reward: 474.39
               Mean episode length: 210.86
    Episode_Reward/reaching_object: 0.8161
     Episode_Reward/lifting_object: 91.0484
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.09s
                      Time elapsed: 00:31:44
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 46097 steps/s (collection: 1.965s, learning 0.168s)
             Mean action noise std: 1.83
          Mean value_function loss: 173.5910
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.5732
                       Mean reward: 434.35
               Mean episode length: 196.84
    Episode_Reward/reaching_object: 0.8016
     Episode_Reward/lifting_object: 88.9856
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.13s
                      Time elapsed: 00:31:46
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 46450 steps/s (collection: 1.947s, learning 0.170s)
             Mean action noise std: 1.83
          Mean value_function loss: 165.4225
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.5855
                       Mean reward: 446.96
               Mean episode length: 199.66
    Episode_Reward/reaching_object: 0.8265
     Episode_Reward/lifting_object: 91.8294
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.12s
                      Time elapsed: 00:31:48
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 48180 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 184.4682
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.5921
                       Mean reward: 449.61
               Mean episode length: 199.44
    Episode_Reward/reaching_object: 0.8250
     Episode_Reward/lifting_object: 91.8963
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.04s
                      Time elapsed: 00:31:50
                               ETA: 00:38:10

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 47972 steps/s (collection: 1.943s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 196.4121
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.5980
                       Mean reward: 442.57
               Mean episode length: 200.08
    Episode_Reward/reaching_object: 0.7990
     Episode_Reward/lifting_object: 89.2006
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.05s
                      Time elapsed: 00:31:52
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 46371 steps/s (collection: 2.029s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 174.9122
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.6025
                       Mean reward: 467.81
               Mean episode length: 209.37
    Episode_Reward/reaching_object: 0.8281
     Episode_Reward/lifting_object: 92.1539
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.12s
                      Time elapsed: 00:31:54
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 47779 steps/s (collection: 1.955s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 168.6210
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.6044
                       Mean reward: 451.66
               Mean episode length: 203.95
    Episode_Reward/reaching_object: 0.8069
     Episode_Reward/lifting_object: 89.9993
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.06s
                      Time elapsed: 00:31:56
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 48497 steps/s (collection: 1.931s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 157.8691
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6067
                       Mean reward: 452.94
               Mean episode length: 206.01
    Episode_Reward/reaching_object: 0.8239
     Episode_Reward/lifting_object: 91.3514
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.03s
                      Time elapsed: 00:31:58
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 47582 steps/s (collection: 1.952s, learning 0.114s)
             Mean action noise std: 1.83
          Mean value_function loss: 189.7355
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.6117
                       Mean reward: 443.42
               Mean episode length: 197.32
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 87.1953
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.07s
                      Time elapsed: 00:32:00
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 43700 steps/s (collection: 2.123s, learning 0.127s)
             Mean action noise std: 1.83
          Mean value_function loss: 164.5506
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.6128
                       Mean reward: 488.26
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 92.5985
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.25s
                      Time elapsed: 00:32:03
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 47735 steps/s (collection: 1.963s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 159.1072
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.6132
                       Mean reward: 493.22
               Mean episode length: 215.28
    Episode_Reward/reaching_object: 0.8287
     Episode_Reward/lifting_object: 92.3018
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.06s
                      Time elapsed: 00:32:05
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 45300 steps/s (collection: 2.055s, learning 0.116s)
             Mean action noise std: 1.83
          Mean value_function loss: 184.7990
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.6132
                       Mean reward: 434.40
               Mean episode length: 195.91
    Episode_Reward/reaching_object: 0.8092
     Episode_Reward/lifting_object: 89.9532
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.17s
                      Time elapsed: 00:32:07
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 46601 steps/s (collection: 2.009s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 190.2536
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.6140
                       Mean reward: 449.12
               Mean episode length: 202.86
    Episode_Reward/reaching_object: 0.8164
     Episode_Reward/lifting_object: 91.1160
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.11s
                      Time elapsed: 00:32:09
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 46846 steps/s (collection: 1.976s, learning 0.122s)
             Mean action noise std: 1.83
          Mean value_function loss: 210.7999
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.6149
                       Mean reward: 459.96
               Mean episode length: 202.69
    Episode_Reward/reaching_object: 0.8153
     Episode_Reward/lifting_object: 91.6374
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.10s
                      Time elapsed: 00:32:11
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 46220 steps/s (collection: 2.019s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 205.0159
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.6128
                       Mean reward: 445.53
               Mean episode length: 194.52
    Episode_Reward/reaching_object: 0.8000
     Episode_Reward/lifting_object: 90.0302
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.13s
                      Time elapsed: 00:32:13
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 48205 steps/s (collection: 1.919s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 228.4258
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.6124
                       Mean reward: 436.40
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 89.5433
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.04s
                      Time elapsed: 00:32:15
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 44631 steps/s (collection: 2.104s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 206.1329
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.6134
                       Mean reward: 437.00
               Mean episode length: 198.65
    Episode_Reward/reaching_object: 0.8049
     Episode_Reward/lifting_object: 90.0940
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.20s
                      Time elapsed: 00:32:17
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 48290 steps/s (collection: 1.917s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 207.3668
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.6179
                       Mean reward: 414.71
               Mean episode length: 183.57
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 86.5544
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.04s
                      Time elapsed: 00:32:19
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 42070 steps/s (collection: 2.169s, learning 0.168s)
             Mean action noise std: 1.83
          Mean value_function loss: 221.6916
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.6214
                       Mean reward: 424.55
               Mean episode length: 186.24
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 86.9368
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.34s
                      Time elapsed: 00:32:22
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 43817 steps/s (collection: 2.075s, learning 0.169s)
             Mean action noise std: 1.83
          Mean value_function loss: 226.5419
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.6275
                       Mean reward: 450.35
               Mean episode length: 197.81
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 85.1546
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.24s
                      Time elapsed: 00:32:24
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 47784 steps/s (collection: 1.944s, learning 0.114s)
             Mean action noise std: 1.84
          Mean value_function loss: 214.8792
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.6362
                       Mean reward: 447.43
               Mean episode length: 198.49
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 87.9824
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.06s
                      Time elapsed: 00:32:26
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 48311 steps/s (collection: 1.928s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 219.0292
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.6428
                       Mean reward: 444.44
               Mean episode length: 196.44
    Episode_Reward/reaching_object: 0.7864
     Episode_Reward/lifting_object: 88.9141
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.03s
                      Time elapsed: 00:32:28
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 47628 steps/s (collection: 1.954s, learning 0.110s)
             Mean action noise std: 1.84
          Mean value_function loss: 239.6785
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.6483
                       Mean reward: 429.98
               Mean episode length: 191.77
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 87.5890
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.06s
                      Time elapsed: 00:32:30
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 47374 steps/s (collection: 1.959s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 204.7227
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.6575
                       Mean reward: 467.32
               Mean episode length: 203.61
    Episode_Reward/reaching_object: 0.7818
     Episode_Reward/lifting_object: 88.4559
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.08s
                      Time elapsed: 00:32:32
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 47534 steps/s (collection: 1.974s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 202.4120
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.6627
                       Mean reward: 468.37
               Mean episode length: 203.89
    Episode_Reward/reaching_object: 0.8028
     Episode_Reward/lifting_object: 90.6630
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.07s
                      Time elapsed: 00:32:34
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 45346 steps/s (collection: 2.001s, learning 0.167s)
             Mean action noise std: 1.84
          Mean value_function loss: 239.9648
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.6665
                       Mean reward: 425.63
               Mean episode length: 190.33
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 87.0741
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.17s
                      Time elapsed: 00:32:37
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 46228 steps/s (collection: 2.005s, learning 0.121s)
             Mean action noise std: 1.84
          Mean value_function loss: 279.3818
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.6717
                       Mean reward: 441.89
               Mean episode length: 193.23
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 88.1583
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.13s
                      Time elapsed: 00:32:39
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 46522 steps/s (collection: 1.997s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 253.6870
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.6802
                       Mean reward: 445.56
               Mean episode length: 194.06
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 88.9029
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.11s
                      Time elapsed: 00:32:41
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 45306 steps/s (collection: 2.063s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 215.3955
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.6895
                       Mean reward: 450.00
               Mean episode length: 195.47
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 86.8780
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.17s
                      Time elapsed: 00:32:43
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 45913 steps/s (collection: 2.003s, learning 0.138s)
             Mean action noise std: 1.84
          Mean value_function loss: 219.7335
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6928
                       Mean reward: 441.19
               Mean episode length: 195.37
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 86.6477
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.14s
                      Time elapsed: 00:32:45
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 46210 steps/s (collection: 1.963s, learning 0.165s)
             Mean action noise std: 1.84
          Mean value_function loss: 238.0495
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 35.6970
                       Mean reward: 441.57
               Mean episode length: 196.00
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 87.2493
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.13s
                      Time elapsed: 00:32:47
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 47114 steps/s (collection: 1.955s, learning 0.131s)
             Mean action noise std: 1.84
          Mean value_function loss: 209.0516
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7000
                       Mean reward: 446.74
               Mean episode length: 194.49
    Episode_Reward/reaching_object: 0.7914
     Episode_Reward/lifting_object: 89.3460
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.09s
                      Time elapsed: 00:32:49
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 46396 steps/s (collection: 1.986s, learning 0.133s)
             Mean action noise std: 1.84
          Mean value_function loss: 212.3258
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7052
                       Mean reward: 479.09
               Mean episode length: 208.62
    Episode_Reward/reaching_object: 0.8033
     Episode_Reward/lifting_object: 90.5691
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.12s
                      Time elapsed: 00:32:51
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 45785 steps/s (collection: 2.027s, learning 0.120s)
             Mean action noise std: 1.84
          Mean value_function loss: 206.3017
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.7090
                       Mean reward: 444.46
               Mean episode length: 195.61
    Episode_Reward/reaching_object: 0.7935
     Episode_Reward/lifting_object: 89.6922
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.15s
                      Time elapsed: 00:32:54
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 46142 steps/s (collection: 2.014s, learning 0.117s)
             Mean action noise std: 1.84
          Mean value_function loss: 218.1148
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7134
                       Mean reward: 476.72
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 0.8159
     Episode_Reward/lifting_object: 92.5774
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.13s
                      Time elapsed: 00:32:56
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 46919 steps/s (collection: 1.967s, learning 0.129s)
             Mean action noise std: 1.84
          Mean value_function loss: 215.2051
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.7196
                       Mean reward: 462.80
               Mean episode length: 201.72
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 87.6252
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.10s
                      Time elapsed: 00:32:58
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 47319 steps/s (collection: 1.960s, learning 0.117s)
             Mean action noise std: 1.85
          Mean value_function loss: 231.2033
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.7254
                       Mean reward: 463.77
               Mean episode length: 202.37
    Episode_Reward/reaching_object: 0.8105
     Episode_Reward/lifting_object: 91.6281
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.08s
                      Time elapsed: 00:33:00
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 46352 steps/s (collection: 1.989s, learning 0.132s)
             Mean action noise std: 1.85
          Mean value_function loss: 220.7883
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.7303
                       Mean reward: 465.35
               Mean episode length: 198.17
    Episode_Reward/reaching_object: 0.7913
     Episode_Reward/lifting_object: 90.3153
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.12s
                      Time elapsed: 00:33:02
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 46112 steps/s (collection: 2.016s, learning 0.116s)
             Mean action noise std: 1.85
          Mean value_function loss: 204.8882
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.7349
                       Mean reward: 462.72
               Mean episode length: 203.49
    Episode_Reward/reaching_object: 0.8188
     Episode_Reward/lifting_object: 92.4006
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.13s
                      Time elapsed: 00:33:04
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 47040 steps/s (collection: 1.975s, learning 0.115s)
             Mean action noise std: 1.85
          Mean value_function loss: 204.6948
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.7366
                       Mean reward: 459.54
               Mean episode length: 196.41
    Episode_Reward/reaching_object: 0.8153
     Episode_Reward/lifting_object: 92.7222
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.09s
                      Time elapsed: 00:33:06
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 45891 steps/s (collection: 2.012s, learning 0.130s)
             Mean action noise std: 1.85
          Mean value_function loss: 199.0486
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7403
                       Mean reward: 475.94
               Mean episode length: 204.08
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 94.4948
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.14s
                      Time elapsed: 00:33:08
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 46751 steps/s (collection: 1.975s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 196.8751
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.7501
                       Mean reward: 482.52
               Mean episode length: 204.80
    Episode_Reward/reaching_object: 0.8112
     Episode_Reward/lifting_object: 91.9130
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.10s
                      Time elapsed: 00:33:10
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 42999 steps/s (collection: 2.155s, learning 0.131s)
             Mean action noise std: 1.85
          Mean value_function loss: 208.7866
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7566
                       Mean reward: 477.37
               Mean episode length: 205.85
    Episode_Reward/reaching_object: 0.8142
     Episode_Reward/lifting_object: 92.5158
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.29s
                      Time elapsed: 00:33:13
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 48226 steps/s (collection: 1.940s, learning 0.098s)
             Mean action noise std: 1.85
          Mean value_function loss: 209.3873
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.7632
                       Mean reward: 461.59
               Mean episode length: 198.18
    Episode_Reward/reaching_object: 0.8013
     Episode_Reward/lifting_object: 90.7461
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.04s
                      Time elapsed: 00:33:15
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 46813 steps/s (collection: 1.995s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 221.7178
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7680
                       Mean reward: 463.39
               Mean episode length: 198.81
    Episode_Reward/reaching_object: 0.8060
     Episode_Reward/lifting_object: 91.2923
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.10s
                      Time elapsed: 00:33:17
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 46657 steps/s (collection: 1.993s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 217.8370
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7747
                       Mean reward: 481.77
               Mean episode length: 207.93
    Episode_Reward/reaching_object: 0.8135
     Episode_Reward/lifting_object: 92.0184
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.11s
                      Time elapsed: 00:33:19
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 48075 steps/s (collection: 1.945s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 200.7574
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.7819
                       Mean reward: 459.14
               Mean episode length: 197.95
    Episode_Reward/reaching_object: 0.8123
     Episode_Reward/lifting_object: 92.2036
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.04s
                      Time elapsed: 00:33:21
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43664 steps/s (collection: 2.119s, learning 0.132s)
             Mean action noise std: 1.85
          Mean value_function loss: 238.2779
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.7873
                       Mean reward: 442.61
               Mean episode length: 191.16
    Episode_Reward/reaching_object: 0.8146
     Episode_Reward/lifting_object: 92.4523
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.25s
                      Time elapsed: 00:33:23
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 46679 steps/s (collection: 1.992s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 212.7236
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.7887
                       Mean reward: 468.26
               Mean episode length: 200.44
    Episode_Reward/reaching_object: 0.8413
     Episode_Reward/lifting_object: 95.6109
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.11s
                      Time elapsed: 00:33:25
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 48270 steps/s (collection: 1.941s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 191.2601
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.7922
                       Mean reward: 491.02
               Mean episode length: 207.73
    Episode_Reward/reaching_object: 0.8373
     Episode_Reward/lifting_object: 96.1774
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.04s
                      Time elapsed: 00:33:27
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 47571 steps/s (collection: 1.953s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 181.3551
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7923
                       Mean reward: 485.75
               Mean episode length: 206.17
    Episode_Reward/reaching_object: 0.8258
     Episode_Reward/lifting_object: 93.8261
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.07s
                      Time elapsed: 00:33:29
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 46889 steps/s (collection: 1.969s, learning 0.127s)
             Mean action noise std: 1.86
          Mean value_function loss: 178.6851
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.7940
                       Mean reward: 480.65
               Mean episode length: 204.79
    Episode_Reward/reaching_object: 0.8376
     Episode_Reward/lifting_object: 95.2707
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.10s
                      Time elapsed: 00:33:32
                               ETA: 00:36:30

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 45380 steps/s (collection: 2.052s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 167.8587
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7971
                       Mean reward: 485.87
               Mean episode length: 206.07
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 96.5110
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.17s
                      Time elapsed: 00:33:34
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 47666 steps/s (collection: 1.963s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 174.2506
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.7988
                       Mean reward: 513.18
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 0.8645
     Episode_Reward/lifting_object: 98.7590
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.06s
                      Time elapsed: 00:33:36
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 44837 steps/s (collection: 2.026s, learning 0.166s)
             Mean action noise std: 1.86
          Mean value_function loss: 221.0845
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.8012
                       Mean reward: 509.09
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 0.8398
     Episode_Reward/lifting_object: 95.3487
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.19s
                      Time elapsed: 00:33:38
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 47060 steps/s (collection: 1.983s, learning 0.106s)
             Mean action noise std: 1.86
          Mean value_function loss: 196.9254
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.8061
                       Mean reward: 480.58
               Mean episode length: 207.50
    Episode_Reward/reaching_object: 0.8626
     Episode_Reward/lifting_object: 97.4496
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.09s
                      Time elapsed: 00:33:40
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 45978 steps/s (collection: 2.019s, learning 0.120s)
             Mean action noise std: 1.86
          Mean value_function loss: 178.8217
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.8078
                       Mean reward: 482.59
               Mean episode length: 205.94
    Episode_Reward/reaching_object: 0.8789
     Episode_Reward/lifting_object: 100.2999
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.14s
                      Time elapsed: 00:33:42
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 46267 steps/s (collection: 2.016s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 182.4634
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.8104
                       Mean reward: 502.78
               Mean episode length: 214.69
    Episode_Reward/reaching_object: 0.8528
     Episode_Reward/lifting_object: 96.8186
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.12s
                      Time elapsed: 00:33:44
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 47177 steps/s (collection: 1.982s, learning 0.101s)
             Mean action noise std: 1.86
          Mean value_function loss: 192.6626
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.8185
                       Mean reward: 481.74
               Mean episode length: 207.71
    Episode_Reward/reaching_object: 0.8577
     Episode_Reward/lifting_object: 97.3625
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.08s
                      Time elapsed: 00:33:46
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 45037 steps/s (collection: 2.066s, learning 0.117s)
             Mean action noise std: 1.86
          Mean value_function loss: 215.5279
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.8245
                       Mean reward: 505.68
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 0.8467
     Episode_Reward/lifting_object: 96.0149
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.18s
                      Time elapsed: 00:33:49
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 48163 steps/s (collection: 1.935s, learning 0.106s)
             Mean action noise std: 1.86
          Mean value_function loss: 186.4790
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8298
                       Mean reward: 501.43
               Mean episode length: 215.36
    Episode_Reward/reaching_object: 0.8721
     Episode_Reward/lifting_object: 98.7715
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.04s
                      Time elapsed: 00:33:51
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 46009 steps/s (collection: 1.969s, learning 0.168s)
             Mean action noise std: 1.86
          Mean value_function loss: 189.3014
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.8360
                       Mean reward: 496.38
               Mean episode length: 208.18
    Episode_Reward/reaching_object: 0.8491
     Episode_Reward/lifting_object: 97.3445
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.14s
                      Time elapsed: 00:33:53
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 47060 steps/s (collection: 1.983s, learning 0.106s)
             Mean action noise std: 1.86
          Mean value_function loss: 176.8127
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8405
                       Mean reward: 478.40
               Mean episode length: 201.44
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: 97.0772
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.09s
                      Time elapsed: 00:33:55
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 47033 steps/s (collection: 1.963s, learning 0.128s)
             Mean action noise std: 1.86
          Mean value_function loss: 165.4528
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.8410
                       Mean reward: 495.23
               Mean episode length: 209.90
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 98.2130
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.09s
                      Time elapsed: 00:33:57
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 45153 steps/s (collection: 2.069s, learning 0.108s)
             Mean action noise std: 1.86
          Mean value_function loss: 150.3792
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.8419
                       Mean reward: 486.80
               Mean episode length: 206.72
    Episode_Reward/reaching_object: 0.8555
     Episode_Reward/lifting_object: 97.4240
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.18s
                      Time elapsed: 00:33:59
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 43743 steps/s (collection: 2.068s, learning 0.179s)
             Mean action noise std: 1.86
          Mean value_function loss: 169.8042
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8461
                       Mean reward: 494.63
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 0.8585
     Episode_Reward/lifting_object: 98.0545
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.25s
                      Time elapsed: 00:34:01
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 44614 steps/s (collection: 2.063s, learning 0.140s)
             Mean action noise std: 1.86
          Mean value_function loss: 162.4417
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.8574
                       Mean reward: 523.05
               Mean episode length: 220.44
    Episode_Reward/reaching_object: 0.8807
     Episode_Reward/lifting_object: 100.3240
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.20s
                      Time elapsed: 00:34:04
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 47023 steps/s (collection: 1.957s, learning 0.134s)
             Mean action noise std: 1.87
          Mean value_function loss: 176.0264
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.8660
                       Mean reward: 488.23
               Mean episode length: 207.44
    Episode_Reward/reaching_object: 0.8569
     Episode_Reward/lifting_object: 98.3569
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.09s
                      Time elapsed: 00:34:06
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 47713 steps/s (collection: 1.956s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 180.1408
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.8692
                       Mean reward: 478.52
               Mean episode length: 203.43
    Episode_Reward/reaching_object: 0.8454
     Episode_Reward/lifting_object: 96.6527
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.06s
                      Time elapsed: 00:34:08
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 45296 steps/s (collection: 2.009s, learning 0.161s)
             Mean action noise std: 1.87
          Mean value_function loss: 181.6464
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.8710
                       Mean reward: 497.24
               Mean episode length: 209.75
    Episode_Reward/reaching_object: 0.8713
     Episode_Reward/lifting_object: 100.5050
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.17s
                      Time elapsed: 00:34:10
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 46937 steps/s (collection: 1.984s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 175.2788
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.8716
                       Mean reward: 478.50
               Mean episode length: 203.86
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 98.5552
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.09s
                      Time elapsed: 00:34:12
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 43437 steps/s (collection: 2.071s, learning 0.193s)
             Mean action noise std: 1.87
          Mean value_function loss: 176.0367
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8728
                       Mean reward: 481.23
               Mean episode length: 203.51
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 100.0416
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.26s
                      Time elapsed: 00:34:14
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 45282 steps/s (collection: 2.036s, learning 0.135s)
             Mean action noise std: 1.87
          Mean value_function loss: 148.6718
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.8759
                       Mean reward: 503.58
               Mean episode length: 212.83
    Episode_Reward/reaching_object: 0.8785
     Episode_Reward/lifting_object: 101.0855
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.17s
                      Time elapsed: 00:34:16
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 45678 steps/s (collection: 2.026s, learning 0.127s)
             Mean action noise std: 1.87
          Mean value_function loss: 180.3645
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.8797
                       Mean reward: 489.31
               Mean episode length: 204.49
    Episode_Reward/reaching_object: 0.8681
     Episode_Reward/lifting_object: 100.0850
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.15s
                      Time elapsed: 00:34:19
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 42989 steps/s (collection: 2.160s, learning 0.127s)
             Mean action noise std: 1.87
          Mean value_function loss: 197.2049
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8838
                       Mean reward: 465.46
               Mean episode length: 197.17
    Episode_Reward/reaching_object: 0.8405
     Episode_Reward/lifting_object: 97.0136
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.29s
                      Time elapsed: 00:34:21
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 45511 steps/s (collection: 2.032s, learning 0.128s)
             Mean action noise std: 1.87
          Mean value_function loss: 181.7242
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.8905
                       Mean reward: 490.94
               Mean episode length: 204.33
    Episode_Reward/reaching_object: 0.8510
     Episode_Reward/lifting_object: 98.3911
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.16s
                      Time elapsed: 00:34:23
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 46032 steps/s (collection: 2.017s, learning 0.119s)
             Mean action noise std: 1.87
          Mean value_function loss: 161.0441
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8951
                       Mean reward: 495.62
               Mean episode length: 208.79
    Episode_Reward/reaching_object: 0.8625
     Episode_Reward/lifting_object: 99.5276
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.14s
                      Time elapsed: 00:34:25
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 45149 steps/s (collection: 2.045s, learning 0.132s)
             Mean action noise std: 1.87
          Mean value_function loss: 162.6878
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8976
                       Mean reward: 497.86
               Mean episode length: 207.52
    Episode_Reward/reaching_object: 0.8670
     Episode_Reward/lifting_object: 100.2641
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.18s
                      Time elapsed: 00:34:27
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 44583 steps/s (collection: 2.038s, learning 0.167s)
             Mean action noise std: 1.87
          Mean value_function loss: 158.0619
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.9065
                       Mean reward: 506.73
               Mean episode length: 210.40
    Episode_Reward/reaching_object: 0.8733
     Episode_Reward/lifting_object: 101.5136
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.20s
                      Time elapsed: 00:34:30
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 40800 steps/s (collection: 2.229s, learning 0.181s)
             Mean action noise std: 1.87
          Mean value_function loss: 191.6138
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.9198
                       Mean reward: 516.27
               Mean episode length: 214.54
    Episode_Reward/reaching_object: 0.8683
     Episode_Reward/lifting_object: 100.2256
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.41s
                      Time elapsed: 00:34:32
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 46284 steps/s (collection: 2.013s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 169.4782
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.9238
                       Mean reward: 483.90
               Mean episode length: 201.81
    Episode_Reward/reaching_object: 0.8616
     Episode_Reward/lifting_object: 99.6757
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.12s
                      Time elapsed: 00:34:34
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 47173 steps/s (collection: 1.991s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 159.9444
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.9252
                       Mean reward: 516.83
               Mean episode length: 216.21
    Episode_Reward/reaching_object: 0.8821
     Episode_Reward/lifting_object: 101.7828
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.08s
                      Time elapsed: 00:34:36
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 46719 steps/s (collection: 1.972s, learning 0.133s)
             Mean action noise std: 1.87
          Mean value_function loss: 178.9094
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.9263
                       Mean reward: 477.94
               Mean episode length: 201.95
    Episode_Reward/reaching_object: 0.8493
     Episode_Reward/lifting_object: 98.1097
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.10s
                      Time elapsed: 00:34:38
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 46804 steps/s (collection: 1.975s, learning 0.126s)
             Mean action noise std: 1.87
          Mean value_function loss: 193.3741
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.9256
                       Mean reward: 508.92
               Mean episode length: 217.15
    Episode_Reward/reaching_object: 0.8517
     Episode_Reward/lifting_object: 97.9497
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.10s
                      Time elapsed: 00:34:40
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 47650 steps/s (collection: 1.963s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 172.9749
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.9276
                       Mean reward: 479.78
               Mean episode length: 204.17
    Episode_Reward/reaching_object: 0.8827
     Episode_Reward/lifting_object: 101.4552
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.06s
                      Time elapsed: 00:34:42
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 47595 steps/s (collection: 1.955s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 165.2890
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.9322
                       Mean reward: 528.84
               Mean episode length: 221.00
    Episode_Reward/reaching_object: 0.8856
     Episode_Reward/lifting_object: 102.6846
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.07s
                      Time elapsed: 00:34:45
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 47334 steps/s (collection: 1.944s, learning 0.133s)
             Mean action noise std: 1.88
          Mean value_function loss: 177.6667
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.9370
                       Mean reward: 530.04
               Mean episode length: 217.67
    Episode_Reward/reaching_object: 0.8730
     Episode_Reward/lifting_object: 101.1457
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.08s
                      Time elapsed: 00:34:47
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 46771 steps/s (collection: 1.996s, learning 0.106s)
             Mean action noise std: 1.88
          Mean value_function loss: 155.7022
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.9479
                       Mean reward: 499.24
               Mean episode length: 208.28
    Episode_Reward/reaching_object: 0.8875
     Episode_Reward/lifting_object: 102.0443
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.10s
                      Time elapsed: 00:34:49
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.971s, learning 0.098s)
             Mean action noise std: 1.88
          Mean value_function loss: 178.6681
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.9537
                       Mean reward: 507.82
               Mean episode length: 210.56
    Episode_Reward/reaching_object: 0.8724
     Episode_Reward/lifting_object: 100.9722
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.07s
                      Time elapsed: 00:34:51
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 45725 steps/s (collection: 1.946s, learning 0.204s)
             Mean action noise std: 1.88
          Mean value_function loss: 174.4267
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.9563
                       Mean reward: 539.32
               Mean episode length: 222.33
    Episode_Reward/reaching_object: 0.8777
     Episode_Reward/lifting_object: 101.0346
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.15s
                      Time elapsed: 00:34:53
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 47443 steps/s (collection: 1.954s, learning 0.118s)
             Mean action noise std: 1.88
          Mean value_function loss: 173.0834
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.9621
                       Mean reward: 511.36
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 0.8955
     Episode_Reward/lifting_object: 103.7034
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.07s
                      Time elapsed: 00:34:55
                               ETA: 00:35:10

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 47814 steps/s (collection: 1.956s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 175.6496
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.9680
                       Mean reward: 502.46
               Mean episode length: 211.22
    Episode_Reward/reaching_object: 0.8908
     Episode_Reward/lifting_object: 102.8355
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.06s
                      Time elapsed: 00:34:57
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 47067 steps/s (collection: 1.963s, learning 0.125s)
             Mean action noise std: 1.88
          Mean value_function loss: 170.9120
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.9710
                       Mean reward: 503.40
               Mean episode length: 210.66
    Episode_Reward/reaching_object: 0.8874
     Episode_Reward/lifting_object: 102.1496
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.09s
                      Time elapsed: 00:34:59
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 43798 steps/s (collection: 2.085s, learning 0.160s)
             Mean action noise std: 1.88
          Mean value_function loss: 153.2869
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.9730
                       Mean reward: 537.51
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 0.8660
     Episode_Reward/lifting_object: 100.2101
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.24s
                      Time elapsed: 00:35:01
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14259 steps/s (collection: 6.755s, learning 0.139s)
             Mean action noise std: 1.88
          Mean value_function loss: 159.4037
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.9745
                       Mean reward: 521.70
               Mean episode length: 214.66
    Episode_Reward/reaching_object: 0.8904
     Episode_Reward/lifting_object: 103.3575
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.89s
                      Time elapsed: 00:35:08
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14082 steps/s (collection: 6.856s, learning 0.125s)
             Mean action noise std: 1.88
          Mean value_function loss: 157.3505
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.9786
                       Mean reward: 518.26
               Mean episode length: 215.54
    Episode_Reward/reaching_object: 0.8896
     Episode_Reward/lifting_object: 102.3199
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.98s
                      Time elapsed: 00:35:15
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 13983 steps/s (collection: 6.888s, learning 0.142s)
             Mean action noise std: 1.88
          Mean value_function loss: 184.9609
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.9844
                       Mean reward: 478.97
               Mean episode length: 205.36
    Episode_Reward/reaching_object: 0.8745
     Episode_Reward/lifting_object: 100.9269
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.03s
                      Time elapsed: 00:35:22
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 13885 steps/s (collection: 6.933s, learning 0.147s)
             Mean action noise std: 1.88
          Mean value_function loss: 156.5328
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.9903
                       Mean reward: 495.30
               Mean episode length: 207.08
    Episode_Reward/reaching_object: 0.8745
     Episode_Reward/lifting_object: 100.3563
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.08s
                      Time elapsed: 00:35:29
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14059 steps/s (collection: 6.873s, learning 0.119s)
             Mean action noise std: 1.88
          Mean value_function loss: 164.3232
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.9959
                       Mean reward: 509.06
               Mean episode length: 210.36
    Episode_Reward/reaching_object: 0.9052
     Episode_Reward/lifting_object: 105.2237
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.99s
                      Time elapsed: 00:35:36
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14579 steps/s (collection: 6.612s, learning 0.131s)
             Mean action noise std: 1.89
          Mean value_function loss: 186.2870
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.0008
                       Mean reward: 518.00
               Mean episode length: 213.02
    Episode_Reward/reaching_object: 0.8725
     Episode_Reward/lifting_object: 101.0869
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.74s
                      Time elapsed: 00:35:43
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14090 steps/s (collection: 6.852s, learning 0.125s)
             Mean action noise std: 1.89
          Mean value_function loss: 166.5569
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.0025
                       Mean reward: 524.13
               Mean episode length: 216.82
    Episode_Reward/reaching_object: 0.8839
     Episode_Reward/lifting_object: 102.5985
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.98s
                      Time elapsed: 00:35:50
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14147 steps/s (collection: 6.819s, learning 0.129s)
             Mean action noise std: 1.89
          Mean value_function loss: 176.4600
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.0032
                       Mean reward: 506.22
               Mean episode length: 208.09
    Episode_Reward/reaching_object: 0.8738
     Episode_Reward/lifting_object: 101.6774
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.95s
                      Time elapsed: 00:35:57
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 16398 steps/s (collection: 5.847s, learning 0.148s)
             Mean action noise std: 1.89
          Mean value_function loss: 214.3291
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.0042
                       Mean reward: 517.14
               Mean episode length: 211.99
    Episode_Reward/reaching_object: 0.8890
     Episode_Reward/lifting_object: 103.4487
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.99s
                      Time elapsed: 00:36:03
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 49102 steps/s (collection: 1.864s, learning 0.138s)
             Mean action noise std: 1.89
          Mean value_function loss: 197.4273
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.0063
                       Mean reward: 522.11
               Mean episode length: 215.91
    Episode_Reward/reaching_object: 0.8894
     Episode_Reward/lifting_object: 103.2150
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.00s
                      Time elapsed: 00:36:05
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 50628 steps/s (collection: 1.804s, learning 0.138s)
             Mean action noise std: 1.89
          Mean value_function loss: 188.0289
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.0094
                       Mean reward: 503.01
               Mean episode length: 208.85
    Episode_Reward/reaching_object: 0.8677
     Episode_Reward/lifting_object: 101.5458
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.94s
                      Time elapsed: 00:36:07
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 50204 steps/s (collection: 1.825s, learning 0.133s)
             Mean action noise std: 1.89
          Mean value_function loss: 202.1851
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.0120
                       Mean reward: 531.48
               Mean episode length: 217.57
    Episode_Reward/reaching_object: 0.8708
     Episode_Reward/lifting_object: 101.1284
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.96s
                      Time elapsed: 00:36:09
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 45257 steps/s (collection: 2.007s, learning 0.165s)
             Mean action noise std: 1.89
          Mean value_function loss: 190.5886
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.0172
                       Mean reward: 530.11
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 0.8865
     Episode_Reward/lifting_object: 103.1532
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.17s
                      Time elapsed: 00:36:11
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 50964 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 218.9571
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.0225
                       Mean reward: 477.40
               Mean episode length: 197.00
    Episode_Reward/reaching_object: 0.8536
     Episode_Reward/lifting_object: 99.1725
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.93s
                      Time elapsed: 00:36:13
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 51195 steps/s (collection: 1.814s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 221.8847
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.0293
                       Mean reward: 504.43
               Mean episode length: 208.25
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: 99.5380
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.92s
                      Time elapsed: 00:36:15
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 50497 steps/s (collection: 1.854s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 185.1697
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.0386
                       Mean reward: 517.67
               Mean episode length: 212.67
    Episode_Reward/reaching_object: 0.8609
     Episode_Reward/lifting_object: 100.1335
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.95s
                      Time elapsed: 00:36:17
                               ETA: 00:35:10

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 50453 steps/s (collection: 1.830s, learning 0.119s)
             Mean action noise std: 1.89
          Mean value_function loss: 201.8760
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.0446
                       Mean reward: 523.93
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 0.8773
     Episode_Reward/lifting_object: 101.6529
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.95s
                      Time elapsed: 00:36:19
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 50153 steps/s (collection: 1.853s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 231.0786
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.0560
                       Mean reward: 510.96
               Mean episode length: 207.95
    Episode_Reward/reaching_object: 0.8708
     Episode_Reward/lifting_object: 101.6913
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.96s
                      Time elapsed: 00:36:21
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 49701 steps/s (collection: 1.881s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 240.7319
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.0594
                       Mean reward: 533.34
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 0.8792
     Episode_Reward/lifting_object: 102.5294
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.98s
                      Time elapsed: 00:36:23
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 50427 steps/s (collection: 1.865s, learning 0.084s)
             Mean action noise std: 1.89
          Mean value_function loss: 178.6394
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.0599
                       Mean reward: 519.08
               Mean episode length: 214.84
    Episode_Reward/reaching_object: 0.8863
     Episode_Reward/lifting_object: 103.3293
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.95s
                      Time elapsed: 00:36:25
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 50796 steps/s (collection: 1.846s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 186.1002
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.0619
                       Mean reward: 511.97
               Mean episode length: 210.46
    Episode_Reward/reaching_object: 0.8594
     Episode_Reward/lifting_object: 99.2440
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 1.94s
                      Time elapsed: 00:36:27
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 50284 steps/s (collection: 1.836s, learning 0.119s)
             Mean action noise std: 1.90
          Mean value_function loss: 192.4426
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.0669
                       Mean reward: 518.13
               Mean episode length: 212.56
    Episode_Reward/reaching_object: 0.8817
     Episode_Reward/lifting_object: 102.7087
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 1.95s
                      Time elapsed: 00:36:29
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 51350 steps/s (collection: 1.828s, learning 0.087s)
             Mean action noise std: 1.90
          Mean value_function loss: 170.1185
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.0729
                       Mean reward: 528.77
               Mean episode length: 214.75
    Episode_Reward/reaching_object: 0.8829
     Episode_Reward/lifting_object: 103.0301
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.91s
                      Time elapsed: 00:36:31
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 49871 steps/s (collection: 1.870s, learning 0.102s)
             Mean action noise std: 1.90
          Mean value_function loss: 204.0257
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.0800
                       Mean reward: 529.58
               Mean episode length: 216.12
    Episode_Reward/reaching_object: 0.8905
     Episode_Reward/lifting_object: 103.9741
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 1.97s
                      Time elapsed: 00:36:33
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 50350 steps/s (collection: 1.861s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 193.0713
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.0818
                       Mean reward: 530.76
               Mean episode length: 217.35
    Episode_Reward/reaching_object: 0.8846
     Episode_Reward/lifting_object: 103.2640
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 1.95s
                      Time elapsed: 00:36:34
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 50580 steps/s (collection: 1.840s, learning 0.104s)
             Mean action noise std: 1.90
          Mean value_function loss: 185.0642
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.0852
                       Mean reward: 529.06
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 0.8745
     Episode_Reward/lifting_object: 102.9213
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.94s
                      Time elapsed: 00:36:36
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 47516 steps/s (collection: 1.950s, learning 0.119s)
             Mean action noise std: 1.90
          Mean value_function loss: 174.3315
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.0932
                       Mean reward: 530.97
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 0.8747
     Episode_Reward/lifting_object: 102.2393
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.07s
                      Time elapsed: 00:36:38
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 50958 steps/s (collection: 1.838s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 190.3322
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.1010
                       Mean reward: 549.34
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 0.8838
     Episode_Reward/lifting_object: 103.6514
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.93s
                      Time elapsed: 00:36:40
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 49133 steps/s (collection: 1.912s, learning 0.089s)
             Mean action noise std: 1.90
          Mean value_function loss: 190.8749
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.1066
                       Mean reward: 525.96
               Mean episode length: 216.06
    Episode_Reward/reaching_object: 0.8802
     Episode_Reward/lifting_object: 103.1551
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.00s
                      Time elapsed: 00:36:42
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 50696 steps/s (collection: 1.848s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 245.3249
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.1107
                       Mean reward: 539.57
               Mean episode length: 220.88
    Episode_Reward/reaching_object: 0.8935
     Episode_Reward/lifting_object: 105.0656
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.94s
                      Time elapsed: 00:36:44
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 49855 steps/s (collection: 1.859s, learning 0.113s)
             Mean action noise std: 1.90
          Mean value_function loss: 186.4593
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.1168
                       Mean reward: 523.38
               Mean episode length: 212.80
    Episode_Reward/reaching_object: 0.8835
     Episode_Reward/lifting_object: 104.3803
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.97s
                      Time elapsed: 00:36:46
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 51005 steps/s (collection: 1.818s, learning 0.109s)
             Mean action noise std: 1.90
          Mean value_function loss: 169.6606
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.1212
                       Mean reward: 516.41
               Mean episode length: 212.80
    Episode_Reward/reaching_object: 0.8948
     Episode_Reward/lifting_object: 105.7827
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.93s
                      Time elapsed: 00:36:48
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 50462 steps/s (collection: 1.852s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 185.4255
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.1261
                       Mean reward: 543.84
               Mean episode length: 217.94
    Episode_Reward/reaching_object: 0.9123
     Episode_Reward/lifting_object: 108.2494
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.95s
                      Time elapsed: 00:36:50
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 47918 steps/s (collection: 1.930s, learning 0.121s)
             Mean action noise std: 1.90
          Mean value_function loss: 218.4579
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.1329
                       Mean reward: 556.25
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 0.8745
     Episode_Reward/lifting_object: 103.8504
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.05s
                      Time elapsed: 00:36:52
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 48382 steps/s (collection: 1.912s, learning 0.120s)
             Mean action noise std: 1.91
          Mean value_function loss: 269.0878
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.1410
                       Mean reward: 519.58
               Mean episode length: 210.56
    Episode_Reward/reaching_object: 0.8815
     Episode_Reward/lifting_object: 104.0071
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.03s
                      Time elapsed: 00:36:54
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 50017 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 166.9343
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.1467
                       Mean reward: 534.03
               Mean episode length: 214.78
    Episode_Reward/reaching_object: 0.8785
     Episode_Reward/lifting_object: 104.5707
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.97s
                      Time elapsed: 00:36:56
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 47954 steps/s (collection: 1.948s, learning 0.102s)
             Mean action noise std: 1.91
          Mean value_function loss: 156.1854
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.1495
                       Mean reward: 533.48
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 0.8920
     Episode_Reward/lifting_object: 106.0526
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.05s
                      Time elapsed: 00:36:58
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 50017 steps/s (collection: 1.871s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 174.4284
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.1565
                       Mean reward: 522.22
               Mean episode length: 211.78
    Episode_Reward/reaching_object: 0.8834
     Episode_Reward/lifting_object: 104.9545
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.97s
                      Time elapsed: 00:37:00
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 47456 steps/s (collection: 1.917s, learning 0.154s)
             Mean action noise std: 1.91
          Mean value_function loss: 187.3082
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.1680
                       Mean reward: 511.42
               Mean episode length: 209.74
    Episode_Reward/reaching_object: 0.8879
     Episode_Reward/lifting_object: 105.4439
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.07s
                      Time elapsed: 00:37:02
                               ETA: 00:34:18

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 47826 steps/s (collection: 1.971s, learning 0.084s)
             Mean action noise std: 1.91
          Mean value_function loss: 167.8801
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 36.1756
                       Mean reward: 525.99
               Mean episode length: 211.92
    Episode_Reward/reaching_object: 0.9051
     Episode_Reward/lifting_object: 107.8095
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.06s
                      Time elapsed: 00:37:04
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 50972 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 165.2756
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.1793
                       Mean reward: 521.92
               Mean episode length: 210.22
    Episode_Reward/reaching_object: 0.8731
     Episode_Reward/lifting_object: 103.4698
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 1.93s
                      Time elapsed: 00:37:06
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 49752 steps/s (collection: 1.861s, learning 0.115s)
             Mean action noise std: 1.91
          Mean value_function loss: 179.2246
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.1851
                       Mean reward: 524.66
               Mean episode length: 211.08
    Episode_Reward/reaching_object: 0.9018
     Episode_Reward/lifting_object: 106.5231
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.98s
                      Time elapsed: 00:37:08
                               ETA: 00:34:11

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 47082 steps/s (collection: 1.974s, learning 0.114s)
             Mean action noise std: 1.91
          Mean value_function loss: 167.8473
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.1887
                       Mean reward: 544.30
               Mean episode length: 221.03
    Episode_Reward/reaching_object: 0.9175
     Episode_Reward/lifting_object: 109.0834
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.09s
                      Time elapsed: 00:37:10
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 49386 steps/s (collection: 1.901s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 149.5602
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 36.1925
                       Mean reward: 563.46
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 0.9154
     Episode_Reward/lifting_object: 109.4278
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.99s
                      Time elapsed: 00:37:12
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 48319 steps/s (collection: 1.925s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 179.2177
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.1926
                       Mean reward: 537.79
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 0.8993
     Episode_Reward/lifting_object: 106.8013
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.03s
                      Time elapsed: 00:37:14
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 50196 steps/s (collection: 1.873s, learning 0.085s)
             Mean action noise std: 1.91
          Mean value_function loss: 181.3101
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.1966
                       Mean reward: 533.88
               Mean episode length: 215.73
    Episode_Reward/reaching_object: 0.9042
     Episode_Reward/lifting_object: 106.6077
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.96s
                      Time elapsed: 00:37:16
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 49979 steps/s (collection: 1.873s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 183.8851
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.2005
                       Mean reward: 530.07
               Mean episode length: 214.99
    Episode_Reward/reaching_object: 0.9225
     Episode_Reward/lifting_object: 108.8517
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.97s
                      Time elapsed: 00:37:18
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 49133 steps/s (collection: 1.877s, learning 0.124s)
             Mean action noise std: 1.91
          Mean value_function loss: 167.0104
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.2058
                       Mean reward: 547.26
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 0.8981
     Episode_Reward/lifting_object: 106.8278
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.00s
                      Time elapsed: 00:37:20
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 50342 steps/s (collection: 1.866s, learning 0.087s)
             Mean action noise std: 1.92
          Mean value_function loss: 142.5107
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.2146
                       Mean reward: 534.66
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 0.9182
     Episode_Reward/lifting_object: 109.1307
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 1.95s
                      Time elapsed: 00:37:22
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 49369 steps/s (collection: 1.863s, learning 0.129s)
             Mean action noise std: 1.92
          Mean value_function loss: 206.8206
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.2225
                       Mean reward: 545.34
               Mean episode length: 218.64
    Episode_Reward/reaching_object: 0.9290
     Episode_Reward/lifting_object: 110.9810
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 1.99s
                      Time elapsed: 00:37:24
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 50608 steps/s (collection: 1.839s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 205.6250
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.2264
                       Mean reward: 548.63
               Mean episode length: 221.40
    Episode_Reward/reaching_object: 0.9054
     Episode_Reward/lifting_object: 106.9631
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.94s
                      Time elapsed: 00:37:26
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 47901 steps/s (collection: 1.911s, learning 0.141s)
             Mean action noise std: 1.92
          Mean value_function loss: 211.5301
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.2289
                       Mean reward: 524.10
               Mean episode length: 209.32
    Episode_Reward/reaching_object: 0.8971
     Episode_Reward/lifting_object: 107.0691
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.05s
                      Time elapsed: 00:37:28
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 48163 steps/s (collection: 1.907s, learning 0.134s)
             Mean action noise std: 1.92
          Mean value_function loss: 220.4998
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.2321
                       Mean reward: 547.96
               Mean episode length: 216.03
    Episode_Reward/reaching_object: 0.9078
     Episode_Reward/lifting_object: 108.7058
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.04s
                      Time elapsed: 00:37:30
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 48815 steps/s (collection: 1.866s, learning 0.148s)
             Mean action noise std: 1.92
          Mean value_function loss: 200.2585
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.2376
                       Mean reward: 512.32
               Mean episode length: 203.99
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 109.6619
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.01s
                      Time elapsed: 00:37:32
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 47178 steps/s (collection: 1.965s, learning 0.119s)
             Mean action noise std: 1.92
          Mean value_function loss: 192.8561
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 36.2381
                       Mean reward: 554.65
               Mean episode length: 215.64
    Episode_Reward/reaching_object: 0.8887
     Episode_Reward/lifting_object: 107.7755
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.08s
                      Time elapsed: 00:37:34
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 49312 steps/s (collection: 1.881s, learning 0.113s)
             Mean action noise std: 1.92
          Mean value_function loss: 205.5914
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.2384
                       Mean reward: 516.68
               Mean episode length: 208.69
    Episode_Reward/reaching_object: 0.8683
     Episode_Reward/lifting_object: 104.5752
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 1.99s
                      Time elapsed: 00:37:36
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 50341 steps/s (collection: 1.856s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 185.0811
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.2394
                       Mean reward: 532.86
               Mean episode length: 210.79
    Episode_Reward/reaching_object: 0.8843
     Episode_Reward/lifting_object: 107.2939
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 1.95s
                      Time elapsed: 00:37:38
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 49685 steps/s (collection: 1.863s, learning 0.115s)
             Mean action noise std: 1.92
          Mean value_function loss: 204.5317
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.2409
                       Mean reward: 519.51
               Mean episode length: 208.16
    Episode_Reward/reaching_object: 0.8869
     Episode_Reward/lifting_object: 107.8845
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 1.98s
                      Time elapsed: 00:37:40
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 50208 steps/s (collection: 1.855s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 229.2657
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.2454
                       Mean reward: 547.58
               Mean episode length: 216.33
    Episode_Reward/reaching_object: 0.8975
     Episode_Reward/lifting_object: 109.9072
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.96s
                      Time elapsed: 00:37:42
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 49519 steps/s (collection: 1.853s, learning 0.132s)
             Mean action noise std: 1.92
          Mean value_function loss: 223.3421
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.2492
                       Mean reward: 533.15
               Mean episode length: 209.62
    Episode_Reward/reaching_object: 0.8683
     Episode_Reward/lifting_object: 106.3527
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 1.99s
                      Time elapsed: 00:37:44
                               ETA: 00:33:30

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 50186 steps/s (collection: 1.850s, learning 0.109s)
             Mean action noise std: 1.92
          Mean value_function loss: 175.6683
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.2543
                       Mean reward: 566.02
               Mean episode length: 220.97
    Episode_Reward/reaching_object: 0.8965
     Episode_Reward/lifting_object: 109.9687
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 1.96s
                      Time elapsed: 00:37:46
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 50604 steps/s (collection: 1.846s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 178.1234
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.2575
                       Mean reward: 556.92
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 0.9010
     Episode_Reward/lifting_object: 110.7018
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 1.94s
                      Time elapsed: 00:37:48
                               ETA: 00:33:25

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 49398 steps/s (collection: 1.851s, learning 0.139s)
             Mean action noise std: 1.92
          Mean value_function loss: 180.2324
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.2594
                       Mean reward: 561.82
               Mean episode length: 216.53
    Episode_Reward/reaching_object: 0.8935
     Episode_Reward/lifting_object: 110.2501
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 1.99s
                      Time elapsed: 00:37:50
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 47919 steps/s (collection: 1.906s, learning 0.146s)
             Mean action noise std: 1.92
          Mean value_function loss: 207.7171
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.2592
                       Mean reward: 557.26
               Mean episode length: 213.47
    Episode_Reward/reaching_object: 0.9041
     Episode_Reward/lifting_object: 112.4689
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.05s
                      Time elapsed: 00:37:52
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 47646 steps/s (collection: 1.911s, learning 0.152s)
             Mean action noise std: 1.92
          Mean value_function loss: 201.2107
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.2603
                       Mean reward: 542.51
               Mean episode length: 207.80
    Episode_Reward/reaching_object: 0.8872
     Episode_Reward/lifting_object: 110.9694
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.06s
                      Time elapsed: 00:37:54
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 48854 steps/s (collection: 1.879s, learning 0.133s)
             Mean action noise std: 1.92
          Mean value_function loss: 183.1510
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.2617
                       Mean reward: 558.88
               Mean episode length: 214.45
    Episode_Reward/reaching_object: 0.8994
     Episode_Reward/lifting_object: 111.5135
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.01s
                      Time elapsed: 00:37:56
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 49610 steps/s (collection: 1.860s, learning 0.122s)
             Mean action noise std: 1.92
          Mean value_function loss: 215.9567
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.2634
                       Mean reward: 556.48
               Mean episode length: 211.90
    Episode_Reward/reaching_object: 0.8846
     Episode_Reward/lifting_object: 111.1190
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 1.98s
                      Time elapsed: 00:37:58
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 49896 steps/s (collection: 1.852s, learning 0.119s)
             Mean action noise std: 1.92
          Mean value_function loss: 189.1173
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.2658
                       Mean reward: 513.17
               Mean episode length: 200.40
    Episode_Reward/reaching_object: 0.8736
     Episode_Reward/lifting_object: 109.6827
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.97s
                      Time elapsed: 00:38:00
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 49601 steps/s (collection: 1.838s, learning 0.144s)
             Mean action noise std: 1.92
          Mean value_function loss: 213.4394
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.2668
                       Mean reward: 569.30
               Mean episode length: 217.64
    Episode_Reward/reaching_object: 0.8745
     Episode_Reward/lifting_object: 110.4136
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 1.98s
                      Time elapsed: 00:38:02
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 49450 steps/s (collection: 1.849s, learning 0.139s)
             Mean action noise std: 1.93
          Mean value_function loss: 178.0382
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.2674
                       Mean reward: 586.85
               Mean episode length: 220.45
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 115.9689
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 1.99s
                      Time elapsed: 00:38:04
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 49261 steps/s (collection: 1.859s, learning 0.137s)
             Mean action noise std: 1.93
          Mean value_function loss: 200.6202
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.2729
                       Mean reward: 570.27
               Mean episode length: 214.15
    Episode_Reward/reaching_object: 0.8991
     Episode_Reward/lifting_object: 114.2323
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.00s
                      Time elapsed: 00:38:06
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 48298 steps/s (collection: 1.883s, learning 0.152s)
             Mean action noise std: 1.93
          Mean value_function loss: 185.5365
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.2740
                       Mean reward: 583.03
               Mean episode length: 220.05
    Episode_Reward/reaching_object: 0.8918
     Episode_Reward/lifting_object: 113.7572
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.04s
                      Time elapsed: 00:38:08
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 49857 steps/s (collection: 1.869s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 227.4852
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.2716
                       Mean reward: 596.83
               Mean episode length: 224.07
    Episode_Reward/reaching_object: 0.8729
     Episode_Reward/lifting_object: 111.2137
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.97s
                      Time elapsed: 00:38:10
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 49003 steps/s (collection: 1.877s, learning 0.130s)
             Mean action noise std: 1.93
          Mean value_function loss: 243.5706
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.2767
                       Mean reward: 573.97
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 0.8885
     Episode_Reward/lifting_object: 114.2735
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.01s
                      Time elapsed: 00:38:12
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 47197 steps/s (collection: 1.922s, learning 0.160s)
             Mean action noise std: 1.93
          Mean value_function loss: 229.4875
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.2834
                       Mean reward: 615.43
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 0.8922
     Episode_Reward/lifting_object: 115.3354
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.08s
                      Time elapsed: 00:38:14
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 46668 steps/s (collection: 1.969s, learning 0.138s)
             Mean action noise std: 1.93
          Mean value_function loss: 231.6121
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.2880
                       Mean reward: 591.92
               Mean episode length: 215.09
    Episode_Reward/reaching_object: 0.8894
     Episode_Reward/lifting_object: 115.4200
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.11s
                      Time elapsed: 00:38:16
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 49150 steps/s (collection: 1.896s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 207.3671
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.2915
                       Mean reward: 597.52
               Mean episode length: 219.85
    Episode_Reward/reaching_object: 0.8643
     Episode_Reward/lifting_object: 113.0499
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.00s
                      Time elapsed: 00:38:18
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 45632 steps/s (collection: 2.025s, learning 0.130s)
             Mean action noise std: 1.93
          Mean value_function loss: 225.7495
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.2932
                       Mean reward: 552.38
               Mean episode length: 203.81
    Episode_Reward/reaching_object: 0.8750
     Episode_Reward/lifting_object: 114.4245
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.15s
                      Time elapsed: 00:38:21
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 50495 steps/s (collection: 1.851s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 238.8327
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.2920
                       Mean reward: 555.48
               Mean episode length: 203.98
    Episode_Reward/reaching_object: 0.8654
     Episode_Reward/lifting_object: 114.0612
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 1.95s
                      Time elapsed: 00:38:23
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 50635 steps/s (collection: 1.844s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 216.5153
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.2908
                       Mean reward: 577.19
               Mean episode length: 208.80
    Episode_Reward/reaching_object: 0.8780
     Episode_Reward/lifting_object: 116.4442
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 1.94s
                      Time elapsed: 00:38:24
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 50770 steps/s (collection: 1.843s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 207.2682
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.2915
                       Mean reward: 580.60
               Mean episode length: 210.37
    Episode_Reward/reaching_object: 0.8647
     Episode_Reward/lifting_object: 115.0216
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 1.94s
                      Time elapsed: 00:38:26
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 49897 steps/s (collection: 1.866s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 232.0579
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.2879
                       Mean reward: 576.84
               Mean episode length: 204.02
    Episode_Reward/reaching_object: 0.8710
     Episode_Reward/lifting_object: 117.2427
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 1.97s
                      Time elapsed: 00:38:28
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 50999 steps/s (collection: 1.819s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 253.0809
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.2846
                       Mean reward: 593.27
               Mean episode length: 207.09
    Episode_Reward/reaching_object: 0.8471
     Episode_Reward/lifting_object: 114.9386
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 1.93s
                      Time elapsed: 00:38:30
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 48982 steps/s (collection: 1.906s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 262.2325
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.2851
                       Mean reward: 591.32
               Mean episode length: 209.93
    Episode_Reward/reaching_object: 0.8650
     Episode_Reward/lifting_object: 117.2970
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.01s
                      Time elapsed: 00:38:32
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 50166 steps/s (collection: 1.835s, learning 0.125s)
             Mean action noise std: 1.93
          Mean value_function loss: 271.7606
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.2859
                       Mean reward: 579.26
               Mean episode length: 205.25
    Episode_Reward/reaching_object: 0.8643
     Episode_Reward/lifting_object: 117.9891
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 1.96s
                      Time elapsed: 00:38:34
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 50861 steps/s (collection: 1.838s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 294.2219
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.2863
                       Mean reward: 566.85
               Mean episode length: 200.57
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 116.5916
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 1.93s
                      Time elapsed: 00:38:36
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 51288 steps/s (collection: 1.830s, learning 0.087s)
             Mean action noise std: 1.93
          Mean value_function loss: 319.8314
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.2890
                       Mean reward: 598.63
               Mean episode length: 209.62
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 116.2550
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 1.92s
                      Time elapsed: 00:38:38
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 51253 steps/s (collection: 1.829s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 302.5316
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.2890
                       Mean reward: 581.01
               Mean episode length: 202.46
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 115.3467
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.92s
                      Time elapsed: 00:38:40
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 51023 steps/s (collection: 1.838s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 281.7263
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.2891
                       Mean reward: 552.36
               Mean episode length: 192.84
    Episode_Reward/reaching_object: 0.8244
     Episode_Reward/lifting_object: 114.2016
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 1.93s
                      Time elapsed: 00:38:42
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 50503 steps/s (collection: 1.828s, learning 0.119s)
             Mean action noise std: 1.93
          Mean value_function loss: 309.9045
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.2886
                       Mean reward: 582.48
               Mean episode length: 202.13
    Episode_Reward/reaching_object: 0.8017
     Episode_Reward/lifting_object: 111.8638
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.95s
                      Time elapsed: 00:38:44
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 50748 steps/s (collection: 1.836s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 309.6732
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.2918
                       Mean reward: 559.74
               Mean episode length: 194.09
    Episode_Reward/reaching_object: 0.8142
     Episode_Reward/lifting_object: 114.6849
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.94s
                      Time elapsed: 00:38:46
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 50965 steps/s (collection: 1.840s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 311.5460
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.2955
                       Mean reward: 582.38
               Mean episode length: 201.02
    Episode_Reward/reaching_object: 0.8139
     Episode_Reward/lifting_object: 114.5907
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.93s
                      Time elapsed: 00:38:48
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 50352 steps/s (collection: 1.853s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 296.8131
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.2975
                       Mean reward: 603.34
               Mean episode length: 207.67
    Episode_Reward/reaching_object: 0.8268
     Episode_Reward/lifting_object: 116.9801
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.95s
                      Time elapsed: 00:38:50
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 49711 steps/s (collection: 1.845s, learning 0.133s)
             Mean action noise std: 1.93
          Mean value_function loss: 312.8579
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 36.2983
                       Mean reward: 600.49
               Mean episode length: 204.45
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 119.2468
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 1.98s
                      Time elapsed: 00:38:52
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 50402 steps/s (collection: 1.857s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 285.5451
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.2989
                       Mean reward: 596.73
               Mean episode length: 202.95
    Episode_Reward/reaching_object: 0.8539
     Episode_Reward/lifting_object: 121.8518
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 1.95s
                      Time elapsed: 00:38:54
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 50305 steps/s (collection: 1.849s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 294.2495
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.2984
                       Mean reward: 622.19
               Mean episode length: 205.49
    Episode_Reward/reaching_object: 0.8612
     Episode_Reward/lifting_object: 124.1145
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 1.95s
                      Time elapsed: 00:38:56
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 50034 steps/s (collection: 1.853s, learning 0.112s)
             Mean action noise std: 1.94
          Mean value_function loss: 293.9455
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.2978
                       Mean reward: 602.96
               Mean episode length: 200.65
    Episode_Reward/reaching_object: 0.8522
     Episode_Reward/lifting_object: 122.9624
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 1.96s
                      Time elapsed: 00:38:58
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 49125 steps/s (collection: 1.846s, learning 0.155s)
             Mean action noise std: 1.94
          Mean value_function loss: 272.3643
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.2960
                       Mean reward: 627.09
               Mean episode length: 211.45
    Episode_Reward/reaching_object: 0.8525
     Episode_Reward/lifting_object: 122.9453
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.00s
                      Time elapsed: 00:39:00
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 47652 steps/s (collection: 1.922s, learning 0.141s)
             Mean action noise std: 1.94
          Mean value_function loss: 272.0444
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 36.2937
                       Mean reward: 625.12
               Mean episode length: 206.58
    Episode_Reward/reaching_object: 0.8510
     Episode_Reward/lifting_object: 124.5727
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.06s
                      Time elapsed: 00:39:02
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 50891 steps/s (collection: 1.837s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 274.5459
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.2955
                       Mean reward: 631.04
               Mean episode length: 207.65
    Episode_Reward/reaching_object: 0.8554
     Episode_Reward/lifting_object: 125.5790
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 1.93s
                      Time elapsed: 00:39:04
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 46430 steps/s (collection: 1.993s, learning 0.125s)
             Mean action noise std: 1.94
          Mean value_function loss: 286.7807
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.2977
                       Mean reward: 628.09
               Mean episode length: 206.77
    Episode_Reward/reaching_object: 0.8505
     Episode_Reward/lifting_object: 125.1845
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.12s
                      Time elapsed: 00:39:06
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 47095 steps/s (collection: 1.977s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 270.7326
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.2961
                       Mean reward: 623.13
               Mean episode length: 205.31
    Episode_Reward/reaching_object: 0.8499
     Episode_Reward/lifting_object: 125.5901
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.09s
                      Time elapsed: 00:39:08
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 50512 steps/s (collection: 1.848s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 284.8128
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.2933
                       Mean reward: 639.87
               Mean episode length: 207.12
    Episode_Reward/reaching_object: 0.8632
     Episode_Reward/lifting_object: 128.1620
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.95s
                      Time elapsed: 00:39:10
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 51876 steps/s (collection: 1.793s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 326.0536
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.2923
                       Mean reward: 679.76
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 0.8814
     Episode_Reward/lifting_object: 131.9879
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 1.89s
                      Time elapsed: 00:39:12
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 50658 steps/s (collection: 1.841s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 273.8709
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.2929
                       Mean reward: 641.30
               Mean episode length: 206.69
    Episode_Reward/reaching_object: 0.8784
     Episode_Reward/lifting_object: 131.4666
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 1.94s
                      Time elapsed: 00:39:14
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 50593 steps/s (collection: 1.814s, learning 0.129s)
             Mean action noise std: 1.94
          Mean value_function loss: 287.1025
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.2937
                       Mean reward: 692.03
               Mean episode length: 217.10
    Episode_Reward/reaching_object: 0.8790
     Episode_Reward/lifting_object: 133.0934
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 1.94s
                      Time elapsed: 00:39:15
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 50626 steps/s (collection: 1.798s, learning 0.144s)
             Mean action noise std: 1.94
          Mean value_function loss: 280.1002
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.2961
                       Mean reward: 673.36
               Mean episode length: 210.97
    Episode_Reward/reaching_object: 0.8836
     Episode_Reward/lifting_object: 134.5631
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.94s
                      Time elapsed: 00:39:17
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 49239 steps/s (collection: 1.823s, learning 0.173s)
             Mean action noise std: 1.94
          Mean value_function loss: 417.2000
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.2997
                       Mean reward: 691.98
               Mean episode length: 213.21
    Episode_Reward/reaching_object: 0.8798
     Episode_Reward/lifting_object: 135.7120
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.00s
                      Time elapsed: 00:39:19
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 51790 steps/s (collection: 1.796s, learning 0.103s)
             Mean action noise std: 1.94
          Mean value_function loss: 335.2780
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.3026
                       Mean reward: 691.28
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 0.8881
     Episode_Reward/lifting_object: 137.8053
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.90s
                      Time elapsed: 00:39:21
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 51524 steps/s (collection: 1.809s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 291.4943
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.3049
                       Mean reward: 681.69
               Mean episode length: 208.84
    Episode_Reward/reaching_object: 0.8588
     Episode_Reward/lifting_object: 134.3369
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 1.91s
                      Time elapsed: 00:39:23
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 50623 steps/s (collection: 1.847s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 300.5633
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.3082
                       Mean reward: 687.57
               Mean episode length: 207.37
    Episode_Reward/reaching_object: 0.8768
     Episode_Reward/lifting_object: 137.9868
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.94s
                      Time elapsed: 00:39:25
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 51820 steps/s (collection: 1.806s, learning 0.091s)
             Mean action noise std: 1.94
          Mean value_function loss: 304.7138
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.3104
                       Mean reward: 729.77
               Mean episode length: 218.55
    Episode_Reward/reaching_object: 0.8826
     Episode_Reward/lifting_object: 139.8947
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 1.90s
                      Time elapsed: 00:39:27
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 51310 steps/s (collection: 1.809s, learning 0.107s)
             Mean action noise std: 1.94
          Mean value_function loss: 288.9059
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 36.3128
                       Mean reward: 674.25
               Mean episode length: 201.11
    Episode_Reward/reaching_object: 0.8618
     Episode_Reward/lifting_object: 136.9481
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.92s
                      Time elapsed: 00:39:29
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 51177 steps/s (collection: 1.837s, learning 0.084s)
             Mean action noise std: 1.94
          Mean value_function loss: 309.0714
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 36.3141
                       Mean reward: 697.99
               Mean episode length: 210.45
    Episode_Reward/reaching_object: 0.8765
     Episode_Reward/lifting_object: 139.7218
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 1.92s
                      Time elapsed: 00:39:31
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 51011 steps/s (collection: 1.822s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 282.6551
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 36.3145
                       Mean reward: 708.88
               Mean episode length: 212.99
    Episode_Reward/reaching_object: 0.8749
     Episode_Reward/lifting_object: 138.4493
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 1.93s
                      Time elapsed: 00:39:33
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 49802 steps/s (collection: 1.861s, learning 0.113s)
             Mean action noise std: 1.94
          Mean value_function loss: 274.5234
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.3145
                       Mean reward: 699.05
               Mean episode length: 210.02
    Episode_Reward/reaching_object: 0.8727
     Episode_Reward/lifting_object: 139.0297
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 1.97s
                      Time elapsed: 00:39:35
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 50789 steps/s (collection: 1.819s, learning 0.117s)
             Mean action noise std: 1.94
          Mean value_function loss: 251.7554
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.3136
                       Mean reward: 745.19
               Mean episode length: 220.60
    Episode_Reward/reaching_object: 0.8963
     Episode_Reward/lifting_object: 142.6847
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 1.94s
                      Time elapsed: 00:39:37
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 50887 steps/s (collection: 1.837s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 269.3203
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 36.3122
                       Mean reward: 699.14
               Mean episode length: 209.92
    Episode_Reward/reaching_object: 0.8671
     Episode_Reward/lifting_object: 137.7976
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 1.93s
                      Time elapsed: 00:39:39
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 50722 steps/s (collection: 1.838s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 269.8943
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 36.3122
                       Mean reward: 697.26
               Mean episode length: 209.37
    Episode_Reward/reaching_object: 0.8966
     Episode_Reward/lifting_object: 142.5224
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 1.94s
                      Time elapsed: 00:39:41
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 51147 steps/s (collection: 1.829s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 290.3053
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 36.3125
                       Mean reward: 655.65
               Mean episode length: 196.51
    Episode_Reward/reaching_object: 0.8801
     Episode_Reward/lifting_object: 140.9741
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 1.92s
                      Time elapsed: 00:39:43
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 51006 steps/s (collection: 1.824s, learning 0.103s)
             Mean action noise std: 1.94
          Mean value_function loss: 287.4804
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.3138
                       Mean reward: 737.86
               Mean episode length: 218.51
    Episode_Reward/reaching_object: 0.9256
     Episode_Reward/lifting_object: 148.4225
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 1.93s
                      Time elapsed: 00:39:44
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 51050 steps/s (collection: 1.827s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 284.5035
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 36.3153
                       Mean reward: 723.21
               Mean episode length: 210.91
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 146.5701
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 1.93s
                      Time elapsed: 00:39:46
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 51950 steps/s (collection: 1.807s, learning 0.086s)
             Mean action noise std: 1.94
          Mean value_function loss: 262.3482
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 36.3159
                       Mean reward: 767.36
               Mean episode length: 222.94
    Episode_Reward/reaching_object: 0.9257
     Episode_Reward/lifting_object: 150.0039
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 1.89s
                      Time elapsed: 00:39:48
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 51759 steps/s (collection: 1.810s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 281.5588
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.3160
                       Mean reward: 686.50
               Mean episode length: 202.09
    Episode_Reward/reaching_object: 0.8873
     Episode_Reward/lifting_object: 144.2352
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 1.90s
                      Time elapsed: 00:39:50
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 49554 steps/s (collection: 1.882s, learning 0.101s)
             Mean action noise std: 1.94
          Mean value_function loss: 301.0336
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 36.3161
                       Mean reward: 716.44
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 0.8688
     Episode_Reward/lifting_object: 141.1100
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 1.98s
                      Time elapsed: 00:39:52
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 51799 steps/s (collection: 1.809s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 294.9079
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 36.3165
                       Mean reward: 746.21
               Mean episode length: 216.13
    Episode_Reward/reaching_object: 0.8611
     Episode_Reward/lifting_object: 139.8156
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 1.90s
                      Time elapsed: 00:39:54
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 50830 steps/s (collection: 1.839s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 289.8747
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 36.3167
                       Mean reward: 767.53
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 148.6891
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.93s
                      Time elapsed: 00:39:56
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 50835 steps/s (collection: 1.841s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 235.9405
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.3171
                       Mean reward: 747.14
               Mean episode length: 216.91
    Episode_Reward/reaching_object: 0.9212
     Episode_Reward/lifting_object: 150.0975
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 1.93s
                      Time elapsed: 00:39:58
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 51484 steps/s (collection: 1.820s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 248.6348
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.3186
                       Mean reward: 763.50
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 0.9077
     Episode_Reward/lifting_object: 147.3458
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.91s
                      Time elapsed: 00:40:00
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 51422 steps/s (collection: 1.818s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 265.5748
               Mean surrogate loss: 0.0231
                 Mean entropy loss: 36.3223
                       Mean reward: 763.16
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 0.9331
     Episode_Reward/lifting_object: 150.9377
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.91s
                      Time elapsed: 00:40:02
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 51997 steps/s (collection: 1.801s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 240.6864
               Mean surrogate loss: 0.0155
                 Mean entropy loss: 36.3239
                       Mean reward: 774.41
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 0.9385
     Episode_Reward/lifting_object: 152.6442
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 1.89s
                      Time elapsed: 00:40:04
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 50665 steps/s (collection: 1.825s, learning 0.115s)
             Mean action noise std: 1.94
          Mean value_function loss: 257.7815
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.3249
                       Mean reward: 741.54
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 0.9326
     Episode_Reward/lifting_object: 152.7025
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 1.94s
                      Time elapsed: 00:40:06
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 51862 steps/s (collection: 1.790s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 252.5132
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 36.3274
                       Mean reward: 747.31
               Mean episode length: 212.73
    Episode_Reward/reaching_object: 0.9266
     Episode_Reward/lifting_object: 151.5521
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 1.90s
                      Time elapsed: 00:40:07
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 52528 steps/s (collection: 1.778s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 228.6574
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 36.3285
                       Mean reward: 746.76
               Mean episode length: 214.22
    Episode_Reward/reaching_object: 0.9488
     Episode_Reward/lifting_object: 156.4405
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.87s
                      Time elapsed: 00:40:09
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 50946 steps/s (collection: 1.842s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 264.8982
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 36.3286
                       Mean reward: 773.43
               Mean episode length: 217.25
    Episode_Reward/reaching_object: 0.9401
     Episode_Reward/lifting_object: 156.0016
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 1.93s
                      Time elapsed: 00:40:11
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 52292 steps/s (collection: 1.786s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 286.4907
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 36.3287
                       Mean reward: 733.90
               Mean episode length: 207.90
    Episode_Reward/reaching_object: 0.9167
     Episode_Reward/lifting_object: 152.0688
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.88s
                      Time elapsed: 00:40:13
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 51438 steps/s (collection: 1.800s, learning 0.112s)
             Mean action noise std: 1.94
          Mean value_function loss: 307.5320
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.3290
                       Mean reward: 751.33
               Mean episode length: 213.96
    Episode_Reward/reaching_object: 0.9026
     Episode_Reward/lifting_object: 150.0375
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.91s
                      Time elapsed: 00:40:15
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 51015 steps/s (collection: 1.836s, learning 0.091s)
             Mean action noise std: 1.94
          Mean value_function loss: 293.8197
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.3291
                       Mean reward: 750.89
               Mean episode length: 213.42
    Episode_Reward/reaching_object: 0.9142
     Episode_Reward/lifting_object: 151.1223
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 1.93s
                      Time elapsed: 00:40:17
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 52196 steps/s (collection: 1.789s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 300.2750
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 36.3304
                       Mean reward: 756.65
               Mean episode length: 218.42
    Episode_Reward/reaching_object: 0.8867
     Episode_Reward/lifting_object: 146.0548
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 1.88s
                      Time elapsed: 00:40:19
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 48733 steps/s (collection: 1.918s, learning 0.099s)
             Mean action noise std: 1.95
          Mean value_function loss: 257.8025
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.3314
                       Mean reward: 755.27
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 150.2406
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.02s
                      Time elapsed: 00:40:21
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 49603 steps/s (collection: 1.855s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 261.5904
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.3319
                       Mean reward: 747.82
               Mean episode length: 216.05
    Episode_Reward/reaching_object: 0.9377
     Episode_Reward/lifting_object: 154.6476
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.98s
                      Time elapsed: 00:40:23
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 51096 steps/s (collection: 1.814s, learning 0.110s)
             Mean action noise std: 1.95
          Mean value_function loss: 249.3226
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.3361
                       Mean reward: 799.34
               Mean episode length: 228.00
    Episode_Reward/reaching_object: 0.9551
     Episode_Reward/lifting_object: 157.5301
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.92s
                      Time elapsed: 00:40:25
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 50748 steps/s (collection: 1.811s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 230.0327
               Mean surrogate loss: 0.0195
                 Mean entropy loss: 36.3378
                       Mean reward: 823.61
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 0.9467
     Episode_Reward/lifting_object: 155.7646
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 1.94s
                      Time elapsed: 00:40:27
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 51791 steps/s (collection: 1.788s, learning 0.110s)
             Mean action noise std: 1.95
          Mean value_function loss: 240.9762
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.3386
                       Mean reward: 786.03
               Mean episode length: 223.85
    Episode_Reward/reaching_object: 0.9412
     Episode_Reward/lifting_object: 155.0756
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 1.90s
                      Time elapsed: 00:40:29
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 49117 steps/s (collection: 1.849s, learning 0.153s)
             Mean action noise std: 1.95
          Mean value_function loss: 272.7205
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.3384
                       Mean reward: 775.63
               Mean episode length: 219.31
    Episode_Reward/reaching_object: 0.9351
     Episode_Reward/lifting_object: 154.3861
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.00s
                      Time elapsed: 00:40:31
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 48119 steps/s (collection: 1.925s, learning 0.118s)
             Mean action noise std: 1.95
          Mean value_function loss: 270.5102
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 36.3383
                       Mean reward: 806.69
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 0.9227
     Episode_Reward/lifting_object: 151.3087
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.04s
                      Time elapsed: 00:40:33
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 49455 steps/s (collection: 1.819s, learning 0.169s)
             Mean action noise std: 1.95
          Mean value_function loss: 285.2838
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 36.3387
                       Mean reward: 806.12
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 0.9307
     Episode_Reward/lifting_object: 153.4460
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 1.99s
                      Time elapsed: 00:40:35
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 47742 steps/s (collection: 1.921s, learning 0.138s)
             Mean action noise std: 1.95
          Mean value_function loss: 249.8234
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 36.3390
                       Mean reward: 753.06
               Mean episode length: 219.05
    Episode_Reward/reaching_object: 0.9281
     Episode_Reward/lifting_object: 152.6875
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.06s
                      Time elapsed: 00:40:37
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 50893 steps/s (collection: 1.817s, learning 0.114s)
             Mean action noise std: 1.95
          Mean value_function loss: 257.7341
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 36.3393
                       Mean reward: 719.35
               Mean episode length: 210.36
    Episode_Reward/reaching_object: 0.9012
     Episode_Reward/lifting_object: 146.9488
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 1.93s
                      Time elapsed: 00:40:39
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 51034 steps/s (collection: 1.807s, learning 0.119s)
             Mean action noise std: 1.95
          Mean value_function loss: 279.0247
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.3393
                       Mean reward: 728.04
               Mean episode length: 213.63
    Episode_Reward/reaching_object: 0.8973
     Episode_Reward/lifting_object: 145.7853
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.93s
                      Time elapsed: 00:40:41
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 50250 steps/s (collection: 1.801s, learning 0.155s)
             Mean action noise std: 1.95
          Mean value_function loss: 299.0351
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.3395
                       Mean reward: 703.64
               Mean episode length: 204.55
    Episode_Reward/reaching_object: 0.8699
     Episode_Reward/lifting_object: 141.2761
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 1.96s
                      Time elapsed: 00:40:43
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 47403 steps/s (collection: 1.919s, learning 0.155s)
             Mean action noise std: 1.95
          Mean value_function loss: 350.6808
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.3396
                       Mean reward: 688.43
               Mean episode length: 205.37
    Episode_Reward/reaching_object: 0.8781
     Episode_Reward/lifting_object: 140.4657
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.07s
                      Time elapsed: 00:40:45
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 49005 steps/s (collection: 1.809s, learning 0.197s)
             Mean action noise std: 1.95
          Mean value_function loss: 353.0615
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.3398
                       Mean reward: 684.37
               Mean episode length: 207.25
    Episode_Reward/reaching_object: 0.8436
     Episode_Reward/lifting_object: 132.0782
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.01s
                      Time elapsed: 00:40:47
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 49092 steps/s (collection: 1.877s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 331.9786
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.3400
                       Mean reward: 664.10
               Mean episode length: 204.22
    Episode_Reward/reaching_object: 0.8279
     Episode_Reward/lifting_object: 129.4984
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.00s
                      Time elapsed: 00:40:49
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 49093 steps/s (collection: 1.873s, learning 0.129s)
             Mean action noise std: 1.95
          Mean value_function loss: 313.1273
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.3400
                       Mean reward: 679.26
               Mean episode length: 208.66
    Episode_Reward/reaching_object: 0.8612
     Episode_Reward/lifting_object: 133.0727
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.00s
                      Time elapsed: 00:40:51
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 47645 steps/s (collection: 1.970s, learning 0.093s)
             Mean action noise std: 1.95
          Mean value_function loss: 271.5431
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.3398
                       Mean reward: 744.18
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 142.1779
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.06s
                      Time elapsed: 00:40:53
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 51294 steps/s (collection: 1.802s, learning 0.114s)
             Mean action noise std: 1.95
          Mean value_function loss: 272.8161
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.3403
                       Mean reward: 741.00
               Mean episode length: 221.52
    Episode_Reward/reaching_object: 0.9036
     Episode_Reward/lifting_object: 141.4173
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 1.92s
                      Time elapsed: 00:40:55
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 52101 steps/s (collection: 1.802s, learning 0.085s)
             Mean action noise std: 1.95
          Mean value_function loss: 241.0507
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.3401
                       Mean reward: 745.80
               Mean episode length: 222.53
    Episode_Reward/reaching_object: 0.9208
     Episode_Reward/lifting_object: 144.3452
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 1.89s
                      Time elapsed: 00:40:56
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 53039 steps/s (collection: 1.767s, learning 0.086s)
             Mean action noise std: 1.95
          Mean value_function loss: 276.5730
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 36.3399
                       Mean reward: 693.12
               Mean episode length: 208.37
    Episode_Reward/reaching_object: 0.8928
     Episode_Reward/lifting_object: 140.6454
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 1.85s
                      Time elapsed: 00:40:58
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 51920 steps/s (collection: 1.808s, learning 0.086s)
             Mean action noise std: 1.95
          Mean value_function loss: 219.3801
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.3403
                       Mean reward: 720.72
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 0.9289
     Episode_Reward/lifting_object: 145.2959
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 1.89s
                      Time elapsed: 00:41:00
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 51332 steps/s (collection: 1.799s, learning 0.117s)
             Mean action noise std: 1.95
          Mean value_function loss: 257.3031
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 36.3408
                       Mean reward: 781.35
               Mean episode length: 228.90
    Episode_Reward/reaching_object: 0.9257
     Episode_Reward/lifting_object: 146.4204
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.92s
                      Time elapsed: 00:41:02
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 51214 steps/s (collection: 1.824s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 228.4597
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.3417
                       Mean reward: 752.57
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 0.9234
     Episode_Reward/lifting_object: 147.0682
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 1.92s
                      Time elapsed: 00:41:04
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 51242 steps/s (collection: 1.807s, learning 0.112s)
             Mean action noise std: 1.95
          Mean value_function loss: 246.6556
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.3453
                       Mean reward: 739.42
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 0.9473
     Episode_Reward/lifting_object: 153.4019
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.92s
                      Time elapsed: 00:41:06
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 52416 steps/s (collection: 1.783s, learning 0.093s)
             Mean action noise std: 1.95
          Mean value_function loss: 200.0214
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 36.3519
                       Mean reward: 798.69
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 0.9689
     Episode_Reward/lifting_object: 157.6378
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 1.88s
                      Time elapsed: 00:41:08
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 51155 steps/s (collection: 1.807s, learning 0.115s)
             Mean action noise std: 1.95
          Mean value_function loss: 195.7122
               Mean surrogate loss: 0.0148
                 Mean entropy loss: 36.3549
                       Mean reward: 821.38
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.9730
     Episode_Reward/lifting_object: 159.9830
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 1.92s
                      Time elapsed: 00:41:10
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 51271 steps/s (collection: 1.802s, learning 0.115s)
             Mean action noise std: 1.95
          Mean value_function loss: 208.1808
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.3566
                       Mean reward: 769.88
               Mean episode length: 214.10
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 160.5899
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.92s
                      Time elapsed: 00:41:12
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 47677 steps/s (collection: 1.954s, learning 0.108s)
             Mean action noise std: 1.95
          Mean value_function loss: 236.4224
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.3622
                       Mean reward: 842.75
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 0.9689
     Episode_Reward/lifting_object: 161.8641
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.06s
                      Time elapsed: 00:41:14
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 49192 steps/s (collection: 1.873s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 211.9733
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.3688
                       Mean reward: 827.09
               Mean episode length: 225.72
    Episode_Reward/reaching_object: 0.9491
     Episode_Reward/lifting_object: 159.3283
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.00s
                      Time elapsed: 00:41:16
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 51938 steps/s (collection: 1.776s, learning 0.117s)
             Mean action noise std: 1.95
          Mean value_function loss: 186.0857
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.3708
                       Mean reward: 848.87
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 0.9848
     Episode_Reward/lifting_object: 165.9238
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.89s
                      Time elapsed: 00:41:18
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 52586 steps/s (collection: 1.760s, learning 0.110s)
             Mean action noise std: 1.95
          Mean value_function loss: 230.2262
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.3719
                       Mean reward: 847.03
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 0.9797
     Episode_Reward/lifting_object: 165.4277
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.87s
                      Time elapsed: 00:41:20
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 51439 steps/s (collection: 1.795s, learning 0.116s)
             Mean action noise std: 1.95
          Mean value_function loss: 180.6809
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.3731
                       Mean reward: 863.34
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.0064
     Episode_Reward/lifting_object: 170.4937
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.91s
                      Time elapsed: 00:41:21
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 51824 steps/s (collection: 1.798s, learning 0.099s)
             Mean action noise std: 1.95
          Mean value_function loss: 196.7708
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 36.3743
                       Mean reward: 850.85
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 0.9921
     Episode_Reward/lifting_object: 168.5960
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.90s
                      Time elapsed: 00:41:23
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 52384 steps/s (collection: 1.766s, learning 0.111s)
             Mean action noise std: 1.95
          Mean value_function loss: 183.6988
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 36.3754
                       Mean reward: 811.67
               Mean episode length: 220.32
    Episode_Reward/reaching_object: 0.9876
     Episode_Reward/lifting_object: 168.2799
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.88s
                      Time elapsed: 00:41:25
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 52401 steps/s (collection: 1.780s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 174.9528
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 36.3781
                       Mean reward: 879.27
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.0193
     Episode_Reward/lifting_object: 173.7037
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.88s
                      Time elapsed: 00:41:27
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 52156 steps/s (collection: 1.788s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 192.4564
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 36.3790
                       Mean reward: 840.08
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.9872
     Episode_Reward/lifting_object: 167.7912
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.88s
                      Time elapsed: 00:41:29
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 51346 steps/s (collection: 1.799s, learning 0.116s)
             Mean action noise std: 1.95
          Mean value_function loss: 161.5498
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 36.3796
                       Mean reward: 870.67
               Mean episode length: 234.44
    Episode_Reward/reaching_object: 0.9963
     Episode_Reward/lifting_object: 169.8126
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 1.91s
                      Time elapsed: 00:41:31
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 51652 steps/s (collection: 1.780s, learning 0.123s)
             Mean action noise std: 1.95
          Mean value_function loss: 162.8028
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 36.3803
                       Mean reward: 856.02
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.0111
     Episode_Reward/lifting_object: 172.4955
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 1.90s
                      Time elapsed: 00:41:33
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 52442 steps/s (collection: 1.773s, learning 0.101s)
             Mean action noise std: 1.95
          Mean value_function loss: 167.4742
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.3809
                       Mean reward: 849.90
               Mean episode length: 229.31
    Episode_Reward/reaching_object: 1.0099
     Episode_Reward/lifting_object: 172.2716
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 1.87s
                      Time elapsed: 00:41:35
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 51478 steps/s (collection: 1.798s, learning 0.112s)
             Mean action noise std: 1.95
          Mean value_function loss: 162.0521
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.3822
                       Mean reward: 819.56
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 0.9870
     Episode_Reward/lifting_object: 168.6039
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.91s
                      Time elapsed: 00:41:37
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 51184 steps/s (collection: 1.837s, learning 0.084s)
             Mean action noise std: 1.95
          Mean value_function loss: 162.8418
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.3872
                       Mean reward: 838.40
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 0.9987
     Episode_Reward/lifting_object: 170.2480
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 1.92s
                      Time elapsed: 00:41:38
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 51952 steps/s (collection: 1.775s, learning 0.117s)
             Mean action noise std: 1.95
          Mean value_function loss: 183.1030
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.3911
                       Mean reward: 880.60
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 1.0156
     Episode_Reward/lifting_object: 173.3281
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 1.89s
                      Time elapsed: 00:41:40
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 53226 steps/s (collection: 1.758s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 159.7347
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 36.3974
                       Mean reward: 862.24
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.0019
     Episode_Reward/lifting_object: 171.3353
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 1.85s
                      Time elapsed: 00:41:42
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 51215 steps/s (collection: 1.798s, learning 0.122s)
             Mean action noise std: 1.96
          Mean value_function loss: 158.6740
               Mean surrogate loss: 0.0153
                 Mean entropy loss: 36.3993
                       Mean reward: 873.94
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.0115
     Episode_Reward/lifting_object: 173.0008
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 1.92s
                      Time elapsed: 00:41:44
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 50789 steps/s (collection: 1.813s, learning 0.123s)
             Mean action noise std: 1.96
          Mean value_function loss: 174.7241
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 36.3998
                       Mean reward: 859.21
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.0011
     Episode_Reward/lifting_object: 171.6067
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 1.94s
                      Time elapsed: 00:41:46
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 49804 steps/s (collection: 1.826s, learning 0.148s)
             Mean action noise std: 1.96
          Mean value_function loss: 176.0883
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4006
                       Mean reward: 881.03
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.0051
     Episode_Reward/lifting_object: 172.3877
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.97s
                      Time elapsed: 00:41:48
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 49408 steps/s (collection: 1.836s, learning 0.154s)
             Mean action noise std: 1.96
          Mean value_function loss: 227.8388
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.4014
                       Mean reward: 835.42
               Mean episode length: 223.85
    Episode_Reward/reaching_object: 0.9608
     Episode_Reward/lifting_object: 164.3397
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 1.99s
                      Time elapsed: 00:41:50
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 48816 steps/s (collection: 1.883s, learning 0.130s)
             Mean action noise std: 1.96
          Mean value_function loss: 233.0765
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.4036
                       Mean reward: 833.41
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 0.9613
     Episode_Reward/lifting_object: 164.2748
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.01s
                      Time elapsed: 00:41:52
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 51915 steps/s (collection: 1.788s, learning 0.106s)
             Mean action noise std: 1.96
          Mean value_function loss: 224.7256
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.4080
                       Mean reward: 797.84
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 0.9693
     Episode_Reward/lifting_object: 165.1613
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 1.89s
                      Time elapsed: 00:41:54
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 52352 steps/s (collection: 1.760s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 226.2182
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 36.4104
                       Mean reward: 848.34
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 0.9673
     Episode_Reward/lifting_object: 164.9317
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.88s
                      Time elapsed: 00:41:56
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 50241 steps/s (collection: 1.840s, learning 0.117s)
             Mean action noise std: 1.96
          Mean value_function loss: 260.3545
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.4114
                       Mean reward: 771.52
               Mean episode length: 209.77
    Episode_Reward/reaching_object: 0.9228
     Episode_Reward/lifting_object: 157.2219
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 1.96s
                      Time elapsed: 00:41:58
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 49396 steps/s (collection: 1.863s, learning 0.127s)
             Mean action noise std: 1.96
          Mean value_function loss: 238.2224
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.4158
                       Mean reward: 821.88
               Mean episode length: 222.20
    Episode_Reward/reaching_object: 0.9662
     Episode_Reward/lifting_object: 165.2731
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.99s
                      Time elapsed: 00:42:00
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 51240 steps/s (collection: 1.815s, learning 0.104s)
             Mean action noise std: 1.96
          Mean value_function loss: 222.1722
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 36.4230
                       Mean reward: 837.16
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 0.9714
     Episode_Reward/lifting_object: 165.3271
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 1.92s
                      Time elapsed: 00:42:02
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 48881 steps/s (collection: 1.860s, learning 0.152s)
             Mean action noise std: 1.96
          Mean value_function loss: 239.0217
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 36.4286
                       Mean reward: 828.47
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 0.9637
     Episode_Reward/lifting_object: 164.4746
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.01s
                      Time elapsed: 00:42:04
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 50665 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 490.6743
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.4295
                       Mean reward: 832.76
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 0.9710
     Episode_Reward/lifting_object: 164.9619
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.94s
                      Time elapsed: 00:42:06
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 52632 steps/s (collection: 1.777s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 229.4399
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 36.4308
                       Mean reward: 789.83
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 0.9606
     Episode_Reward/lifting_object: 163.5142
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.87s
                      Time elapsed: 00:42:08
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 51800 steps/s (collection: 1.808s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 281.5757
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.4315
                       Mean reward: 786.97
               Mean episode length: 214.24
    Episode_Reward/reaching_object: 0.9525
     Episode_Reward/lifting_object: 161.9354
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 1.90s
                      Time elapsed: 00:42:09
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 52661 steps/s (collection: 1.781s, learning 0.086s)
             Mean action noise std: 1.96
          Mean value_function loss: 262.0948
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.4314
                       Mean reward: 798.76
               Mean episode length: 216.76
    Episode_Reward/reaching_object: 0.9665
     Episode_Reward/lifting_object: 164.6595
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.87s
                      Time elapsed: 00:42:11
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 50304 steps/s (collection: 1.834s, learning 0.120s)
             Mean action noise std: 1.96
          Mean value_function loss: 257.6985
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.4319
                       Mean reward: 803.29
               Mean episode length: 220.86
    Episode_Reward/reaching_object: 0.9210
     Episode_Reward/lifting_object: 155.7686
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.95s
                      Time elapsed: 00:42:13
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 51210 steps/s (collection: 1.814s, learning 0.105s)
             Mean action noise std: 1.96
          Mean value_function loss: 219.6211
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.4355
                       Mean reward: 797.42
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 0.9769
     Episode_Reward/lifting_object: 165.3345
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 1.92s
                      Time elapsed: 00:42:15
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 50928 steps/s (collection: 1.821s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 223.6587
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 36.4415
                       Mean reward: 855.79
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 0.9655
     Episode_Reward/lifting_object: 163.5955
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 1.93s
                      Time elapsed: 00:42:17
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 50782 steps/s (collection: 1.818s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 227.1766
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 36.4427
                       Mean reward: 845.41
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.9684
     Episode_Reward/lifting_object: 163.8401
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 1.94s
                      Time elapsed: 00:42:19
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 51044 steps/s (collection: 1.788s, learning 0.138s)
             Mean action noise std: 1.96
          Mean value_function loss: 203.1311
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 36.4430
                       Mean reward: 847.56
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 0.9763
     Episode_Reward/lifting_object: 165.1693
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 1.93s
                      Time elapsed: 00:42:21
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 51110 steps/s (collection: 1.784s, learning 0.139s)
             Mean action noise std: 1.96
          Mean value_function loss: 251.5390
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.4431
                       Mean reward: 791.85
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 0.9484
     Episode_Reward/lifting_object: 158.8586
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 1.92s
                      Time elapsed: 00:42:23
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 52117 steps/s (collection: 1.787s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 241.7779
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4468
                       Mean reward: 797.28
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 0.9370
     Episode_Reward/lifting_object: 157.2888
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 1.89s
                      Time elapsed: 00:42:25
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 51681 steps/s (collection: 1.795s, learning 0.107s)
             Mean action noise std: 1.96
          Mean value_function loss: 218.9914
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 36.4497
                       Mean reward: 808.62
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 0.9613
     Episode_Reward/lifting_object: 161.8880
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 1.90s
                      Time elapsed: 00:42:27
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 49993 steps/s (collection: 1.829s, learning 0.138s)
             Mean action noise std: 1.96
          Mean value_function loss: 189.6424
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 36.4502
                       Mean reward: 866.94
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 0.9904
     Episode_Reward/lifting_object: 167.8338
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 1.97s
                      Time elapsed: 00:42:29
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 50414 steps/s (collection: 1.848s, learning 0.102s)
             Mean action noise std: 1.96
          Mean value_function loss: 196.6163
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 36.4507
                       Mean reward: 863.06
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 0.9940
     Episode_Reward/lifting_object: 168.2705
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 1.95s
                      Time elapsed: 00:42:31
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 51723 steps/s (collection: 1.803s, learning 0.097s)
             Mean action noise std: 1.96
          Mean value_function loss: 169.4856
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 36.4515
                       Mean reward: 847.67
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.0131
     Episode_Reward/lifting_object: 171.8004
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.90s
                      Time elapsed: 00:42:32
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 52628 steps/s (collection: 1.764s, learning 0.103s)
             Mean action noise std: 1.96
          Mean value_function loss: 189.6761
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 36.4520
                       Mean reward: 864.32
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 0.9835
     Episode_Reward/lifting_object: 167.2773
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 1.87s
                      Time elapsed: 00:42:34
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 50927 steps/s (collection: 1.830s, learning 0.100s)
             Mean action noise std: 1.96
          Mean value_function loss: 188.0503
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.4523
                       Mean reward: 802.51
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 0.9789
     Episode_Reward/lifting_object: 166.4605
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 1.93s
                      Time elapsed: 00:42:36
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 49917 steps/s (collection: 1.838s, learning 0.132s)
             Mean action noise std: 1.96
          Mean value_function loss: 202.6025
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 36.4531
                       Mean reward: 804.72
               Mean episode length: 221.52
    Episode_Reward/reaching_object: 0.9783
     Episode_Reward/lifting_object: 166.5584
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 1.97s
                      Time elapsed: 00:42:38
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 50040 steps/s (collection: 1.821s, learning 0.143s)
             Mean action noise std: 1.96
          Mean value_function loss: 193.8010
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.4534
                       Mean reward: 832.55
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 0.9768
     Episode_Reward/lifting_object: 165.2728
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 1.96s
                      Time elapsed: 00:42:40
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 50954 steps/s (collection: 1.778s, learning 0.151s)
             Mean action noise std: 1.96
          Mean value_function loss: 208.6925
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 36.4537
                       Mean reward: 852.96
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.9765
     Episode_Reward/lifting_object: 166.8363
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 1.93s
                      Time elapsed: 00:42:42
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 48945 steps/s (collection: 1.855s, learning 0.154s)
             Mean action noise std: 1.96
          Mean value_function loss: 227.6479
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 36.4539
                       Mean reward: 819.60
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 0.9749
     Episode_Reward/lifting_object: 165.4729
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.01s
                      Time elapsed: 00:42:44
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 51545 steps/s (collection: 1.819s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 250.1542
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.4538
                       Mean reward: 822.22
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 0.9615
     Episode_Reward/lifting_object: 162.5031
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 1.91s
                      Time elapsed: 00:42:46
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 50798 steps/s (collection: 1.818s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 258.0407
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 36.4543
                       Mean reward: 815.07
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 0.9381
     Episode_Reward/lifting_object: 157.0689
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 1.94s
                      Time elapsed: 00:42:48
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 48096 steps/s (collection: 1.912s, learning 0.132s)
             Mean action noise std: 1.96
          Mean value_function loss: 176.3990
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4550
                       Mean reward: 838.67
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.9712
     Episode_Reward/lifting_object: 163.1098
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.04s
                      Time elapsed: 00:42:50
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 50960 steps/s (collection: 1.839s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 206.3572
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 36.4554
                       Mean reward: 813.76
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 163.3602
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 1.93s
                      Time elapsed: 00:42:52
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 52127 steps/s (collection: 1.785s, learning 0.101s)
             Mean action noise std: 1.96
          Mean value_function loss: 211.6670
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 36.4557
                       Mean reward: 785.95
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 0.9655
     Episode_Reward/lifting_object: 161.7771
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.89s
                      Time elapsed: 00:42:54
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 49853 steps/s (collection: 1.863s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 198.9276
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 36.4561
                       Mean reward: 834.43
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 0.9666
     Episode_Reward/lifting_object: 162.1937
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 1.97s
                      Time elapsed: 00:42:56
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 52478 steps/s (collection: 1.783s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 169.3462
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 36.4569
                       Mean reward: 839.91
               Mean episode length: 229.23
    Episode_Reward/reaching_object: 0.9877
     Episode_Reward/lifting_object: 165.9452
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 1.87s
                      Time elapsed: 00:42:58
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 50828 steps/s (collection: 1.832s, learning 0.102s)
             Mean action noise std: 1.96
          Mean value_function loss: 165.8665
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 36.4577
                       Mean reward: 816.31
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 0.9672
     Episode_Reward/lifting_object: 161.6176
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 1.93s
                      Time elapsed: 00:43:00
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 51029 steps/s (collection: 1.812s, learning 0.115s)
             Mean action noise std: 1.96
          Mean value_function loss: 110.4516
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 36.4581
                       Mean reward: 883.00
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 1.0205
     Episode_Reward/lifting_object: 173.0927
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 1.93s
                      Time elapsed: 00:43:02
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 49437 steps/s (collection: 1.886s, learning 0.102s)
             Mean action noise std: 1.96
          Mean value_function loss: 142.3749
               Mean surrogate loss: 0.0119
                 Mean entropy loss: 36.4588
                       Mean reward: 860.90
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.0063
     Episode_Reward/lifting_object: 170.7505
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 1.99s
                      Time elapsed: 00:43:04
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 48547 steps/s (collection: 1.931s, learning 0.094s)
             Mean action noise std: 1.96
          Mean value_function loss: 128.1409
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 36.4594
                       Mean reward: 853.73
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 1.0213
     Episode_Reward/lifting_object: 173.4081
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.02s
                      Time elapsed: 00:43:06
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 50754 steps/s (collection: 1.845s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 133.1162
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 36.4602
                       Mean reward: 861.36
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 1.0005
     Episode_Reward/lifting_object: 170.2401
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 1.94s
                      Time elapsed: 00:43:08
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 51259 steps/s (collection: 1.825s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 126.7736
               Mean surrogate loss: 0.0139
                 Mean entropy loss: 36.4608
                       Mean reward: 869.07
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.0069
     Episode_Reward/lifting_object: 172.2017
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 1.92s
                      Time elapsed: 00:43:09
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 51771 steps/s (collection: 1.811s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 137.7163
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.4616
                       Mean reward: 885.70
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.9879
     Episode_Reward/lifting_object: 168.8328
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 1.90s
                      Time elapsed: 00:43:11
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 52127 steps/s (collection: 1.785s, learning 0.101s)
             Mean action noise std: 1.97
          Mean value_function loss: 158.9319
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 36.4622
                       Mean reward: 887.38
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.0092
     Episode_Reward/lifting_object: 174.5687
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 1.89s
                      Time elapsed: 00:43:13
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 50335 steps/s (collection: 1.806s, learning 0.147s)
             Mean action noise std: 1.97
          Mean value_function loss: 208.6249
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.4630
                       Mean reward: 835.08
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 0.9917
     Episode_Reward/lifting_object: 171.2775
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 1.95s
                      Time elapsed: 00:43:15
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 52096 steps/s (collection: 1.787s, learning 0.100s)
             Mean action noise std: 1.97
          Mean value_function loss: 242.7563
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.4645
                       Mean reward: 838.26
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 0.9879
     Episode_Reward/lifting_object: 171.3446
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 1.89s
                      Time elapsed: 00:43:17
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 49371 steps/s (collection: 1.838s, learning 0.153s)
             Mean action noise std: 1.97
          Mean value_function loss: 336.8522
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.4686
                       Mean reward: 803.65
               Mean episode length: 214.85
    Episode_Reward/reaching_object: 0.9516
     Episode_Reward/lifting_object: 165.3417
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 1.99s
                      Time elapsed: 00:43:19
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 50864 steps/s (collection: 1.829s, learning 0.103s)
             Mean action noise std: 1.97
          Mean value_function loss: 357.4344
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.4761
                       Mean reward: 807.95
               Mean episode length: 215.85
    Episode_Reward/reaching_object: 0.9329
     Episode_Reward/lifting_object: 161.8712
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 1.93s
                      Time elapsed: 00:43:21
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 51375 steps/s (collection: 1.823s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 349.8729
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.4882
                       Mean reward: 806.71
               Mean episode length: 215.81
    Episode_Reward/reaching_object: 0.9567
     Episode_Reward/lifting_object: 166.6409
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 1.91s
                      Time elapsed: 00:43:23
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 49480 steps/s (collection: 1.855s, learning 0.132s)
             Mean action noise std: 1.97
          Mean value_function loss: 388.2415
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.4962
                       Mean reward: 802.64
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 160.6125
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 1.99s
                      Time elapsed: 00:43:25
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 50385 steps/s (collection: 1.856s, learning 0.095s)
             Mean action noise std: 1.97
          Mean value_function loss: 316.0107
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.5057
                       Mean reward: 822.59
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 0.9593
     Episode_Reward/lifting_object: 167.2748
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 1.95s
                      Time elapsed: 00:43:27
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 49075 steps/s (collection: 1.909s, learning 0.095s)
             Mean action noise std: 1.97
          Mean value_function loss: 367.6283
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5120
                       Mean reward: 813.57
               Mean episode length: 217.05
    Episode_Reward/reaching_object: 0.9602
     Episode_Reward/lifting_object: 167.3118
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.00s
                      Time elapsed: 00:43:29
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 48154 steps/s (collection: 1.832s, learning 0.210s)
             Mean action noise std: 1.97
          Mean value_function loss: 297.5495
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.5187
                       Mean reward: 825.97
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 0.9673
     Episode_Reward/lifting_object: 168.3061
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.04s
                      Time elapsed: 00:43:31
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 49656 steps/s (collection: 1.846s, learning 0.134s)
             Mean action noise std: 1.97
          Mean value_function loss: 276.0888
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 36.5251
                       Mean reward: 855.08
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 0.9532
     Episode_Reward/lifting_object: 165.9909
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 1.98s
                      Time elapsed: 00:43:33
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 50515 steps/s (collection: 1.850s, learning 0.096s)
             Mean action noise std: 1.97
          Mean value_function loss: 255.3622
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.5334
                       Mean reward: 870.97
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 0.9829
     Episode_Reward/lifting_object: 171.0591
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 1.95s
                      Time elapsed: 00:43:35
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 50921 steps/s (collection: 1.832s, learning 0.099s)
             Mean action noise std: 1.97
          Mean value_function loss: 240.8575
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.5392
                       Mean reward: 901.17
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 0.9977
     Episode_Reward/lifting_object: 173.8780
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 1.93s
                      Time elapsed: 00:43:37
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 49961 steps/s (collection: 1.840s, learning 0.128s)
             Mean action noise std: 1.97
          Mean value_function loss: 245.0407
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 36.5437
                       Mean reward: 876.61
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 1.0066
     Episode_Reward/lifting_object: 175.5382
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 1.97s
                      Time elapsed: 00:43:39
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 47836 steps/s (collection: 1.921s, learning 0.134s)
             Mean action noise std: 1.98
          Mean value_function loss: 199.3640
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.5482
                       Mean reward: 884.34
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 1.0167
     Episode_Reward/lifting_object: 177.2248
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.05s
                      Time elapsed: 00:43:41
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 48555 steps/s (collection: 1.883s, learning 0.142s)
             Mean action noise std: 1.98
          Mean value_function loss: 221.5499
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 36.5606
                       Mean reward: 897.03
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.9875
     Episode_Reward/lifting_object: 171.9183
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.02s
                      Time elapsed: 00:43:43
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 50187 steps/s (collection: 1.860s, learning 0.099s)
             Mean action noise std: 1.98
          Mean value_function loss: 192.1560
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.5723
                       Mean reward: 921.24
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 1.0063
     Episode_Reward/lifting_object: 175.8492
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 1.96s
                      Time elapsed: 00:43:45
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 50425 steps/s (collection: 1.856s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 182.6295
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.5841
                       Mean reward: 898.04
               Mean episode length: 237.41
    Episode_Reward/reaching_object: 1.0085
     Episode_Reward/lifting_object: 175.9240
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 1.95s
                      Time elapsed: 00:43:47
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 47435 steps/s (collection: 1.889s, learning 0.183s)
             Mean action noise std: 1.98
          Mean value_function loss: 153.3565
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.5876
                       Mean reward: 916.68
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.0285
     Episode_Reward/lifting_object: 179.5913
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.07s
                      Time elapsed: 00:43:49
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 45474 steps/s (collection: 2.001s, learning 0.161s)
             Mean action noise std: 1.98
          Mean value_function loss: 127.9953
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.5906
                       Mean reward: 905.63
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 1.0365
     Episode_Reward/lifting_object: 181.1681
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.16s
                      Time elapsed: 00:43:51
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 50082 steps/s (collection: 1.868s, learning 0.095s)
             Mean action noise std: 1.98
          Mean value_function loss: 129.8773
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 36.5939
                       Mean reward: 905.29
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 178.8438
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 1.96s
                      Time elapsed: 00:43:53
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 49597 steps/s (collection: 1.869s, learning 0.113s)
             Mean action noise std: 1.98
          Mean value_function loss: 146.0245
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 36.5954
                       Mean reward: 889.15
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 1.0258
     Episode_Reward/lifting_object: 179.0845
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 1.98s
                      Time elapsed: 00:43:55
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 47005 steps/s (collection: 1.970s, learning 0.122s)
             Mean action noise std: 1.98
          Mean value_function loss: 132.0095
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 36.5962
                       Mean reward: 876.10
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 1.0227
     Episode_Reward/lifting_object: 178.6296
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.09s
                      Time elapsed: 00:43:57
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 47799 steps/s (collection: 1.958s, learning 0.099s)
             Mean action noise std: 1.98
          Mean value_function loss: 100.5514
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.5975
                       Mean reward: 927.39
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 1.0439
     Episode_Reward/lifting_object: 182.9517
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.06s
                      Time elapsed: 00:43:59
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 45058 steps/s (collection: 1.985s, learning 0.197s)
             Mean action noise std: 1.98
          Mean value_function loss: 111.2759
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 36.6006
                       Mean reward: 921.44
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 182.2792
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.18s
                      Time elapsed: 00:44:01
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 47710 steps/s (collection: 1.893s, learning 0.167s)
             Mean action noise std: 1.98
          Mean value_function loss: 109.6498
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.6015
                       Mean reward: 919.43
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 1.0421
     Episode_Reward/lifting_object: 182.0582
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.06s
                      Time elapsed: 00:44:03
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 48165 steps/s (collection: 1.887s, learning 0.153s)
             Mean action noise std: 1.98
          Mean value_function loss: 133.6706
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.6054
                       Mean reward: 894.85
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.0278
     Episode_Reward/lifting_object: 179.7669
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.04s
                      Time elapsed: 00:44:05
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 49007 steps/s (collection: 1.853s, learning 0.153s)
             Mean action noise std: 1.98
          Mean value_function loss: 117.0382
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 36.6115
                       Mean reward: 908.83
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.0385
     Episode_Reward/lifting_object: 181.4528
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.01s
                      Time elapsed: 00:44:07
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 49899 steps/s (collection: 1.836s, learning 0.134s)
             Mean action noise std: 1.98
          Mean value_function loss: 114.8846
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.6139
                       Mean reward: 889.68
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 1.0353
     Episode_Reward/lifting_object: 181.0241
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 1.97s
                      Time elapsed: 00:44:09
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 49525 steps/s (collection: 1.894s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 105.6840
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 36.6201
                       Mean reward: 882.47
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 182.7579
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 1.98s
                      Time elapsed: 00:44:11
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 51086 steps/s (collection: 1.825s, learning 0.099s)
             Mean action noise std: 1.98
          Mean value_function loss: 102.4083
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.6233
                       Mean reward: 904.34
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 1.0304
     Episode_Reward/lifting_object: 180.2262
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 1.92s
                      Time elapsed: 00:44:13
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 49032 steps/s (collection: 1.892s, learning 0.113s)
             Mean action noise std: 1.98
          Mean value_function loss: 94.9947
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.6274
                       Mean reward: 924.21
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 183.4907
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.00s
                      Time elapsed: 00:44:15
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 44778 steps/s (collection: 2.009s, learning 0.186s)
             Mean action noise std: 1.98
          Mean value_function loss: 102.3034
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 36.6289
                       Mean reward: 899.55
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: 179.9613
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.20s
                      Time elapsed: 00:44:17
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 50576 steps/s (collection: 1.822s, learning 0.122s)
             Mean action noise std: 1.98
          Mean value_function loss: 114.8587
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 36.6300
                       Mean reward: 927.13
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 1.0392
     Episode_Reward/lifting_object: 182.4576
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 1.94s
                      Time elapsed: 00:44:19
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 48560 steps/s (collection: 1.849s, learning 0.175s)
             Mean action noise std: 1.98
          Mean value_function loss: 103.2672
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.6312
                       Mean reward: 925.60
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 1.0380
     Episode_Reward/lifting_object: 182.0632
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.02s
                      Time elapsed: 00:44:21
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 49369 steps/s (collection: 1.826s, learning 0.165s)
             Mean action noise std: 1.98
          Mean value_function loss: 130.1673
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.6323
                       Mean reward: 915.19
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 181.4985
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 1.99s
                      Time elapsed: 00:44:23
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 48170 steps/s (collection: 1.917s, learning 0.124s)
             Mean action noise std: 1.99
          Mean value_function loss: 151.7334
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.6347
                       Mean reward: 903.39
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.0318
     Episode_Reward/lifting_object: 181.7067
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.04s
                      Time elapsed: 00:44:25
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 44509 steps/s (collection: 2.048s, learning 0.161s)
             Mean action noise std: 1.99
          Mean value_function loss: 120.2402
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6367
                       Mean reward: 895.70
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.0286
     Episode_Reward/lifting_object: 181.4101
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.21s
                      Time elapsed: 00:44:28
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 47847 steps/s (collection: 1.915s, learning 0.140s)
             Mean action noise std: 1.99
          Mean value_function loss: 148.8675
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 36.6379
                       Mean reward: 879.49
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 1.0147
     Episode_Reward/lifting_object: 179.1284
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.05s
                      Time elapsed: 00:44:30
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 49019 steps/s (collection: 1.909s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 173.0212
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.6389
                       Mean reward: 923.26
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 1.0246
     Episode_Reward/lifting_object: 181.4835
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.01s
                      Time elapsed: 00:44:32
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 50398 steps/s (collection: 1.816s, learning 0.135s)
             Mean action noise std: 1.99
          Mean value_function loss: 197.9596
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.6417
                       Mean reward: 887.19
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 0.9891
     Episode_Reward/lifting_object: 175.0822
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 1.95s
                      Time elapsed: 00:44:34
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 48013 steps/s (collection: 1.945s, learning 0.103s)
             Mean action noise std: 1.99
          Mean value_function loss: 228.8533
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.6495
                       Mean reward: 858.87
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 0.9829
     Episode_Reward/lifting_object: 174.6680
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.05s
                      Time elapsed: 00:44:36
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 48224 steps/s (collection: 1.930s, learning 0.108s)
             Mean action noise std: 1.99
          Mean value_function loss: 231.2069
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 36.6547
                       Mean reward: 876.70
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 0.9717
     Episode_Reward/lifting_object: 172.9879
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.04s
                      Time elapsed: 00:44:38
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 48185 steps/s (collection: 1.919s, learning 0.121s)
             Mean action noise std: 1.99
          Mean value_function loss: 292.0198
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.6566
                       Mean reward: 854.64
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 0.9471
     Episode_Reward/lifting_object: 168.9886
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.04s
                      Time elapsed: 00:44:40
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 50516 steps/s (collection: 1.828s, learning 0.118s)
             Mean action noise std: 1.99
          Mean value_function loss: 325.8308
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.6582
                       Mean reward: 879.78
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 0.9719
     Episode_Reward/lifting_object: 174.0775
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 1.95s
                      Time elapsed: 00:44:42
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 48576 steps/s (collection: 1.904s, learning 0.120s)
             Mean action noise std: 1.99
          Mean value_function loss: 210.5417
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.6612
                       Mean reward: 856.85
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 0.9753
     Episode_Reward/lifting_object: 174.7845
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.02s
                      Time elapsed: 00:44:44
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 49527 steps/s (collection: 1.883s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 221.9656
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 36.6715
                       Mean reward: 889.61
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 0.9740
     Episode_Reward/lifting_object: 174.6740
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 1.98s
                      Time elapsed: 00:44:46
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 49023 steps/s (collection: 1.883s, learning 0.122s)
             Mean action noise std: 1.99
          Mean value_function loss: 177.3572
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.6768
                       Mean reward: 863.80
               Mean episode length: 229.10
    Episode_Reward/reaching_object: 0.9894
     Episode_Reward/lifting_object: 177.1609
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.01s
                      Time elapsed: 00:44:48
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 47051 steps/s (collection: 1.900s, learning 0.189s)
             Mean action noise std: 1.99
          Mean value_function loss: 165.9896
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 36.6782
                       Mean reward: 871.21
               Mean episode length: 231.50
    Episode_Reward/reaching_object: 0.9721
     Episode_Reward/lifting_object: 173.8383
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.09s
                      Time elapsed: 00:44:50
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 47613 steps/s (collection: 1.932s, learning 0.133s)
             Mean action noise std: 1.99
          Mean value_function loss: 124.0493
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.6799
                       Mean reward: 887.57
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 0.9946
     Episode_Reward/lifting_object: 178.1022
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.06s
                      Time elapsed: 00:44:52
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 48033 steps/s (collection: 1.938s, learning 0.108s)
             Mean action noise std: 1.99
          Mean value_function loss: 99.8365
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 36.6840
                       Mean reward: 887.61
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.0038
     Episode_Reward/lifting_object: 179.6867
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.05s
                      Time elapsed: 00:44:54
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 50556 steps/s (collection: 1.851s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 96.6335
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 36.6862
                       Mean reward: 893.13
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.9995
     Episode_Reward/lifting_object: 178.6401
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 1.94s
                      Time elapsed: 00:44:56
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 49178 steps/s (collection: 1.873s, learning 0.126s)
             Mean action noise std: 1.99
          Mean value_function loss: 85.0706
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.6895
                       Mean reward: 910.96
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 1.0095
     Episode_Reward/lifting_object: 180.1677
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.00s
                      Time elapsed: 00:44:58
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 49280 steps/s (collection: 1.878s, learning 0.117s)
             Mean action noise std: 1.99
          Mean value_function loss: 102.4920
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.6983
                       Mean reward: 921.09
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 1.0116
     Episode_Reward/lifting_object: 180.7583
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 1.99s
                      Time elapsed: 00:45:00
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 46439 steps/s (collection: 1.970s, learning 0.147s)
             Mean action noise std: 1.99
          Mean value_function loss: 76.0491
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 36.7034
                       Mean reward: 923.74
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 1.0323
     Episode_Reward/lifting_object: 184.3877
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.12s
                      Time elapsed: 00:45:02
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 48397 steps/s (collection: 1.915s, learning 0.117s)
             Mean action noise std: 1.99
          Mean value_function loss: 79.4871
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 36.7055
                       Mean reward: 938.04
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 1.0232
     Episode_Reward/lifting_object: 183.0206
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.03s
                      Time elapsed: 00:45:04
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 46238 steps/s (collection: 2.014s, learning 0.112s)
             Mean action noise std: 1.99
          Mean value_function loss: 73.6903
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.7077
                       Mean reward: 907.00
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 1.0183
     Episode_Reward/lifting_object: 181.6825
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.13s
                      Time elapsed: 00:45:06
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 48224 steps/s (collection: 1.922s, learning 0.116s)
             Mean action noise std: 1.99
          Mean value_function loss: 89.9911
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 36.7095
                       Mean reward: 879.58
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 1.0028
     Episode_Reward/lifting_object: 179.0683
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.04s
                      Time elapsed: 00:45:08
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 49529 steps/s (collection: 1.871s, learning 0.114s)
             Mean action noise std: 1.99
          Mean value_function loss: 69.8527
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.7120
                       Mean reward: 924.23
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 1.0116
     Episode_Reward/lifting_object: 180.0668
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 1.98s
                      Time elapsed: 00:45:10
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 45556 steps/s (collection: 1.980s, learning 0.178s)
             Mean action noise std: 1.99
          Mean value_function loss: 80.0841
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.7165
                       Mean reward: 919.47
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 1.0203
     Episode_Reward/lifting_object: 182.1776
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.16s
                      Time elapsed: 00:45:12
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 47652 steps/s (collection: 1.913s, learning 0.150s)
             Mean action noise std: 1.99
          Mean value_function loss: 71.3615
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.7180
                       Mean reward: 891.89
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.0012
     Episode_Reward/lifting_object: 178.6524
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.06s
                      Time elapsed: 00:45:14
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 47274 steps/s (collection: 1.922s, learning 0.157s)
             Mean action noise std: 2.00
          Mean value_function loss: 78.7654
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 36.7189
                       Mean reward: 876.59
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.9980
     Episode_Reward/lifting_object: 177.8713
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.08s
                      Time elapsed: 00:45:16
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 42557 steps/s (collection: 2.163s, learning 0.147s)
             Mean action noise std: 2.00
          Mean value_function loss: 68.0387
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 36.7216
                       Mean reward: 922.66
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 1.0283
     Episode_Reward/lifting_object: 183.5691
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.31s
                      Time elapsed: 00:45:19
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 45449 steps/s (collection: 2.035s, learning 0.128s)
             Mean action noise std: 2.00
          Mean value_function loss: 50.5563
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 36.7233
                       Mean reward: 936.95
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 1.0372
     Episode_Reward/lifting_object: 184.9666
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.16s
                      Time elapsed: 00:45:21
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 47628 steps/s (collection: 1.941s, learning 0.123s)
             Mean action noise std: 2.00
          Mean value_function loss: 59.2351
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 36.7238
                       Mean reward: 938.50
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 1.0336
     Episode_Reward/lifting_object: 184.2714
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.06s
                      Time elapsed: 00:45:23
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 48778 steps/s (collection: 1.904s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 50.8323
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 36.7247
                       Mean reward: 908.20
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.0363
     Episode_Reward/lifting_object: 184.5511
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.02s
                      Time elapsed: 00:45:25
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 48960 steps/s (collection: 1.882s, learning 0.126s)
             Mean action noise std: 2.00
          Mean value_function loss: 59.3035
               Mean surrogate loss: 0.0151
                 Mean entropy loss: 36.7262
                       Mean reward: 930.59
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 1.0349
     Episode_Reward/lifting_object: 184.2001
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.01s
                      Time elapsed: 00:45:27
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 47995 steps/s (collection: 1.939s, learning 0.109s)
             Mean action noise std: 2.00
          Mean value_function loss: 39.3358
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.7277
                       Mean reward: 939.09
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 186.9388
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.05s
                      Time elapsed: 00:45:29
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 49690 steps/s (collection: 1.863s, learning 0.115s)
             Mean action noise std: 2.00
          Mean value_function loss: 49.3386
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 36.7293
                       Mean reward: 925.42
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 1.0423
     Episode_Reward/lifting_object: 185.6528
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 1.98s
                      Time elapsed: 00:45:31
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 48925 steps/s (collection: 1.907s, learning 0.102s)
             Mean action noise std: 2.00
          Mean value_function loss: 61.6405
               Mean surrogate loss: 0.0256
                 Mean entropy loss: 36.7299
                       Mean reward: 925.58
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 184.9329
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.01s
                      Time elapsed: 00:45:33
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 49668 steps/s (collection: 1.876s, learning 0.103s)
             Mean action noise std: 2.00
          Mean value_function loss: 50.3110
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 36.7311
                       Mean reward: 930.78
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 1.0396
     Episode_Reward/lifting_object: 184.7002
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 1.98s
                      Time elapsed: 00:45:35
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 49137 steps/s (collection: 1.892s, learning 0.109s)
             Mean action noise std: 2.00
          Mean value_function loss: 53.5265
               Mean surrogate loss: 0.0165
                 Mean entropy loss: 36.7325
                       Mean reward: 921.76
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 184.9055
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.00s
                      Time elapsed: 00:45:37
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 47302 steps/s (collection: 1.943s, learning 0.135s)
             Mean action noise std: 2.00
          Mean value_function loss: 47.8018
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.7351
                       Mean reward: 945.61
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 1.0498
     Episode_Reward/lifting_object: 186.8324
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.08s
                      Time elapsed: 00:45:39
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 47023 steps/s (collection: 1.958s, learning 0.133s)
             Mean action noise std: 2.00
          Mean value_function loss: 55.6581
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 36.7420
                       Mean reward: 906.21
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 184.5115
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.09s
                      Time elapsed: 00:45:41
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 48246 steps/s (collection: 1.923s, learning 0.115s)
             Mean action noise std: 2.00
          Mean value_function loss: 57.7070
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.7459
                       Mean reward: 947.79
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 184.2437
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.04s
                      Time elapsed: 00:45:43
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 44926 steps/s (collection: 2.023s, learning 0.165s)
             Mean action noise std: 2.00
          Mean value_function loss: 85.1489
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 36.7525
                       Mean reward: 940.79
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 186.2234
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.19s
                      Time elapsed: 00:45:45
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 44944 steps/s (collection: 2.023s, learning 0.164s)
             Mean action noise std: 2.00
          Mean value_function loss: 54.2513
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.7580
                       Mean reward: 916.79
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 1.0375
     Episode_Reward/lifting_object: 183.6953
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.19s
                      Time elapsed: 00:45:48
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 47288 steps/s (collection: 1.921s, learning 0.158s)
             Mean action noise std: 2.00
          Mean value_function loss: 51.7618
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 36.7642
                       Mean reward: 920.66
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 184.6117
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.08s
                      Time elapsed: 00:45:50
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 46578 steps/s (collection: 1.989s, learning 0.121s)
             Mean action noise std: 2.00
          Mean value_function loss: 59.5123
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.7681
                       Mean reward: 924.37
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 185.1788
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.11s
                      Time elapsed: 00:45:52
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 49339 steps/s (collection: 1.897s, learning 0.095s)
             Mean action noise std: 2.00
          Mean value_function loss: 53.8086
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 36.7722
                       Mean reward: 917.03
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 185.0742
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 1.99s
                      Time elapsed: 00:45:54
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 48119 steps/s (collection: 1.914s, learning 0.129s)
             Mean action noise std: 2.00
          Mean value_function loss: 59.8200
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.7778
                       Mean reward: 923.88
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 186.0252
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.04s
                      Time elapsed: 00:45:56
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 48372 steps/s (collection: 1.900s, learning 0.133s)
             Mean action noise std: 2.00
          Mean value_function loss: 80.0703
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 36.7897
                       Mean reward: 904.66
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.0276
     Episode_Reward/lifting_object: 181.7623
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.03s
                      Time elapsed: 00:45:58
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 48239 steps/s (collection: 1.935s, learning 0.103s)
             Mean action noise std: 2.01
          Mean value_function loss: 59.9249
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.8027
                       Mean reward: 929.71
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 186.3361
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.04s
                      Time elapsed: 00:46:00
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 48642 steps/s (collection: 1.912s, learning 0.109s)
             Mean action noise std: 2.01
          Mean value_function loss: 91.6419
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 36.8115
                       Mean reward: 902.84
               Mean episode length: 238.66
    Episode_Reward/reaching_object: 1.0248
     Episode_Reward/lifting_object: 181.2467
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.02s
                      Time elapsed: 00:46:02
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 49128 steps/s (collection: 1.864s, learning 0.137s)
             Mean action noise std: 2.01
          Mean value_function loss: 79.3583
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.8152
                       Mean reward: 901.82
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 1.0355
     Episode_Reward/lifting_object: 182.8317
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.00s
                      Time elapsed: 00:46:04
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 48444 steps/s (collection: 1.908s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 92.0490
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.8177
                       Mean reward: 926.67
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 1.0276
     Episode_Reward/lifting_object: 181.4436
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.03s
                      Time elapsed: 00:46:06
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 45505 steps/s (collection: 1.967s, learning 0.193s)
             Mean action noise std: 2.01
          Mean value_function loss: 75.3243
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 36.8232
                       Mean reward: 905.78
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 1.0426
     Episode_Reward/lifting_object: 184.5551
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.16s
                      Time elapsed: 00:46:08
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 49272 steps/s (collection: 1.897s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 63.2919
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 36.8249
                       Mean reward: 941.57
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 185.9655
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.00s
                      Time elapsed: 00:46:10
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 48347 steps/s (collection: 1.927s, learning 0.106s)
             Mean action noise std: 2.01
          Mean value_function loss: 67.6892
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 36.8257
                       Mean reward: 929.32
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 1.0299
     Episode_Reward/lifting_object: 181.8778
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.03s
                      Time elapsed: 00:46:12
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 47234 steps/s (collection: 1.977s, learning 0.105s)
             Mean action noise std: 2.01
          Mean value_function loss: 62.2539
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 36.8265
                       Mean reward: 895.66
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 1.0339
     Episode_Reward/lifting_object: 182.7299
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.08s
                      Time elapsed: 00:46:14
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 49758 steps/s (collection: 1.884s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 71.1824
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 36.8274
                       Mean reward: 898.70
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 1.0286
     Episode_Reward/lifting_object: 181.4509
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 1.98s
                      Time elapsed: 00:46:16
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 45562 steps/s (collection: 2.022s, learning 0.135s)
             Mean action noise std: 2.01
          Mean value_function loss: 62.2799
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 36.8290
                       Mean reward: 907.16
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 1.0288
     Episode_Reward/lifting_object: 181.6136
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.16s
                      Time elapsed: 00:46:18
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 49812 steps/s (collection: 1.870s, learning 0.103s)
             Mean action noise std: 2.01
          Mean value_function loss: 52.7535
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 36.8308
                       Mean reward: 936.62
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 1.0485
     Episode_Reward/lifting_object: 185.5927
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 1.97s
                      Time elapsed: 00:46:20
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 49699 steps/s (collection: 1.870s, learning 0.108s)
             Mean action noise std: 2.01
          Mean value_function loss: 57.8216
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.8316
                       Mean reward: 937.35
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 184.9438
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 1.98s
                      Time elapsed: 00:46:22
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 47568 steps/s (collection: 1.941s, learning 0.125s)
             Mean action noise std: 2.01
          Mean value_function loss: 82.0373
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.8328
                       Mean reward: 905.95
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 1.0177
     Episode_Reward/lifting_object: 179.2499
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.07s
                      Time elapsed: 00:46:24
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 47648 steps/s (collection: 1.944s, learning 0.119s)
             Mean action noise std: 2.01
          Mean value_function loss: 64.3311
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.8365
                       Mean reward: 928.74
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 1.0391
     Episode_Reward/lifting_object: 182.9949
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.06s
                      Time elapsed: 00:46:26
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 45776 steps/s (collection: 1.974s, learning 0.173s)
             Mean action noise std: 2.01
          Mean value_function loss: 62.5468
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 36.8376
                       Mean reward: 933.42
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 1.0451
     Episode_Reward/lifting_object: 183.8277
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.15s
                      Time elapsed: 00:46:29
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 47690 steps/s (collection: 1.904s, learning 0.158s)
             Mean action noise std: 2.01
          Mean value_function loss: 43.0837
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 36.8383
                       Mean reward: 924.37
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 1.0485
     Episode_Reward/lifting_object: 184.1631
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.06s
                      Time elapsed: 00:46:31
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 46252 steps/s (collection: 1.869s, learning 0.256s)
             Mean action noise std: 2.01
          Mean value_function loss: 46.2885
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.8397
                       Mean reward: 953.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0478
     Episode_Reward/lifting_object: 184.1790
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.13s
                      Time elapsed: 00:46:33
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 43530 steps/s (collection: 2.138s, learning 0.120s)
             Mean action noise std: 2.01
          Mean value_function loss: 54.1553
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 36.8410
                       Mean reward: 920.69
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 1.0459
     Episode_Reward/lifting_object: 183.4747
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.26s
                      Time elapsed: 00:46:35
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 49045 steps/s (collection: 1.894s, learning 0.110s)
             Mean action noise std: 2.01
          Mean value_function loss: 45.8654
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 36.8434
                       Mean reward: 940.63
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 1.0511
     Episode_Reward/lifting_object: 184.7159
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.00s
                      Time elapsed: 00:46:37
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 50209 steps/s (collection: 1.836s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 50.4114
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.8501
                       Mean reward: 923.38
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 1.0511
     Episode_Reward/lifting_object: 184.6696
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 1.96s
                      Time elapsed: 00:46:39
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 49175 steps/s (collection: 1.887s, learning 0.112s)
             Mean action noise std: 2.01
          Mean value_function loss: 40.2470
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 36.8587
                       Mean reward: 925.53
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 186.1149
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.00s
                      Time elapsed: 00:46:41
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 48298 steps/s (collection: 1.924s, learning 0.111s)
             Mean action noise std: 2.01
          Mean value_function loss: 48.7231
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.8642
                       Mean reward: 927.35
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.0488
     Episode_Reward/lifting_object: 184.7647
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.04s
                      Time elapsed: 00:46:43
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 47633 steps/s (collection: 1.938s, learning 0.126s)
             Mean action noise std: 2.01
          Mean value_function loss: 47.8521
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 36.8733
                       Mean reward: 941.07
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 1.0536
     Episode_Reward/lifting_object: 185.8397
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.06s
                      Time elapsed: 00:46:45
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19597 steps/s (collection: 4.887s, learning 0.130s)
             Mean action noise std: 2.02
          Mean value_function loss: 68.5833
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 36.8838
                       Mean reward: 932.24
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 1.0525
     Episode_Reward/lifting_object: 185.9695
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.02s
                      Time elapsed: 00:46:50
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14190 steps/s (collection: 6.796s, learning 0.131s)
             Mean action noise std: 2.02
          Mean value_function loss: 53.8662
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 36.8874
                       Mean reward: 928.30
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 1.0526
     Episode_Reward/lifting_object: 186.0492
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.93s
                      Time elapsed: 00:46:57
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14259 steps/s (collection: 6.786s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 69.1722
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 36.8890
                       Mean reward: 920.10
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 1.0321
     Episode_Reward/lifting_object: 182.5405
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.89s
                      Time elapsed: 00:47:04
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14254 steps/s (collection: 6.750s, learning 0.146s)
             Mean action noise std: 2.02
          Mean value_function loss: 60.2336
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 36.8908
                       Mean reward: 917.77
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 184.8840
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.90s
                      Time elapsed: 00:47:11
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14275 steps/s (collection: 6.770s, learning 0.116s)
             Mean action noise std: 2.02
          Mean value_function loss: 72.7035
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.8933
                       Mean reward: 923.15
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 1.0334
     Episode_Reward/lifting_object: 182.7238
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.89s
                      Time elapsed: 00:47:18
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14560 steps/s (collection: 6.644s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 52.2220
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 36.9015
                       Mean reward: 927.98
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 1.0393
     Episode_Reward/lifting_object: 183.8314
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.75s
                      Time elapsed: 00:47:24
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14468 steps/s (collection: 6.646s, learning 0.148s)
             Mean action noise std: 2.02
          Mean value_function loss: 56.3897
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.9137
                       Mean reward: 921.19
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 184.3332
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.79s
                      Time elapsed: 00:47:31
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14476 steps/s (collection: 6.682s, learning 0.109s)
             Mean action noise std: 2.02
          Mean value_function loss: 49.3954
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 36.9255
                       Mean reward: 946.04
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 1.0448
     Episode_Reward/lifting_object: 184.4246
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.79s
                      Time elapsed: 00:47:38
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12876 steps/s (collection: 7.501s, learning 0.134s)
             Mean action noise std: 2.02
          Mean value_function loss: 62.0543
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.9286
                       Mean reward: 946.95
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 184.9464
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.63s
                      Time elapsed: 00:47:46
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 46271 steps/s (collection: 2.005s, learning 0.120s)
             Mean action noise std: 2.02
          Mean value_function loss: 53.2285
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.9317
                       Mean reward: 899.20
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 1.0355
     Episode_Reward/lifting_object: 182.6249
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.12s
                      Time elapsed: 00:47:48
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 47340 steps/s (collection: 1.881s, learning 0.196s)
             Mean action noise std: 2.02
          Mean value_function loss: 56.6034
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 36.9374
                       Mean reward: 917.04
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 1.0453
     Episode_Reward/lifting_object: 184.3133
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.08s
                      Time elapsed: 00:47:50
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 50425 steps/s (collection: 1.833s, learning 0.116s)
             Mean action noise std: 2.02
          Mean value_function loss: 45.0467
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 36.9435
                       Mean reward: 923.28
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 183.9932
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 1.95s
                      Time elapsed: 00:47:52
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 52093 steps/s (collection: 1.794s, learning 0.093s)
             Mean action noise std: 2.02
          Mean value_function loss: 48.6390
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 36.9470
                       Mean reward: 924.00
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 1.0487
     Episode_Reward/lifting_object: 184.9482
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 1.89s
                      Time elapsed: 00:47:54
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 52118 steps/s (collection: 1.788s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 56.0022
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 36.9484
                       Mean reward: 896.78
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 1.0305
     Episode_Reward/lifting_object: 181.5703
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 1.89s
                      Time elapsed: 00:47:56
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 52786 steps/s (collection: 1.772s, learning 0.090s)
             Mean action noise std: 2.02
          Mean value_function loss: 53.6018
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.9514
                       Mean reward: 939.96
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 1.0323
     Episode_Reward/lifting_object: 181.8630
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.86s
                      Time elapsed: 00:47:57
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 51606 steps/s (collection: 1.816s, learning 0.089s)
             Mean action noise std: 2.02
          Mean value_function loss: 71.4199
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 36.9563
                       Mean reward: 867.26
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 1.0293
     Episode_Reward/lifting_object: 181.4024
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.90s
                      Time elapsed: 00:47:59
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 49628 steps/s (collection: 1.888s, learning 0.093s)
             Mean action noise std: 2.02
          Mean value_function loss: 68.0643
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.9604
                       Mean reward: 884.66
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.0180
     Episode_Reward/lifting_object: 179.4513
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.98s
                      Time elapsed: 00:48:01
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 52194 steps/s (collection: 1.790s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 67.1561
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.9688
                       Mean reward: 903.92
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.0414
     Episode_Reward/lifting_object: 183.9478
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.88s
                      Time elapsed: 00:48:03
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 51016 steps/s (collection: 1.798s, learning 0.129s)
             Mean action noise std: 2.03
          Mean value_function loss: 56.5395
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.9757
                       Mean reward: 934.07
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.0366
     Episode_Reward/lifting_object: 183.2198
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 1.93s
                      Time elapsed: 00:48:05
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 52982 steps/s (collection: 1.763s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 68.6396
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 36.9825
                       Mean reward: 917.50
               Mean episode length: 241.45
    Episode_Reward/reaching_object: 1.0239
     Episode_Reward/lifting_object: 181.2993
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.86s
                      Time elapsed: 00:48:07
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 51549 steps/s (collection: 1.821s, learning 0.086s)
             Mean action noise std: 2.03
          Mean value_function loss: 56.2910
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.9931
                       Mean reward: 919.42
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 1.0255
     Episode_Reward/lifting_object: 181.3901
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.91s
                      Time elapsed: 00:48:09
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 49635 steps/s (collection: 1.893s, learning 0.088s)
             Mean action noise std: 2.03
          Mean value_function loss: 47.1929
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.0037
                       Mean reward: 941.13
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 186.0060
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.98s
                      Time elapsed: 00:48:11
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 51230 steps/s (collection: 1.827s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 60.8509
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.0103
                       Mean reward: 927.37
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 1.0368
     Episode_Reward/lifting_object: 184.2348
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.92s
                      Time elapsed: 00:48:13
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 53289 steps/s (collection: 1.755s, learning 0.090s)
             Mean action noise std: 2.03
          Mean value_function loss: 65.9922
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0131
                       Mean reward: 922.17
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.0243
     Episode_Reward/lifting_object: 182.0005
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.84s
                      Time elapsed: 00:48:15
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 51559 steps/s (collection: 1.783s, learning 0.124s)
             Mean action noise std: 2.03
          Mean value_function loss: 52.1568
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.0231
                       Mean reward: 953.30
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 1.0418
     Episode_Reward/lifting_object: 185.1089
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.91s
                      Time elapsed: 00:48:17
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 52798 steps/s (collection: 1.759s, learning 0.103s)
             Mean action noise std: 2.03
          Mean value_function loss: 64.9569
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.0355
                       Mean reward: 911.82
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 1.0375
     Episode_Reward/lifting_object: 184.1436
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.86s
                      Time elapsed: 00:48:18
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 48700 steps/s (collection: 1.816s, learning 0.203s)
             Mean action noise std: 2.04
          Mean value_function loss: 63.0242
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.0537
                       Mean reward: 928.37
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 1.0463
     Episode_Reward/lifting_object: 186.2786
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.02s
                      Time elapsed: 00:48:20
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 49526 steps/s (collection: 1.843s, learning 0.142s)
             Mean action noise std: 2.04
          Mean value_function loss: 52.1558
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 37.0685
                       Mean reward: 939.51
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 1.0288
     Episode_Reward/lifting_object: 182.8765
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.98s
                      Time elapsed: 00:48:22
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 50623 steps/s (collection: 1.774s, learning 0.168s)
             Mean action noise std: 2.04
          Mean value_function loss: 60.0950
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.0803
                       Mean reward: 936.61
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 184.6986
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.94s
                      Time elapsed: 00:48:24
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 46244 steps/s (collection: 1.939s, learning 0.187s)
             Mean action noise std: 2.04
          Mean value_function loss: 45.9686
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0916
                       Mean reward: 936.39
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 1.0370
     Episode_Reward/lifting_object: 184.2867
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.13s
                      Time elapsed: 00:48:26
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 45818 steps/s (collection: 1.985s, learning 0.160s)
             Mean action noise std: 2.04
          Mean value_function loss: 72.9286
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.1087
                       Mean reward: 928.75
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 1.0230
     Episode_Reward/lifting_object: 181.6706
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.15s
                      Time elapsed: 00:48:29
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 50428 steps/s (collection: 1.768s, learning 0.181s)
             Mean action noise std: 2.04
          Mean value_function loss: 36.2027
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.1372
                       Mean reward: 940.16
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 186.7081
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.95s
                      Time elapsed: 00:48:31
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 51681 steps/s (collection: 1.791s, learning 0.111s)
             Mean action noise std: 2.05
          Mean value_function loss: 47.2735
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1510
                       Mean reward: 946.73
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 1.0492
     Episode_Reward/lifting_object: 186.3234
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.90s
                      Time elapsed: 00:48:32
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 51375 steps/s (collection: 1.793s, learning 0.120s)
             Mean action noise std: 2.05
          Mean value_function loss: 68.8666
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.1571
                       Mean reward: 928.31
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 185.6904
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.91s
                      Time elapsed: 00:48:34
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 48319 steps/s (collection: 1.901s, learning 0.133s)
             Mean action noise std: 2.05
          Mean value_function loss: 56.6674
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.1702
                       Mean reward: 933.10
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 1.0380
     Episode_Reward/lifting_object: 184.1623
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.03s
                      Time elapsed: 00:48:36
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 51593 steps/s (collection: 1.777s, learning 0.129s)
             Mean action noise std: 2.05
          Mean value_function loss: 57.8656
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.1777
                       Mean reward: 930.75
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 1.0481
     Episode_Reward/lifting_object: 186.2002
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 1.91s
                      Time elapsed: 00:48:38
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 52281 steps/s (collection: 1.755s, learning 0.126s)
             Mean action noise std: 2.05
          Mean value_function loss: 69.2844
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.1856
                       Mean reward: 922.69
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 1.0380
     Episode_Reward/lifting_object: 184.2041
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.88s
                      Time elapsed: 00:48:40
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 48891 steps/s (collection: 1.905s, learning 0.106s)
             Mean action noise std: 2.05
          Mean value_function loss: 73.4551
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.1960
                       Mean reward: 923.64
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 1.0319
     Episode_Reward/lifting_object: 183.3493
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.01s
                      Time elapsed: 00:48:42
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 51743 steps/s (collection: 1.790s, learning 0.110s)
             Mean action noise std: 2.05
          Mean value_function loss: 75.8774
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.2123
                       Mean reward: 907.72
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 1.0228
     Episode_Reward/lifting_object: 181.3672
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.90s
                      Time elapsed: 00:48:44
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 51150 steps/s (collection: 1.813s, learning 0.109s)
             Mean action noise std: 2.05
          Mean value_function loss: 46.3764
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.2306
                       Mean reward: 932.13
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 1.0447
     Episode_Reward/lifting_object: 185.5620
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 1.92s
                      Time elapsed: 00:48:46
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 51890 steps/s (collection: 1.808s, learning 0.087s)
             Mean action noise std: 2.06
          Mean value_function loss: 52.3193
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.2401
                       Mean reward: 916.25
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 185.8759
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.89s
                      Time elapsed: 00:48:48
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 49451 steps/s (collection: 1.878s, learning 0.110s)
             Mean action noise std: 2.06
          Mean value_function loss: 43.1016
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.2482
                       Mean reward: 950.09
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 1.0523
     Episode_Reward/lifting_object: 187.1618
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 1.99s
                      Time elapsed: 00:48:50
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 52673 steps/s (collection: 1.772s, learning 0.094s)
             Mean action noise std: 2.06
          Mean value_function loss: 35.4749
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.2585
                       Mean reward: 931.65
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 1.0480
     Episode_Reward/lifting_object: 185.9697
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.87s
                      Time elapsed: 00:48:52
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 48071 steps/s (collection: 1.941s, learning 0.104s)
             Mean action noise std: 2.06
          Mean value_function loss: 38.9499
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.2656
                       Mean reward: 932.64
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 1.0478
     Episode_Reward/lifting_object: 185.8857
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.04s
                      Time elapsed: 00:48:54
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 49876 steps/s (collection: 1.831s, learning 0.140s)
             Mean action noise std: 2.06
          Mean value_function loss: 33.7878
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.2736
                       Mean reward: 939.50
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 1.0564
     Episode_Reward/lifting_object: 187.4219
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.97s
                      Time elapsed: 00:48:56
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 48983 steps/s (collection: 1.908s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 44.4050
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.2837
                       Mean reward: 942.51
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 1.0373
     Episode_Reward/lifting_object: 183.6567
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.01s
                      Time elapsed: 00:48:58
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 50820 steps/s (collection: 1.831s, learning 0.103s)
             Mean action noise std: 2.06
          Mean value_function loss: 46.5856
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.2937
                       Mean reward: 926.81
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 184.9577
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 1.93s
                      Time elapsed: 00:49:00
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 50467 steps/s (collection: 1.858s, learning 0.090s)
             Mean action noise std: 2.06
          Mean value_function loss: 38.5025
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.3019
                       Mean reward: 945.01
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 1.0451
     Episode_Reward/lifting_object: 185.1401
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.95s
                      Time elapsed: 00:49:02
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 48883 steps/s (collection: 1.919s, learning 0.092s)
             Mean action noise std: 2.06
          Mean value_function loss: 43.9470
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.3150
                       Mean reward: 917.95
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 1.0426
     Episode_Reward/lifting_object: 184.4944
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.01s
                      Time elapsed: 00:49:04
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 51392 steps/s (collection: 1.818s, learning 0.095s)
             Mean action noise std: 2.07
          Mean value_function loss: 42.2309
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.3308
                       Mean reward: 936.00
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 1.0484
     Episode_Reward/lifting_object: 185.5601
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.91s
                      Time elapsed: 00:49:06
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 52644 steps/s (collection: 1.781s, learning 0.086s)
             Mean action noise std: 2.07
          Mean value_function loss: 33.8001
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.3461
                       Mean reward: 927.84
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 185.3444
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.87s
                      Time elapsed: 00:49:07
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 52005 steps/s (collection: 1.808s, learning 0.083s)
             Mean action noise std: 2.07
          Mean value_function loss: 50.0183
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 37.3727
                       Mean reward: 940.66
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: 185.1505
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 1.89s
                      Time elapsed: 00:49:09
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 49216 steps/s (collection: 1.820s, learning 0.177s)
             Mean action noise std: 2.07
          Mean value_function loss: 37.3685
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.3808
                       Mean reward: 938.66
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 185.8788
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.00s
                      Time elapsed: 00:49:11
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 48287 steps/s (collection: 1.921s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 37.8089
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.3893
                       Mean reward: 943.73
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 185.2770
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.04s
                      Time elapsed: 00:49:13
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 51905 steps/s (collection: 1.802s, learning 0.092s)
             Mean action noise std: 2.07
          Mean value_function loss: 54.3158
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.3975
                       Mean reward: 929.88
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 184.9438
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.89s
                      Time elapsed: 00:49:15
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 46194 steps/s (collection: 1.932s, learning 0.197s)
             Mean action noise std: 2.07
          Mean value_function loss: 43.3238
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.4005
                       Mean reward: 932.32
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 1.0377
     Episode_Reward/lifting_object: 183.4260
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.13s
                      Time elapsed: 00:49:17
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 50826 steps/s (collection: 1.815s, learning 0.120s)
             Mean action noise std: 2.07
          Mean value_function loss: 38.3637
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 37.4031
                       Mean reward: 938.58
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 1.0481
     Episode_Reward/lifting_object: 185.5196
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.93s
                      Time elapsed: 00:49:19
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 48654 steps/s (collection: 1.845s, learning 0.175s)
             Mean action noise std: 2.07
          Mean value_function loss: 47.8780
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.4084
                       Mean reward: 928.41
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 1.0444
     Episode_Reward/lifting_object: 184.8109
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.02s
                      Time elapsed: 00:49:21
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 47132 steps/s (collection: 1.977s, learning 0.109s)
             Mean action noise std: 2.08
          Mean value_function loss: 50.0819
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.4208
                       Mean reward: 914.01
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 1.0349
     Episode_Reward/lifting_object: 182.8620
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.09s
                      Time elapsed: 00:49:23
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 49290 steps/s (collection: 1.865s, learning 0.129s)
             Mean action noise std: 2.08
          Mean value_function loss: 31.3774
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.4329
                       Mean reward: 939.10
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 187.4049
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 1.99s
                      Time elapsed: 00:49:25
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 50337 steps/s (collection: 1.832s, learning 0.121s)
             Mean action noise std: 2.08
          Mean value_function loss: 44.5021
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.4444
                       Mean reward: 934.27
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 185.0750
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.95s
                      Time elapsed: 00:49:27
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 52077 steps/s (collection: 1.758s, learning 0.130s)
             Mean action noise std: 2.08
          Mean value_function loss: 48.9303
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.4579
                       Mean reward: 922.21
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 1.0367
     Episode_Reward/lifting_object: 183.5779
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.89s
                      Time elapsed: 00:49:29
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 51860 steps/s (collection: 1.801s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 32.7544
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.4727
                       Mean reward: 934.83
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 186.2739
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.90s
                      Time elapsed: 00:49:31
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 47160 steps/s (collection: 1.971s, learning 0.114s)
             Mean action noise std: 2.08
          Mean value_function loss: 41.8764
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.4922
                       Mean reward: 931.22
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 185.1581
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.08s
                      Time elapsed: 00:49:33
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 50635 steps/s (collection: 1.827s, learning 0.115s)
             Mean action noise std: 2.08
          Mean value_function loss: 57.3566
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.5049
                       Mean reward: 922.59
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 183.8813
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.94s
                      Time elapsed: 00:49:35
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 50136 steps/s (collection: 1.843s, learning 0.118s)
             Mean action noise std: 2.09
          Mean value_function loss: 38.2497
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.5161
                       Mean reward: 912.12
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 184.9241
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.96s
                      Time elapsed: 00:49:37
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 51074 steps/s (collection: 1.833s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 43.7885
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 37.5315
                       Mean reward: 943.60
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 1.0417
     Episode_Reward/lifting_object: 184.4120
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.92s
                      Time elapsed: 00:49:39
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 51096 steps/s (collection: 1.822s, learning 0.102s)
             Mean action noise std: 2.09
          Mean value_function loss: 39.9333
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.5427
                       Mean reward: 935.85
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 1.0543
     Episode_Reward/lifting_object: 186.6802
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.92s
                      Time elapsed: 00:49:41
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 44231 steps/s (collection: 2.072s, learning 0.151s)
             Mean action noise std: 2.09
          Mean value_function loss: 37.9118
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 37.5571
                       Mean reward: 943.53
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 1.0409
     Episode_Reward/lifting_object: 184.1040
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.22s
                      Time elapsed: 00:49:43
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 51913 steps/s (collection: 1.798s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 69.8320
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.5702
                       Mean reward: 897.43
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.0398
     Episode_Reward/lifting_object: 183.9314
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.89s
                      Time elapsed: 00:49:45
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 49954 steps/s (collection: 1.854s, learning 0.114s)
             Mean action noise std: 2.10
          Mean value_function loss: 93.4548
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 37.5963
                       Mean reward: 907.98
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 1.0382
     Episode_Reward/lifting_object: 183.4721
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 1.97s
                      Time elapsed: 00:49:47
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 48500 steps/s (collection: 1.877s, learning 0.150s)
             Mean action noise std: 2.10
          Mean value_function loss: 40.6773
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.6184
                       Mean reward: 913.94
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 184.6327
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.03s
                      Time elapsed: 00:49:49
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 47250 steps/s (collection: 1.873s, learning 0.208s)
             Mean action noise std: 2.10
          Mean value_function loss: 59.8673
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.6296
                       Mean reward: 900.09
               Mean episode length: 238.17
    Episode_Reward/reaching_object: 1.0229
     Episode_Reward/lifting_object: 180.9226
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.08s
                      Time elapsed: 00:49:51
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 50123 steps/s (collection: 1.851s, learning 0.110s)
             Mean action noise std: 2.10
          Mean value_function loss: 51.6280
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.6465
                       Mean reward: 909.53
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 1.0330
     Episode_Reward/lifting_object: 182.8941
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.96s
                      Time elapsed: 00:49:53
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 50500 steps/s (collection: 1.832s, learning 0.115s)
             Mean action noise std: 2.10
          Mean value_function loss: 38.3219
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.6580
                       Mean reward: 943.03
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 186.3077
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.95s
                      Time elapsed: 00:49:55
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 47960 steps/s (collection: 1.890s, learning 0.160s)
             Mean action noise std: 2.10
          Mean value_function loss: 44.3199
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.6741
                       Mean reward: 935.07
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 184.8473
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.05s
                      Time elapsed: 00:49:57
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 47185 steps/s (collection: 1.954s, learning 0.129s)
             Mean action noise std: 2.10
          Mean value_function loss: 42.6195
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.6809
                       Mean reward: 935.33
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 185.2660
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.08s
                      Time elapsed: 00:49:59
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 49202 steps/s (collection: 1.887s, learning 0.111s)
             Mean action noise std: 2.11
          Mean value_function loss: 51.4449
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.6915
                       Mean reward: 929.60
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 1.0334
     Episode_Reward/lifting_object: 182.6034
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.00s
                      Time elapsed: 00:50:01
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 49302 steps/s (collection: 1.874s, learning 0.120s)
             Mean action noise std: 2.11
          Mean value_function loss: 38.9824
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.7059
                       Mean reward: 943.27
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 1.0544
     Episode_Reward/lifting_object: 186.8064
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.99s
                      Time elapsed: 00:50:03
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 50873 steps/s (collection: 1.837s, learning 0.096s)
             Mean action noise std: 2.11
          Mean value_function loss: 58.3923
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.7210
                       Mean reward: 899.73
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 1.0236
     Episode_Reward/lifting_object: 180.7169
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.93s
                      Time elapsed: 00:50:05
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 51126 steps/s (collection: 1.825s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 50.9596
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 37.7346
                       Mean reward: 897.72
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.0373
     Episode_Reward/lifting_object: 182.8738
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 1.92s
                      Time elapsed: 00:50:07
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 50417 steps/s (collection: 1.843s, learning 0.107s)
             Mean action noise std: 2.11
          Mean value_function loss: 46.5308
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.7384
                       Mean reward: 935.63
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 184.3226
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 1.95s
                      Time elapsed: 00:50:09
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 50352 steps/s (collection: 1.844s, learning 0.109s)
             Mean action noise std: 2.11
          Mean value_function loss: 56.5709
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 37.7399
                       Mean reward: 906.35
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.0334
     Episode_Reward/lifting_object: 182.6985
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 1.95s
                      Time elapsed: 00:50:11
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 50540 steps/s (collection: 1.847s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 45.7578
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 37.7420
                       Mean reward: 897.97
               Mean episode length: 237.31
    Episode_Reward/reaching_object: 1.0451
     Episode_Reward/lifting_object: 184.7494
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 1.95s
                      Time elapsed: 00:50:13
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 50100 steps/s (collection: 1.851s, learning 0.111s)
             Mean action noise std: 2.11
          Mean value_function loss: 41.4952
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.7453
                       Mean reward: 936.14
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 1.0485
     Episode_Reward/lifting_object: 185.0841
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.96s
                      Time elapsed: 00:50:15
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 49658 steps/s (collection: 1.862s, learning 0.118s)
             Mean action noise std: 2.11
          Mean value_function loss: 50.9655
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.7510
                       Mean reward: 941.40
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 1.0426
     Episode_Reward/lifting_object: 184.2519
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 1.98s
                      Time elapsed: 00:50:17
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 50442 steps/s (collection: 1.845s, learning 0.103s)
             Mean action noise std: 2.11
          Mean value_function loss: 43.4504
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 37.7529
                       Mean reward: 927.71
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 1.0377
     Episode_Reward/lifting_object: 183.0320
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.95s
                      Time elapsed: 00:50:19
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 51938 steps/s (collection: 1.771s, learning 0.122s)
             Mean action noise std: 2.11
          Mean value_function loss: 56.8499
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.7570
                       Mean reward: 912.02
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 1.0267
     Episode_Reward/lifting_object: 180.8129
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.89s
                      Time elapsed: 00:50:21
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 49397 steps/s (collection: 1.846s, learning 0.145s)
             Mean action noise std: 2.11
          Mean value_function loss: 50.2700
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.7678
                       Mean reward: 922.39
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 1.0444
     Episode_Reward/lifting_object: 183.9064
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.99s
                      Time elapsed: 00:50:23
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 52751 steps/s (collection: 1.756s, learning 0.108s)
             Mean action noise std: 2.11
          Mean value_function loss: 46.6939
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 37.7782
                       Mean reward: 923.38
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 183.2192
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.86s
                      Time elapsed: 00:50:25
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 52125 steps/s (collection: 1.761s, learning 0.124s)
             Mean action noise std: 2.11
          Mean value_function loss: 36.6221
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.7836
                       Mean reward: 937.47
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 1.0538
     Episode_Reward/lifting_object: 185.8198
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.89s
                      Time elapsed: 00:50:26
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 52859 steps/s (collection: 1.764s, learning 0.095s)
             Mean action noise std: 2.12
          Mean value_function loss: 53.5914
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.7921
                       Mean reward: 925.32
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 1.0332
     Episode_Reward/lifting_object: 181.6968
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.86s
                      Time elapsed: 00:50:28
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 50948 steps/s (collection: 1.814s, learning 0.115s)
             Mean action noise std: 2.12
          Mean value_function loss: 41.3593
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.8002
                       Mean reward: 889.17
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 182.9948
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 1.93s
                      Time elapsed: 00:50:30
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 51308 steps/s (collection: 1.826s, learning 0.090s)
             Mean action noise std: 2.12
          Mean value_function loss: 43.8561
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.8088
                       Mean reward: 892.85
               Mean episode length: 237.55
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 184.5934
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 1.92s
                      Time elapsed: 00:50:32
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 50482 steps/s (collection: 1.844s, learning 0.103s)
             Mean action noise std: 2.12
          Mean value_function loss: 37.9387
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.8187
                       Mean reward: 945.27
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 185.8066
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.95s
                      Time elapsed: 00:50:34
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 51438 steps/s (collection: 1.797s, learning 0.115s)
             Mean action noise std: 2.12
          Mean value_function loss: 50.4678
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.8276
                       Mean reward: 950.66
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 1.0540
     Episode_Reward/lifting_object: 185.6345
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.91s
                      Time elapsed: 00:50:36
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 51259 steps/s (collection: 1.813s, learning 0.105s)
             Mean action noise std: 2.12
          Mean value_function loss: 59.2770
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.8375
                       Mean reward: 920.06
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 184.2322
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.92s
                      Time elapsed: 00:50:38
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 50673 steps/s (collection: 1.827s, learning 0.113s)
             Mean action noise std: 2.12
          Mean value_function loss: 43.7085
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.8493
                       Mean reward: 913.02
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 1.0359
     Episode_Reward/lifting_object: 182.2629
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.94s
                      Time elapsed: 00:50:40
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 52127 steps/s (collection: 1.787s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 43.0285
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 37.8587
                       Mean reward: 929.31
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 1.0408
     Episode_Reward/lifting_object: 182.8997
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.89s
                      Time elapsed: 00:50:42
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 50866 steps/s (collection: 1.822s, learning 0.111s)
             Mean action noise std: 2.12
          Mean value_function loss: 33.8004
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.8653
                       Mean reward: 942.62
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 1.0472
     Episode_Reward/lifting_object: 184.3192
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 1.93s
                      Time elapsed: 00:50:44
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 50263 steps/s (collection: 1.847s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 38.4482
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 37.8775
                       Mean reward: 930.07
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 186.0634
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.96s
                      Time elapsed: 00:50:46
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 48194 steps/s (collection: 1.943s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 44.0096
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.8850
                       Mean reward: 948.41
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 1.0504
     Episode_Reward/lifting_object: 184.6973
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.04s
                      Time elapsed: 00:50:48
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 49311 steps/s (collection: 1.865s, learning 0.129s)
             Mean action noise std: 2.13
          Mean value_function loss: 33.5621
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.8900
                       Mean reward: 951.02
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 184.2476
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.99s
                      Time elapsed: 00:50:50
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 48122 steps/s (collection: 1.885s, learning 0.158s)
             Mean action noise std: 2.13
          Mean value_function loss: 43.7042
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.8923
                       Mean reward: 945.30
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 183.6747
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.04s
                      Time elapsed: 00:50:52
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 49671 steps/s (collection: 1.868s, learning 0.112s)
             Mean action noise std: 2.13
          Mean value_function loss: 48.4890
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.9014
                       Mean reward: 927.98
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.0456
     Episode_Reward/lifting_object: 183.8935
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 1.98s
                      Time elapsed: 00:50:54
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 50649 steps/s (collection: 1.826s, learning 0.115s)
             Mean action noise std: 2.13
          Mean value_function loss: 39.8057
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.9159
                       Mean reward: 931.18
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 1.0409
     Episode_Reward/lifting_object: 183.0077
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.94s
                      Time elapsed: 00:50:56
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 51145 steps/s (collection: 1.817s, learning 0.106s)
             Mean action noise std: 2.13
          Mean value_function loss: 60.2524
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9279
                       Mean reward: 935.54
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 184.2160
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.92s
                      Time elapsed: 00:50:58
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 50020 steps/s (collection: 1.831s, learning 0.135s)
             Mean action noise std: 2.13
          Mean value_function loss: 26.9453
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.9392
                       Mean reward: 939.00
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 1.0589
     Episode_Reward/lifting_object: 186.3265
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.97s
                      Time elapsed: 00:51:00
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 50444 steps/s (collection: 1.817s, learning 0.132s)
             Mean action noise std: 2.13
          Mean value_function loss: 58.4402
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 37.9549
                       Mean reward: 931.05
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 1.0342
     Episode_Reward/lifting_object: 181.4096
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.95s
                      Time elapsed: 00:51:02
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 50943 steps/s (collection: 1.823s, learning 0.106s)
             Mean action noise std: 2.14
          Mean value_function loss: 41.1579
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.9669
                       Mean reward: 929.96
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 1.0524
     Episode_Reward/lifting_object: 185.0091
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 1.93s
                      Time elapsed: 00:51:03
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 51441 steps/s (collection: 1.816s, learning 0.095s)
             Mean action noise std: 2.14
          Mean value_function loss: 42.7895
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.9787
                       Mean reward: 915.74
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 184.2902
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.91s
                      Time elapsed: 00:51:05
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 51061 steps/s (collection: 1.817s, learning 0.108s)
             Mean action noise std: 2.14
          Mean value_function loss: 35.5914
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.9870
                       Mean reward: 935.96
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 184.3700
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.93s
                      Time elapsed: 00:51:07
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 51707 steps/s (collection: 1.816s, learning 0.085s)
             Mean action noise std: 2.14
          Mean value_function loss: 42.0783
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.9969
                       Mean reward: 945.56
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 186.1526
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.90s
                      Time elapsed: 00:51:09
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 51125 steps/s (collection: 1.825s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 43.7053
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.0101
                       Mean reward: 933.27
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 185.5755
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.92s
                      Time elapsed: 00:51:11
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 50019 steps/s (collection: 1.865s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 37.4625
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.0207
                       Mean reward: 931.65
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 1.0524
     Episode_Reward/lifting_object: 184.8404
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 1.97s
                      Time elapsed: 00:51:13
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 50807 steps/s (collection: 1.847s, learning 0.088s)
             Mean action noise std: 2.14
          Mean value_function loss: 31.4520
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.0347
                       Mean reward: 930.33
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 1.0484
     Episode_Reward/lifting_object: 184.1067
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.93s
                      Time elapsed: 00:51:15
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 47954 steps/s (collection: 1.911s, learning 0.139s)
             Mean action noise std: 2.14
          Mean value_function loss: 41.0751
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.0427
                       Mean reward: 927.90
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 185.6872
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.05s
                      Time elapsed: 00:51:17
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 51221 steps/s (collection: 1.825s, learning 0.094s)
             Mean action noise std: 2.15
          Mean value_function loss: 34.9461
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.0556
                       Mean reward: 922.51
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 184.2075
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 1.92s
                      Time elapsed: 00:51:19
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 50039 steps/s (collection: 1.866s, learning 0.099s)
             Mean action noise std: 2.15
          Mean value_function loss: 38.1493
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.0679
                       Mean reward: 939.95
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 185.6651
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 1.96s
                      Time elapsed: 00:51:21
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 50860 steps/s (collection: 1.840s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 34.6892
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.0777
                       Mean reward: 927.74
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: 185.8361
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 1.93s
                      Time elapsed: 00:51:23
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 50870 steps/s (collection: 1.802s, learning 0.130s)
             Mean action noise std: 2.15
          Mean value_function loss: 34.7108
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.0905
                       Mean reward: 931.31
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 1.0449
     Episode_Reward/lifting_object: 183.8132
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 1.93s
                      Time elapsed: 00:51:25
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 49548 steps/s (collection: 1.840s, learning 0.144s)
             Mean action noise std: 2.15
          Mean value_function loss: 34.5459
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.1011
                       Mean reward: 933.15
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 1.0633
     Episode_Reward/lifting_object: 187.7128
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 1.98s
                      Time elapsed: 00:51:27
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 52174 steps/s (collection: 1.796s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 46.3474
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.1090
                       Mean reward: 924.82
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 183.7535
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 1.88s
                      Time elapsed: 00:51:29
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 50021 steps/s (collection: 1.855s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 36.0175
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 38.1154
                       Mean reward: 922.93
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 185.5871
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.97s
                      Time elapsed: 00:51:31
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 50633 steps/s (collection: 1.831s, learning 0.111s)
             Mean action noise std: 2.15
          Mean value_function loss: 24.7489
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.1268
                       Mean reward: 945.74
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 185.9664
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 1.94s
                      Time elapsed: 00:51:33
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 50816 steps/s (collection: 1.847s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 36.5885
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.1359
                       Mean reward: 921.87
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 1.0575
     Episode_Reward/lifting_object: 186.7153
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 1.93s
                      Time elapsed: 00:51:35
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 51986 steps/s (collection: 1.804s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 42.0124
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.1470
                       Mean reward: 941.03
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 1.0539
     Episode_Reward/lifting_object: 186.0924
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.89s
                      Time elapsed: 00:51:36
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 51968 steps/s (collection: 1.792s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 41.1709
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.1577
                       Mean reward: 943.09
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 184.9110
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 1.89s
                      Time elapsed: 00:51:38
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 50350 steps/s (collection: 1.863s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 50.7688
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.1681
                       Mean reward: 909.88
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 183.0832
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 1.95s
                      Time elapsed: 00:51:40
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 51100 steps/s (collection: 1.824s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 31.5678
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.1816
                       Mean reward: 930.90
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 1.0523
     Episode_Reward/lifting_object: 185.7490
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.92s
                      Time elapsed: 00:51:42
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 50623 steps/s (collection: 1.830s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 26.7652
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.1931
                       Mean reward: 934.61
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 1.0565
     Episode_Reward/lifting_object: 186.2555
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.94s
                      Time elapsed: 00:51:44
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 50765 steps/s (collection: 1.823s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 53.4734
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.2010
                       Mean reward: 924.76
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 184.6838
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 1.94s
                      Time elapsed: 00:51:46
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 50545 steps/s (collection: 1.817s, learning 0.128s)
             Mean action noise std: 2.16
          Mean value_function loss: 36.8959
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.2099
                       Mean reward: 940.59
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 184.6237
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 1.94s
                      Time elapsed: 00:51:48
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 49817 steps/s (collection: 1.856s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 33.7458
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.2209
                       Mean reward: 935.77
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 1.0307
     Episode_Reward/lifting_object: 181.8514
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.97s
                      Time elapsed: 00:51:50
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 50080 steps/s (collection: 1.825s, learning 0.138s)
             Mean action noise std: 2.16
          Mean value_function loss: 34.6479
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.2282
                       Mean reward: 924.95
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 186.9919
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 1.96s
                      Time elapsed: 00:51:52
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 51822 steps/s (collection: 1.778s, learning 0.119s)
             Mean action noise std: 2.17
          Mean value_function loss: 42.9687
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.2341
                       Mean reward: 935.76
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 185.4392
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 1.90s
                      Time elapsed: 00:51:54
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 48654 steps/s (collection: 1.890s, learning 0.130s)
             Mean action noise std: 2.17
          Mean value_function loss: 54.7279
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.2422
                       Mean reward: 937.54
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 1.0460
     Episode_Reward/lifting_object: 184.7083
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.02s
                      Time elapsed: 00:51:56
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 50271 steps/s (collection: 1.822s, learning 0.134s)
             Mean action noise std: 2.17
          Mean value_function loss: 47.2517
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.2474
                       Mean reward: 923.21
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 183.8263
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 1.96s
                      Time elapsed: 00:51:58
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 49813 steps/s (collection: 1.858s, learning 0.115s)
             Mean action noise std: 2.17
          Mean value_function loss: 51.0766
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.2537
                       Mean reward: 924.79
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 1.0417
     Episode_Reward/lifting_object: 183.9743
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 1.97s
                      Time elapsed: 00:52:00
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 48968 steps/s (collection: 1.885s, learning 0.123s)
             Mean action noise std: 2.17
          Mean value_function loss: 41.4866
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 38.2602
                       Mean reward: 895.59
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 182.2361
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.01s
                      Time elapsed: 00:52:02
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 49997 steps/s (collection: 1.848s, learning 0.119s)
             Mean action noise std: 2.17
          Mean value_function loss: 33.9933
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.2756
                       Mean reward: 930.09
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 1.0362
     Episode_Reward/lifting_object: 182.4770
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 1.97s
                      Time elapsed: 00:52:04
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 47954 steps/s (collection: 1.946s, learning 0.104s)
             Mean action noise std: 2.17
          Mean value_function loss: 34.6339
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.2852
                       Mean reward: 947.71
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 1.0544
     Episode_Reward/lifting_object: 185.9988
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.05s
                      Time elapsed: 00:52:06
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 49452 steps/s (collection: 1.883s, learning 0.105s)
             Mean action noise std: 2.17
          Mean value_function loss: 25.4462
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.2932
                       Mean reward: 944.77
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 1.0564
     Episode_Reward/lifting_object: 186.7057
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 1.99s
                      Time elapsed: 00:52:08
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 47771 steps/s (collection: 1.896s, learning 0.161s)
             Mean action noise std: 2.17
          Mean value_function loss: 39.5703
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.3014
                       Mean reward: 926.51
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 184.5713
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.06s
                      Time elapsed: 00:52:10
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 46413 steps/s (collection: 1.908s, learning 0.210s)
             Mean action noise std: 2.17
          Mean value_function loss: 35.2924
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.3081
                       Mean reward: 942.93
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 185.2253
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.12s
                      Time elapsed: 00:52:12
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 50585 steps/s (collection: 1.832s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 39.7973
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.3156
                       Mean reward: 914.61
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 1.0345
     Episode_Reward/lifting_object: 182.6376
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 1.94s
                      Time elapsed: 00:52:14
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 49460 steps/s (collection: 1.843s, learning 0.144s)
             Mean action noise std: 2.18
          Mean value_function loss: 39.9333
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.3267
                       Mean reward: 920.81
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 1.0472
     Episode_Reward/lifting_object: 184.6175
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.99s
                      Time elapsed: 00:52:16
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 48504 steps/s (collection: 1.857s, learning 0.170s)
             Mean action noise std: 2.18
          Mean value_function loss: 25.5029
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.3346
                       Mean reward: 940.75
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 187.0468
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.03s
                      Time elapsed: 00:52:18
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 49025 steps/s (collection: 1.871s, learning 0.135s)
             Mean action noise std: 2.18
          Mean value_function loss: 29.5466
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.3414
                       Mean reward: 929.01
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 1.0597
     Episode_Reward/lifting_object: 187.1891
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.01s
                      Time elapsed: 00:52:20
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 49947 steps/s (collection: 1.824s, learning 0.144s)
             Mean action noise std: 2.18
          Mean value_function loss: 30.6041
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.3477
                       Mean reward: 929.97
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 186.9437
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 1.97s
                      Time elapsed: 00:52:22
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 50911 steps/s (collection: 1.812s, learning 0.119s)
             Mean action noise std: 2.18
          Mean value_function loss: 33.0370
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.3492
                       Mean reward: 925.79
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 184.9603
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 1.93s
                      Time elapsed: 00:52:24
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 50242 steps/s (collection: 1.831s, learning 0.126s)
             Mean action noise std: 2.18
          Mean value_function loss: 23.4189
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.3506
                       Mean reward: 947.82
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 1.0579
     Episode_Reward/lifting_object: 186.7854
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.96s
                      Time elapsed: 00:52:26
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 47727 steps/s (collection: 1.938s, learning 0.122s)
             Mean action noise std: 2.18
          Mean value_function loss: 34.8798
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.3549
                       Mean reward: 936.09
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 1.0525
     Episode_Reward/lifting_object: 185.5786
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.06s
                      Time elapsed: 00:52:28
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 47672 steps/s (collection: 1.896s, learning 0.166s)
             Mean action noise std: 2.18
          Mean value_function loss: 36.3098
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.3638
                       Mean reward: 944.61
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 1.0612
     Episode_Reward/lifting_object: 187.2063
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.06s
                      Time elapsed: 00:52:30
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 48043 steps/s (collection: 1.930s, learning 0.116s)
             Mean action noise std: 2.18
          Mean value_function loss: 26.5330
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.3773
                       Mean reward: 940.97
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 1.0513
     Episode_Reward/lifting_object: 185.3810
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.05s
                      Time elapsed: 00:52:32
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 49271 steps/s (collection: 1.860s, learning 0.136s)
             Mean action noise std: 2.18
          Mean value_function loss: 26.6089
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.3902
                       Mean reward: 939.91
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 186.3294
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.00s
                      Time elapsed: 00:52:34
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 48143 steps/s (collection: 1.935s, learning 0.107s)
             Mean action noise std: 2.18
          Mean value_function loss: 25.4063
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.3969
                       Mean reward: 932.55
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 1.0623
     Episode_Reward/lifting_object: 187.5918
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.04s
                      Time elapsed: 00:52:36
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 50113 steps/s (collection: 1.837s, learning 0.125s)
             Mean action noise std: 2.19
          Mean value_function loss: 27.8966
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 38.4069
                       Mean reward: 914.91
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 186.0244
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 1.96s
                      Time elapsed: 00:52:38
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 47878 steps/s (collection: 1.875s, learning 0.179s)
             Mean action noise std: 2.19
          Mean value_function loss: 31.8317
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.4150
                       Mean reward: 933.87
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 1.0517
     Episode_Reward/lifting_object: 185.7977
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.05s
                      Time elapsed: 00:52:40
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 49281 steps/s (collection: 1.865s, learning 0.130s)
             Mean action noise std: 2.19
          Mean value_function loss: 30.6787
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.4185
                       Mean reward: 948.27
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: 186.2717
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.99s
                      Time elapsed: 00:52:42
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 50414 steps/s (collection: 1.839s, learning 0.111s)
             Mean action noise std: 2.19
          Mean value_function loss: 38.0426
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.4257
                       Mean reward: 908.58
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 1.0431
     Episode_Reward/lifting_object: 184.2226
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 1.95s
                      Time elapsed: 00:52:44
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 49136 steps/s (collection: 1.845s, learning 0.156s)
             Mean action noise std: 2.19
          Mean value_function loss: 24.1474
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.4376
                       Mean reward: 952.30
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 186.4467
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.00s
                      Time elapsed: 00:52:46
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 46893 steps/s (collection: 1.920s, learning 0.177s)
             Mean action noise std: 2.19
          Mean value_function loss: 27.4525
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.4482
                       Mean reward: 936.05
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 186.9238
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.10s
                      Time elapsed: 00:52:48
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 49177 steps/s (collection: 1.874s, learning 0.125s)
             Mean action noise std: 2.19
          Mean value_function loss: 28.7426
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.4551
                       Mean reward: 954.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 184.6732
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.00s
                      Time elapsed: 00:52:50
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 49356 steps/s (collection: 1.854s, learning 0.138s)
             Mean action noise std: 2.19
          Mean value_function loss: 27.6310
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.4631
                       Mean reward: 941.25
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 1.0575
     Episode_Reward/lifting_object: 186.4536
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 1.99s
                      Time elapsed: 00:52:52
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 50184 steps/s (collection: 1.857s, learning 0.102s)
             Mean action noise std: 2.19
          Mean value_function loss: 32.5221
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.4700
                       Mean reward: 948.01
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 1.0513
     Episode_Reward/lifting_object: 185.3057
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 1.96s
                      Time elapsed: 00:52:54
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 49992 steps/s (collection: 1.860s, learning 0.107s)
             Mean action noise std: 2.19
          Mean value_function loss: 28.5097
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.4782
                       Mean reward: 933.90
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 1.0632
     Episode_Reward/lifting_object: 187.3517
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.97s
                      Time elapsed: 00:52:56
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 48266 steps/s (collection: 1.875s, learning 0.162s)
             Mean action noise std: 2.20
          Mean value_function loss: 30.4540
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.4888
                       Mean reward: 933.16
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.0524
     Episode_Reward/lifting_object: 185.6134
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.04s
                      Time elapsed: 00:52:58
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 47443 steps/s (collection: 1.887s, learning 0.185s)
             Mean action noise std: 2.20
          Mean value_function loss: 36.7017
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.4992
                       Mean reward: 917.69
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: 185.5929
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.07s
                      Time elapsed: 00:53:00
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 50487 steps/s (collection: 1.846s, learning 0.101s)
             Mean action noise std: 2.20
          Mean value_function loss: 44.3939
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.5114
                       Mean reward: 934.77
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 184.7004
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 1.95s
                      Time elapsed: 00:53:02
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 48679 steps/s (collection: 1.855s, learning 0.165s)
             Mean action noise std: 2.20
          Mean value_function loss: 47.5781
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.5188
                       Mean reward: 908.81
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 1.0393
     Episode_Reward/lifting_object: 182.7574
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.02s
                      Time elapsed: 00:53:04
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 49334 steps/s (collection: 1.851s, learning 0.142s)
             Mean action noise std: 2.20
          Mean value_function loss: 30.2636
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.5280
                       Mean reward: 943.59
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 1.0554
     Episode_Reward/lifting_object: 185.7227
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 1.99s
                      Time elapsed: 00:53:06
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 49750 steps/s (collection: 1.862s, learning 0.114s)
             Mean action noise std: 2.20
          Mean value_function loss: 46.6628
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.5380
                       Mean reward: 924.77
               Mean episode length: 242.86
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 186.2251
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 1.98s
                      Time elapsed: 00:53:08
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 49513 steps/s (collection: 1.855s, learning 0.130s)
             Mean action noise std: 2.20
          Mean value_function loss: 37.3243
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.5483
                       Mean reward: 928.95
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 185.8814
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 1.99s
                      Time elapsed: 00:53:10
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 47853 steps/s (collection: 1.955s, learning 0.099s)
             Mean action noise std: 2.20
          Mean value_function loss: 33.5525
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.5564
                       Mean reward: 924.69
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 1.0573
     Episode_Reward/lifting_object: 185.9544
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.05s
                      Time elapsed: 00:53:12
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 48679 steps/s (collection: 1.880s, learning 0.139s)
             Mean action noise std: 2.20
          Mean value_function loss: 240.1757
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.5646
                       Mean reward: 928.72
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 185.6794
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.02s
                      Time elapsed: 00:53:14
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 46300 steps/s (collection: 1.961s, learning 0.162s)
             Mean action noise std: 2.21
          Mean value_function loss: 346.9162
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.5722
                       Mean reward: 930.79
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 183.8904
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.12s
                      Time elapsed: 00:53:16
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 46548 steps/s (collection: 1.952s, learning 0.160s)
             Mean action noise std: 2.21
          Mean value_function loss: 42.6716
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 38.5874
                       Mean reward: 925.53
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 1.0443
     Episode_Reward/lifting_object: 184.1339
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.11s
                      Time elapsed: 00:53:18
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 48121 steps/s (collection: 1.947s, learning 0.096s)
             Mean action noise std: 2.21
          Mean value_function loss: 44.7677
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.5909
                       Mean reward: 934.13
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 1.0472
     Episode_Reward/lifting_object: 184.4476
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.04s
                      Time elapsed: 00:53:20
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 48197 steps/s (collection: 1.896s, learning 0.144s)
             Mean action noise std: 2.21
          Mean value_function loss: 19.3241
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.5960
                       Mean reward: 951.94
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 1.0703
     Episode_Reward/lifting_object: 189.1839
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.04s
                      Time elapsed: 00:53:22
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 48243 steps/s (collection: 1.918s, learning 0.120s)
             Mean action noise std: 2.21
          Mean value_function loss: 29.7620
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.6008
                       Mean reward: 938.68
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 186.6367
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.04s
                      Time elapsed: 00:53:24
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 48194 steps/s (collection: 1.940s, learning 0.099s)
             Mean action noise std: 2.21
          Mean value_function loss: 42.9922
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 38.6036
                       Mean reward: 940.05
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 185.9640
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.04s
                      Time elapsed: 00:53:26
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 49121 steps/s (collection: 1.892s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 34.1864
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.6051
                       Mean reward: 946.76
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 186.2074
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.00s
                      Time elapsed: 00:53:28
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 46835 steps/s (collection: 1.970s, learning 0.129s)
             Mean action noise std: 2.21
          Mean value_function loss: 33.9444
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.6082
                       Mean reward: 932.23
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 186.4181
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.10s
                      Time elapsed: 00:53:31
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 48783 steps/s (collection: 1.918s, learning 0.097s)
             Mean action noise std: 2.21
          Mean value_function loss: 36.3975
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.6171
                       Mean reward: 953.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 186.2643
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.02s
                      Time elapsed: 00:53:33
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 48400 steps/s (collection: 1.922s, learning 0.109s)
             Mean action noise std: 2.21
          Mean value_function loss: 38.7699
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.6249
                       Mean reward: 927.54
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 183.9411
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.03s
                      Time elapsed: 00:53:35
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 48643 steps/s (collection: 1.912s, learning 0.109s)
             Mean action noise std: 2.21
          Mean value_function loss: 43.1866
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.6290
                       Mean reward: 934.30
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: 185.7361
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.02s
                      Time elapsed: 00:53:37
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 48256 steps/s (collection: 1.917s, learning 0.120s)
             Mean action noise std: 2.21
          Mean value_function loss: 49.6465
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.6352
                       Mean reward: 936.99
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 182.2982
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.04s
                      Time elapsed: 00:53:39
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 48091 steps/s (collection: 1.924s, learning 0.120s)
             Mean action noise std: 2.21
          Mean value_function loss: 43.4695
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.6444
                       Mean reward: 950.98
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 1.0569
     Episode_Reward/lifting_object: 186.7692
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.04s
                      Time elapsed: 00:53:41
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 48224 steps/s (collection: 1.914s, learning 0.124s)
             Mean action noise std: 2.21
          Mean value_function loss: 45.2908
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 38.6567
                       Mean reward: 927.62
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 184.6141
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.04s
                      Time elapsed: 00:53:43
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 48889 steps/s (collection: 1.918s, learning 0.093s)
             Mean action noise std: 2.22
          Mean value_function loss: 32.0767
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 38.6657
                       Mean reward: 931.79
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 1.0460
     Episode_Reward/lifting_object: 184.8205
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.01s
                      Time elapsed: 00:53:45
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 49959 steps/s (collection: 1.859s, learning 0.109s)
             Mean action noise std: 2.22
          Mean value_function loss: 42.9301
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.6736
                       Mean reward: 938.57
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 186.4537
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 1.97s
                      Time elapsed: 00:53:47
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 48567 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 2.22
          Mean value_function loss: 47.5266
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.6819
                       Mean reward: 905.58
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 1.0364
     Episode_Reward/lifting_object: 183.1639
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.02s
                      Time elapsed: 00:53:49
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 48234 steps/s (collection: 1.915s, learning 0.124s)
             Mean action noise std: 2.22
          Mean value_function loss: 24.9058
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.6936
                       Mean reward: 950.24
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 187.4779
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.04s
                      Time elapsed: 00:53:51
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 46834 steps/s (collection: 1.970s, learning 0.129s)
             Mean action noise std: 2.22
          Mean value_function loss: 40.1620
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.7008
                       Mean reward: 927.82
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 185.6789
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.10s
                      Time elapsed: 00:53:53
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 49244 steps/s (collection: 1.878s, learning 0.118s)
             Mean action noise std: 2.22
          Mean value_function loss: 36.4958
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.7115
                       Mean reward: 946.99
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 1.0543
     Episode_Reward/lifting_object: 186.5331
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.00s
                      Time elapsed: 00:53:55
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 49140 steps/s (collection: 1.904s, learning 0.097s)
             Mean action noise std: 2.22
          Mean value_function loss: 41.1603
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.7182
                       Mean reward: 939.28
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 1.0431
     Episode_Reward/lifting_object: 184.3377
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.00s
                      Time elapsed: 00:53:57
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 49249 steps/s (collection: 1.894s, learning 0.102s)
             Mean action noise std: 2.22
          Mean value_function loss: 27.6537
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.7264
                       Mean reward: 941.92
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 1.0633
     Episode_Reward/lifting_object: 187.8689
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.00s
                      Time elapsed: 00:53:59
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 47237 steps/s (collection: 1.971s, learning 0.111s)
             Mean action noise std: 2.22
          Mean value_function loss: 29.1635
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 38.7360
                       Mean reward: 939.82
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 1.0613
     Episode_Reward/lifting_object: 187.1297
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.08s
                      Time elapsed: 00:54:01
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 46217 steps/s (collection: 2.005s, learning 0.122s)
             Mean action noise std: 2.22
          Mean value_function loss: 28.6787
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.7386
                       Mean reward: 941.22
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 186.0954
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.13s
                      Time elapsed: 00:54:03
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 48480 steps/s (collection: 1.924s, learning 0.104s)
             Mean action noise std: 2.22
          Mean value_function loss: 30.3145
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.7404
                       Mean reward: 935.88
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 1.0588
     Episode_Reward/lifting_object: 186.3256
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.03s
                      Time elapsed: 00:54:05
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 45527 steps/s (collection: 1.976s, learning 0.184s)
             Mean action noise std: 2.23
          Mean value_function loss: 42.2621
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.7466
                       Mean reward: 930.65
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 1.0512
     Episode_Reward/lifting_object: 185.0815
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.16s
                      Time elapsed: 00:54:07
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 48205 steps/s (collection: 1.930s, learning 0.109s)
             Mean action noise std: 2.23
          Mean value_function loss: 37.0664
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.7582
                       Mean reward: 907.35
               Mean episode length: 240.06
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 184.8969
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.04s
                      Time elapsed: 00:54:09
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 49142 steps/s (collection: 1.863s, learning 0.138s)
             Mean action noise std: 2.23
          Mean value_function loss: 26.2305
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.7695
                       Mean reward: 924.43
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.0640
     Episode_Reward/lifting_object: 186.8049
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.00s
                      Time elapsed: 00:54:11
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 50147 steps/s (collection: 1.855s, learning 0.106s)
             Mean action noise std: 2.23
          Mean value_function loss: 52.5774
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 38.7806
                       Mean reward: 901.79
               Mean episode length: 237.92
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 185.5085
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 1.96s
                      Time elapsed: 00:54:13
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 49230 steps/s (collection: 1.881s, learning 0.116s)
             Mean action noise std: 2.23
          Mean value_function loss: 31.9161
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.7863
                       Mean reward: 938.82
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 1.0629
     Episode_Reward/lifting_object: 186.4440
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.00s
                      Time elapsed: 00:54:15
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 47050 steps/s (collection: 1.966s, learning 0.124s)
             Mean action noise std: 2.23
          Mean value_function loss: 28.6558
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.7917
                       Mean reward: 947.60
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 185.5604
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.09s
                      Time elapsed: 00:54:17
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 49144 steps/s (collection: 1.901s, learning 0.100s)
             Mean action noise std: 2.23
          Mean value_function loss: 48.0061
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.7996
                       Mean reward: 927.15
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.0524
     Episode_Reward/lifting_object: 184.4703
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.00s
                      Time elapsed: 00:54:19
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 48421 steps/s (collection: 1.868s, learning 0.162s)
             Mean action noise std: 2.23
          Mean value_function loss: 35.6564
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.8084
                       Mean reward: 919.64
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 186.0531
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.03s
                      Time elapsed: 00:54:21
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 42767 steps/s (collection: 2.180s, learning 0.119s)
             Mean action noise std: 2.23
          Mean value_function loss: 31.0967
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8221
                       Mean reward: 935.85
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 1.0641
     Episode_Reward/lifting_object: 186.4664
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.30s
                      Time elapsed: 00:54:24
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 44649 steps/s (collection: 2.034s, learning 0.168s)
             Mean action noise std: 2.24
          Mean value_function loss: 34.4246
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 38.8352
                       Mean reward: 943.53
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 1.0696
     Episode_Reward/lifting_object: 187.5078
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.20s
                      Time elapsed: 00:54:26
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 48461 steps/s (collection: 1.920s, learning 0.109s)
             Mean action noise std: 2.24
          Mean value_function loss: 39.3280
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.8447
                       Mean reward: 933.99
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 1.0471
     Episode_Reward/lifting_object: 183.3101
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.03s
                      Time elapsed: 00:54:28
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 49454 steps/s (collection: 1.872s, learning 0.115s)
             Mean action noise std: 2.24
          Mean value_function loss: 32.0359
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.8575
                       Mean reward: 938.45
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 1.0624
     Episode_Reward/lifting_object: 186.1463
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 1.99s
                      Time elapsed: 00:54:30
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 48657 steps/s (collection: 1.896s, learning 0.124s)
             Mean action noise std: 2.24
          Mean value_function loss: 28.6843
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.8704
                       Mean reward: 935.80
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 186.3608
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.02s
                      Time elapsed: 00:54:32
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 49719 steps/s (collection: 1.880s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 32.3079
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.8818
                       Mean reward: 926.15
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 184.8595
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 1.98s
                      Time elapsed: 00:54:34
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 49170 steps/s (collection: 1.879s, learning 0.120s)
             Mean action noise std: 2.24
          Mean value_function loss: 191.6310
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8924
                       Mean reward: 945.85
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 1.0605
     Episode_Reward/lifting_object: 186.2326
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.00s
                      Time elapsed: 00:54:36
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 49893 steps/s (collection: 1.847s, learning 0.123s)
             Mean action noise std: 2.24
          Mean value_function loss: 246.9169
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.9016
                       Mean reward: 933.95
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 185.1986
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 1.97s
                      Time elapsed: 00:54:38
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 49481 steps/s (collection: 1.866s, learning 0.121s)
             Mean action noise std: 2.24
          Mean value_function loss: 41.9459
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.9134
                       Mean reward: 936.52
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 183.2903
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 1.99s
                      Time elapsed: 00:54:40
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 47671 steps/s (collection: 1.956s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 23.5639
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.9191
                       Mean reward: 920.60
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 1.0656
     Episode_Reward/lifting_object: 186.7957
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.06s
                      Time elapsed: 00:54:42
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 49269 steps/s (collection: 1.896s, learning 0.099s)
             Mean action noise std: 2.25
          Mean value_function loss: 26.4190
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.9260
                       Mean reward: 918.42
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 1.0696
     Episode_Reward/lifting_object: 187.6509
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.00s
                      Time elapsed: 00:54:44
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 49947 steps/s (collection: 1.874s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 182.3869
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 38.9341
                       Mean reward: 934.12
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 1.0757
     Episode_Reward/lifting_object: 188.7088
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 1.97s
                      Time elapsed: 00:54:46
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 48534 steps/s (collection: 1.918s, learning 0.108s)
             Mean action noise std: 2.25
          Mean value_function loss: 69.0109
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.9391
                       Mean reward: 913.92
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 1.0569
     Episode_Reward/lifting_object: 185.0656
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.03s
                      Time elapsed: 00:54:48
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 49635 steps/s (collection: 1.862s, learning 0.118s)
             Mean action noise std: 2.25
          Mean value_function loss: 28.8724
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.9491
                       Mean reward: 946.28
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 187.0573
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 1.98s
                      Time elapsed: 00:54:50
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 48968 steps/s (collection: 1.894s, learning 0.114s)
             Mean action noise std: 2.25
          Mean value_function loss: 29.3416
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 38.9569
                       Mean reward: 948.01
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 1.0626
     Episode_Reward/lifting_object: 186.7985
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.01s
                      Time elapsed: 00:54:52
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 48970 steps/s (collection: 1.903s, learning 0.104s)
             Mean action noise std: 2.25
          Mean value_function loss: 40.9136
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 38.9618
                       Mean reward: 946.30
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 1.0620
     Episode_Reward/lifting_object: 186.1818
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.01s
                      Time elapsed: 00:54:54
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 49019 steps/s (collection: 1.908s, learning 0.098s)
             Mean action noise std: 2.25
          Mean value_function loss: 30.0961
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.9679
                       Mean reward: 940.58
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 185.5320
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.01s
                      Time elapsed: 00:54:56
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 47708 steps/s (collection: 1.943s, learning 0.118s)
             Mean action noise std: 2.25
          Mean value_function loss: 25.3018
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.9752
                       Mean reward: 951.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 186.4594
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.06s
                      Time elapsed: 00:54:58
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 48177 steps/s (collection: 1.919s, learning 0.121s)
             Mean action noise std: 2.25
          Mean value_function loss: 30.8789
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.9817
                       Mean reward: 945.48
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 1.0722
     Episode_Reward/lifting_object: 188.7945
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.04s
                      Time elapsed: 00:55:00
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 49102 steps/s (collection: 1.887s, learning 0.115s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.7779
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 38.9840
                       Mean reward: 925.86
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 185.7647
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.00s
                      Time elapsed: 00:55:02
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 48958 steps/s (collection: 1.899s, learning 0.109s)
             Mean action noise std: 2.25
          Mean value_function loss: 21.5548
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.9856
                       Mean reward: 947.50
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 1.0633
     Episode_Reward/lifting_object: 187.1573
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.01s
                      Time elapsed: 00:55:04
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 49164 steps/s (collection: 1.899s, learning 0.101s)
             Mean action noise std: 2.25
          Mean value_function loss: 38.7683
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 38.9880
                       Mean reward: 942.31
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 1.0497
     Episode_Reward/lifting_object: 184.7607
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.00s
                      Time elapsed: 00:55:06
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 49464 steps/s (collection: 1.884s, learning 0.103s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.7583
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.9942
                       Mean reward: 918.14
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 185.9732
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 1.99s
                      Time elapsed: 00:55:08
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 48724 steps/s (collection: 1.916s, learning 0.102s)
             Mean action noise std: 2.26
          Mean value_function loss: 18.0093
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.0063
                       Mean reward: 948.65
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 186.0957
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.02s
                      Time elapsed: 00:55:10
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 48729 steps/s (collection: 1.899s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 19.3841
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.0164
                       Mean reward: 954.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0704
     Episode_Reward/lifting_object: 188.3804
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.02s
                      Time elapsed: 00:55:12
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 48630 steps/s (collection: 1.920s, learning 0.101s)
             Mean action noise std: 2.26
          Mean value_function loss: 37.3787
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.0303
                       Mean reward: 925.61
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 185.5069
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.02s
                      Time elapsed: 00:55:14
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 49934 steps/s (collection: 1.872s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 39.2507
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.0433
                       Mean reward: 923.55
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 185.1509
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 1.97s
                      Time elapsed: 00:55:16
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 49420 steps/s (collection: 1.891s, learning 0.098s)
             Mean action noise std: 2.26
          Mean value_function loss: 29.8730
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.0544
                       Mean reward: 938.60
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 1.0640
     Episode_Reward/lifting_object: 187.4123
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 1.99s
                      Time elapsed: 00:55:18
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 49974 steps/s (collection: 1.861s, learning 0.106s)
             Mean action noise std: 2.26
          Mean value_function loss: 30.9893
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.0676
                       Mean reward: 949.86
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 185.7737
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 1.97s
                      Time elapsed: 00:55:20
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 48905 steps/s (collection: 1.883s, learning 0.128s)
             Mean action noise std: 2.26
          Mean value_function loss: 42.9127
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 39.0795
                       Mean reward: 951.71
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 183.8696
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.01s
                      Time elapsed: 00:55:22
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 49652 steps/s (collection: 1.885s, learning 0.095s)
             Mean action noise std: 2.26
          Mean value_function loss: 26.5533
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.0870
                       Mean reward: 938.47
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 1.0555
     Episode_Reward/lifting_object: 186.1422
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 1.98s
                      Time elapsed: 00:55:24
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 50070 steps/s (collection: 1.860s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 26.6704
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.0995
                       Mean reward: 914.81
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 1.0638
     Episode_Reward/lifting_object: 187.3724
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 1.96s
                      Time elapsed: 00:55:26
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 49496 steps/s (collection: 1.887s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 26.3634
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1108
                       Mean reward: 946.56
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 1.0567
     Episode_Reward/lifting_object: 186.5141
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 1.99s
                      Time elapsed: 00:55:28
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 45401 steps/s (collection: 1.994s, learning 0.171s)
             Mean action noise std: 2.27
          Mean value_function loss: 19.3257
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.1250
                       Mean reward: 942.20
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 1.0673
     Episode_Reward/lifting_object: 188.1311
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.17s
                      Time elapsed: 00:55:30
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 47634 steps/s (collection: 1.960s, learning 0.104s)
             Mean action noise std: 2.27
          Mean value_function loss: 23.0183
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.1363
                       Mean reward: 953.96
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 186.0049
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.06s
                      Time elapsed: 00:55:32
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 50221 steps/s (collection: 1.866s, learning 0.091s)
             Mean action noise std: 2.27
          Mean value_function loss: 28.2361
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1413
                       Mean reward: 923.64
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 185.8107
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 1.96s
                      Time elapsed: 00:55:34
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 49856 steps/s (collection: 1.872s, learning 0.100s)
             Mean action noise std: 2.27
          Mean value_function loss: 17.2823
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.1480
                       Mean reward: 946.92
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 1.0689
     Episode_Reward/lifting_object: 188.6140
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 1.97s
                      Time elapsed: 00:55:36
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 49150 steps/s (collection: 1.901s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 32.5046
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.1573
                       Mean reward: 948.04
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 186.7066
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.00s
                      Time elapsed: 00:55:38
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 49960 steps/s (collection: 1.873s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 28.4317
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.1636
                       Mean reward: 925.33
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 1.0566
     Episode_Reward/lifting_object: 186.3626
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 1.97s
                      Time elapsed: 00:55:40
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 49416 steps/s (collection: 1.878s, learning 0.111s)
             Mean action noise std: 2.28
          Mean value_function loss: 30.9227
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.1736
                       Mean reward: 948.42
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 1.0525
     Episode_Reward/lifting_object: 185.9046
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 1.99s
                      Time elapsed: 00:55:42
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 46494 steps/s (collection: 1.938s, learning 0.177s)
             Mean action noise std: 2.28
          Mean value_function loss: 33.3130
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.1817
                       Mean reward: 915.23
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 1.0597
     Episode_Reward/lifting_object: 187.0159
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.11s
                      Time elapsed: 00:55:44
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 49864 steps/s (collection: 1.857s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 28.3369
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.1907
                       Mean reward: 941.38
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 184.9550
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 1.97s
                      Time elapsed: 00:55:46
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 48231 steps/s (collection: 1.939s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 44.3408
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.2049
                       Mean reward: 925.15
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 185.5309
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.04s
                      Time elapsed: 00:55:48
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 43697 steps/s (collection: 2.068s, learning 0.182s)
             Mean action noise std: 2.28
          Mean value_function loss: 30.3600
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.2151
                       Mean reward: 923.93
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 185.1332
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.25s
                      Time elapsed: 00:55:50
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 48887 steps/s (collection: 1.865s, learning 0.146s)
             Mean action noise std: 2.28
          Mean value_function loss: 21.5739
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.2219
                       Mean reward: 927.42
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.0604
     Episode_Reward/lifting_object: 186.9637
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.01s
                      Time elapsed: 00:55:52
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 47797 steps/s (collection: 1.882s, learning 0.175s)
             Mean action noise std: 2.28
          Mean value_function loss: 50.1373
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.2277
                       Mean reward: 890.23
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 184.4655
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.06s
                      Time elapsed: 00:55:54
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 48507 steps/s (collection: 1.865s, learning 0.162s)
             Mean action noise std: 2.28
          Mean value_function loss: 55.3381
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.2342
                       Mean reward: 932.18
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 186.1851
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.03s
                      Time elapsed: 00:55:57
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 48157 steps/s (collection: 1.911s, learning 0.130s)
             Mean action noise std: 2.28
          Mean value_function loss: 32.6563
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.2384
                       Mean reward: 955.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 186.7734
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.04s
                      Time elapsed: 00:55:59
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 48680 steps/s (collection: 1.879s, learning 0.140s)
             Mean action noise std: 2.28
          Mean value_function loss: 37.6095
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.2464
                       Mean reward: 945.08
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 1.0646
     Episode_Reward/lifting_object: 187.5396
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.02s
                      Time elapsed: 00:56:01
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 48619 steps/s (collection: 1.870s, learning 0.152s)
             Mean action noise std: 2.28
          Mean value_function loss: 34.4837
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.2578
                       Mean reward: 902.33
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 183.4077
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.02s
                      Time elapsed: 00:56:03
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 49463 steps/s (collection: 1.868s, learning 0.119s)
             Mean action noise std: 2.29
          Mean value_function loss: 31.6464
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.2657
                       Mean reward: 936.50
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 186.8582
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 1.99s
                      Time elapsed: 00:56:05
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 48522 steps/s (collection: 1.911s, learning 0.115s)
             Mean action noise std: 2.29
          Mean value_function loss: 34.4546
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.2753
                       Mean reward: 927.53
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 1.0514
     Episode_Reward/lifting_object: 185.5688
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.03s
                      Time elapsed: 00:56:07
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 47471 steps/s (collection: 1.944s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 36.8687
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 39.2879
                       Mean reward: 928.85
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 1.0460
     Episode_Reward/lifting_object: 184.6810
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.07s
                      Time elapsed: 00:56:09
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 46758 steps/s (collection: 1.970s, learning 0.132s)
             Mean action noise std: 2.29
          Mean value_function loss: 30.9057
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.2899
                       Mean reward: 940.88
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 1.0582
     Episode_Reward/lifting_object: 187.2212
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.10s
                      Time elapsed: 00:56:11
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 48925 steps/s (collection: 1.901s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 34.1457
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.2918
                       Mean reward: 928.62
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 185.3650
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.01s
                      Time elapsed: 00:56:13
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 48467 steps/s (collection: 1.901s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 28.4354
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.2969
                       Mean reward: 926.94
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 184.4215
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.03s
                      Time elapsed: 00:56:15
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 48679 steps/s (collection: 1.913s, learning 0.107s)
             Mean action noise std: 2.29
          Mean value_function loss: 39.9299
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.3080
                       Mean reward: 927.44
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 1.0444
     Episode_Reward/lifting_object: 184.5934
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.02s
                      Time elapsed: 00:56:17
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 48111 steps/s (collection: 1.933s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 34.1109
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.3177
                       Mean reward: 928.66
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 184.6592
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.04s
                      Time elapsed: 00:56:19
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 43148 steps/s (collection: 2.111s, learning 0.167s)
             Mean action noise std: 2.29
          Mean value_function loss: 31.9643
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.3247
                       Mean reward: 925.11
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 185.6869
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.28s
                      Time elapsed: 00:56:21
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 45873 steps/s (collection: 2.009s, learning 0.134s)
             Mean action noise std: 2.29
          Mean value_function loss: 35.3189
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.3320
                       Mean reward: 931.79
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 1.0442
     Episode_Reward/lifting_object: 184.9952
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.14s
                      Time elapsed: 00:56:23
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 46564 steps/s (collection: 2.008s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 40.2526
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.3390
                       Mean reward: 943.68
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 187.3832
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.11s
                      Time elapsed: 00:56:25
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 48664 steps/s (collection: 1.916s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 29.3526
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.3447
                       Mean reward: 941.67
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 1.0495
     Episode_Reward/lifting_object: 185.9843
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.02s
                      Time elapsed: 00:56:27
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 45113 steps/s (collection: 1.961s, learning 0.218s)
             Mean action noise std: 2.30
          Mean value_function loss: 24.9811
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.3530
                       Mean reward: 945.16
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 185.7141
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.18s
                      Time elapsed: 00:56:30
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 47647 steps/s (collection: 1.941s, learning 0.123s)
             Mean action noise std: 2.30
          Mean value_function loss: 25.9577
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.3611
                       Mean reward: 938.23
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 186.8467
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.06s
                      Time elapsed: 00:56:32
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 48335 steps/s (collection: 1.908s, learning 0.126s)
             Mean action noise std: 2.30
          Mean value_function loss: 46.2936
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.3693
                       Mean reward: 919.80
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 186.8840
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.03s
                      Time elapsed: 00:56:34
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 46793 steps/s (collection: 1.999s, learning 0.102s)
             Mean action noise std: 2.30
          Mean value_function loss: 33.0457
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.3750
                       Mean reward: 923.11
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 1.0341
     Episode_Reward/lifting_object: 184.2384
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.10s
                      Time elapsed: 00:56:36
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 48239 steps/s (collection: 1.932s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 33.1097
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.3859
                       Mean reward: 939.71
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 1.0482
     Episode_Reward/lifting_object: 187.1124
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.04s
                      Time elapsed: 00:56:38
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 49873 steps/s (collection: 1.877s, learning 0.095s)
             Mean action noise std: 2.30
          Mean value_function loss: 23.9935
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.3967
                       Mean reward: 953.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0453
     Episode_Reward/lifting_object: 186.5669
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 1.97s
                      Time elapsed: 00:56:40
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 48690 steps/s (collection: 1.895s, learning 0.124s)
             Mean action noise std: 2.30
          Mean value_function loss: 30.5937
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4014
                       Mean reward: 945.21
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 1.0512
     Episode_Reward/lifting_object: 187.8739
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.02s
                      Time elapsed: 00:56:42
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 48034 steps/s (collection: 1.953s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 33.0646
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.4059
                       Mean reward: 947.14
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 187.5815
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.05s
                      Time elapsed: 00:56:44
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 47353 steps/s (collection: 1.971s, learning 0.105s)
             Mean action noise std: 2.30
          Mean value_function loss: 24.5824
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.4139
                       Mean reward: 954.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 188.0792
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.08s
                      Time elapsed: 00:56:46
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 44862 steps/s (collection: 2.047s, learning 0.144s)
             Mean action noise std: 2.30
          Mean value_function loss: 35.0177
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.4255
                       Mean reward: 906.60
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 1.0469
     Episode_Reward/lifting_object: 186.2373
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.19s
                      Time elapsed: 00:56:48
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 46312 steps/s (collection: 1.977s, learning 0.145s)
             Mean action noise std: 2.30
          Mean value_function loss: 38.7449
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.4292
                       Mean reward: 914.96
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 1.0362
     Episode_Reward/lifting_object: 184.0329
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.12s
                      Time elapsed: 00:56:50
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 46883 steps/s (collection: 1.959s, learning 0.138s)
             Mean action noise std: 2.31
          Mean value_function loss: 21.5687
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.4360
                       Mean reward: 953.92
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 1.0575
     Episode_Reward/lifting_object: 188.0414
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.10s
                      Time elapsed: 00:56:52
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 48091 steps/s (collection: 1.918s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 26.2858
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 39.4520
                       Mean reward: 954.04
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 188.1367
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.04s
                      Time elapsed: 00:56:54
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 48190 steps/s (collection: 1.914s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 23.4535
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.4568
                       Mean reward: 928.36
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 186.6107
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.04s
                      Time elapsed: 00:56:56
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 46073 steps/s (collection: 1.978s, learning 0.156s)
             Mean action noise std: 2.31
          Mean value_function loss: 17.7371
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4609
                       Mean reward: 925.36
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 1.0484
     Episode_Reward/lifting_object: 185.0307
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.13s
                      Time elapsed: 00:56:59
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 45033 steps/s (collection: 2.025s, learning 0.158s)
             Mean action noise std: 2.31
          Mean value_function loss: 27.7248
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.4680
                       Mean reward: 944.62
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 1.0666
     Episode_Reward/lifting_object: 188.3405
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.18s
                      Time elapsed: 00:57:01
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 48913 steps/s (collection: 1.894s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 29.9390
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.4780
                       Mean reward: 908.19
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 185.7178
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.01s
                      Time elapsed: 00:57:03
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 49126 steps/s (collection: 1.856s, learning 0.145s)
             Mean action noise std: 2.31
          Mean value_function loss: 27.7340
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.4859
                       Mean reward: 930.69
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 1.0523
     Episode_Reward/lifting_object: 185.5442
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.00s
                      Time elapsed: 00:57:05
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 48424 steps/s (collection: 1.928s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 24.8275
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.4939
                       Mean reward: 934.32
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 1.0505
     Episode_Reward/lifting_object: 185.4609
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.03s
                      Time elapsed: 00:57:07
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 48205 steps/s (collection: 1.909s, learning 0.130s)
             Mean action noise std: 2.31
          Mean value_function loss: 23.0786
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.5043
                       Mean reward: 951.54
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 186.8030
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.04s
                      Time elapsed: 00:57:09
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 47904 steps/s (collection: 1.934s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 30.0093
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.5193
                       Mean reward: 945.82
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 187.0430
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.05s
                      Time elapsed: 00:57:11
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 48679 steps/s (collection: 1.919s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 17.9315
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5290
                       Mean reward: 943.39
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 188.4999
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.02s
                      Time elapsed: 00:57:13
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 49630 steps/s (collection: 1.888s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 33.1284
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.5344
                       Mean reward: 949.64
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 186.7523
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 1.98s
                      Time elapsed: 00:57:15
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 49509 steps/s (collection: 1.890s, learning 0.096s)
             Mean action noise std: 2.32
          Mean value_function loss: 35.3796
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.5406
                       Mean reward: 916.02
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 185.8032
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 1.99s
                      Time elapsed: 00:57:17
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 48950 steps/s (collection: 1.898s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 21.1822
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 39.5493
                       Mean reward: 947.54
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 185.3666
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.01s
                      Time elapsed: 00:57:19
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 45863 steps/s (collection: 2.045s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 23.2134
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.5582
                       Mean reward: 920.31
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 1.0553
     Episode_Reward/lifting_object: 186.3406
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.14s
                      Time elapsed: 00:57:21
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 49397 steps/s (collection: 1.866s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 26.1318
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 39.5629
                       Mean reward: 923.81
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 186.7815
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 1.99s
                      Time elapsed: 00:57:23
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 48794 steps/s (collection: 1.891s, learning 0.123s)
             Mean action noise std: 2.32
          Mean value_function loss: 28.6242
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.5666
                       Mean reward: 939.53
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 184.9753
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.01s
                      Time elapsed: 00:57:25
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 49079 steps/s (collection: 1.882s, learning 0.121s)
             Mean action noise std: 2.32
          Mean value_function loss: 24.4435
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.5788
                       Mean reward: 935.12
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 1.0553
     Episode_Reward/lifting_object: 186.5593
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.00s
                      Time elapsed: 00:57:27
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 50060 steps/s (collection: 1.857s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 30.3084
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.5889
                       Mean reward: 939.41
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 1.0396
     Episode_Reward/lifting_object: 183.7950
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 1.96s
                      Time elapsed: 00:57:29
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 47666 steps/s (collection: 1.956s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 28.1207
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.5920
                       Mean reward: 935.99
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 186.0282
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.06s
                      Time elapsed: 00:57:31
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 48776 steps/s (collection: 1.881s, learning 0.134s)
             Mean action noise std: 2.32
          Mean value_function loss: 30.0525
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.5951
                       Mean reward: 931.24
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 1.0503
     Episode_Reward/lifting_object: 185.3253
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.02s
                      Time elapsed: 00:57:33
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 48639 steps/s (collection: 1.887s, learning 0.135s)
             Mean action noise std: 2.33
          Mean value_function loss: 29.7969
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.5997
                       Mean reward: 945.46
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 1.0602
     Episode_Reward/lifting_object: 187.1395
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.02s
                      Time elapsed: 00:57:35
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 49815 steps/s (collection: 1.873s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 23.4409
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.6067
                       Mean reward: 949.53
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 1.0594
     Episode_Reward/lifting_object: 187.1131
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 1.97s
                      Time elapsed: 00:57:37
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 48675 steps/s (collection: 1.912s, learning 0.107s)
             Mean action noise std: 2.33
          Mean value_function loss: 37.1128
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.6153
                       Mean reward: 925.80
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 1.0472
     Episode_Reward/lifting_object: 184.8824
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.02s
                      Time elapsed: 00:57:39
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 49881 steps/s (collection: 1.867s, learning 0.104s)
             Mean action noise std: 2.33
          Mean value_function loss: 33.1675
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.6219
                       Mean reward: 935.13
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 1.0553
     Episode_Reward/lifting_object: 186.2564
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 1.97s
                      Time elapsed: 00:57:41
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 49663 steps/s (collection: 1.877s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 29.2031
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.6249
                       Mean reward: 946.95
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 186.3572
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 1.98s
                      Time elapsed: 00:57:43
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 49805 steps/s (collection: 1.872s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 19.4183
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.6314
                       Mean reward: 954.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0667
     Episode_Reward/lifting_object: 188.1516
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 1.97s
                      Time elapsed: 00:57:45
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 49078 steps/s (collection: 1.901s, learning 0.102s)
             Mean action noise std: 2.33
          Mean value_function loss: 26.1843
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.6417
                       Mean reward: 944.67
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 185.8581
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.00s
                      Time elapsed: 00:57:47
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 49178 steps/s (collection: 1.882s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 18.0596
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.6473
                       Mean reward: 937.06
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 186.6187
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.00s
                      Time elapsed: 00:57:49
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 50218 steps/s (collection: 1.859s, learning 0.098s)
             Mean action noise std: 2.33
          Mean value_function loss: 20.2356
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.6542
                       Mean reward: 945.38
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 1.0540
     Episode_Reward/lifting_object: 186.1563
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 1.96s
                      Time elapsed: 00:57:51
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 49555 steps/s (collection: 1.884s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 19.2177
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.6630
                       Mean reward: 947.23
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0663
     Episode_Reward/lifting_object: 188.4212
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 1.98s
                      Time elapsed: 00:57:53
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 49450 steps/s (collection: 1.874s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 32.6597
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.6659
                       Mean reward: 917.80
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 185.7677
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 1.99s
                      Time elapsed: 00:57:55
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 49403 steps/s (collection: 1.877s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 23.7729
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.6699
                       Mean reward: 937.26
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 186.9624
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 1.99s
                      Time elapsed: 00:57:57
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 49589 steps/s (collection: 1.868s, learning 0.115s)
             Mean action noise std: 2.34
          Mean value_function loss: 26.6197
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 39.6814
                       Mean reward: 930.99
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.0603
     Episode_Reward/lifting_object: 186.7142
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 1.98s
                      Time elapsed: 00:57:59
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 49498 steps/s (collection: 1.877s, learning 0.109s)
             Mean action noise std: 2.34
          Mean value_function loss: 29.9759
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.6879
                       Mean reward: 931.65
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 186.3342
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 1.99s
                      Time elapsed: 00:58:01
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 48464 steps/s (collection: 1.909s, learning 0.119s)
             Mean action noise std: 2.34
          Mean value_function loss: 30.4356
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.6967
                       Mean reward: 934.50
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 185.6725
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.03s
                      Time elapsed: 00:58:03
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 49062 steps/s (collection: 1.902s, learning 0.102s)
             Mean action noise std: 2.34
          Mean value_function loss: 29.6183
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.7029
                       Mean reward: 933.10
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 1.0613
     Episode_Reward/lifting_object: 186.7957
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.00s
                      Time elapsed: 00:58:05
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 49332 steps/s (collection: 1.867s, learning 0.126s)
             Mean action noise std: 2.34
          Mean value_function loss: 35.2564
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.7104
                       Mean reward: 934.79
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 185.3030
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 1.99s
                      Time elapsed: 00:58:07
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 51012 steps/s (collection: 1.833s, learning 0.094s)
             Mean action noise std: 2.34
          Mean value_function loss: 29.9767
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.7162
                       Mean reward: 924.04
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 1.0551
     Episode_Reward/lifting_object: 185.2531
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 1.93s
                      Time elapsed: 00:58:09
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 46637 steps/s (collection: 1.954s, learning 0.154s)
             Mean action noise std: 2.34
          Mean value_function loss: 27.5928
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.7215
                       Mean reward: 936.05
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 186.0533
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.11s
                      Time elapsed: 00:58:11
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 51008 steps/s (collection: 1.822s, learning 0.105s)
             Mean action noise std: 2.34
          Mean value_function loss: 28.6813
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.7279
                       Mean reward: 921.54
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 1.0611
     Episode_Reward/lifting_object: 185.5591
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 1.93s
                      Time elapsed: 00:58:13
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 47024 steps/s (collection: 1.893s, learning 0.198s)
             Mean action noise std: 2.34
          Mean value_function loss: 29.0378
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.7331
                       Mean reward: 927.64
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0697
     Episode_Reward/lifting_object: 187.1852
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.09s
                      Time elapsed: 00:58:15
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 47491 steps/s (collection: 1.941s, learning 0.129s)
             Mean action noise std: 2.34
          Mean value_function loss: 32.8352
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.7408
                       Mean reward: 938.26
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 184.7200
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.07s
                      Time elapsed: 00:58:17
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 49395 steps/s (collection: 1.883s, learning 0.107s)
             Mean action noise std: 2.34
          Mean value_function loss: 27.8175
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.7539
                       Mean reward: 945.54
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 187.9425
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 1.99s
                      Time elapsed: 00:58:19
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 47034 steps/s (collection: 1.983s, learning 0.108s)
             Mean action noise std: 2.35
          Mean value_function loss: 24.5261
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.7687
                       Mean reward: 943.66
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 1.0529
     Episode_Reward/lifting_object: 183.9980
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.09s
                      Time elapsed: 00:58:21
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 46993 steps/s (collection: 1.970s, learning 0.122s)
             Mean action noise std: 2.35
          Mean value_function loss: 30.6481
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.7801
                       Mean reward: 938.29
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 1.0687
     Episode_Reward/lifting_object: 187.1436
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.09s
                      Time elapsed: 00:58:23
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 44393 steps/s (collection: 2.050s, learning 0.164s)
             Mean action noise std: 2.35
          Mean value_function loss: 34.4199
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.7865
                       Mean reward: 927.12
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 183.2496
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.21s
                      Time elapsed: 00:58:25
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 45194 steps/s (collection: 2.007s, learning 0.168s)
             Mean action noise std: 2.35
          Mean value_function loss: 27.6539
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.7909
                       Mean reward: 946.14
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 1.0668
     Episode_Reward/lifting_object: 186.7160
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.18s
                      Time elapsed: 00:58:28
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 49845 steps/s (collection: 1.878s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 28.8244
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.7971
                       Mean reward: 939.53
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 185.7474
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 1.97s
                      Time elapsed: 00:58:30
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 49668 steps/s (collection: 1.863s, learning 0.117s)
             Mean action noise std: 2.35
          Mean value_function loss: 43.2173
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8030
                       Mean reward: 926.36
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 183.3729
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 1.98s
                      Time elapsed: 00:58:32
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 49036 steps/s (collection: 1.896s, learning 0.109s)
             Mean action noise std: 2.35
          Mean value_function loss: 36.2904
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.8102
                       Mean reward: 946.77
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 185.0979
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.00s
                      Time elapsed: 00:58:34
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 27677 steps/s (collection: 3.412s, learning 0.140s)
             Mean action noise std: 2.35
          Mean value_function loss: 37.1647
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.8137
                       Mean reward: 880.55
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.0310
     Episode_Reward/lifting_object: 180.3280
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.55s
                      Time elapsed: 00:58:37
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14205 steps/s (collection: 6.780s, learning 0.140s)
             Mean action noise std: 2.35
          Mean value_function loss: 22.7283
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.8195
                       Mean reward: 939.38
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 186.1761
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.92s
                      Time elapsed: 00:58:44
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14743 steps/s (collection: 6.552s, learning 0.115s)
             Mean action noise std: 2.35
          Mean value_function loss: 40.1535
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.8254
                       Mean reward: 906.57
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 184.2765
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.67s
                      Time elapsed: 00:58:51
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 15723 steps/s (collection: 6.110s, learning 0.143s)
             Mean action noise std: 2.35
          Mean value_function loss: 27.3351
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.8280
                       Mean reward: 920.41
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 1.0636
     Episode_Reward/lifting_object: 186.0051
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.25s
                      Time elapsed: 00:58:57
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 15308 steps/s (collection: 6.298s, learning 0.124s)
             Mean action noise std: 2.35
          Mean value_function loss: 31.2566
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.8317
                       Mean reward: 948.12
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 1.0694
     Episode_Reward/lifting_object: 186.9608
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.42s
                      Time elapsed: 00:59:03
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 15404 steps/s (collection: 6.186s, learning 0.196s)
             Mean action noise std: 2.35
          Mean value_function loss: 34.0573
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.8346
                       Mean reward: 912.58
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.0665
     Episode_Reward/lifting_object: 186.3782
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.38s
                      Time elapsed: 00:59:10
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 15085 steps/s (collection: 6.376s, learning 0.141s)
             Mean action noise std: 2.35
          Mean value_function loss: 33.7630
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.8386
                       Mean reward: 930.31
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 1.0676
     Episode_Reward/lifting_object: 186.3571
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.52s
                      Time elapsed: 00:59:16
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 15454 steps/s (collection: 6.207s, learning 0.154s)
             Mean action noise std: 2.36
          Mean value_function loss: 36.4204
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.8467
                       Mean reward: 912.51
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 183.9515
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.36s
                      Time elapsed: 00:59:23
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 15474 steps/s (collection: 6.229s, learning 0.124s)
             Mean action noise std: 2.36
          Mean value_function loss: 33.0420
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.8545
                       Mean reward: 929.70
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 1.0584
     Episode_Reward/lifting_object: 184.6000
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.35s
                      Time elapsed: 00:59:29
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 20657 steps/s (collection: 4.626s, learning 0.133s)
             Mean action noise std: 2.36
          Mean value_function loss: 24.5962
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8606
                       Mean reward: 948.04
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 187.7805
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.76s
                      Time elapsed: 00:59:34
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 48756 steps/s (collection: 1.884s, learning 0.132s)
             Mean action noise std: 2.36
          Mean value_function loss: 29.4031
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.8677
                       Mean reward: 920.25
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 185.7708
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.02s
                      Time elapsed: 00:59:36
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 49343 steps/s (collection: 1.831s, learning 0.162s)
             Mean action noise std: 2.36
          Mean value_function loss: 21.9452
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8707
                       Mean reward: 948.15
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 1.0682
     Episode_Reward/lifting_object: 186.5627
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.99s
                      Time elapsed: 00:59:38
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 48435 steps/s (collection: 1.865s, learning 0.165s)
             Mean action noise std: 2.36
          Mean value_function loss: 25.2587
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 39.8753
                       Mean reward: 941.40
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 1.0685
     Episode_Reward/lifting_object: 186.6585
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.03s
                      Time elapsed: 00:59:40
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 50734 steps/s (collection: 1.822s, learning 0.116s)
             Mean action noise std: 2.36
          Mean value_function loss: 22.5475
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 39.8768
                       Mean reward: 917.23
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 1.0658
     Episode_Reward/lifting_object: 186.0771
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 1.94s
                      Time elapsed: 00:59:42
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 50404 steps/s (collection: 1.805s, learning 0.146s)
             Mean action noise std: 2.36
          Mean value_function loss: 37.2564
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 39.8781
                       Mean reward: 935.31
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 1.0611
     Episode_Reward/lifting_object: 185.5104
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 1.95s
                      Time elapsed: 00:59:44
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 50359 steps/s (collection: 1.764s, learning 0.188s)
             Mean action noise std: 2.36
          Mean value_function loss: 29.6793
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 39.8791
                       Mean reward: 949.82
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 1.0779
     Episode_Reward/lifting_object: 188.9097
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.95s
                      Time elapsed: 00:59:46
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 48676 steps/s (collection: 1.871s, learning 0.148s)
             Mean action noise std: 2.36
          Mean value_function loss: 22.7937
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 39.8802
                       Mean reward: 932.70
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 1.0674
     Episode_Reward/lifting_object: 186.9425
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.02s
                      Time elapsed: 00:59:48
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 51669 steps/s (collection: 1.795s, learning 0.108s)
             Mean action noise std: 2.36
          Mean value_function loss: 32.2389
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8811
                       Mean reward: 940.62
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 184.0158
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 1.90s
                      Time elapsed: 00:59:50
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 52893 steps/s (collection: 1.758s, learning 0.101s)
             Mean action noise std: 2.36
          Mean value_function loss: 39.5024
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.8831
                       Mean reward: 920.71
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 185.8691
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.86s
                      Time elapsed: 00:59:51
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 52748 steps/s (collection: 1.748s, learning 0.116s)
             Mean action noise std: 2.36
          Mean value_function loss: 31.6042
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.8864
                       Mean reward: 919.46
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 1.0599
     Episode_Reward/lifting_object: 185.4570
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 1.86s
                      Time elapsed: 00:59:53
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 50574 steps/s (collection: 1.824s, learning 0.120s)
             Mean action noise std: 2.36
          Mean value_function loss: 24.6395
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.8934
                       Mean reward: 944.44
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 1.0837
     Episode_Reward/lifting_object: 189.6341
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 1.94s
                      Time elapsed: 00:59:55
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 52595 steps/s (collection: 1.756s, learning 0.113s)
             Mean action noise std: 2.36
          Mean value_function loss: 29.1914
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.9015
                       Mean reward: 941.86
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 185.0735
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.87s
                      Time elapsed: 00:59:57
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 48339 steps/s (collection: 1.890s, learning 0.143s)
             Mean action noise std: 2.36
          Mean value_function loss: 27.1122
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.9078
                       Mean reward: 944.13
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 1.0722
     Episode_Reward/lifting_object: 187.0430
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.03s
                      Time elapsed: 00:59:59
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 50783 steps/s (collection: 1.822s, learning 0.114s)
             Mean action noise std: 2.36
          Mean value_function loss: 28.5598
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.9114
                       Mean reward: 921.01
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 1.0689
     Episode_Reward/lifting_object: 186.0099
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.94s
                      Time elapsed: 01:00:01
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 52435 steps/s (collection: 1.765s, learning 0.110s)
             Mean action noise std: 2.36
          Mean value_function loss: 18.6276
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9188
                       Mean reward: 947.83
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0747
     Episode_Reward/lifting_object: 187.2323
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 1.87s
                      Time elapsed: 01:00:03
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 53088 steps/s (collection: 1.736s, learning 0.116s)
             Mean action noise std: 2.37
          Mean value_function loss: 36.8037
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9264
                       Mean reward: 918.00
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 1.0612
     Episode_Reward/lifting_object: 184.5101
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 1.85s
                      Time elapsed: 01:00:05
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 52771 steps/s (collection: 1.759s, learning 0.104s)
             Mean action noise std: 2.37
          Mean value_function loss: 24.7372
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 39.9346
                       Mean reward: 932.36
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 1.0735
     Episode_Reward/lifting_object: 186.5495
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 1.86s
                      Time elapsed: 01:00:07
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 53495 steps/s (collection: 1.740s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 25.9185
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.9421
                       Mean reward: 922.55
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 1.0714
     Episode_Reward/lifting_object: 186.2281
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.84s
                      Time elapsed: 01:00:08
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 52568 steps/s (collection: 1.755s, learning 0.115s)
             Mean action noise std: 2.37
          Mean value_function loss: 35.7419
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9508
                       Mean reward: 936.92
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 1.0675
     Episode_Reward/lifting_object: 185.1342
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.87s
                      Time elapsed: 01:00:10
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 52315 steps/s (collection: 1.777s, learning 0.102s)
             Mean action noise std: 2.37
          Mean value_function loss: 25.9333
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.9563
                       Mean reward: 946.55
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 1.0765
     Episode_Reward/lifting_object: 187.0891
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.88s
                      Time elapsed: 01:00:12
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 52657 steps/s (collection: 1.765s, learning 0.101s)
             Mean action noise std: 2.37
          Mean value_function loss: 35.7254
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.9601
                       Mean reward: 936.75
               Mean episode length: 245.63
    Episode_Reward/reaching_object: 1.0618
     Episode_Reward/lifting_object: 184.1028
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.87s
                      Time elapsed: 01:00:14
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 54113 steps/s (collection: 1.731s, learning 0.086s)
             Mean action noise std: 2.37
          Mean value_function loss: 42.0109
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.9665
                       Mean reward: 920.35
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 182.3829
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.82s
                      Time elapsed: 01:00:16
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 52555 steps/s (collection: 1.773s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 31.3282
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.9725
                       Mean reward: 901.03
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.0586
     Episode_Reward/lifting_object: 184.1273
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 1.87s
                      Time elapsed: 01:00:18
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 52088 steps/s (collection: 1.770s, learning 0.117s)
             Mean action noise std: 2.37
          Mean value_function loss: 25.7723
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.9821
                       Mean reward: 950.67
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 1.0834
     Episode_Reward/lifting_object: 188.6995
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 1.89s
                      Time elapsed: 01:00:20
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 53073 steps/s (collection: 1.729s, learning 0.123s)
             Mean action noise std: 2.37
          Mean value_function loss: 32.2021
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.9910
                       Mean reward: 949.89
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 1.0742
     Episode_Reward/lifting_object: 187.0838
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 1.85s
                      Time elapsed: 01:00:22
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 52543 steps/s (collection: 1.783s, learning 0.088s)
             Mean action noise std: 2.37
          Mean value_function loss: 26.0045
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 39.9978
                       Mean reward: 937.12
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 185.0795
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 1.87s
                      Time elapsed: 01:00:23
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 53359 steps/s (collection: 1.743s, learning 0.100s)
             Mean action noise std: 2.37
          Mean value_function loss: 42.8872
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.0008
                       Mean reward: 910.66
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 183.8220
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 1.84s
                      Time elapsed: 01:00:25
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 53678 steps/s (collection: 1.729s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 90.8341
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.0028
                       Mean reward: 904.18
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 181.0949
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 1.83s
                      Time elapsed: 01:00:27
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 51762 steps/s (collection: 1.780s, learning 0.119s)
             Mean action noise std: 2.37
          Mean value_function loss: 54.6883
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.0059
                       Mean reward: 939.67
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 183.7148
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.90s
                      Time elapsed: 01:00:29
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 52884 steps/s (collection: 1.738s, learning 0.121s)
             Mean action noise std: 2.38
          Mean value_function loss: 40.3674
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.0090
                       Mean reward: 913.05
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 183.1327
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.86s
                      Time elapsed: 01:00:31
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 54416 steps/s (collection: 1.722s, learning 0.085s)
             Mean action noise std: 2.38
          Mean value_function loss: 45.0984
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.0157
                       Mean reward: 925.66
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 1.0649
     Episode_Reward/lifting_object: 183.9826
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.81s
                      Time elapsed: 01:00:33
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 50432 steps/s (collection: 1.862s, learning 0.088s)
             Mean action noise std: 2.38
          Mean value_function loss: 27.2056
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.0241
                       Mean reward: 945.91
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 1.0819
     Episode_Reward/lifting_object: 186.9140
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.95s
                      Time elapsed: 01:00:35
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 51329 steps/s (collection: 1.788s, learning 0.127s)
             Mean action noise std: 2.38
          Mean value_function loss: 33.3040
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.0337
                       Mean reward: 936.98
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 1.0687
     Episode_Reward/lifting_object: 184.5819
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.92s
                      Time elapsed: 01:00:37
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 52469 steps/s (collection: 1.773s, learning 0.101s)
             Mean action noise std: 2.38
          Mean value_function loss: 34.8783
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0421
                       Mean reward: 925.69
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 1.0803
     Episode_Reward/lifting_object: 186.3487
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 1.87s
                      Time elapsed: 01:00:38
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 50813 steps/s (collection: 1.843s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 27.6298
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.0506
                       Mean reward: 932.92
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 186.0429
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 1.93s
                      Time elapsed: 01:00:40
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 52982 steps/s (collection: 1.761s, learning 0.094s)
             Mean action noise std: 2.38
          Mean value_function loss: 28.1419
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.0583
                       Mean reward: 935.17
               Mean episode length: 245.63
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 185.6698
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 1.86s
                      Time elapsed: 01:00:42
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 51960 steps/s (collection: 1.798s, learning 0.094s)
             Mean action noise std: 2.38
          Mean value_function loss: 29.3525
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.0637
                       Mean reward: 925.87
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 1.0695
     Episode_Reward/lifting_object: 184.8020
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.89s
                      Time elapsed: 01:00:44
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 49488 steps/s (collection: 1.885s, learning 0.101s)
             Mean action noise std: 2.38
          Mean value_function loss: 27.4417
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0714
                       Mean reward: 944.40
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0755
     Episode_Reward/lifting_object: 186.3176
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 1.99s
                      Time elapsed: 01:00:46
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 50860 steps/s (collection: 1.837s, learning 0.096s)
             Mean action noise std: 2.38
          Mean value_function loss: 20.6459
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.0790
                       Mean reward: 942.18
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 1.0772
     Episode_Reward/lifting_object: 187.1023
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 1.93s
                      Time elapsed: 01:00:48
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 52572 steps/s (collection: 1.774s, learning 0.096s)
             Mean action noise std: 2.38
          Mean value_function loss: 36.9812
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.0823
                       Mean reward: 928.13
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 1.0607
     Episode_Reward/lifting_object: 184.3398
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 1.87s
                      Time elapsed: 01:00:50
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 50623 steps/s (collection: 1.816s, learning 0.126s)
             Mean action noise std: 2.39
          Mean value_function loss: 24.8520
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0884
                       Mean reward: 946.51
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 1.0689
     Episode_Reward/lifting_object: 185.9084
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.94s
                      Time elapsed: 01:00:52
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 49774 steps/s (collection: 1.824s, learning 0.151s)
             Mean action noise std: 2.39
          Mean value_function loss: 44.0829
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0985
                       Mean reward: 914.11
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 185.2440
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 1.97s
                      Time elapsed: 01:00:54
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 50398 steps/s (collection: 1.813s, learning 0.138s)
             Mean action noise std: 2.39
          Mean value_function loss: 28.8529
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.1031
                       Mean reward: 936.10
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 1.0657
     Episode_Reward/lifting_object: 185.1757
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.95s
                      Time elapsed: 01:00:56
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 51780 steps/s (collection: 1.796s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 23.1339
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.1075
                       Mean reward: 947.01
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 1.0748
     Episode_Reward/lifting_object: 186.7546
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 1.90s
                      Time elapsed: 01:00:58
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 47118 steps/s (collection: 1.927s, learning 0.160s)
             Mean action noise std: 2.39
          Mean value_function loss: 29.7822
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.1144
                       Mean reward: 947.16
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 1.0737
     Episode_Reward/lifting_object: 186.7535
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.09s
                      Time elapsed: 01:01:00
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 47038 steps/s (collection: 1.930s, learning 0.160s)
             Mean action noise std: 2.39
          Mean value_function loss: 46.9654
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.1211
                       Mean reward: 928.88
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 184.2826
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.09s
                      Time elapsed: 01:01:02
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 50909 steps/s (collection: 1.761s, learning 0.170s)
             Mean action noise std: 2.39
          Mean value_function loss: 30.9853
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.1263
                       Mean reward: 940.14
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 1.0658
     Episode_Reward/lifting_object: 184.4572
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 1.93s
                      Time elapsed: 01:01:04
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 49252 steps/s (collection: 1.873s, learning 0.123s)
             Mean action noise std: 2.39
          Mean value_function loss: 34.9620
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.1371
                       Mean reward: 947.86
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 1.0870
     Episode_Reward/lifting_object: 188.3185
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.00s
                      Time elapsed: 01:01:06
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 51161 steps/s (collection: 1.771s, learning 0.151s)
             Mean action noise std: 2.39
          Mean value_function loss: 42.5153
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 40.1448
                       Mean reward: 938.83
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.0613
     Episode_Reward/lifting_object: 183.1601
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 1.92s
                      Time elapsed: 01:01:08
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 52404 steps/s (collection: 1.790s, learning 0.086s)
             Mean action noise std: 2.39
          Mean value_function loss: 36.7583
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.1477
                       Mean reward: 933.45
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 1.0818
     Episode_Reward/lifting_object: 186.3026
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 1.88s
                      Time elapsed: 01:01:10
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 51005 steps/s (collection: 1.803s, learning 0.124s)
             Mean action noise std: 2.39
          Mean value_function loss: 32.1079
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.1515
                       Mean reward: 951.03
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 1.0815
     Episode_Reward/lifting_object: 185.7765
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 1.93s
                      Time elapsed: 01:01:11
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 52767 steps/s (collection: 1.772s, learning 0.091s)
             Mean action noise std: 2.39
          Mean value_function loss: 35.3370
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.1545
                       Mean reward: 919.06
               Mean episode length: 241.68
    Episode_Reward/reaching_object: 1.0797
     Episode_Reward/lifting_object: 185.3392
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 1.86s
                      Time elapsed: 01:01:13
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 52769 steps/s (collection: 1.767s, learning 0.096s)
             Mean action noise std: 2.39
          Mean value_function loss: 23.2550
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.1625
                       Mean reward: 939.97
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 1.0850
     Episode_Reward/lifting_object: 186.5589
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 1.86s
                      Time elapsed: 01:01:15
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 52860 steps/s (collection: 1.772s, learning 0.088s)
             Mean action noise std: 2.40
          Mean value_function loss: 38.3228
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1703
                       Mean reward: 931.09
               Mean episode length: 244.16
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: 185.4667
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 1.86s
                      Time elapsed: 01:01:17
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 52536 steps/s (collection: 1.776s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 40.0376
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.1786
                       Mean reward: 935.66
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 1.0776
     Episode_Reward/lifting_object: 184.9563
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 1.87s
                      Time elapsed: 01:01:19
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 52488 steps/s (collection: 1.785s, learning 0.088s)
             Mean action noise std: 2.40
          Mean value_function loss: 49.3466
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.1872
                       Mean reward: 893.56
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 182.8708
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 1.87s
                      Time elapsed: 01:01:21
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 51403 steps/s (collection: 1.784s, learning 0.128s)
             Mean action noise std: 2.40
          Mean value_function loss: 20.3715
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.1890
                       Mean reward: 939.96
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 1.0879
     Episode_Reward/lifting_object: 186.8055
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 1.91s
                      Time elapsed: 01:01:23
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 51681 steps/s (collection: 1.786s, learning 0.116s)
             Mean action noise std: 2.40
          Mean value_function loss: 29.8824
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.1917
                       Mean reward: 929.30
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 1.0831
     Episode_Reward/lifting_object: 186.3280
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 1.90s
                      Time elapsed: 01:01:25
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 51576 steps/s (collection: 1.815s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 28.0427
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.1957
                       Mean reward: 943.41
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 1.0793
     Episode_Reward/lifting_object: 185.8136
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 1.91s
                      Time elapsed: 01:01:26
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 51522 steps/s (collection: 1.808s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 33.8937
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.1993
                       Mean reward: 911.08
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 1.0801
     Episode_Reward/lifting_object: 186.1578
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 1.91s
                      Time elapsed: 01:01:28
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 52622 steps/s (collection: 1.783s, learning 0.085s)
             Mean action noise std: 2.40
          Mean value_function loss: 37.3024
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2014
                       Mean reward: 911.58
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 182.2375
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 1.87s
                      Time elapsed: 01:01:30
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 52003 steps/s (collection: 1.799s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 28.1780
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.2076
                       Mean reward: 946.17
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 1.0769
     Episode_Reward/lifting_object: 185.3998
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 1.89s
                      Time elapsed: 01:01:32
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 52365 steps/s (collection: 1.789s, learning 0.089s)
             Mean action noise std: 2.40
          Mean value_function loss: 28.4595
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.2192
                       Mean reward: 935.78
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 1.0865
     Episode_Reward/lifting_object: 187.4114
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 1.88s
                      Time elapsed: 01:01:34
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 51443 steps/s (collection: 1.777s, learning 0.134s)
             Mean action noise std: 2.40
          Mean value_function loss: 43.8080
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 40.2248
                       Mean reward: 921.12
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 1.0700
     Episode_Reward/lifting_object: 184.3727
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 1.91s
                      Time elapsed: 01:01:36
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 52239 steps/s (collection: 1.795s, learning 0.086s)
             Mean action noise std: 2.40
          Mean value_function loss: 32.0232
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2269
                       Mean reward: 916.05
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.0721
     Episode_Reward/lifting_object: 185.1239
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 1.88s
                      Time elapsed: 01:01:38
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 51427 steps/s (collection: 1.801s, learning 0.110s)
             Mean action noise std: 2.40
          Mean value_function loss: 34.1165
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.2321
                       Mean reward: 924.13
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 1.0773
     Episode_Reward/lifting_object: 186.4680
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 1.91s
                      Time elapsed: 01:01:40
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 51131 steps/s (collection: 1.838s, learning 0.085s)
             Mean action noise std: 2.41
          Mean value_function loss: 31.3240
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.2430
                       Mean reward: 953.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0624
     Episode_Reward/lifting_object: 183.6837
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 1.92s
                      Time elapsed: 01:01:42
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 51872 steps/s (collection: 1.782s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 56.5660
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.2537
                       Mean reward: 888.74
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 183.3784
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 1.90s
                      Time elapsed: 01:01:44
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 50072 steps/s (collection: 1.828s, learning 0.135s)
             Mean action noise std: 2.41
          Mean value_function loss: 29.7926
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.2614
                       Mean reward: 930.67
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 1.0669
     Episode_Reward/lifting_object: 185.1918
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 1.96s
                      Time elapsed: 01:01:46
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 52095 steps/s (collection: 1.800s, learning 0.087s)
             Mean action noise std: 2.41
          Mean value_function loss: 29.0249
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.2695
                       Mean reward: 936.38
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 1.0756
     Episode_Reward/lifting_object: 186.9697
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 1.89s
                      Time elapsed: 01:01:47
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 50931 steps/s (collection: 1.837s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 32.9231
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.2740
                       Mean reward: 947.72
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0787
     Episode_Reward/lifting_object: 187.7438
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 1.93s
                      Time elapsed: 01:01:49
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 52289 steps/s (collection: 1.783s, learning 0.097s)
             Mean action noise std: 2.41
          Mean value_function loss: 39.2641
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.2794
                       Mean reward: 941.64
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 1.0683
     Episode_Reward/lifting_object: 185.7430
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 1.88s
                      Time elapsed: 01:01:51
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 51754 steps/s (collection: 1.785s, learning 0.115s)
             Mean action noise std: 2.41
          Mean value_function loss: 34.2385
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.2871
                       Mean reward: 941.04
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 1.0614
     Episode_Reward/lifting_object: 184.5350
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 1.90s
                      Time elapsed: 01:01:53
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 52096 steps/s (collection: 1.782s, learning 0.105s)
             Mean action noise std: 2.41
          Mean value_function loss: 33.3146
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.2925
                       Mean reward: 928.58
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 1.0650
     Episode_Reward/lifting_object: 184.7945
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 1.89s
                      Time elapsed: 01:01:55
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 52265 steps/s (collection: 1.768s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 29.7660
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.2986
                       Mean reward: 946.15
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0670
     Episode_Reward/lifting_object: 185.8172
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 1.88s
                      Time elapsed: 01:01:57
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 52103 steps/s (collection: 1.776s, learning 0.111s)
             Mean action noise std: 2.41
          Mean value_function loss: 27.2101
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.3060
                       Mean reward: 927.62
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 1.0673
     Episode_Reward/lifting_object: 185.4478
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 1.89s
                      Time elapsed: 01:01:59
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 52408 steps/s (collection: 1.783s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 30.8411
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.3150
                       Mean reward: 933.73
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 1.0734
     Episode_Reward/lifting_object: 186.1801
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.88s
                      Time elapsed: 01:02:01
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 51905 steps/s (collection: 1.775s, learning 0.119s)
             Mean action noise std: 2.42
          Mean value_function loss: 27.3300
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.3252
                       Mean reward: 915.21
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 1.0745
     Episode_Reward/lifting_object: 186.3725
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 1.89s
                      Time elapsed: 01:02:03
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 51066 steps/s (collection: 1.813s, learning 0.112s)
             Mean action noise std: 2.42
          Mean value_function loss: 20.0073
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3384
                       Mean reward: 938.45
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 1.0822
     Episode_Reward/lifting_object: 187.7659
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 1.93s
                      Time elapsed: 01:02:04
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 52228 steps/s (collection: 1.796s, learning 0.086s)
             Mean action noise std: 2.42
          Mean value_function loss: 29.9027
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.3479
                       Mean reward: 928.88
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 1.0808
     Episode_Reward/lifting_object: 187.3580
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 1.88s
                      Time elapsed: 01:02:06
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 51787 steps/s (collection: 1.790s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 40.0512
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.3515
                       Mean reward: 927.02
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 1.0711
     Episode_Reward/lifting_object: 185.7877
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 1.90s
                      Time elapsed: 01:02:08
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 52078 steps/s (collection: 1.779s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 29.7813
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.3558
                       Mean reward: 936.27
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 1.0794
     Episode_Reward/lifting_object: 187.4398
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 1.89s
                      Time elapsed: 01:02:10
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 51817 steps/s (collection: 1.775s, learning 0.122s)
             Mean action noise std: 2.42
          Mean value_function loss: 34.2847
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3641
                       Mean reward: 930.31
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 184.2551
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 1.90s
                      Time elapsed: 01:02:12
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 50900 steps/s (collection: 1.791s, learning 0.141s)
             Mean action noise std: 2.42
          Mean value_function loss: 26.0945
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.3725
                       Mean reward: 944.71
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 1.0786
     Episode_Reward/lifting_object: 187.3217
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 1.93s
                      Time elapsed: 01:02:14
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 50679 steps/s (collection: 1.794s, learning 0.146s)
             Mean action noise std: 2.42
          Mean value_function loss: 19.0257
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 40.3747
                       Mean reward: 945.07
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 1.0862
     Episode_Reward/lifting_object: 188.8712
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 1.94s
                      Time elapsed: 01:02:16
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 50052 steps/s (collection: 1.830s, learning 0.134s)
             Mean action noise std: 2.42
          Mean value_function loss: 26.5289
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.3809
                       Mean reward: 938.88
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 187.5925
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 1.96s
                      Time elapsed: 01:02:18
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 51866 steps/s (collection: 1.760s, learning 0.135s)
             Mean action noise std: 2.42
          Mean value_function loss: 35.5813
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.3875
                       Mean reward: 936.75
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 1.0664
     Episode_Reward/lifting_object: 185.1943
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 1.90s
                      Time elapsed: 01:02:20
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 47123 steps/s (collection: 1.917s, learning 0.169s)
             Mean action noise std: 2.42
          Mean value_function loss: 31.0868
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.3949
                       Mean reward: 933.69
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 1.0695
     Episode_Reward/lifting_object: 185.8794
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.09s
                      Time elapsed: 01:02:22
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 51670 steps/s (collection: 1.791s, learning 0.112s)
             Mean action noise std: 2.42
          Mean value_function loss: 31.2042
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.3998
                       Mean reward: 944.24
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 1.0750
     Episode_Reward/lifting_object: 186.7011
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 1.90s
                      Time elapsed: 01:02:24
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 51782 steps/s (collection: 1.800s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 34.9519
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.4064
                       Mean reward: 936.63
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 1.0764
     Episode_Reward/lifting_object: 186.8681
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 1.90s
                      Time elapsed: 01:02:26
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 52041 steps/s (collection: 1.796s, learning 0.093s)
             Mean action noise std: 2.43
          Mean value_function loss: 51.4697
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.4145
                       Mean reward: 932.70
               Mean episode length: 244.51
    Episode_Reward/reaching_object: 1.0629
     Episode_Reward/lifting_object: 184.2528
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.89s
                      Time elapsed: 01:02:28
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 51434 steps/s (collection: 1.794s, learning 0.118s)
             Mean action noise std: 2.43
          Mean value_function loss: 41.9148
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.4218
                       Mean reward: 922.17
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 1.0689
     Episode_Reward/lifting_object: 184.9800
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 1.91s
                      Time elapsed: 01:02:29
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 51217 steps/s (collection: 1.815s, learning 0.105s)
             Mean action noise std: 2.43
          Mean value_function loss: 47.7786
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.4327
                       Mean reward: 910.87
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 184.3435
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 1.92s
                      Time elapsed: 01:02:31
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 52221 steps/s (collection: 1.784s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 48.3861
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.4409
                       Mean reward: 897.76
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 183.4238
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 1.88s
                      Time elapsed: 01:02:33
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 51489 steps/s (collection: 1.800s, learning 0.109s)
             Mean action noise std: 2.43
          Mean value_function loss: 54.2632
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.4464
                       Mean reward: 925.38
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 183.8833
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 1.91s
                      Time elapsed: 01:02:35
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 51497 steps/s (collection: 1.815s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 41.0380
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.4508
                       Mean reward: 915.55
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.0764
     Episode_Reward/lifting_object: 186.4120
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 1.91s
                      Time elapsed: 01:02:37
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 51443 steps/s (collection: 1.796s, learning 0.115s)
             Mean action noise std: 2.43
          Mean value_function loss: 46.7093
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.4585
                       Mean reward: 923.65
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 1.0602
     Episode_Reward/lifting_object: 183.4646
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.91s
                      Time elapsed: 01:02:39
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 51640 steps/s (collection: 1.814s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 54.1129
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.4675
                       Mean reward: 910.26
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 1.0567
     Episode_Reward/lifting_object: 182.2105
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 1.90s
                      Time elapsed: 01:02:41
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 50826 steps/s (collection: 1.811s, learning 0.123s)
             Mean action noise std: 2.43
          Mean value_function loss: 45.5325
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.4746
                       Mean reward: 933.19
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 1.0619
     Episode_Reward/lifting_object: 183.8136
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 1.93s
                      Time elapsed: 01:02:43
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 48709 steps/s (collection: 1.857s, learning 0.161s)
             Mean action noise std: 2.44
          Mean value_function loss: 45.3556
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.4848
                       Mean reward: 916.21
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.0656
     Episode_Reward/lifting_object: 184.6079
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.02s
                      Time elapsed: 01:02:45
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 51207 steps/s (collection: 1.813s, learning 0.107s)
             Mean action noise std: 2.44
          Mean value_function loss: 52.6465
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.4899
                       Mean reward: 940.10
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 183.4335
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 1.92s
                      Time elapsed: 01:02:47
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 51912 steps/s (collection: 1.770s, learning 0.124s)
             Mean action noise std: 2.44
          Mean value_function loss: 57.4850
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.4948
                       Mean reward: 929.56
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 182.4090
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 1.89s
                      Time elapsed: 01:02:49
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 51558 steps/s (collection: 1.821s, learning 0.086s)
             Mean action noise std: 2.44
          Mean value_function loss: 30.9134
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4988
                       Mean reward: 948.56
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0672
     Episode_Reward/lifting_object: 185.4002
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 1.91s
                      Time elapsed: 01:02:51
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 51284 steps/s (collection: 1.819s, learning 0.098s)
             Mean action noise std: 2.44
          Mean value_function loss: 24.9321
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.5053
                       Mean reward: 940.54
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 1.0725
     Episode_Reward/lifting_object: 186.8331
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 1.92s
                      Time elapsed: 01:02:52
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 50548 steps/s (collection: 1.846s, learning 0.098s)
             Mean action noise std: 2.44
          Mean value_function loss: 33.0995
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 40.5125
                       Mean reward: 951.45
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 1.0605
     Episode_Reward/lifting_object: 184.6691
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 1.94s
                      Time elapsed: 01:02:54
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 52073 steps/s (collection: 1.792s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 32.9706
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.5142
                       Mean reward: 930.84
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 186.3075
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 1.89s
                      Time elapsed: 01:02:56
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 51541 steps/s (collection: 1.801s, learning 0.106s)
             Mean action noise std: 2.44
          Mean value_function loss: 43.6888
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.5201
                       Mean reward: 917.57
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: 182.5194
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 1.91s
                      Time elapsed: 01:02:58
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 50284 steps/s (collection: 1.841s, learning 0.114s)
             Mean action noise std: 2.44
          Mean value_function loss: 28.0350
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.5252
                       Mean reward: 924.56
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 1.0627
     Episode_Reward/lifting_object: 185.7420
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 1.95s
                      Time elapsed: 01:03:00
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 50762 steps/s (collection: 1.821s, learning 0.116s)
             Mean action noise std: 2.44
          Mean value_function loss: 36.9546
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.5274
                       Mean reward: 922.47
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 1.0639
     Episode_Reward/lifting_object: 186.4741
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 1.94s
                      Time elapsed: 01:03:02
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 50257 steps/s (collection: 1.830s, learning 0.127s)
             Mean action noise std: 2.44
          Mean value_function loss: 32.4679
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 40.5290
                       Mean reward: 941.43
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 1.0697
     Episode_Reward/lifting_object: 187.4487
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 1.96s
                      Time elapsed: 01:03:04
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 50823 steps/s (collection: 1.816s, learning 0.118s)
             Mean action noise std: 2.44
          Mean value_function loss: 48.3794
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.5330
                       Mean reward: 946.32
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 1.0513
     Episode_Reward/lifting_object: 184.4618
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 1.93s
                      Time elapsed: 01:03:06
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 47822 steps/s (collection: 1.908s, learning 0.148s)
             Mean action noise std: 2.44
          Mean value_function loss: 38.8068
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.5409
                       Mean reward: 956.58
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 186.0431
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.06s
                      Time elapsed: 01:03:08
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 50872 steps/s (collection: 1.824s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 39.5810
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.5517
                       Mean reward: 943.80
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 185.8892
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 1.93s
                      Time elapsed: 01:03:10
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 51321 steps/s (collection: 1.828s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 28.6943
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.5589
                       Mean reward: 928.12
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 185.6360
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 1.92s
                      Time elapsed: 01:03:12
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 51496 steps/s (collection: 1.816s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 56.3091
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.5635
                       Mean reward: 939.81
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 1.0472
     Episode_Reward/lifting_object: 184.1490
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 1.91s
                      Time elapsed: 01:03:14
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 50182 steps/s (collection: 1.826s, learning 0.133s)
             Mean action noise std: 2.45
          Mean value_function loss: 32.6044
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.5689
                       Mean reward: 945.19
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 1.0602
     Episode_Reward/lifting_object: 186.9418
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 1.96s
                      Time elapsed: 01:03:16
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 52019 steps/s (collection: 1.790s, learning 0.100s)
             Mean action noise std: 2.45
          Mean value_function loss: 40.0850
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.5762
                       Mean reward: 942.72
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 1.0645
     Episode_Reward/lifting_object: 187.7266
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 1.89s
                      Time elapsed: 01:03:18
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 51475 steps/s (collection: 1.819s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 41.6794
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.5795
                       Mean reward: 942.07
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 186.2215
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 1.91s
                      Time elapsed: 01:03:20
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 51914 steps/s (collection: 1.803s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 48.3337
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.5825
                       Mean reward: 938.29
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 1.0510
     Episode_Reward/lifting_object: 185.1106
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 1.89s
                      Time elapsed: 01:03:21
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 51947 steps/s (collection: 1.783s, learning 0.110s)
             Mean action noise std: 2.45
          Mean value_function loss: 39.1485
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.5903
                       Mean reward: 936.61
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 185.4761
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 1.89s
                      Time elapsed: 01:03:23
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 50957 steps/s (collection: 1.831s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 39.5071
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.6004
                       Mean reward: 928.90
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 186.5677
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 1.93s
                      Time elapsed: 01:03:25
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 51214 steps/s (collection: 1.828s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 34.2241
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.6071
                       Mean reward: 946.75
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 186.4694
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 1.92s
                      Time elapsed: 01:03:27
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 51020 steps/s (collection: 1.815s, learning 0.112s)
             Mean action noise std: 2.45
          Mean value_function loss: 35.5486
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.6145
                       Mean reward: 932.04
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 186.6545
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 1.93s
                      Time elapsed: 01:03:29
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 50736 steps/s (collection: 1.820s, learning 0.118s)
             Mean action noise std: 2.45
          Mean value_function loss: 44.3056
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.6241
                       Mean reward: 920.47
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 1.0554
     Episode_Reward/lifting_object: 185.2248
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 1.94s
                      Time elapsed: 01:03:31
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 50050 steps/s (collection: 1.850s, learning 0.115s)
             Mean action noise std: 2.45
          Mean value_function loss: 43.2601
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 40.6284
                       Mean reward: 920.37
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 182.8509
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 1.96s
                      Time elapsed: 01:03:33
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 51006 steps/s (collection: 1.839s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 28.4342
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.6314
                       Mean reward: 955.00
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 1.0716
     Episode_Reward/lifting_object: 187.8225
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 1.93s
                      Time elapsed: 01:03:35
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 51075 steps/s (collection: 1.798s, learning 0.127s)
             Mean action noise std: 2.45
          Mean value_function loss: 33.5187
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.6352
                       Mean reward: 930.27
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 184.8429
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 1.92s
                      Time elapsed: 01:03:37
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 51757 steps/s (collection: 1.810s, learning 0.089s)
             Mean action noise std: 2.46
          Mean value_function loss: 34.8134
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.6399
                       Mean reward: 940.69
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 1.0640
     Episode_Reward/lifting_object: 186.1178
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 1.90s
                      Time elapsed: 01:03:39
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 51180 steps/s (collection: 1.828s, learning 0.093s)
             Mean action noise std: 2.46
          Mean value_function loss: 27.7781
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.6431
                       Mean reward: 918.74
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.0680
     Episode_Reward/lifting_object: 186.9151
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 1.92s
                      Time elapsed: 01:03:41
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 49478 steps/s (collection: 1.841s, learning 0.146s)
             Mean action noise std: 2.46
          Mean value_function loss: 35.0805
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.6485
                       Mean reward: 935.53
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 1.0672
     Episode_Reward/lifting_object: 186.8967
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 1.99s
                      Time elapsed: 01:03:43
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 48801 steps/s (collection: 1.884s, learning 0.131s)
             Mean action noise std: 2.46
          Mean value_function loss: 48.5597
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.6525
                       Mean reward: 917.63
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 1.0429
     Episode_Reward/lifting_object: 182.5395
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.01s
                      Time elapsed: 01:03:45
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 50368 steps/s (collection: 1.844s, learning 0.108s)
             Mean action noise std: 2.46
          Mean value_function loss: 22.8372
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.6559
                       Mean reward: 947.14
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 1.0715
     Episode_Reward/lifting_object: 188.0583
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 1.95s
                      Time elapsed: 01:03:47
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 50426 steps/s (collection: 1.848s, learning 0.101s)
             Mean action noise std: 2.46
          Mean value_function loss: 46.2636
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.6641
                       Mean reward: 936.44
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 183.6858
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 1.95s
                      Time elapsed: 01:03:49
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 52310 steps/s (collection: 1.781s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 45.5516
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.6767
                       Mean reward: 919.16
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.0433
     Episode_Reward/lifting_object: 182.8579
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 1.88s
                      Time elapsed: 01:03:50
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 51703 steps/s (collection: 1.807s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 41.4586
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.6853
                       Mean reward: 912.44
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 1.0502
     Episode_Reward/lifting_object: 184.1940
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 1.90s
                      Time elapsed: 01:03:52
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 51757 steps/s (collection: 1.782s, learning 0.118s)
             Mean action noise std: 2.46
          Mean value_function loss: 24.2233
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.6969
                       Mean reward: 929.25
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 1.0657
     Episode_Reward/lifting_object: 186.9496
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 1.90s
                      Time elapsed: 01:03:54
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 50778 steps/s (collection: 1.781s, learning 0.155s)
             Mean action noise std: 2.46
          Mean value_function loss: 27.8520
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.7040
                       Mean reward: 946.46
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 185.4131
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 1.94s
                      Time elapsed: 01:03:56
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 50146 steps/s (collection: 1.798s, learning 0.162s)
             Mean action noise std: 2.46
          Mean value_function loss: 28.3432
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.7109
                       Mean reward: 944.39
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 1.0706
     Episode_Reward/lifting_object: 187.6435
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 1.96s
                      Time elapsed: 01:03:58
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 51756 steps/s (collection: 1.804s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 30.3181
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.7132
                       Mean reward: 906.51
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 1.0621
     Episode_Reward/lifting_object: 185.6893
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 1.90s
                      Time elapsed: 01:04:00
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 51656 steps/s (collection: 1.802s, learning 0.101s)
             Mean action noise std: 2.47
          Mean value_function loss: 27.7916
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.7175
                       Mean reward: 929.35
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 186.3362
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 1.90s
                      Time elapsed: 01:04:02
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 50699 steps/s (collection: 1.843s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 39.9954
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.7255
                       Mean reward: 933.38
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 1.0599
     Episode_Reward/lifting_object: 184.8807
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 1.94s
                      Time elapsed: 01:04:04
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 50961 steps/s (collection: 1.836s, learning 0.093s)
             Mean action noise std: 2.47
          Mean value_function loss: 39.1943
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.7390
                       Mean reward: 932.90
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.0691
     Episode_Reward/lifting_object: 186.4150
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 1.93s
                      Time elapsed: 01:04:06
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 50991 steps/s (collection: 1.818s, learning 0.110s)
             Mean action noise std: 2.47
          Mean value_function loss: 43.9390
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.7453
                       Mean reward: 925.76
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 1.0639
     Episode_Reward/lifting_object: 185.0747
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 1.93s
                      Time elapsed: 01:04:08
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 51870 steps/s (collection: 1.790s, learning 0.105s)
             Mean action noise std: 2.47
          Mean value_function loss: 14.7040
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.7532
                       Mean reward: 939.31
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 1.0823
     Episode_Reward/lifting_object: 188.2617
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 1.90s
                      Time elapsed: 01:04:10
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 51486 steps/s (collection: 1.802s, learning 0.107s)
             Mean action noise std: 2.47
          Mean value_function loss: 37.8416
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.7592
                       Mean reward: 927.69
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 184.8002
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 1.91s
                      Time elapsed: 01:04:12
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 48847 steps/s (collection: 1.878s, learning 0.134s)
             Mean action noise std: 2.47
          Mean value_function loss: 27.6209
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.7631
                       Mean reward: 947.12
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 1.0635
     Episode_Reward/lifting_object: 184.5463
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.01s
                      Time elapsed: 01:04:14
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 50201 steps/s (collection: 1.847s, learning 0.111s)
             Mean action noise std: 2.47
          Mean value_function loss: 41.0818
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 40.7659
                       Mean reward: 914.82
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 183.5729
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 1.96s
                      Time elapsed: 01:04:16
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 49504 steps/s (collection: 1.865s, learning 0.121s)
             Mean action noise std: 2.47
          Mean value_function loss: 44.0868
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 40.7687
                       Mean reward: 924.49
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 183.7924
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 1.99s
                      Time elapsed: 01:04:18
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 46731 steps/s (collection: 1.964s, learning 0.139s)
             Mean action noise std: 2.47
          Mean value_function loss: 21.7354
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.7728
                       Mean reward: 956.26
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 1.0823
     Episode_Reward/lifting_object: 188.5317
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.10s
                      Time elapsed: 01:04:20
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 50872 steps/s (collection: 1.836s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 25.4098
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.7795
                       Mean reward: 953.21
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 1.0788
     Episode_Reward/lifting_object: 187.9506
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 1.93s
                      Time elapsed: 01:04:22
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 50337 steps/s (collection: 1.840s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 30.0001
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.7870
                       Mean reward: 946.24
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 1.0612
     Episode_Reward/lifting_object: 185.1311
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 1.95s
                      Time elapsed: 01:04:24
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 51768 steps/s (collection: 1.805s, learning 0.094s)
             Mean action noise std: 2.48
          Mean value_function loss: 27.4103
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.7975
                       Mean reward: 940.63
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 186.6701
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 1.90s
                      Time elapsed: 01:04:25
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 50988 steps/s (collection: 1.807s, learning 0.121s)
             Mean action noise std: 2.48
          Mean value_function loss: 25.6197
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.8049
                       Mean reward: 950.96
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 1.0736
     Episode_Reward/lifting_object: 188.1606
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 1.93s
                      Time elapsed: 01:04:27
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 50655 steps/s (collection: 1.830s, learning 0.111s)
             Mean action noise std: 2.48
          Mean value_function loss: 26.6574
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.8130
                       Mean reward: 948.51
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0767
     Episode_Reward/lifting_object: 188.9762
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 1.94s
                      Time elapsed: 01:04:29
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 49276 steps/s (collection: 1.836s, learning 0.159s)
             Mean action noise std: 2.48
          Mean value_function loss: 25.1280
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.8193
                       Mean reward: 946.29
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 1.0742
     Episode_Reward/lifting_object: 188.6119
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 1.99s
                      Time elapsed: 01:04:31
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 48574 steps/s (collection: 1.843s, learning 0.181s)
             Mean action noise std: 2.48
          Mean value_function loss: 38.0483
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.8277
                       Mean reward: 940.13
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 185.9416
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.02s
                      Time elapsed: 01:04:33
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 49157 steps/s (collection: 1.880s, learning 0.120s)
             Mean action noise std: 2.48
          Mean value_function loss: 51.3984
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.8329
                       Mean reward: 908.21
               Mean episode length: 239.07
    Episode_Reward/reaching_object: 1.0510
     Episode_Reward/lifting_object: 184.5163
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.00s
                      Time elapsed: 01:04:35
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 51088 steps/s (collection: 1.797s, learning 0.127s)
             Mean action noise std: 2.48
          Mean value_function loss: 39.6385
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.8400
                       Mean reward: 948.58
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 1.0584
     Episode_Reward/lifting_object: 185.7819
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 1.92s
                      Time elapsed: 01:04:37
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 50636 steps/s (collection: 1.836s, learning 0.105s)
             Mean action noise std: 2.48
          Mean value_function loss: 31.7179
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.8434
                       Mean reward: 929.49
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.0589
     Episode_Reward/lifting_object: 185.7468
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 1.94s
                      Time elapsed: 01:04:39
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 50825 steps/s (collection: 1.815s, learning 0.119s)
             Mean action noise std: 2.48
          Mean value_function loss: 25.1345
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.8494
                       Mean reward: 934.67
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 1.0680
     Episode_Reward/lifting_object: 187.4599
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 1.93s
                      Time elapsed: 01:04:41
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 50627 steps/s (collection: 1.856s, learning 0.086s)
             Mean action noise std: 2.48
          Mean value_function loss: 42.4385
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.8566
                       Mean reward: 911.55
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 1.0481
     Episode_Reward/lifting_object: 183.7007
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.94s
                      Time elapsed: 01:04:43
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 49956 steps/s (collection: 1.853s, learning 0.115s)
             Mean action noise std: 2.49
          Mean value_function loss: 30.7325
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.8648
                       Mean reward: 945.71
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 1.0661
     Episode_Reward/lifting_object: 186.8679
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.97s
                      Time elapsed: 01:04:45
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 50157 steps/s (collection: 1.854s, learning 0.106s)
             Mean action noise std: 2.49
          Mean value_function loss: 38.0058
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.8770
                       Mean reward: 937.34
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 1.0586
     Episode_Reward/lifting_object: 185.0835
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 1.96s
                      Time elapsed: 01:04:47
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 52172 steps/s (collection: 1.785s, learning 0.099s)
             Mean action noise std: 2.49
          Mean value_function loss: 35.2872
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 40.8864
                       Mean reward: 952.40
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 1.0700
     Episode_Reward/lifting_object: 187.0718
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 1.88s
                      Time elapsed: 01:04:49
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 51514 steps/s (collection: 1.819s, learning 0.089s)
             Mean action noise std: 2.49
          Mean value_function loss: 35.1375
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 40.8894
                       Mean reward: 940.32
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 1.0641
     Episode_Reward/lifting_object: 185.7857
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 1.91s
                      Time elapsed: 01:04:51
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 51995 steps/s (collection: 1.791s, learning 0.100s)
             Mean action noise std: 2.49
          Mean value_function loss: 41.0223
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.8908
                       Mean reward: 906.21
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.0599
     Episode_Reward/lifting_object: 184.7543
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 1.89s
                      Time elapsed: 01:04:53
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 47167 steps/s (collection: 1.915s, learning 0.169s)
             Mean action noise std: 2.49
          Mean value_function loss: 24.3888
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.8949
                       Mean reward: 937.43
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 1.0636
     Episode_Reward/lifting_object: 184.8210
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.08s
                      Time elapsed: 01:04:55
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 50258 steps/s (collection: 1.856s, learning 0.100s)
             Mean action noise std: 2.49
          Mean value_function loss: 32.7159
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.9017
                       Mean reward: 933.13
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 1.0690
     Episode_Reward/lifting_object: 186.8392
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.96s
                      Time elapsed: 01:04:57
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 48864 steps/s (collection: 1.886s, learning 0.126s)
             Mean action noise std: 2.49
          Mean value_function loss: 30.7487
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.9071
                       Mean reward: 925.64
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 1.0647
     Episode_Reward/lifting_object: 186.4550
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.01s
                      Time elapsed: 01:04:59
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 49654 steps/s (collection: 1.886s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 54.9366
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.9153
                       Mean reward: 913.58
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.0447
     Episode_Reward/lifting_object: 182.6642
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 1.98s
                      Time elapsed: 01:05:01
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 52124 steps/s (collection: 1.793s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 30.7658
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.9197
                       Mean reward: 931.15
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 1.0690
     Episode_Reward/lifting_object: 187.3362
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 1.89s
                      Time elapsed: 01:05:03
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 51400 steps/s (collection: 1.798s, learning 0.115s)
             Mean action noise std: 2.49
          Mean value_function loss: 55.0925
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.9236
                       Mean reward: 930.14
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 184.9164
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.91s
                      Time elapsed: 01:05:04
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 51226 steps/s (collection: 1.798s, learning 0.121s)
             Mean action noise std: 2.49
          Mean value_function loss: 45.1751
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.9265
                       Mean reward: 956.18
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 183.2389
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 1.92s
                      Time elapsed: 01:05:06
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 49778 steps/s (collection: 1.850s, learning 0.125s)
             Mean action noise std: 2.49
          Mean value_function loss: 55.3810
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.9277
                       Mean reward: 914.55
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 1.0531
     Episode_Reward/lifting_object: 184.9993
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.97s
                      Time elapsed: 01:05:08
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 49792 steps/s (collection: 1.829s, learning 0.146s)
             Mean action noise std: 2.49
          Mean value_function loss: 55.3765
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.9313
                       Mean reward: 928.38
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 186.1593
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 1.97s
                      Time elapsed: 01:05:10
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 49394 steps/s (collection: 1.858s, learning 0.133s)
             Mean action noise std: 2.49
          Mean value_function loss: 35.4745
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.9370
                       Mean reward: 937.46
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 1.0483
     Episode_Reward/lifting_object: 184.6746
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 1.99s
                      Time elapsed: 01:05:12
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 49641 steps/s (collection: 1.840s, learning 0.141s)
             Mean action noise std: 2.50
          Mean value_function loss: 24.0375
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.9424
                       Mean reward: 929.30
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 1.0624
     Episode_Reward/lifting_object: 187.1382
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 1.98s
                      Time elapsed: 01:05:14
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 50851 steps/s (collection: 1.822s, learning 0.112s)
             Mean action noise std: 2.50
          Mean value_function loss: 53.6320
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.9513
                       Mean reward: 901.64
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 1.0381
     Episode_Reward/lifting_object: 182.7157
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 1.93s
                      Time elapsed: 01:05:16
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 47033 steps/s (collection: 1.960s, learning 0.131s)
             Mean action noise std: 2.50
          Mean value_function loss: 40.6405
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.9588
                       Mean reward: 940.48
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 1.0504
     Episode_Reward/lifting_object: 185.0980
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.09s
                      Time elapsed: 01:05:18
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 46156 steps/s (collection: 1.989s, learning 0.141s)
             Mean action noise std: 2.50
          Mean value_function loss: 28.6833
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 40.9660
                       Mean reward: 941.24
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 185.5472
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.13s
                      Time elapsed: 01:05:20
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 50603 steps/s (collection: 1.855s, learning 0.088s)
             Mean action noise std: 2.50
          Mean value_function loss: 34.9272
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.9698
                       Mean reward: 932.28
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 186.0336
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 1.94s
                      Time elapsed: 01:05:22
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 51593 steps/s (collection: 1.804s, learning 0.101s)
             Mean action noise std: 2.50
          Mean value_function loss: 16.4211
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.9744
                       Mean reward: 951.47
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 188.0642
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 1.91s
                      Time elapsed: 01:05:24
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 51507 steps/s (collection: 1.783s, learning 0.126s)
             Mean action noise std: 2.50
          Mean value_function loss: 49.0004
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 40.9784
                       Mean reward: 947.53
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 1.0538
     Episode_Reward/lifting_object: 185.2889
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 1.91s
                      Time elapsed: 01:05:26
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 52648 steps/s (collection: 1.781s, learning 0.087s)
             Mean action noise std: 2.50
          Mean value_function loss: 41.5629
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.9817
                       Mean reward: 911.26
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 185.8979
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 1.87s
                      Time elapsed: 01:05:28
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 51418 steps/s (collection: 1.816s, learning 0.096s)
             Mean action noise std: 2.50
          Mean value_function loss: 32.1209
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 40.9886
                       Mean reward: 948.50
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 1.0610
     Episode_Reward/lifting_object: 186.4346
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 1.91s
                      Time elapsed: 01:05:30
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 51844 steps/s (collection: 1.799s, learning 0.097s)
             Mean action noise std: 2.50
          Mean value_function loss: 90.5079
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.9978
                       Mean reward: 903.55
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.0529
     Episode_Reward/lifting_object: 185.3793
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 1.90s
                      Time elapsed: 01:05:32
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 47428 steps/s (collection: 1.918s, learning 0.155s)
             Mean action noise std: 2.50
          Mean value_function loss: 38.2323
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.0084
                       Mean reward: 936.27
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 184.3027
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.07s
                      Time elapsed: 01:05:34
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 50695 steps/s (collection: 1.839s, learning 0.100s)
             Mean action noise std: 2.50
          Mean value_function loss: 30.6229
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.0169
                       Mean reward: 930.85
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 1.0434
     Episode_Reward/lifting_object: 183.5939
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 1.94s
                      Time elapsed: 01:05:36
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 51684 steps/s (collection: 1.806s, learning 0.096s)
             Mean action noise std: 2.51
          Mean value_function loss: 32.0068
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.0231
                       Mean reward: 933.90
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 1.0550
     Episode_Reward/lifting_object: 185.5790
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 1.90s
                      Time elapsed: 01:05:38
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 49694 steps/s (collection: 1.823s, learning 0.155s)
             Mean action noise std: 2.51
          Mean value_function loss: 23.2094
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.0292
                       Mean reward: 945.53
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 185.6987
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 1.98s
                      Time elapsed: 01:05:40
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 46865 steps/s (collection: 2.004s, learning 0.093s)
             Mean action noise std: 2.51
          Mean value_function loss: 30.7335
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.0387
                       Mean reward: 927.67
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 185.7823
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.10s
                      Time elapsed: 01:05:42
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 49384 steps/s (collection: 1.839s, learning 0.151s)
             Mean action noise std: 2.51
          Mean value_function loss: 50.1112
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.0436
                       Mean reward: 918.68
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.0481
     Episode_Reward/lifting_object: 184.1914
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 1.99s
                      Time elapsed: 01:05:44
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 49178 steps/s (collection: 1.880s, learning 0.118s)
             Mean action noise std: 2.51
          Mean value_function loss: 344.6712
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.0486
                       Mean reward: 939.03
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 1.0544
     Episode_Reward/lifting_object: 185.3563
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.00s
                      Time elapsed: 01:05:46
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 51137 steps/s (collection: 1.835s, learning 0.087s)
             Mean action noise std: 2.51
          Mean value_function loss: 748.2758
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 41.0533
                       Mean reward: 941.58
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 185.8891
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 1.92s
                      Time elapsed: 01:05:48
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 51552 steps/s (collection: 1.822s, learning 0.085s)
             Mean action noise std: 2.51
          Mean value_function loss: 41.1947
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.0568
                       Mean reward: 929.46
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 184.8398
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 1.91s
                      Time elapsed: 01:05:50
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 50327 steps/s (collection: 1.839s, learning 0.114s)
             Mean action noise std: 2.51
          Mean value_function loss: 29.6696
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.0636
                       Mean reward: 946.51
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 187.0519
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 1.95s
                      Time elapsed: 01:05:52
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 51589 steps/s (collection: 1.785s, learning 0.121s)
             Mean action noise std: 2.51
          Mean value_function loss: 46.5370
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.0703
                       Mean reward: 910.06
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 186.1306
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 1.91s
                      Time elapsed: 01:05:54
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 50933 steps/s (collection: 1.836s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 57.5548
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.0734
                       Mean reward: 901.73
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 182.7850
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.93s
                      Time elapsed: 01:05:56
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 46647 steps/s (collection: 2.000s, learning 0.108s)
             Mean action noise std: 2.51
          Mean value_function loss: 39.7812
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.0798
                       Mean reward: 931.40
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: 184.4064
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.11s
                      Time elapsed: 01:05:58
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 51205 steps/s (collection: 1.832s, learning 0.088s)
             Mean action noise std: 2.51
          Mean value_function loss: 44.5262
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.0861
                       Mean reward: 940.35
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 1.0607
     Episode_Reward/lifting_object: 185.4827
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 1.92s
                      Time elapsed: 01:06:00
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 50707 steps/s (collection: 1.818s, learning 0.121s)
             Mean action noise std: 2.51
          Mean value_function loss: 48.9885
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.0915
                       Mean reward: 907.48
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.0526
     Episode_Reward/lifting_object: 183.8306
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.94s
                      Time elapsed: 01:06:01
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 51230 steps/s (collection: 1.772s, learning 0.147s)
             Mean action noise std: 2.51
          Mean value_function loss: 32.6781
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.0954
                       Mean reward: 927.59
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 1.0714
     Episode_Reward/lifting_object: 187.0919
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.92s
                      Time elapsed: 01:06:03
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 51269 steps/s (collection: 1.776s, learning 0.141s)
             Mean action noise std: 2.51
          Mean value_function loss: 37.7093
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 41.0967
                       Mean reward: 934.63
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 1.0698
     Episode_Reward/lifting_object: 186.7257
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.92s
                      Time elapsed: 01:06:05
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 49681 steps/s (collection: 1.837s, learning 0.142s)
             Mean action noise std: 2.51
          Mean value_function loss: 36.3214
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.0977
                       Mean reward: 916.29
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 1.0630
     Episode_Reward/lifting_object: 185.8477
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.98s
                      Time elapsed: 01:06:07
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 49964 steps/s (collection: 1.861s, learning 0.106s)
             Mean action noise std: 2.51
          Mean value_function loss: 31.8759
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 41.0986
                       Mean reward: 927.31
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 1.0643
     Episode_Reward/lifting_object: 185.6636
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.97s
                      Time elapsed: 01:06:09
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 47825 steps/s (collection: 1.911s, learning 0.144s)
             Mean action noise std: 2.51
          Mean value_function loss: 48.0169
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 41.0997
                       Mean reward: 920.72
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 184.6841
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.06s
                      Time elapsed: 01:06:11
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 48738 steps/s (collection: 1.883s, learning 0.134s)
             Mean action noise std: 2.52
          Mean value_function loss: 28.5793
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.1022
                       Mean reward: 918.49
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 185.4319
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.02s
                      Time elapsed: 01:06:13
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 48919 steps/s (collection: 1.870s, learning 0.140s)
             Mean action noise std: 2.52
          Mean value_function loss: 39.9746
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.1073
                       Mean reward: 937.71
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 184.3998
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.01s
                      Time elapsed: 01:06:15
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 49450 steps/s (collection: 1.856s, learning 0.132s)
             Mean action noise std: 2.52
          Mean value_function loss: 25.5739
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.1110
                       Mean reward: 950.45
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 1.0763
     Episode_Reward/lifting_object: 188.8594
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 1.99s
                      Time elapsed: 01:06:17
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 49915 steps/s (collection: 1.882s, learning 0.087s)
             Mean action noise std: 2.52
          Mean value_function loss: 37.7802
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.1135
                       Mean reward: 932.95
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 185.3141
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.97s
                      Time elapsed: 01:06:19
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 50289 steps/s (collection: 1.836s, learning 0.119s)
             Mean action noise std: 2.52
          Mean value_function loss: 29.7129
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.1157
                       Mean reward: 932.52
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 1.0715
     Episode_Reward/lifting_object: 188.1036
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 1.95s
                      Time elapsed: 01:06:21
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 48480 steps/s (collection: 1.885s, learning 0.143s)
             Mean action noise std: 2.52
          Mean value_function loss: 23.6525
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.1182
                       Mean reward: 943.47
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 185.9769
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.03s
                      Time elapsed: 01:06:23
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 50189 steps/s (collection: 1.860s, learning 0.099s)
             Mean action noise std: 2.52
          Mean value_function loss: 33.4483
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.1226
                       Mean reward: 921.50
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 185.0048
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.96s
                      Time elapsed: 01:06:25
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 46414 steps/s (collection: 1.993s, learning 0.125s)
             Mean action noise std: 2.52
          Mean value_function loss: 38.4704
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.1280
                       Mean reward: 929.92
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 184.8470
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.12s
                      Time elapsed: 01:06:27
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 48752 steps/s (collection: 1.855s, learning 0.162s)
             Mean action noise std: 2.52
          Mean value_function loss: 46.5852
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.1339
                       Mean reward: 929.91
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 181.7560
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.02s
                      Time elapsed: 01:06:29
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 49722 steps/s (collection: 1.834s, learning 0.143s)
             Mean action noise std: 2.52
          Mean value_function loss: 49.6519
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.1411
                       Mean reward: 928.87
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 182.3471
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 1.98s
                      Time elapsed: 01:06:31
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 50573 steps/s (collection: 1.831s, learning 0.113s)
             Mean action noise std: 2.52
          Mean value_function loss: 36.6395
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.1480
                       Mean reward: 912.07
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 1.0601
     Episode_Reward/lifting_object: 185.4474
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 1.94s
                      Time elapsed: 01:06:33
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 50750 steps/s (collection: 1.845s, learning 0.092s)
             Mean action noise std: 2.52
          Mean value_function loss: 29.6206
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.1561
                       Mean reward: 924.12
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 186.1434
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 1.94s
                      Time elapsed: 01:06:35
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 50579 steps/s (collection: 1.838s, learning 0.106s)
             Mean action noise std: 2.52
          Mean value_function loss: 40.4804
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.1632
                       Mean reward: 947.66
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 1.0789
     Episode_Reward/lifting_object: 188.3407
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 1.94s
                      Time elapsed: 01:06:37
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 50629 steps/s (collection: 1.829s, learning 0.112s)
             Mean action noise std: 2.52
          Mean value_function loss: 31.7067
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.1690
                       Mean reward: 956.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 184.2155
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 1.94s
                      Time elapsed: 01:06:39
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 50352 steps/s (collection: 1.836s, learning 0.116s)
             Mean action noise std: 2.53
          Mean value_function loss: 33.0310
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.1763
                       Mean reward: 929.35
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 1.0623
     Episode_Reward/lifting_object: 185.1517
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 1.95s
                      Time elapsed: 01:06:41
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 49649 steps/s (collection: 1.891s, learning 0.089s)
             Mean action noise std: 2.53
          Mean value_function loss: 37.6198
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.1859
                       Mean reward: 935.42
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 1.0638
     Episode_Reward/lifting_object: 185.8114
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 1.98s
                      Time elapsed: 01:06:43
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 48390 steps/s (collection: 1.940s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 36.8888
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.1953
                       Mean reward: 939.49
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 1.0585
     Episode_Reward/lifting_object: 184.5386
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.03s
                      Time elapsed: 01:06:45
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 48130 steps/s (collection: 1.948s, learning 0.095s)
             Mean action noise std: 2.53
          Mean value_function loss: 42.9387
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.2013
                       Mean reward: 932.04
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 184.0387
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.04s
                      Time elapsed: 01:06:47
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 51989 steps/s (collection: 1.793s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 27.0071
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.2072
                       Mean reward: 942.78
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 186.5107
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 1.89s
                      Time elapsed: 01:06:49
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 47839 steps/s (collection: 1.912s, learning 0.143s)
             Mean action noise std: 2.53
          Mean value_function loss: 35.8529
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.2103
                       Mean reward: 934.29
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 186.4584
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.05s
                      Time elapsed: 01:06:51
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 50044 steps/s (collection: 1.861s, learning 0.103s)
             Mean action noise std: 2.53
          Mean value_function loss: 61.0989
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.2138
                       Mean reward: 907.59
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 181.7337
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 1.96s
                      Time elapsed: 01:06:53
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 50451 steps/s (collection: 1.841s, learning 0.107s)
             Mean action noise std: 2.53
          Mean value_function loss: 70.4784
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.2155
                       Mean reward: 904.97
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.0352
     Episode_Reward/lifting_object: 180.6133
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 1.95s
                      Time elapsed: 01:06:55
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 50925 steps/s (collection: 1.823s, learning 0.108s)
             Mean action noise std: 2.53
          Mean value_function loss: 51.7003
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.2182
                       Mean reward: 920.78
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 184.1809
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 1.93s
                      Time elapsed: 01:06:57
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 49552 steps/s (collection: 1.875s, learning 0.109s)
             Mean action noise std: 2.53
          Mean value_function loss: 43.8627
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.2243
                       Mean reward: 925.08
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 1.0543
     Episode_Reward/lifting_object: 184.5765
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 1.98s
                      Time elapsed: 01:06:59
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 49228 steps/s (collection: 1.899s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 53.2348
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.2279
                       Mean reward: 935.94
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 185.0302
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.00s
                      Time elapsed: 01:07:01
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 49652 steps/s (collection: 1.873s, learning 0.107s)
             Mean action noise std: 2.53
          Mean value_function loss: 40.2843
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.2310
                       Mean reward: 917.68
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 184.4070
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 1.98s
                      Time elapsed: 01:07:03
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 50385 steps/s (collection: 1.858s, learning 0.093s)
             Mean action noise std: 2.53
          Mean value_function loss: 37.0965
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.2372
                       Mean reward: 937.18
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 185.0583
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 1.95s
                      Time elapsed: 01:07:05
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 50107 steps/s (collection: 1.813s, learning 0.149s)
             Mean action noise std: 2.54
          Mean value_function loss: 42.0706
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.2459
                       Mean reward: 929.86
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0417
     Episode_Reward/lifting_object: 182.5401
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 1.96s
                      Time elapsed: 01:07:07
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 48619 steps/s (collection: 1.835s, learning 0.187s)
             Mean action noise std: 2.54
          Mean value_function loss: 43.0008
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.2528
                       Mean reward: 944.17
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 1.0582
     Episode_Reward/lifting_object: 185.7802
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.02s
                      Time elapsed: 01:07:09
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 49269 steps/s (collection: 1.850s, learning 0.146s)
             Mean action noise std: 2.54
          Mean value_function loss: 36.1327
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 41.2547
                       Mean reward: 928.65
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 185.9027
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.00s
                      Time elapsed: 01:07:11
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 50760 steps/s (collection: 1.806s, learning 0.131s)
             Mean action noise std: 2.54
          Mean value_function loss: 44.0782
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 41.2554
                       Mean reward: 953.98
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 185.0155
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 1.94s
                      Time elapsed: 01:07:13
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 51756 steps/s (collection: 1.796s, learning 0.103s)
             Mean action noise std: 2.54
          Mean value_function loss: 66.9126
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 41.2559
                       Mean reward: 921.38
               Mean episode length: 241.99
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: 183.5659
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 1.90s
                      Time elapsed: 01:07:15
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 51673 steps/s (collection: 1.791s, learning 0.111s)
             Mean action noise std: 2.54
          Mean value_function loss: 83.1128
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.2563
                       Mean reward: 924.17
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 1.0498
     Episode_Reward/lifting_object: 183.9103
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 1.90s
                      Time elapsed: 01:07:17
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 51839 steps/s (collection: 1.807s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 93.7780
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.2570
                       Mean reward: 905.94
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.0344
     Episode_Reward/lifting_object: 180.7940
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 1.90s
                      Time elapsed: 01:07:18
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 50780 steps/s (collection: 1.815s, learning 0.121s)
             Mean action noise std: 2.54
          Mean value_function loss: 70.9608
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.2601
                       Mean reward: 937.20
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 181.8465
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 1.94s
                      Time elapsed: 01:07:20
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 46196 steps/s (collection: 1.993s, learning 0.135s)
             Mean action noise std: 2.54
          Mean value_function loss: 57.8645
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.2648
                       Mean reward: 934.75
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 1.0444
     Episode_Reward/lifting_object: 182.1689
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.13s
                      Time elapsed: 01:07:23
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 50348 steps/s (collection: 1.858s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 48.2924
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.2697
                       Mean reward: 932.43
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 1.0671
     Episode_Reward/lifting_object: 186.0718
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 1.95s
                      Time elapsed: 01:07:24
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 50280 steps/s (collection: 1.852s, learning 0.103s)
             Mean action noise std: 2.54
          Mean value_function loss: 43.6689
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 41.2779
                       Mean reward: 935.72
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 184.0412
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 1.96s
                      Time elapsed: 01:07:26
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 48571 steps/s (collection: 1.929s, learning 0.095s)
             Mean action noise std: 2.54
          Mean value_function loss: 46.9622
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.2794
                       Mean reward: 918.91
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 183.9639
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.02s
                      Time elapsed: 01:07:28
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 50842 steps/s (collection: 1.843s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 36.3541
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 41.2807
                       Mean reward: 944.24
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 1.0666
     Episode_Reward/lifting_object: 186.3777
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 1.93s
                      Time elapsed: 01:07:30
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 51129 steps/s (collection: 1.830s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 43.1080
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.2821
                       Mean reward: 946.49
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 1.0740
     Episode_Reward/lifting_object: 187.5611
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 1.92s
                      Time elapsed: 01:07:32
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 51115 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 2.54
          Mean value_function loss: 40.8439
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.2851
                       Mean reward: 926.94
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.0659
     Episode_Reward/lifting_object: 186.1144
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 1.92s
                      Time elapsed: 01:07:34
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 50647 steps/s (collection: 1.834s, learning 0.107s)
             Mean action noise std: 2.54
          Mean value_function loss: 42.4340
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.2902
                       Mean reward: 917.68
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.0488
     Episode_Reward/lifting_object: 183.1466
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 1.94s
                      Time elapsed: 01:07:36
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 50301 steps/s (collection: 1.848s, learning 0.106s)
             Mean action noise std: 2.54
          Mean value_function loss: 43.9216
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.2981
                       Mean reward: 949.44
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 1.0607
     Episode_Reward/lifting_object: 185.6212
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 1.95s
                      Time elapsed: 01:07:38
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 50326 steps/s (collection: 1.848s, learning 0.105s)
             Mean action noise std: 2.54
          Mean value_function loss: 41.5245
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3066
                       Mean reward: 934.36
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 1.0461
     Episode_Reward/lifting_object: 183.1051
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 1.95s
                      Time elapsed: 01:07:40
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 50180 steps/s (collection: 1.860s, learning 0.099s)
             Mean action noise std: 2.54
          Mean value_function loss: 37.9966
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.3119
                       Mean reward: 943.73
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0550
     Episode_Reward/lifting_object: 184.5286
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 1.96s
                      Time elapsed: 01:07:42
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 49449 steps/s (collection: 1.856s, learning 0.132s)
             Mean action noise std: 2.55
          Mean value_function loss: 34.8454
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.3187
                       Mean reward: 941.55
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 185.5339
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 1.99s
                      Time elapsed: 01:07:44
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 49120 steps/s (collection: 1.863s, learning 0.138s)
             Mean action noise std: 2.55
          Mean value_function loss: 35.5842
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3259
                       Mean reward: 921.37
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 186.0877
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.00s
                      Time elapsed: 01:07:46
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 49430 steps/s (collection: 1.865s, learning 0.124s)
             Mean action noise std: 2.55
          Mean value_function loss: 23.7688
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 41.3283
                       Mean reward: 942.32
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 184.6087
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 1.99s
                      Time elapsed: 01:07:48
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 49247 steps/s (collection: 1.864s, learning 0.133s)
             Mean action noise std: 2.55
          Mean value_function loss: 28.8098
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 41.3296
                       Mean reward: 936.64
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 1.0655
     Episode_Reward/lifting_object: 186.5716
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.00s
                      Time elapsed: 01:07:50
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 49339 steps/s (collection: 1.867s, learning 0.125s)
             Mean action noise std: 2.55
          Mean value_function loss: 23.5117
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.3310
                       Mean reward: 957.11
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 1.0756
     Episode_Reward/lifting_object: 188.6498
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 1.99s
                      Time elapsed: 01:07:52
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 49777 steps/s (collection: 1.847s, learning 0.128s)
             Mean action noise std: 2.55
          Mean value_function loss: 28.3024
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3322
                       Mean reward: 940.66
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 187.0925
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 1.97s
                      Time elapsed: 01:07:54
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 49352 steps/s (collection: 1.860s, learning 0.132s)
             Mean action noise std: 2.55
          Mean value_function loss: 35.7803
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.3345
                       Mean reward: 921.29
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 186.1738
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 1.99s
                      Time elapsed: 01:07:56
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 49714 steps/s (collection: 1.866s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 37.8804
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.3366
                       Mean reward: 943.93
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 186.0007
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 1.98s
                      Time elapsed: 01:07:58
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 48578 steps/s (collection: 1.899s, learning 0.125s)
             Mean action noise std: 2.55
          Mean value_function loss: 37.2090
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.3390
                       Mean reward: 942.58
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 186.1502
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.02s
                      Time elapsed: 01:08:00
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 49207 steps/s (collection: 1.869s, learning 0.129s)
             Mean action noise std: 2.55
          Mean value_function loss: 34.6814
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.3454
                       Mean reward: 947.32
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 186.5523
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.00s
                      Time elapsed: 01:08:02
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 50335 steps/s (collection: 1.840s, learning 0.113s)
             Mean action noise std: 2.55
          Mean value_function loss: 43.0265
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.3503
                       Mean reward: 945.91
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0575
     Episode_Reward/lifting_object: 186.1250
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 1.95s
                      Time elapsed: 01:08:04
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 50414 steps/s (collection: 1.829s, learning 0.121s)
             Mean action noise std: 2.55
          Mean value_function loss: 32.7395
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.3547
                       Mean reward: 899.03
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 183.8692
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 1.95s
                      Time elapsed: 01:08:06
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 50281 steps/s (collection: 1.856s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 25.3952
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.3585
                       Mean reward: 949.78
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 1.0711
     Episode_Reward/lifting_object: 189.1649
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 1.96s
                      Time elapsed: 01:08:08
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 49710 steps/s (collection: 1.864s, learning 0.114s)
             Mean action noise std: 2.55
          Mean value_function loss: 30.4573
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.3672
                       Mean reward: 938.08
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 186.5361
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 1.98s
                      Time elapsed: 01:08:10
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 49407 steps/s (collection: 1.880s, learning 0.110s)
             Mean action noise std: 2.55
          Mean value_function loss: 34.2731
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.3750
                       Mean reward: 945.68
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 186.3057
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 1.99s
                      Time elapsed: 01:08:12
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 49979 steps/s (collection: 1.872s, learning 0.095s)
             Mean action noise std: 2.55
          Mean value_function loss: 36.1099
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3832
                       Mean reward: 954.60
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 1.0562
     Episode_Reward/lifting_object: 186.6313
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 1.97s
                      Time elapsed: 01:08:14
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 49431 steps/s (collection: 1.881s, learning 0.108s)
             Mean action noise std: 2.56
          Mean value_function loss: 49.1898
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3917
                       Mean reward: 937.12
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 184.8984
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 1.99s
                      Time elapsed: 01:08:16
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 47793 steps/s (collection: 1.944s, learning 0.113s)
             Mean action noise std: 2.56
          Mean value_function loss: 40.6253
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.3978
                       Mean reward: 929.80
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.0495
     Episode_Reward/lifting_object: 184.8381
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.06s
                      Time elapsed: 01:08:18
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 49846 steps/s (collection: 1.863s, learning 0.109s)
             Mean action noise std: 2.56
          Mean value_function loss: 29.2117
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.4066
                       Mean reward: 939.37
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 1.0476
     Episode_Reward/lifting_object: 184.3362
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 1.97s
                      Time elapsed: 01:08:20
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 49288 steps/s (collection: 1.881s, learning 0.113s)
             Mean action noise std: 2.56
          Mean value_function loss: 31.4091
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.4149
                       Mean reward: 947.50
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 1.0677
     Episode_Reward/lifting_object: 188.2949
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 1.99s
                      Time elapsed: 01:08:22
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 48568 steps/s (collection: 1.896s, learning 0.128s)
             Mean action noise std: 2.56
          Mean value_function loss: 39.5964
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 41.4209
                       Mean reward: 928.15
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 1.0590
     Episode_Reward/lifting_object: 186.6989
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.02s
                      Time elapsed: 01:08:24
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 48688 steps/s (collection: 1.913s, learning 0.106s)
             Mean action noise std: 2.56
          Mean value_function loss: 36.4163
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.4225
                       Mean reward: 923.41
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 186.1003
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.02s
                      Time elapsed: 01:08:26
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 49759 steps/s (collection: 1.870s, learning 0.106s)
             Mean action noise std: 2.56
          Mean value_function loss: 37.1611
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.4257
                       Mean reward: 909.54
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 185.0623
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 1.98s
                      Time elapsed: 01:08:28
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 49532 steps/s (collection: 1.883s, learning 0.102s)
             Mean action noise std: 2.56
          Mean value_function loss: 38.0895
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.4304
                       Mean reward: 951.99
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 186.7468
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 1.98s
                      Time elapsed: 01:08:30
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 47045 steps/s (collection: 1.901s, learning 0.188s)
             Mean action noise std: 2.56
          Mean value_function loss: 44.6949
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.4389
                       Mean reward: 900.92
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.0439
     Episode_Reward/lifting_object: 184.1277
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.09s
                      Time elapsed: 01:08:32
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 49405 steps/s (collection: 1.876s, learning 0.114s)
             Mean action noise std: 2.56
          Mean value_function loss: 43.6195
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.4499
                       Mean reward: 935.05
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 1.0469
     Episode_Reward/lifting_object: 185.0641
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 1.99s
                      Time elapsed: 01:08:34
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 48557 steps/s (collection: 1.897s, learning 0.127s)
             Mean action noise std: 2.56
          Mean value_function loss: 43.9552
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 41.4607
                       Mean reward: 885.73
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.0378
     Episode_Reward/lifting_object: 183.0611
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.02s
                      Time elapsed: 01:08:36
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 48650 steps/s (collection: 1.900s, learning 0.121s)
             Mean action noise std: 2.57
          Mean value_function loss: 47.4302
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.4659
                       Mean reward: 935.09
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 1.0492
     Episode_Reward/lifting_object: 185.6740
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.02s
                      Time elapsed: 01:08:38
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 48135 steps/s (collection: 1.880s, learning 0.163s)
             Mean action noise std: 2.57
          Mean value_function loss: 42.4030
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.4710
                       Mean reward: 920.09
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 1.0448
     Episode_Reward/lifting_object: 184.8881
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.04s
                      Time elapsed: 01:08:40
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 48991 steps/s (collection: 1.865s, learning 0.141s)
             Mean action noise std: 2.57
          Mean value_function loss: 29.5180
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.4773
                       Mean reward: 945.07
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 187.0830
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.01s
                      Time elapsed: 01:08:42
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 47963 steps/s (collection: 1.903s, learning 0.147s)
             Mean action noise std: 2.57
          Mean value_function loss: 25.3995
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.4836
                       Mean reward: 943.37
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 1.0575
     Episode_Reward/lifting_object: 187.0575
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.05s
                      Time elapsed: 01:08:44
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 49556 steps/s (collection: 1.880s, learning 0.104s)
             Mean action noise std: 2.57
          Mean value_function loss: 35.1381
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.4887
                       Mean reward: 929.54
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 187.2614
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 1.98s
                      Time elapsed: 01:08:46
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 48039 steps/s (collection: 1.909s, learning 0.138s)
             Mean action noise std: 2.57
          Mean value_function loss: 34.0839
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4913
                       Mean reward: 922.82
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 1.0398
     Episode_Reward/lifting_object: 183.4503
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.05s
                      Time elapsed: 01:08:48
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 49603 steps/s (collection: 1.859s, learning 0.123s)
             Mean action noise std: 2.57
          Mean value_function loss: 27.6631
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.4972
                       Mean reward: 933.64
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 1.0597
     Episode_Reward/lifting_object: 186.9740
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 1.98s
                      Time elapsed: 01:08:50
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 47859 steps/s (collection: 1.912s, learning 0.142s)
             Mean action noise std: 2.57
          Mean value_function loss: 30.7001
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.5019
                       Mean reward: 942.53
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 1.0643
     Episode_Reward/lifting_object: 187.7090
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.05s
                      Time elapsed: 01:08:52
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 47536 steps/s (collection: 1.924s, learning 0.144s)
             Mean action noise std: 2.57
          Mean value_function loss: 36.8384
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.5053
                       Mean reward: 913.42
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 186.6248
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.07s
                      Time elapsed: 01:08:54
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 48376 steps/s (collection: 1.915s, learning 0.117s)
             Mean action noise std: 2.57
          Mean value_function loss: 42.5567
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.5110
                       Mean reward: 935.81
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 185.9111
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.03s
                      Time elapsed: 01:08:56
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 47373 steps/s (collection: 1.949s, learning 0.126s)
             Mean action noise std: 2.57
          Mean value_function loss: 47.8450
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.5147
                       Mean reward: 933.04
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 1.0456
     Episode_Reward/lifting_object: 184.2614
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.08s
                      Time elapsed: 01:08:58
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 49070 steps/s (collection: 1.881s, learning 0.122s)
             Mean action noise std: 2.57
          Mean value_function loss: 32.8537
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.5173
                       Mean reward: 938.18
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 186.4621
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.00s
                      Time elapsed: 01:09:00
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 49199 steps/s (collection: 1.878s, learning 0.120s)
             Mean action noise std: 2.57
          Mean value_function loss: 32.2555
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.5221
                       Mean reward: 936.64
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: 185.0009
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.00s
                      Time elapsed: 01:09:02
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 49036 steps/s (collection: 1.906s, learning 0.099s)
             Mean action noise std: 2.57
          Mean value_function loss: 41.2359
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.5309
                       Mean reward: 920.40
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 1.0416
     Episode_Reward/lifting_object: 184.2771
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.00s
                      Time elapsed: 01:09:04
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 48787 steps/s (collection: 1.921s, learning 0.094s)
             Mean action noise std: 2.57
          Mean value_function loss: 38.1637
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.5365
                       Mean reward: 929.64
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 187.3438
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.01s
                      Time elapsed: 01:09:06
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 48197 steps/s (collection: 1.918s, learning 0.122s)
             Mean action noise std: 2.58
          Mean value_function loss: 44.2071
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.5405
                       Mean reward: 912.03
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 184.6657
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.04s
                      Time elapsed: 01:09:08
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 49257 steps/s (collection: 1.892s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 32.0838
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.5469
                       Mean reward: 949.50
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 1.0485
     Episode_Reward/lifting_object: 186.3130
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.00s
                      Time elapsed: 01:09:10
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 48651 steps/s (collection: 1.923s, learning 0.098s)
             Mean action noise std: 2.58
          Mean value_function loss: 44.5163
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.5574
                       Mean reward: 900.73
               Mean episode length: 237.14
    Episode_Reward/reaching_object: 1.0424
     Episode_Reward/lifting_object: 185.2700
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.02s
                      Time elapsed: 01:09:12
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 48353 steps/s (collection: 1.934s, learning 0.099s)
             Mean action noise std: 2.58
          Mean value_function loss: 42.0676
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 41.5679
                       Mean reward: 925.41
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 1.0317
     Episode_Reward/lifting_object: 183.2210
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.03s
                      Time elapsed: 01:09:14
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 47981 steps/s (collection: 1.933s, learning 0.116s)
             Mean action noise std: 2.58
          Mean value_function loss: 40.0389
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.5706
                       Mean reward: 930.80
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 1.0268
     Episode_Reward/lifting_object: 182.4181
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.05s
                      Time elapsed: 01:09:16
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 48426 steps/s (collection: 1.911s, learning 0.119s)
             Mean action noise std: 2.58
          Mean value_function loss: 40.1919
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 41.5716
                       Mean reward: 948.29
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 186.0357
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.03s
                      Time elapsed: 01:09:18
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 49330 steps/s (collection: 1.880s, learning 0.113s)
             Mean action noise std: 2.58
          Mean value_function loss: 46.7155
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 41.5725
                       Mean reward: 946.96
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 187.2539
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 1.99s
                      Time elapsed: 01:09:20
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 49844 steps/s (collection: 1.871s, learning 0.101s)
             Mean action noise std: 2.58
          Mean value_function loss: 45.5631
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 41.5733
                       Mean reward: 928.34
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.0266
     Episode_Reward/lifting_object: 182.0856
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 1.97s
                      Time elapsed: 01:09:22
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 49616 steps/s (collection: 1.870s, learning 0.111s)
             Mean action noise std: 2.58
          Mean value_function loss: 48.0148
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 41.5744
                       Mean reward: 915.24
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 1.0270
     Episode_Reward/lifting_object: 181.6706
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 1.98s
                      Time elapsed: 01:09:24
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 49652 steps/s (collection: 1.881s, learning 0.099s)
             Mean action noise std: 2.58
          Mean value_function loss: 31.8478
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.5769
                       Mean reward: 952.65
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 184.8660
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 1.98s
                      Time elapsed: 01:09:26
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 48152 steps/s (collection: 1.914s, learning 0.128s)
             Mean action noise std: 2.58
          Mean value_function loss: 45.1420
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.5829
                       Mean reward: 919.92
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 185.4425
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.04s
                      Time elapsed: 01:09:28
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 48022 steps/s (collection: 1.947s, learning 0.100s)
             Mean action noise std: 2.58
          Mean value_function loss: 32.8475
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.5873
                       Mean reward: 948.58
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 185.7771
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.05s
                      Time elapsed: 01:09:30
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 48141 steps/s (collection: 1.919s, learning 0.123s)
             Mean action noise std: 2.58
          Mean value_function loss: 41.8614
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.5924
                       Mean reward: 906.57
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 1.0397
     Episode_Reward/lifting_object: 183.6163
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.04s
                      Time elapsed: 01:09:32
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 48490 steps/s (collection: 1.904s, learning 0.123s)
             Mean action noise std: 2.58
          Mean value_function loss: 31.2556
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.6023
                       Mean reward: 937.48
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.0525
     Episode_Reward/lifting_object: 185.8962
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.03s
                      Time elapsed: 01:09:35
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 48425 steps/s (collection: 1.898s, learning 0.132s)
             Mean action noise std: 2.58
          Mean value_function loss: 38.7912
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.6076
                       Mean reward: 921.92
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 186.6583
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.03s
                      Time elapsed: 01:09:37
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 49132 steps/s (collection: 1.901s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 31.2454
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.6115
                       Mean reward: 948.44
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 1.0585
     Episode_Reward/lifting_object: 186.8063
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.00s
                      Time elapsed: 01:09:39
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 49090 steps/s (collection: 1.903s, learning 0.099s)
             Mean action noise std: 2.59
          Mean value_function loss: 34.1477
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.6192
                       Mean reward: 916.34
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 186.8428
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.00s
                      Time elapsed: 01:09:41
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 49188 steps/s (collection: 1.903s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 34.8782
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.6272
                       Mean reward: 942.43
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 1.0501
     Episode_Reward/lifting_object: 185.6266
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.00s
                      Time elapsed: 01:09:43
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 48488 steps/s (collection: 1.931s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 54.9965
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.6316
                       Mean reward: 899.33
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 1.0396
     Episode_Reward/lifting_object: 183.8136
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.03s
                      Time elapsed: 01:09:45
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 48913 steps/s (collection: 1.899s, learning 0.110s)
             Mean action noise std: 2.59
          Mean value_function loss: 40.6905
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.6386
                       Mean reward: 928.64
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 1.0343
     Episode_Reward/lifting_object: 182.8137
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.01s
                      Time elapsed: 01:09:47
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 49913 steps/s (collection: 1.863s, learning 0.106s)
             Mean action noise std: 2.59
          Mean value_function loss: 35.5642
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.6462
                       Mean reward: 925.11
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 1.0423
     Episode_Reward/lifting_object: 184.1138
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 1.97s
                      Time elapsed: 01:09:49
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 48981 steps/s (collection: 1.862s, learning 0.145s)
             Mean action noise std: 2.59
          Mean value_function loss: 28.9961
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.6512
                       Mean reward: 940.34
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 186.7121
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.01s
                      Time elapsed: 01:09:51
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 49199 steps/s (collection: 1.890s, learning 0.108s)
             Mean action noise std: 2.59
          Mean value_function loss: 37.4375
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.6562
                       Mean reward: 891.29
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 184.5688
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.00s
                      Time elapsed: 01:09:53
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 48982 steps/s (collection: 1.907s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 36.7689
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.6596
                       Mean reward: 933.97
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 185.2237
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.01s
                      Time elapsed: 01:09:55
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 49185 steps/s (collection: 1.891s, learning 0.108s)
             Mean action noise std: 2.59
          Mean value_function loss: 47.9027
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.6641
                       Mean reward: 911.13
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.0294
     Episode_Reward/lifting_object: 181.9326
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.00s
                      Time elapsed: 01:09:57
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 50619 steps/s (collection: 1.841s, learning 0.101s)
             Mean action noise std: 2.59
          Mean value_function loss: 48.0164
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.6711
                       Mean reward: 934.21
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 1.0398
     Episode_Reward/lifting_object: 183.2231
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 1.94s
                      Time elapsed: 01:09:58
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 49637 steps/s (collection: 1.880s, learning 0.101s)
             Mean action noise std: 2.59
          Mean value_function loss: 49.2778
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 41.6753
                       Mean reward: 931.74
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 1.0475
     Episode_Reward/lifting_object: 184.8540
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 1.98s
                      Time elapsed: 01:10:00
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 48310 steps/s (collection: 1.921s, learning 0.114s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.0796
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.6808
                       Mean reward: 941.24
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 184.0650
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.03s
                      Time elapsed: 01:10:03
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 49417 steps/s (collection: 1.880s, learning 0.109s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.8085
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.6889
                       Mean reward: 939.93
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 1.0568
     Episode_Reward/lifting_object: 186.3866
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 1.99s
                      Time elapsed: 01:10:04
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 48129 steps/s (collection: 1.925s, learning 0.117s)
             Mean action noise std: 2.60
          Mean value_function loss: 42.1840
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.6937
                       Mean reward: 935.16
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 1.0540
     Episode_Reward/lifting_object: 185.8225
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.04s
                      Time elapsed: 01:10:07
                               ETA: 00:00:02

